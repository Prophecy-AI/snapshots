{
  "query": "What techniques do top Kaggle competitors use to reduce the gap between cross-validation and leaderboard scores in chemistry prediction competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Top Kaggle teams narrow the CV\u2011leaderboard gap in chemistry contests by treating the validation scheme as a proxy for the hidden test distribution and then layering several robustness\u2011boosting tactics.  They replace na\u00efve random\u202fK\u2011fold splits with **chemistry\u2011aware folds**\u2014for example scaffold\u2011 or molecular\u2011weight\u2011based splits that keep whole chemical families out of the training folds\u2014so that CV scores better reflect the leaderboard\u2019s out\u2011of\u2011sample chemistry\u202f([Kaggle Handbook](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e)).  The resulting out\u2011of\u2011fold (OOF) predictions are then used for **stacking and blending**: multiple strong base models (gradient\u2011boosted trees, graph neural networks, transformer\u2011based encoders) are trained on different seeds or feature sets, their OOF outputs are fed to a meta\u2011learner, and the final submission is a weighted average of the top\u2011performing models.  This ensemble strategy is highlighted in the Kaggle Grandmasters Playbook as one of the most reliable ways to lift leaderboard performance beyond raw CV scores ([NVIDIA Blog](https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data)).\n\nIn parallel, competitors apply classic **over\u2011fitting controls**: early stopping based on a held\u2011out validation slice, strong regularisation (L2, dropout for neural nets), and **adversarial validation** to spot subtle distribution shifts between train and test sets, discarding or re\u2011weighting problematic samples ([Safjan\u202f2023](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)).  They also enrich the data with domain\u2011specific descriptors\u2014physicochemical properties, Morgan fingerprints, and learned graph embeddings\u2014and perform **feature selection** to keep only the most predictive signals, which reduces noise that can inflate CV scores.  The winning solution for the 2025 Open Polymer Prediction Challenge exemplifies this blend of scaffold\u2011aware CV, extensive feature engineering, and multi\u2011model ensembling to achieve a small CV\u2011leaderboard gap and secure the top spot ([Xebia Blog](https://xebia.com/blog/winning-recipe-for-predicting-polymer-properties)).  \n\nTogether, chemistry\u2011aware validation, OOF\u2011driven stacking/blending, disciplined regularisation, and targeted feature engineering form the core toolkit that elite Kaggle participants use to align cross\u2011validation performance with the true leaderboard outcome in molecular\u2011property prediction contests.",
      "url": ""
    },
    {
      "title": "Kaggle Handbook: Tips & Tricks To Survive a Kaggle Shake-up",
      "text": "Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up | by Ertu\u011frul Demir | Global Maksimum Data &amp; Information Technologies | Medium\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[\nWrite\n](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[\nSearch\n](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n[## Global Maksimum Data &amp; Information Technologies\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---publication_nav-19a31620126f-23675beed05e---------------------------------------)\n\u00b7[\n![Global Maksimum Data &amp; Information Technologies](https://miro.medium.com/v2/resize:fill:76:76/1*zx7E3SekvcuY0u-IrGmojQ.png)\n](https://medium.com/global-maksimum-data-information-technologies?source=post_page---post_publication_sidebar-19a31620126f-23675beed05e---------------------------------------)\nGlobal Maksimum\u2019s enthusiastic data engineers write this blog to encourage you to learn more about AI, machine learning, data visualization, and relational database systems.\n# Kaggle Handbook: Tips &amp; Tricks To Survive a Kaggle Shake-up\n[\n![Ertu\u011frul Demir](https://miro.medium.com/v2/resize:fill:64:64/1*vpZN8RoCNs5fZslUDi5Gpg.jpeg)\n](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n[Ertu\u011frul Demir](https://medium.com/@ertugruldemir?source=post_page---byline--23675beed05e---------------------------------------)\n7 min read\n\u00b7May 24, 2022\n[\n](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/global-maksimum-data-information-technologies/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;user=Ertu%C4%9Frul+Demir&amp;userId=c2ba54d15a24&amp;source=---header_actions--23675beed05e---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/23675beed05e&amp;operation=register&amp;redirect=https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e&amp;source=---header_actions--23675beed05e---------------------bookmark_footer------------------)\nListen\nShare\nHi folks! In the first[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)of this series, I talked about a set of fundamental techniques \u2014including bias-variance tradeoff, validation set, and cross validation approach \u2014and how to use these techniques to set up a solid foundation for the Kaggle Shake-up and the private leaderboard.\nI highly recommend reading the first post before diving into this one, because there will be references. This post picks up right where we left off and shows some more techniques using analogies and case studies. Let\u2019s not waste any time.\n**Leakage**\nBefore we start with the techniques that will help you in Kaggle competitions, let\u2019s talk about data leakage. Data leakage or simply leakage can lead to create overly optimistic or even completely wrong predictive models. It happens when a model learns something it shouldn\u2019t have seen \u2014such as validation data \u2014and makes your cross validation unreliable. You don\u2019t want the most important part of your pipeline to be useless. Leakage is a common result among Kaggle participants who use external data.\nWith leakage people usually mean a**Data Leakage,**but especially in a competition setting, we should also be aware of what I like to call a**Model Leakage**. To avoid data leakage in cross validation:\n* Always make sure to drop duplicates.\n* Never include your external data in validation sets.\nModel leakage is more tricky to catch and it\u2019s related to data leakage in essence. It happens when you use**pre-trained**models to get the help of**transfer learning**. To avoid model leakage in cross validation:\n* Explore the pre-trained model\u2019s training data. Make sure it does not contain indistinguishable if not identical data.\n**Adversarial Validation**\nAdversarial Validation is a simple yet clever technique for finding out whether training and test sets come from similar feature distributions. It is very a straightforward case of binary classification:\n1. Drop the training set target column.\n2. Add a new target column for the training and test sets and label them as 0 and 1 respectively.\n3. Merge both datasets and shuffle them.\n4. Train a binary classifier and note the metric.\nBasically, we train a model that tries to predict whether an instance comes from the training set or the private test set. The metric for this task is usually a score of*Area Under the Receiver Operating Characteristic Curve*(ROC AUC). We have a problem if our model gets good scores on this task. Good scores mean that the train and private test set feature distributions are separable and different from each other, and we don\u2019t want that. We want our adversarial validation models to score close to 0.5 to have reliable cross validation schemes.\n**Out-of-fold Predictions**\nCross validation is not only a strong validation strategy, but it has another property: Out-of-fold Predictions (OOF).\nThese predictions are a form of out-of-sample predictions. They\u2019re based on data that is not used to train models. So basically, for every k-1 training fold, we get a set of validation fold predictions. In this way, we end up with out-of-sample predictions for the entire training set! We can use these OOF predictions to estimate our models, but beyond that, we can use them for stacking.\nIn the previous[post](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8), I mentioned the downsides of using a hold-out set, which leads to biased models depending on data split and training data size. We fixed that with cross validation, and again, with its byproduct OOF predictions, we can use all the data and end up with better ensembles.\n**Ensemble Learning**\nSuppose you want to buy a new smartphone. You can\u2019t decide, so you ask your friends for recommendations and aggregate their answers. The aggregated answer is likely to be better than what you\u2019d learn from an expert in a tech store. This simple idea (using weak learners) is the main element of ensemble learning, and it remains one of the most powerful techniques today. A similar logic is the basis of many compelling algorithms such as Random Forests and Gradient Boosting, which I will not cover in this post. Our interest is in stacking or blending, depending on your validation strategy (OOF for the first and Hold-out for the second). Let\u2019s continue with that.\n**Blending and Stacking**\nTo understand the intuition behind these ...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-tips-tricks-to-survive-a-kaggle-shake-up-23675beed05e"
    },
    {
      "title": "The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data",
      "text": "The Kaggle Grandmasters Playbook: 7 Battle&#x2d;Tested Modeling Techniques for Tabular Data | NVIDIA Technical Blog\n[](https://developer.nvidia.com/)[DEVELOPER](https://developer.nvidia.com/)\n[Technical Blog](https://developer.nvidia.com/blog)\n[Subscribe](https://developer.nvidia.com/email-signup)\n[Related Resources**](#main-content-end)\n[Data Science](https://developer.nvidia.com/blog/category/data-science/)\nEnglish\u4e2d\u6587\n# The Kaggle Grandmasters Playbook: 7 Battle-Tested Modeling Techniques for Tabular Data\nLessons from years of competitions, made practical with GPU acceleration.\n![](https://developer-blogs.nvidia.com/wp-content/uploads/2025/09/Kaggle-Grandmasters-Playbook-featured-1024x576-png.webp)\nSep 18, 2025\nBy[Kazuki Onodera](https://developer.nvidia.com/blog/author/kazuki-onodera/),[Th\u00e9o Viel](https://developer.nvidia.com/blog/author/tviel/),[Gilberto Titericz](https://developer.nvidia.com/blog/author/gillbertotitericz/)and[Chris Deotte](https://developer.nvidia.com/blog/author/cdeotte/)\n**\nLike\n[**Discuss (1)](#entry-content-comments)\n* [L](https://www.linkedin.com/sharing/share-offsite/?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [T](https://twitter.com/intent/tweet?text=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog+https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [F](https://www.facebook.com/sharer/sharer.php?u=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/)\n* [R](https://www.reddit.com/submit?url=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/&amp;title=The+Kaggle+Grandmasters+Playbook:+7+Battle-Tested+Modeling+Techniques+for+Tabular+Data+|+NVIDIA+Technical+Blog)\n* [E](<mailto:?subject=I'd like to share a link with you&body=https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/>)\nAI-Generated Summary\nLike\nDislike\n* The authors have developed a playbook for solving real-world tabular problems, refined through hundreds of Kaggle competitions, which emphasizes fast experimentation and careful validation.\n* Key techniques include using GPU acceleration for dataframe operations and model training with NVIDIA cuML or GPU backends of XGBoost, LightGBM, and CatBoost to enable rapid iteration.\n* The authors also stress the importance of techniques like k-fold cross-validation, diverse baselines, feature engineering, hill climbing, stacking, pseudo-labeling, and extra training to improve model performance and robustness.\nAI-generated content may summarize information incompletely. Verify important information.[Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\nOver hundreds of Kaggle competitions, we&#8217;ve refined a playbook that consistently lands us near the top of the leaderboard\u2014no matter if we\u2019re working with millions of rows, missing values, or test sets that behave nothing like the training data. This isn\u2019t just a collection of modeling tricks\u2014it\u2019s a repeatable system for solving real-world tabular problems fast.\nBelow are seven of our most battle-tested techniques, each one made practical through GPU acceleration. Whether you\u2019re climbing the leaderboard or deploying models in production, these strategies can give you an edge.\nWe\u2019ve included links to example write-ups or notebooks from past competitions for each technique.\n**Note:**Kaggle and Google Colab notebooks come with free GPUs and accelerated drop-ins like the ones you\u2019ll see below pre-installed.\n## Core principles: the foundations of a winning workflow[**](#core_principles_the_foundations_of_a_winning_workflow)\nBefore diving into techniques, it\u2019s worth pausing to cover the two principles that power everything in this playbook: fast experimentation and careful validation. These aren\u2019t optional best practices\u2014they\u2019re the foundation of how we approach every tabular modeling problem.\n### Fast experimentation[**](#fast_experimentation)\nThe biggest lever we have in any competition or real-world project is the number of high-quality experiments we can run. The more we iterate, the more patterns we discover\u2014and the faster we catch when a model is failing, drifting, or overfitting\u2014so we can course-correct early and improve faster.\nIn practice, that means we optimize our entire pipeline for speed, not just our model training step.\n**Here\u2019s how we make it work:**\n* Accelerate dataframe operations using[GPU drop-in replacements](https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126)for pandas or Polars to transform and engineer features at scale.\n* Train models with[NVIDIA cuML](https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml)or GPU backends of XGBoost, LightGBM, and CatBoost.\nGPU acceleration isn\u2019t just for deep learning\u2014it&#8217;s often the only way to make advanced tabular techniques practical at scale.\n### Local Validation[**](#local_validation)\nIf you can\u2019t trust your validation score, you\u2019re flying blind. That\u2019s why cross-validation (CV) is a cornerstone to our workflow.\n**Our approach:**\n* Use k-fold cross-validation, where the model trains on most of the data and tests on the part that\u2019s held out.\n* Rotate through folds so every part of the data is tested once.\nThis gives a much more reliable measure of performance than a single train/validation split.\n**Pro tip:**Match your CV strategy to how the test data is structured.\n**For example:**\n* Use TimeSeriesSplit for time-dependent data\n* Use GroupKFold for grouped data (like users or patients)\nWith those foundations in place\u2014moving fast and validating carefully\u2014we can now dive into the techniques themselves. Each one builds on these principles and shows how we turn raw data into world-class models.\n## 1. Start with smarter EDA, not just the basics[**](#1_start_with_smarter_eda_not_just_the_basics)\nMost practitioners know the basics: Check for missing values, outliers, correlations, and feature ranges. Those steps are important, but they\u2019re table stakes. To build models that hold up in the real world, you need to explore the data a little deeper\u2014a couple of quick checks that we\u2019ve found useful, but many people miss:\n**Train vs. test distribution checks:**Spot when evaluation data differs from training, since distribution shift can cause models to validate well but fail in deployment.\n*Figure 1. Comparing feature distributions between train (blue) and test (red) reveals a clear shift\u2014test data is concentrated in a higher range, with minimal overlap. This kind of distribution shift can cause models to validate well but fail in deployment.*\n**Analyze target variable for temporal patterns:**Check for trends or seasonality, since ignoring temporal patterns can lead to models that look accurate in training but break in production.\n*Figure 2. Analyzing the target variable over time uncovers a strong upward trend with seasonal fluctuations and accelerating growth. Ignoring temporal patterns like these can mislead models unless time-aware validation is used.*\nThese techniques aren&#8217;t brand new\u2014but they\u2019re often overlooked, and ignoring them can sink a project.\n**Why it matters:**Skipping these checks can derail an otherwise solid workflow.\n**In action:**In the winning solution to the Amazon KDD Cup \u201823, the team uncovered both a train\u2014test distribution shift and temporal patterns in the target\u2014insights that shaped the final approach.[Read the full write-up &gt;](https://openreview.net/forum?id=J3wj55kK5t)\n**Made practical with GPUs:**Real-world datasets are often millions of rows, which can slow to a crawl in pandas. By adding GPU acceleration with[NVIDIA cuDF](https:...",
      "url": "https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data"
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "Winning Recipe for Predicting Polymer Properties",
      "text": "Winning Recipe For Predicting Polymer Properties | Xebia\n[](https://xebia.com/)\n* [Applied &amp; Gen AI](https://xebia.com/artificial-intelligence/)\n* [Intelligent Automation](https://xebia.com/intelligent-automation/)\n* [Cloud &amp; Data](https://xebia.com/cloud-data-modernization/)\n* [Digital Products &amp; Platforms](https://xebia.com/digital-products-platforms/)\n* [Industries](https://xebia.com/industries/)\n* [Partners](https://xebia.com/partners/)\n* [Solutions](https://xebia.com/solutions/)\n* [Training](https://academy.xebia.com/)\n* [Careers](https://xebia.com/careers/)\n* [About](#)\n[Contact](https://xebia.com/about-us/contact/)\n![English](https://kcdn.xebia.com/cdn-cgi/image/scq=50,format=auto,quality=75,onerror=redirect,width=48/https://xebia.com/_next/static/media/usa_flag.371b2b09.png)\nAiOverview\nThis AI search assistant is currently in a pilot program and is still being refined. Responses, generated in English, may take a few seconds to appear. We aim for accuracy, but occasional inaccuracies may occur.\nPlease verify key details before making decisions or[contacting us](https://xebia.com/about-us/contact/)directly.\n##### Response\n\u200c\u200c\u200c##### Related Topics\n\u200c\u200c\u200c\u200c##### Context Files\n1. \u200c\u200c\u200c\u200c\u200c\u200c2. \u200c\u200c\u200c\u200c\u200c\u200c3. \u200c\u200c\u200c\u200c\u200c\u200c4. \u200c\u200c\u200c\u200c\u200c\u200c##### Related Topics\n\u200c\u200c\u200c\u200cBlog\n# Winning Recipe for Predicting Polymer Properties\n![Jetze Schuurmans](https://kcdn.xebia.com/cdn-cgi/image/scq=50,format=auto,quality=75,onerror=redirect,width=3840/https://xebia.com/media/2025/03/20230905-120453-jti-96x96-1.jpg)\n#### Jetze Schuurmans\nUpdatedOctober 16, 2025\n7minutes\nShare[](https://www.linkedin.com/shareArticle?mini=true&amp;url=)[](mailto:?subject=&amp;body=)\n## Open Polymer Prediction Challenge: Analysis of the Winning Approach\nThe NeurIPS Open Polymer Prediction Challenge 2025 attracted over 2,240 teams competing to predict five polymer properties from SMILES representations: glass transition temperature (Tg), thermal conductivity (Tc), density (De), fractional free volume (FFV), and radius of gyration (Rg). We analyzed the winning solution by[James Day](https://www.kaggle.com/jsday96)and identified several key insights that challenge current research trends while demonstrating the continued effectiveness of classical machine learning techniques.\n### Key Takeaways\n**Property-specific models remain superior for limited data**: Despite the research community&#x27;s push toward general-purpose foundation models, property-specific models proved more effective when working with constrained datasets.\n**Ensemble methods continue to excel**: This traditional machine learning technique delivered exceptional performance, reinforcing its value in modern competitions.\n**External data demands careful curation**: As discussed in[The challenges of molecular property datasets](https://xebia.com/blog/splitting-data-for-molecular-properties/), integrating external data sources requires meticulous preprocessing to address inconsistencies and noise.\n**General-purpose BERT outperformed domain-specific models**: ModernBERT exceeded the performance of chemistry-specific models, though polyBERT embeddings were retained as valuable tabular features.\n**Strategic 3D model selection**: The winning solution employed Uni-Mol-2-84M as its 3D model. This choice is particularly interesting given that[Praski et al.](https://arxiv.org/abs/2508.06199)demonstrated superior performance from graph transformer models like[R-MAT](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00789-7)on molecular property prediction tasks, especially for drug-related properties. R-MAT models offer easy implementation and reduced memory requirements, making the Uni-Mol-2 choice worth examining.\n## Architecture Overview\n![data_overview](https://xebia.com/media/2025/10/winning-recipe-for-predicting-polymer-properties-inference.svg)\nThe winning approach generated property-specific predictions using ensembles of ModernBERT, AutoGluon, and Uni-Mol-2 models through a multi-stage pipeline:\n1. Initial training on externally labeled datasets\n2. BERT model retraining on a pseudolabeled PI1M subset\n3. Extensive feature engineering for tabular models\n4. Post-processing adjustment for glass transition temperature predictions to compensate for distribution shift between training and leaderboard datasets\n### Data Strategy\n![data_overview](https://xebia.com/media/2025/10/winning-recipe-for-predicting-polymer-properties-data.svg)\n#### Dataset Composition and Augmentation\nModel validation relied on 5-fold cross-validation using the competition&#x27;s original training data. The training data was substantially augmented with external datasets and locally executed MD simulations. The winner identified significant data quality challenges and a distribution shift between the training and leaderboard datasets.\n#### Addressing Distribution Shift\nInvestigation revealed a pronounced distribution shift in glass transition temperature (Tg) between training and leaderboard datasets.\n![TG_BIAS_COEFFICIENT](https://xebia.com/media/2025/10/winning-recipe-for-predicting-polymer-properties-Tg_Bias_Coefficient.png)The lower bound (LB) score is the[wMAE](www.kaggle.com/competitions/neurips-open-polymer-prediction-2025/overview/evaluation)metric used in the competition. The Bias Coefficient is a factor that is multiplied with the standard deviation of the glass transition predictions, this product is then added to the original Tg predictions.\nTo correct this systematic bias, predictions underwent post-processing:`submission\\_df[&quot;&quot;Tg&quot;&quot;] += (submission\\_df[&quot;&quot;Tg&quot;&quot;].std() \\* 0.5644)`\n#### External Data Sources\nThe solution incorporated several external datasets:\n* [Metrics from 1,116 locally run MD simulations](https://www.kaggle.com/datasets/jsday96/md-simulation-results/)\n* [PI1M (pseudolabeled subset)](https://www.kaggle.com/datasets/jsday96/pi1m-pseudolabels)\n* [LAMALAB curated Tg](https://zenodo.org/records/15210035)\n* [RadonPy sample data](https://github.com/RadonPy/RadonPy/blob/develop/data/PI1070.csv)\n* [Seok et al.](https://springernature.figshare.com/articles/dataset/dataset_with_glass_transition_temperature/24219958?file=42507037)\n* [Borredon et al.](https://www.sciencedirect.com/science/article/pii/S2590159123000377#ec0005)\n* [Duke ChemProps](https://github.com/Duke-MatSci/ChemProps)\nThese datasets presented multiple challenges: random label noise, non-linear relationships with ground truth, constant bias factors, and out-of-distribution outliers.\n#### Data Cleaning Methodology\nThree general strategies were applied across all datasets:\n**Label rescaling via isotonic regression**: An[isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression)model transformed raw labels by learning to predict ensemble predictions from the original training data. This approach effectively corrected for constant bias factors and non-linear relationships with ground truth. Final labels often represented Optuna-tuned weighted averages of raw and rescaled values to minimize overfitting.\n**Error-based filtering**: The ensembles&#x27; predictions were used to identify samples exceeding an error threshold, which were discarded. Thresholds were defined as ratios of sample error to mean absolute error from ensemble testing on the host dataset, ensuring consistent threshold ranges across properties and facilitating Optuna hyperparameter search.\n**Sample weighting**: Optuna tuned per-dataset sample weights, enabling models to discount lower-quality training examples appropriately.\nDataset-specific interventions included:\n**RadonPy**: Manual inspection identified and removed outliers, particularly thermal conductivity values exceeding 0.402 that appeared inconsistent with ensemble predictions. Optuna frequently favored this filtered version during hyperparameter tuning.\n**MD Simulations**: Rather than applying general cleaning strategies, the solution implemented model stacking. An ensemble of 41 XGBoost models predicted simulati...",
      "url": "https://xebia.com/blog/winning-recipe-for-predicting-polymer-properties"
    },
    {
      "title": "Cross-Validation - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=f2bc5978f08b75213e7a:1:11021)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/alexisbcook/cross-validation"
    },
    {
      "title": "Tutorial: K Fold Cross Validation - Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Fsatishgunjal%2Ftutorial-k-fold-cross-validation)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Fsatishgunjal%2Ftutorial-k-fold-cross-validation)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\nSatish Gunjal \u00b7 4y ago \u00b7 92,586 views\n\narrow\\_drop\\_up131\n\nCopy & Edit249\n\n![gold medal](https://www.kaggle.com/static/images/medals/notebooks/goldl@1x.png)\n\nmore\\_vert\n\n# Tutorial: K Fold Cross Validation\n\n## Tutorial: K Fold Cross Validation\n\n[Notebook](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/notebook) [Input](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/input) [Output](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/output) [Logs](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/log) [Comments (5)](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/comments)\n\nhistoryVersion 7 of 7chevron\\_right\n\n## Runtime\n\nplay\\_arrow\n\n2m 35s\n\n## Input\n\nCOMPETITIONS\n\n![](https://www.kaggle.com/competitions/5407/images/thumbnail)\n\nHouse Prices - Advanced Regression Techniques\n\n![](https://www.kaggle.com/competitions/3136/images/thumbnail)\n\nTitanic - Machine Learning from Disaster\n\n## Tags\n\n[Classification](https://www.kaggle.com/code?tagIds=13302-Classification) [Regression](https://www.kaggle.com/code?tagIds=14203-Regression)\n\n## Language\n\nPython\n\n## Table of Contents\n\n[Index](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Index) [Introduction](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Introduction-) [Inner Working of Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Inner-Working-of-Cross-Validation-) [K Fold Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold-Cross-Validation-) [Stratified K Fold Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Stratified-K-Fold-Cross-Validation-) [Hyperparameter Tuning and Model Selection](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Hyperparameter-Tuning-and-Model-Selection-) [Advantages](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Advantages-) [Disadvantages](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Disadvantages-) [K Fold: Regression Example](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold:-Regression-Example-) [Import Libraries](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Import-Libraries-) [Load Dataset](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Load-Dataset-) [Understanding the Data](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Understanding-the-Data-) [Model Score Using KFold](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Model-Score-Using-KFold-) [Model Tuning using KFold](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Model-Tuning-using-KFold-) [K Fold: Classification Example](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold:-Classification-Example-) [Using Logistic Regression](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Logistic-Regression-) [Using Decision Classifier](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Decision-Classifier-) [Using Random Forest Classifier](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Random-Forest-Classifier-) [Decision Tree Classifier Tuning](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Decision-Tree-Classifier-Tuning-) [Reference](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Reference-)\n\n![Profile picture for undefined](https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1739697666&Signature=JVVWuDBwsOVtiIWfxdVES7uWSyf%2Bps87MWLfWJ4GthhX9l2wvME9fn8paJlGoTXB%2B61Ng1U%2F9Dws7W9CvZLWlNwwBrzkLSOK1MuCj%2FR3z4XcSWskcH8Ntiy74IZbEqlkNoEWj%2FI%2FVrI%2Fb1tGr3RWFKn7s61Z0MJ4ZbFPcPEO8kd5aLgU1t3IIiua3V8i1kbO%2B6iqnK92oZfS1io1qVBCS2L77pxDaV%2F0TCelxozeV%2B5e0B4Um7uVOg5mipGLJ74%2BGnkdf562hcqIUI3yQFBS68SRnnbbyMwbhIpfWtpHK56itpjIA5o6kbQwjg%2BMyKxnyCPPXNKEtDC93IVbQZvXWg%3D%3D)\n\nCompetition Notebook\n\n[Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n\n[iframe](https://www.kaggleusercontent.com/kf/48636546/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..NNcXRlz1r7wVA_ZK7hc27A.fenyRROP0MgZU1Tk_PeeaOXMw36nI8G_UByr6iwE5r3uL9ZGOxHPxUrpb82HBc2ig7ZhFkyRQcXV-3-nyGWrXUhjUsFKXK7Xjjt4d3FEzA6Xenj3IXnN4K8f_ZcqQTdXSkkaTPTTSRixgMLYi2KoE0J8sWKlQ-63xQaj8ancpZ0H7B-QWFnfLvTmwqUDOaaenkJ73CwKvLmUVFNxP3Q7ki2xSCozzg5G6d3SLaj6v-dR4D8qYvWRr-vNiOkZJVZixLTk1vx5H-SVNTzxka7JSHpPCcpPj2KTtq9ngeuLjO_bHOX1ldNbGLq_uRIu9LbkV3sRFz2_IJs2pdU6-qtOU_WjhkAqyR6v9sXudACSV1x-MXbrttJvEB2i3kBoJzXcNRN8XH5K1Sss5-OdY2xj2q8PQCnqDlJBnWIWx4RSJeFxpaSPb1x3U7sWTF_4EYsOLCKM9Jo6QmWCFOUYLpwZ-vJVX5lEmSoHWiRGillmyCKXwGHTCvC9vApjgvYcwj6Cklu8EvuldNOYKp1Q6h6B0GKkjM2xoWDyflCBN6KdxVieDeYSY1kXLBrcT9eWdaHwmzhva1zHPVDZSocdHapOAfHFunmzkYnusqHpI66mFybxcKWRDTZBSDQSjEet401qkneujsqnaiHPiuUef9YLl1goUwXngf2oI0NwHICm0-Y.98Dh2ehJc4THR-HE0iz1SQ/__results__.html?sharingControls=true)\n\n## License\n\nThis Notebook has been released under the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) open source license.\n\n## Continue exploring\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/input_light.svg)\n\n\n\n\n\n\n\nInput\n\n2 files\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/output_light.svg)\n\n\n\n\n\n\n\nOutput\n\n0 files\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/logs_light.svg)\n\n\n\n\n\n\n\nLogs\n\n154.6 second run - successful\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/comments_light.svg)\n\n\n\n\n\n\n\nComments\n\n5 comments\n\n\n\n\narrow\\_right\\_alt",
      "url": "https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation"
    },
    {
      "title": "",
      "text": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions\nMoritz Hardt M@MRTZ.ORG\nAvrim Blum AVRIM@CS.CMU.EDU\nCarnegie Mellon University\nAbstract\nThe organizer of a machine learning competi\u0002tion faces the problem of maintaining an accurate\nleaderboard that faithfully represents the quality\nof the best submission of each competing team.\nWhat makes this estimation problem particularly\nchallenging is its sequential and adaptive nature.\nAs participants are allowed to repeatedly evaluate\ntheir submissions on the leaderboard, they may\nbegin to overfit to the holdout data that supports\nthe leaderboard. Few theoretical results give ac\u0002tionable advice on how to design a reliable leader\u0002board. Existing approaches therefore often resort\nto poorly understood heuristics such as limiting\nthe bit precision of answers and the rate of re\u0002submission.\nIn this work, we introduce a notion of leader\u0002board accuracy tailored to the format of a com\u0002petition. We introduce a natural algorithm called\nthe Ladder and demonstrate that it simultaneously\nsupports strong theoretical guarantees in a fully\nadaptive model of estimation, withstands practical\nadversarial attacks, and achieves high utility on\nreal submission files from an actual competition\nhosted by Kaggle.\nNotably, we are able to sidestep a powerful recent\nhardness result for adaptive risk estimation that\nrules out algorithms such as ours under a seem\u0002ingly very similar notion of accuracy. On a practi\u0002cal note, we provide a completely parameter-free\nvariant of our algorithm that can be deployed in a\nreal competition with no tuning required whatso\u0002ever.\nProceedings of the 32 nd International Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy\u0002right 2015 by the author(s).\n1. Introduction\nMachine learning competitions have become an extremely\npopular format for solving prediction and classification prob\u0002lems of all kinds. A number of companies such as Netflix\nhave organized major competitions in the past and some\nstart-ups like Kaggle specialize in hosting machine learning\ncompetitions. In a typical competition hundreds of partici\u0002pants will compete for prize money by repeatedly submitting\nclassifiers to the host in an attempt to improve on their pre\u0002viously best score. The score reflects the performance of\nthe classifier on some subset of the data, which are typically\npartitioned into two sets: a training set and a test set. The\ntraining set is publicly available with both the individual\ninstances and their corresponding class labels. The test set is\npublicly available as well, but the class labels are withheld.\nPredicting these missing class labels is the goal of the partic\u0002ipant and a valid submission is simply a list of labels\u2014one\nfor each point in the test set.\nThe central component of any competition is the leaderboard\nwhich ranks all teams in the competition by the score of their\nbest submission. This leads to the fundamental problem\nof maintaining a leaderboard that accurately reflects the\ntrue strength of a classifier. What makes this problem so\nchallenging is that participants may begin to incorporate\nthe feedback from the leaderboard into the design of their\nclassifier thus creating a dependence between the classifier\nand the data on which it is evaluated. In such cases, it is\nwell known that the holdout set no longer gives an unbiased\nestimate of the classifier\u2019s true performance. To counteract\nthis problem, existing solutions such as the one used by\nKaggle further partition the test set into two parts. One part\nof the test set is used for computing scores on the public\nleaderboard. The other is used to rank all submissions after\nthe competition ended. This final ranking is often referred\nto as the private leaderboard. While this solution increases\nthe quality of the private leaderboard, it does not address the\nproblem of maintaining accuracy on the public leaderboard.\nIndeed, numerous posts on the forums of Kaggle report\non the problem of \u201coverfitting to the holdout\u201d meaning that\nThe Ladder: A Reliable Leaderboard for Machine Learning Competitions\nsome scores on the public leaderboard are inflated compared\nto final scores. To mitigate this problem Kaggle primarily\nrestricts the rate of re-submission and to some extent the\nnumerical precision of the released scores.\nYet, in spite of its obvious importance, there is relatively\nlittle theory on how to design a leaderboard with rigorous\nquality guarantees. Basic questions remain difficult to as\u0002sess, such as, can we a priori quantify how accurate existing\nleaderboard mechanisms are and can we design better meth\u0002ods?\nWhile the theory of estimating the true loss of a classifier or\nset of classifiers from a finite sample is decades old, much\nof theory breaks down due to the sequential and adaptive na\u0002ture of the estimation problem that arises when maintaining\na leaderboard. First of all, there is no a priori understanding\nof which learning algorithms are going to be used, the com\u0002plexity of the classifiers they are producing, and how many\nsubmissions there are going to be. Indeed, submissions are\njust a list of labels and do not even specify how these labels\nwere obtained. Second, any submission might incorporate\nstatistical information about the withheld class labels that\nwas revealed by the score of previous submissions. In such\ncases, the public leaderboard may no longer provide an unbi\u0002ased estimate of the true score. To make matters worse, very\nrecent results suggest that maintaining accurate estimates\non a sequence of many adaptively chosen classifiers may be\ncomputationally intractable (Hardt & Ullman, 2014; Steinke\n& Ullman, 2014).\n1.1. Our Contributions\nWe introduce a notion of accuracy called leaderboard accu\u0002racy tailored to the format of a competition. Intuitively, high\nleaderboard accuracy entails that each score represented on\nthe leaderboard is close to the true score of the correspond\u0002ing classifier on the unknown distribution from which the\ndata were drawn. Our primary theoretical contributions are\nthe following.\n1. We show that there is a simple and natural algorithm\nwe call Ladder that achieves high leaderboard accuracy\nin a fully adaptive model of estimation in which we\nplace no restrictions on the data analyst whatsoever. In\nfact, we don\u2019t even limit the number of submissions\nan analyst can make. Formally, our worst-case upper\nbound shows that if we normalize scores to be in [0, 1]\nthe maximum error of our algorithm on any estimate\nis never worse than O((log(kn)/n)\n1/3\n) where k is the\nnumber of submissions and n is the size of the data\nused to compute the leaderboard. In contrast, we ob\u0002serve that the error of the Kaggle mechanism (and sim\u0002ilar solutions) scales with the number of submissions\nas \u221ak so that our algorithm features an exponential\nimprovement in k.\n2. We also prove an information-theoretic lower bound on\nthe leaderboard accuracy demonstrating that no estima\u0002tor can achieve error smaller than \u2126((log(k)/n)\n1/2\n).\nComplementing our theoretical worst-case upper bound and\nlower bound, we make a number of practical contributions:\n1. We provide a parameter-free variant of our algorithm\nthat can be deployed in a real competition with no\ntuning required whatsoever.\n2. To demonstrate the strength of our parameter-free algo\u0002rithm we conduct two opposing experiments. The first\nis an adversarial\u2014yet practical\u2014attack on the leader\u0002board that aims to create as much of a bias as possible\nwith a given number of submissions. We compare\nthe performance of the Kaggle mechanism to that of\nthe Ladder mechanism under this attack. We observe\nthat the accuracy of the Kaggle mechanism diminishes\nrapidly with the number of submissions, while our\nalgorithm encounters only a small bias in its estimates.\n3. In a second experiment, we evaluate our algorithm on\nreal submission files from a Kaggle competition. The\ndata set presents a difficult benchmark as little over\u0002fitting occurred and the errors of the Kaggle leader\u0002board...",
      "url": "https://proceedings.mlr.press/v37/blum15.pdf"
    }
  ]
}