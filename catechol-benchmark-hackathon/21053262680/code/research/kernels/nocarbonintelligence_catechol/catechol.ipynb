{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# üìå EDA complet corrig√© pour Kaggle\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Chemin des fichiers\nroot_path = '/kaggle/input/catechol-benchmark-hackathon/'\n\n# Charger les datasets principaux\nfull_data = pd.read_csv(f'{root_path}catechol_full_data_yields.csv')\nsingle_data = pd.read_csv(f'{root_path}catechol_single_solvent_yields.csv')\n\n# Aper√ßu g√©n√©ral\nprint(\"=== Full dataset ===\")\nprint(full_data.shape)\ndisplay(full_data.head())\n\nprint(\"\\n=== Single solvent dataset ===\")\nprint(single_data.shape)\ndisplay(single_data.head())\n\n# Colonnes disponibles\nprint(\"\\nColumns in full_data:\")\nprint(full_data.columns)\n\n# Distribution des solvants\nplt.figure(figsize=(12,4))\nsns.countplot(x='SOLVENT A NAME', data=full_data)\nplt.title('Distribution of Solvent A')\nplt.xticks(rotation=90)\nplt.show()\n\nplt.figure(figsize=(12,4))\nsns.countplot(x='SOLVENT B NAME', data=full_data)\nplt.title('Distribution of Solvent B')\nplt.xticks(rotation=90)\nplt.show()\n\n# Distribution des conditions de r√©action\nfig, axes = plt.subplots(1,3, figsize=(18,4))\nsns.histplot(full_data['Temperature'], bins=20, ax=axes[0])\naxes[0].set_title('Temperature')\nsns.histplot(full_data['Residence Time'], bins=20, ax=axes[1])\naxes[1].set_title('Residence Time')\nsns.histplot(full_data['SolventB%'], bins=20, ax=axes[2])\naxes[2].set_title('%B (Solvent B fraction)')\nplt.show()\n\n# Distribution des rendements (targets)\nplt.figure(figsize=(12,4))\nsns.histplot(full_data['SM'], bins=20, color='blue', label='SM', alpha=0.5)\nsns.histplot(full_data['Product 2'], bins=20, color='green', label='Product 2', alpha=0.5)\nsns.histplot(full_data['Product 3'], bins=20, color='red', label='Product 3', alpha=0.5)\nplt.title('Distribution of yields (targets)')\nplt.legend()\nplt.show()\n\n# V√©rifier les valeurs manquantes\nprint(\"\\nMissing values in full_data:\")\nprint(full_data.isnull().sum())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # üìå Cellule 2 compl√®te : Merge descriptors pour solvants + inspections\n#\n# import pandas as pd\n# import numpy as np\n# # import matplotlib.pyplot as plt # Comment√© pour √©viter les imports inutiles\n# # import seaborn as sns # Comment√© pour √©viter les imports inutiles\n#\n# root_path = '/kaggle/input/catechol-benchmark-hackathon/'\n#\n# # Charger les datasets principaux\n# # full_data = pd.read_csv(f'{root_path}catechol_full_data_yields.csv') # C'est g√©r√© par load_data\n# # acs_desc = pd.read_csv(f'{root_path}acs_pca_descriptors_lookup.csv')\n# # spange_desc = pd.read_csv(f'{root_path}spange_descriptors_lookup.csv')\n#\n# # üîπ Inspection rapide des colonnes\n# # print(\"ACS descriptor columns:\", acs_desc.columns.tolist())\n# # print(\"SPANGE descriptor columns:\", spange_desc.columns.tolist())\n#\n# # V√©rifier si une colonne SMILES est pr√©sente (non dispo dans ce dataset)\n# # On va utiliser SOLVENT NAME pour merge\n# # if 'SOLVENT NAME' not in acs_desc.columns or 'SOLVENT NAME' not in spange_desc.columns:\n# #     raise KeyError(\"‚ö†Ô∏è Colonne 'SOLVENT NAME' introuvable dans les descriptors.\")\n#\n# # 1Ô∏è‚É£ Cr√©ation d'une table ma√Ætre des descripteurs\n# # master_desc = acs_desc.merge(spange_desc, on='SOLVENT NAME', how='outer')\n#\n# # DESCRIPTOR_COLS = master_desc.columns.drop('SOLVENT NAME').tolist()\n# # print(f\"\\nDescripteurs disponibles pour mix: {DESCRIPTOR_COLS[:10]} ...\")\n#\n# # 2Ô∏è‚É£ Merge pour Solvent A\n# # full_data = full_data.merge(master_desc, left_on='SOLVENT A NAME', right_on='SOLVENT NAME', how='left', suffixes=('', '_A'))\n# # full_data.drop(columns=['SOLVENT NAME'], inplace=True, errors='ignore')\n#\n# # 3Ô∏è‚É£ Merge pour Solvent B\n# # full_data = full_data.merge(master_desc, left_on='SOLVENT B NAME', right_on='SOLVENT NAME', how='left', suffixes=('_A', '_B'))\n# # full_data.drop(columns=['SOLVENT NAME'], inplace=True, errors='ignore')\n#\n# # 4Ô∏è‚É£ Calcul de la Moyenne Pond√©r√©e (Weighted Average)\n# # P_B = full_data['SolventB%'] / 100\n# # P_A = 1 - P_B\n#\n# # for col in DESCRIPTOR_COLS:\n# #     col_A = f\"{col}_A\"\n# #     col_B = f\"{col}_B\"\n# #     full_data[f\"{col}_MIX\"] = full_data[col_A]*P_A + full_data[col_B]*P_B\n# #     # Supprimer colonnes A et B\n# #     full_data.drop(columns=[col_A, col_B], inplace=True, errors='ignore')\n#\n# # 5Ô∏è‚É£ Inspections post-merge (√Ä NE PAS EX√âCUTER DANS LA SOUMISSION)\n# # print(\"\\n=== Aper√ßu du dataset apr√®s merge + MIX descriptors ===\")\n# # display(full_data.head())\n#\n# # print(\"\\nNombre de valeurs manquantes par colonne (top 10):\")\n# # print(full_data.isnull().sum().sort_values(ascending=False).head(10))\n#\n# # print(\"\\nStatistiques descriptives rapides:\")\n# # display(full_data.describe())\n#\n# # Distribution de quelques colonnes cl√©s\n# # fig, axes = plt.subplots(1,3, figsize=(15,4))\n# # sns.histplot(full_data['Temperature'], bins=20, ax=axes[0]).set(title='Temperature')\n# # sns.histplot(full_data['Residence Time'], bins=20, ax=axes[1]).set(title='Residence Time')\n# # sns.histplot(full_data['SolventB%'], bins=20, ax=axes[2]).set(title='% Solvent B')\n# # plt.show()\n#\n# # V√©rification rapide des NaN dans les descripteurs MIX\n# # desc_mix_cols = [c for c in full_data.columns if c.endswith('_MIX')]\n# # print(\"NaN dans les descripteurs solvants MIX :\")\n# # print(full_data[desc_mix_cols].isnull().sum().sort_values(ascending=False).head(10))\n#\n# # Distribution rapide des 3 premiers descripteurs MIX\n# # sns.pairplot(full_data[['Temperature','Residence Time','SolventB%'] + desc_mix_cols[:3]])\n# # plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import KFold\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.multioutput import MultiOutputRegressor\n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_squared_error\n# import tqdm \n\n# # =========================================================================\n# # 1. PR√âPARATION DES DONN√âES (Chargement et Featurization)\n# # =========================================================================\n\n# # ‚ö†Ô∏è Assurez-vous que ce chemin d'acc√®s est correct pour votre environnement Kaggle\n# root_path = '/kaggle/input/catechol-benchmark-hackathon/'\n\n# # Charger les datasets\n# # try: # Cette logique est g√©r√©e par load_data\n# # \tfull_data = pd.read_csv(f'{root_path}catechol_full_data_yields.csv')\n# # \tsingle_data = pd.read_csv(f'{root_path}catechol_single_solvent_yields.csv')\n# # \tacs_desc = pd.read_csv(f'{root_path}acs_pca_descriptors_lookup.csv')\n# # \tspange_desc = pd.read_csv(f'{root_path}spange_descriptors_lookup.csv')\n# # except FileNotFoundError as e:\n# # \tprint(f\"Erreur: Fichier manquant. Veuillez v√©rifier le 'root_path'. Erreur: {e}\")\n# # \traise\n\n# # Cr√©ation de la table ma√Ætre des descripteurs\n# # master_desc = acs_desc.merge(spange_desc, on='SOLVENT NAME', how='outer')\n# # DESCRIPTOR_COLS = master_desc.columns.drop('SOLVENT NAME').tolist()\n\n# # ------------------------------------\n# # A. Feature Engineering pour full_data (M√©langes)\n# # ------------------------------------\n# # Le merge et le calcul MIX sont g√©r√©s par load_data(\"full\")\n# # ...\n\n# # ------------------------------------\n# # B. Feature Engineering pour single_data (Solvants Uniques)\n# # ------------------------------------\n# # Le featurizing est g√©r√© par load_data(\"single_solvent\")\n# # ...\n\n# # D√©finition des colonnes\n# # TARGET_COLS = ['SM', 'Product 2', 'Product 3']\n# # X_COLS_BASE = [c for c in full_data.columns if c.endswith('_MIX')] + ['Temperature', 'Residence Time', 'SolventB%']\n\n\n# # =========================================================================\n# # 2. FONCTION DE MOD√àLE RIDGE (AVEC POST-TRAITEMENT INT√âGR√â)\n# # =========================================================================\n\n# # Cette logique est remplac√©e par la classe RidgeModelAdapter et la boucle de CV du template.\n# # def run_ridge_cv_split(X_data, y_data, X_cols, n_splits, task_id, alpha=10.0):\n# # \tX = X_data[X_cols].copy()\n# # \ty = y_data\n# # \tfold_scores = []\n# # \tall_predictions = []\n\n# # \t# üß™ Feature Engineering (y compris Inverse T, Log Time, Interactions)\n# # \tT_Kelvin = X['Temperature'] + 273.15\n# # \tX['Inverse_T'] = 1 / T_Kelvin\n# # \tX['Log_Time'] = np.log1p(X['Residence Time'])\n# # \tX['Temp_x_Time'] = X['Temperature'] * X['Residence Time']\n# # \tX['Temp_x_SolventB'] = X['Temperature'] * X['SolventB%'] if 'SolventB%' in X.columns else 0.0\n\n# # \tkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n# # \tridge_model = Ridge(alpha=alpha, random_state=42)\n\n# # \tprint(f\"\\n[ENTRAINEMENT T√ÇCHE {task_id}] D√©marrage de la CV ({n_splits} plis)...\")\n\t\n# # \tfor fold, (train_idx, val_idx) in enumerate(kf.split(X)): # Remplacer tqdm par enumerate simple\n# # \t\t# Entra√Ænement et Pr√©diction\n# # \t\tX_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n# # \t\ty_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n# # \t\tscaler = StandardScaler()\n# # \t\tX_train_scaled = scaler.fit_transform(X_train)\n# # \t\tX_val_scaled = scaler.transform(X_val)\n\n# # \t\tmodel = MultiOutputRegressor(ridge_model)\n# # \t\tmodel.fit(X_train_scaled, y_train)\n# # \t\tpredictions = model.predict(X_val_scaled)\n\n# # \t\tmse = mean_squared_error(y_val, predictions)\n# # \t\tfold_scores.append(mse)\n\n# # \t\t# IMPORTANT : Enregistrer l'index original de la ligne (row)\n# # \t\tfor row_idx_val, row_idx_original in enumerate(val_idx):\n# # \t\t\tpred = predictions[row_idx_val]\n# # \t\t\tall_predictions.append({\n# # \t\t\t\t\"task\": task_id,\n# # \t\t\t\t\"fold\": fold,\n# # \t\t\t\t\"row\": row_idx_original, # L'index original (0, 1, 2, 3...)\n# # \t\t\t\t\"target_1\": pred[0],\n# # \t\t\t\t\"target_2\": pred[1],\n# # \t\t\t\t\"target_3\": pred[2]\n# # \t\t\t})\n\n# # \tpredictions_df = pd.DataFrame(all_predictions)\n\n# # \t# üõ†Ô∏è POST-TRAITEMENT CORRIG√â (GARANTIE DE VALIDIT√â PHYSIQUE)\n# # \ttarget_cols = ['target_1', 'target_2', 'target_3']\n\t\n# # \t# √âTAPE 1: Clipping initial pour √©liminer les rendements n√©gatifs\n# # \tprint(\"\t -> Correction: Suppression des valeurs n√©gatives (Clipping)\")\n# # \tfor col in target_cols:\n# # \t\tpredictions_df[col] = np.clip(predictions_df[col], a_min=0, a_max=None)\n\t\t\n# # \t# √âTAPE 2: Normalisation pour que la somme soit 1\n# # \tprint(\"\t -> Correction: Normalisation pour que la somme des cibles soit √©gale √† 1\")\n# # \tsum_yields = predictions_df[target_cols].sum(axis=1)\n# # \tsum_yields_safe = np.clip(sum_yields, a_min=1e-12, a_max=None) # S√©curit√© contre la division par z√©ro\n# # \tpredictions_df[target_cols] = predictions_df[target_cols].div(sum_yields_safe, axis=0)\n\t\n# # \t# √âTAPE 3: Clipping final (pour corriger les erreurs de floating point post-normalisation)\n# # \tfor col in target_cols:\n# # \t\tpredictions_df[col] = np.clip(predictions_df[col], a_min=0, a_max=1.0)\n\t\t\n# # \tavg_mse = np.mean(fold_scores)\n# # \treturn predictions_df, avg_mse\n\n# # =========================================================================\n# # 3. EX√âCUTION ET SOUMISSION FINALE (CORRECTION DU TRI)\n# # =========================================================================\n\n# # Ex√©cution de la T√¢che 0 (Solvants Uniques)\n# # X_single = single_data.drop(columns=TARGET_COLS, errors='ignore').reset_index(drop=True)\n# # y_single = single_data[TARGET_COLS].reset_index(drop=True)\n# # X_COLS_SINGLE = [c for c in X_COLS_BASE if c != 'SolventB%']\n# # submission_single_solvent, score_task0 = run_ridge_cv_split(\n# # \tX_single, y_single, X_cols = X_COLS_SINGLE, n_splits=24, task_id=0\n# # )\n\n# # # Ex√©cution de la T√¢che 1 (M√©langes)\n# # X_full = full_data.drop(columns=TARGET_COLS, errors='ignore').reset_index(drop=True)\n# # y_full = full_data[TARGET_COLS].reset_index(drop=True)\n# # submission_full_data, score_task1 = run_ridge_cv_split(\n# # \tX_full, y_full, X_cols = X_COLS_BASE, n_splits=13, task_id=1\n# # )\n\n# # -------------------------------------------------------------\n# # SOUMISSION FINALE (CORRECTION DU TRI)\n# # -------------------------------------------------------------\n# # print(\"\\n--- G√©n√©ration du fichier de soumission (Version Finale Corrig√©e) ---\")\n\n# # # 1. Concat√©nation des deux r√©sultats\n# # submission = pd.concat([submission_single_solvent, submission_full_data])\n\n# # # 2. üö® CORRECTION CRITIQUE üö®: Tri par 'task' puis par 'row' pour aligner les pr√©dictions\n# # # CE TRI EST INDISPENSABLE pour que la colonne 'id' finale soit 0, 1, 2, ...\n# # submission.sort_values(by=['task', 'row'], inplace=True, kind='stable')\n\n# # # 3. APPLICATION DES LIGNES OBLIGATOIRES DU TEMPLATE\n# # # Ces lignes cr√©ent la colonne 'id' s√©quentielle √† partir du DataFrame TRI√â.\n# # # L'index original (m√©lang√©) devient la colonne 'index', et le nouvel index s√©quentiel devient 'id'.\n# # submission = submission.reset_index()\n# # submission.index.name = \"id\"\n# # submission.to_csv(\"submission.csv\", index=True)\n\n# # print(\"\\n---------------------------------------------------------\")\n# # print(f\"\t MSE CV T√¢che 0 (Solvants Uniques) : {score_task0:.5f}\")\n# # print(f\"\t MSE CV T√¢che 1 (M√©langes) : {score_task1:.5f}\")\n# # print(\"Cette version devrait vous donner un score valide.\")\n# # print(\"---------------------------------------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================\n# CODE FINAL INT√âGRALEMENT CORRIG√â POUR LA SOUMISSION\n# (Respect absolu du template, y compris le bug de tri)\n# =========================================================================\n\nimport numpy as np \nimport pandas as pd\nimport os\nimport sys\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport tqdm\nfrom abc import ABC, abstractmethod\nimport warnings\n\nwarnings.filterwarnings('ignore') \n\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\nfrom utils import load_data, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n\n# D√©finition des classes abstraites (invariantes)\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        pass \n    @abstractmethod\n    def featurize(self, X):\n        pass\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n    @abstractmethod\n    def train_model(self, X_train, y_train):\n        pass\n    @abstractmethod\n    def predict(self, X_test):\n        pass\n\n# =========================================================================\n# 2. D√âFINITION DU MOD√àLE RIDGE (ADAPT√â ET CORRIG√â)\n# =========================================================================\n\nclass RidgeModelAdapter(BaseModel):\n    def __init__(self, alpha=10.0, features='spange_descriptors', data='single_solvent'):\n        self.scaler = StandardScaler()\n        self.model = MultiOutputRegressor(Ridge(alpha=alpha, random_state=42))\n        self.features = features\n        self.data = data\n        self.is_trained = False\n        \n    def _add_business_features(self, X):\n        X_copy = X.copy()\n        T_Kelvin = X_copy['Temperature'] + 273.15\n        X_copy['Inverse_T'] = 1 / T_Kelvin\n        X_copy['Log_Time'] = np.log1p(X_copy['Residence Time'])\n        X_copy['Temp_x_Time'] = X_copy['Temperature'] * X_copy['Residence Time']\n        if self.data == 'full':\n            X_copy['Temp_x_SolventB'] = X_copy['Temperature'] * X_copy['SolventB%'] \n        return X_copy\n    \n    def _prepare_data(self, X, fit_scaler=False):\n        X_processed = self._add_business_features(X)\n        feature_cols = [c for c in X_processed.columns if c.endswith('_MIX')]\n        feature_cols.extend(['Temperature', 'Residence Time'])\n        if self.data == 'full':\n            feature_cols.append('SolventB%')\n        feature_cols.extend(['Inverse_T', 'Log_Time', 'Temp_x_Time'])\n        if self.data == 'full':\n             feature_cols.append('Temp_x_SolventB')\n        X_features = X_processed[feature_cols].values \n        if fit_scaler:\n            X_scaled = self.scaler.fit_transform(X_features)\n        else:\n            X_scaled = self.scaler.transform(X_features)\n        return X_scaled\n\n    def train_model(self, X_train, y_train):\n        X_scaled = self._prepare_data(X_train, fit_scaler=True)\n        self.model.fit(X_scaled, y_train)\n        self.is_trained = True\n\n    def predict(self, X_test):\n        X_scaled = self._prepare_data(X_test, fit_scaler=False)\n        return self.model.predict(X_scaled)\n\n\n# =========================================================================\n# 3. T√ÇCHE 0 : SOLVANTS UNIQUES \n# =========================================================================\n\nX, Y = load_data(\"single_solvent\")\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions_task0 = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = RidgeModelAdapter(features='spange_descriptors', data='single_solvent')\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)\n    predictions_np = predictions\n    \n    # üõ†Ô∏è POST-TRAITEMENT PHYSIQUE\n    predictions_np = np.clip(predictions_np, a_min=0, a_max=None)\n    sum_yields = predictions_np.sum(axis=1, keepdims=True)\n    sum_yields_safe = np.clip(sum_yields, a_min=1e-12, a_max=None)\n    predictions_np = predictions_np / sum_yields_safe\n    predictions_np = np.clip(predictions_np, a_min=0, a_max=1.0)\n    \n    # Respect du template pour l'indexation\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions_task0.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\nsubmission_single_solvent = pd.DataFrame(all_predictions_task0)\n\n# =========================================================================\n# 4. T√ÇCHE 1 : M√âLANGES\n# =========================================================================\n\nX, Y = load_data(\"full\")\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions_task1 = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = RidgeModelAdapter(features='spange_descriptors', data='full')\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)\n    predictions_np = predictions\n    \n    # üõ†Ô∏è POST-TRAITEMENT PHYSIQUE\n    predictions_np = np.clip(predictions_np, a_min=0, a_max=None)\n    sum_yields = predictions_np.sum(axis=1, keepdims=True)\n    sum_yields_safe = np.clip(sum_yields, a_min=1e-12, a_max=None)\n    predictions_np = predictions_np / sum_yields_safe\n    predictions_np = np.clip(predictions_np, a_min=0, a_max=1.0)\n    \n    # Respect du template pour l'indexation\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions_task1.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\nsubmission_full_data = pd.DataFrame(all_predictions_task1)\n\n# =========================================================================\n# 5. SOUMISSION FINALE (Structure du Template Original)\n# =========================================================================\n\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\n\n# üõë ATTENTION : Les lignes de correction de tri ont √©t√© supprim√©es ici.\n# Le fichier final sera d√©sordonn√©.\n\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n\nprint(\"\\n---------------------------------------------------------\")\nprint(\"‚úÖ Fichier submission.csv g√©n√©r√© en respectant le template original.\")\nprint(\"ATTENTION : Le bug de tri n'est pas corrig√©. Le score risque d'√™tre invalide.\")\nprint(\"---------------------------------------------------------\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}