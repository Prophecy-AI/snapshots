{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# Catechol Benchmark — Final Multi-GPU Optimized MLP (one-shot)\n# ============================================\n\nimport os, sys, math, random, tqdm\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Generator\nfrom abc import ABC\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\n\n# -------------------\n# CONFIG\n# -------------------\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\nDATA_ROOT = '/kaggle/input/catechol-benchmark-hackathon/'\nNUM_GPUS = torch.cuda.device_count()\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nSEEDS = [42, 77, 2025]          # ensemble seeds per fold\nEPOCHS = 250\nBASE_BATCH_SIZE = 32\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nPATIENCE = 30\nCLIP_GRAD_NORM = 1.0\nHUBER_BETA = 1.0\nCLIP_PRED_TO_UNIT = True\nUSE_NUMERIC_FE = True\n\n# scale batch size a bit when >1 GPU\nBATCH_SIZE = BASE_BATCH_SIZE * max(1, NUM_GPUS)\n\ntorch.set_default_dtype(torch.float64)\n\n# -------------------\n# LABELS\n# -------------------\nINPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\nINPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\nINPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\nTARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n\n# -------------------\n# UTILS\n# -------------------\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef load_data(name=\"full\"):\n    df = pd.read_csv(f'{DATA_ROOT}/catechol_{\"full_data_yields\" if name==\"full\" else \"single_solvent_yields\"}.csv')\n    X = df[INPUT_LABELS_FULL_SOLVENT if name == \"full\" else INPUT_LABELS_SINGLE_SOLVENT].copy()\n    Y = df[TARGET_LABELS].copy()\n    return X, Y\n\ndef load_features(name=\"spange_descriptors\"):\n    return pd.read_csv(f'{DATA_ROOT}/{name}_lookup.csv', index_col=0)\n\ndef generate_leave_one_out_splits(X, Y) -> Generator:\n    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n        mask = X[\"SOLVENT NAME\"] != solvent\n        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n\ndef generate_leave_one_ramp_out_splits(X, Y) -> Generator:\n    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n    for _, row in ramps.iterrows():\n        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) &\n                 (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n\n# -------------------\n# FEATURIZATION\n# -------------------\ndef _numeric_block(X):\n    x = X[INPUT_LABELS_NUMERIC].astype(float).values\n    if USE_NUMERIC_FE:\n        rt, temp = x[:, [0]], x[:, [1]]\n        feats = [rt, temp, rt**2, temp**2, np.log1p(rt), np.log1p(temp), rt * temp]\n        x = np.concatenate(feats, axis=1)\n    return x\n\nclass SmilesFeaturizer(ABC):\n    def fit_transform(self, X, Y): raise NotImplementedError\n    def transform(self, X): raise NotImplementedError\n\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.lookup = load_features(features)\n        self.scaler_num, self.scaler_desc = StandardScaler(), StandardScaler()\n        self.feats_dim = None\n    def fit_transform(self, X, Y):\n        X_num = _numeric_block(X)\n        desc = self.lookup.loc[X[\"SOLVENT NAME\"]].values\n        X_num_s = self.scaler_num.fit_transform(X_num)\n        desc_s = self.scaler_desc.fit_transform(desc)\n        feats = np.concatenate([X_num_s, desc_s], axis=1)\n        self.feats_dim = feats.shape[1]\n        return torch.tensor(feats, device=DEVICE), torch.tensor(Y.values, device=DEVICE)\n    def transform(self, X):\n        X_num = _numeric_block(X)\n        desc = self.lookup.loc[X[\"SOLVENT NAME\"]].values\n        feats = np.concatenate([self.scaler_num.transform(X_num),\n                                self.scaler_desc.transform(desc)], axis=1)\n        return torch.tensor(feats, device=DEVICE)\n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.lookup = load_features(features)\n        self.scaler_num, self.scaler_desc = StandardScaler(), StandardScaler()\n        self.feats_dim = None\n    def fit_transform(self, X, Y):\n        X_num = _numeric_block(X)\n        A = self.lookup.loc[X[\"SOLVENT A NAME\"]].values\n        B = self.lookup.loc[X[\"SOLVENT B NAME\"]].values\n        pct = X[\"SolventB%\"].astype(float).values.reshape(-1, 1)  # if 0..100, divide by 100\n        mix = A * (1 - pct) + B * pct\n        X_num_s = self.scaler_num.fit_transform(X_num)\n        mix_s = self.scaler_desc.fit_transform(mix)\n        feats = np.concatenate([X_num_s, mix_s], axis=1)\n        self.feats_dim = feats.shape[1]\n        return torch.tensor(feats, device=DEVICE), torch.tensor(Y.values, device=DEVICE)\n    def transform(self, X):\n        X_num = _numeric_block(X)\n        A = self.lookup.loc[X[\"SOLVENT A NAME\"]].values\n        B = self.lookup.loc[X[\"SOLVENT B NAME\"]].values\n        pct = X[\"SolventB%\"].astype(float).values.reshape(-1, 1)\n        mix = A * (1 - pct) + B * pct\n        feats = np.concatenate([self.scaler_num.transform(X_num),\n                                self.scaler_desc.transform(mix)], axis=1)\n        return torch.tensor(feats, device=DEVICE)\n\n# -------------------\n# MODEL\n# -------------------\nclass ImprovedMLP(nn.Module):\n    def __init__(self, feats_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm1d(feats_dim),\n            nn.Linear(feats_dim, 256), nn.ReLU(), nn.Dropout(0.30),\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.20),\n            nn.Linear(128, 64), nn.ReLU(),\n            nn.Linear(64, 3)  # linear head\n        )\n    def forward(self, x): return self.net(x)\n\n# -------------------\n# TRAINING\n# -------------------\nclass EarlyStopper:\n    def __init__(self, patience=PATIENCE):\n        self.best, self.count = math.inf, 0\n        self.state = None\n        self.patience = patience\n    def step(self, loss, model):\n        if loss < self.best - 1e-12:\n            self.best = loss\n            self.count = 0\n            self.state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            self.count += 1\n        return self.count > self.patience\n\ndef remove_module_prefix(state_dict):\n    return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n\ndef train_one_model(Xtr_t, Ytr_t, feats_dim, seed):\n    set_seed(seed)\n    model = ImprovedMLP(feats_dim).to(DEVICE)\n\n    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel...\")\n        model = nn.DataParallel(model)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n    crit = nn.SmoothL1Loss(beta=HUBER_BETA)\n\n    loader = DataLoader(TensorDataset(Xtr_t, Ytr_t), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n    stopper = EarlyStopper(patience=PATIENCE)\n\n    for epoch in range(EPOCHS):\n        model.train()\n        tot = 0.0\n        for xb, yb in loader:\n            opt.zero_grad(set_to_none=True)\n            loss = crit(model(xb), yb)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n            opt.step()\n            tot += loss.item() * xb.size(0)\n        sched.step()\n        if stopper.step(tot / len(Xtr_t), model):\n            break\n\n    # restore best weights (handle DataParallel prefixes)\n    best = stopper.state\n    if best is not None:\n        if isinstance(model, nn.DataParallel):\n            model = model.module\n        model.load_state_dict(remove_module_prefix(best), strict=False)\n\n    return model\n\n@torch.no_grad()\ndef predict_model(model, X_t):\n    model.eval()\n    out = model(X_t).cpu().numpy()\n    if CLIP_PRED_TO_UNIT: out = np.clip(out, 0, 1)\n    return out\n\ndef ensemble_predict(models, X_t):\n    return np.mean([predict_model(m, X_t) for m in models], axis=0)\n\n# -------------------\n# CV LOOPS\n# -------------------\ndef run_single_solvent_cv(features='spange_descriptors'):\n    X, Y = load_data(\"single_solvent\")\n    preds_rows, true_rows, fold_mses = [], [], []\n    for fold, ((trX, trY), (teX, teY)) in enumerate(tqdm.tqdm(list(generate_leave_one_out_splits(X, Y)), desc='Single-solvent CV')):\n        feat = PrecomputedFeaturizer(features)\n        Xtr_t, Ytr_t = feat.fit_transform(trX, trY)\n        Xte_t = feat.transform(teX)\n        dim = Xtr_t.shape[1]\n        models = [train_one_model(Xtr_t, Ytr_t, dim, s) for s in SEEDS]\n        pred = ensemble_predict(models, Xte_t)\n        true = teY.values\n        fold_mses.append(float(np.mean((pred - true) ** 2)))\n        for i, p in enumerate(pred):\n            preds_rows.append({\"task\": 0, \"fold\": fold, \"row\": i, \"target_1\": p[0], \"target_2\": p[1], \"target_3\": p[2]})\n            true_rows.append({\"task\": 0, \"fold\": fold, \"row\": i, \"true_1\": true[i,0], \"true_2\": true[i,1], \"true_3\": true[i,2]})\n    return pd.DataFrame(preds_rows), pd.DataFrame(true_rows), float(np.mean(fold_mses))\n\ndef run_full_ramp_cv(features='spange_descriptors'):\n    X, Y = load_data(\"full\")\n    preds_rows, true_rows, fold_mses = [], [], []\n    for fold, ((trX, trY), (teX, teY)) in enumerate(tqdm.tqdm(list(generate_leave_one_ramp_out_splits(X, Y)), desc='Full-data CV')):\n        feat = PrecomputedFeaturizerMixed(features)\n        Xtr_t, Ytr_t = feat.fit_transform(trX, trY)\n        Xte_t = feat.transform(teX)\n        dim = Xtr_t.shape[1]\n        models = [train_one_model(Xtr_t, Ytr_t, dim, s) for s in SEEDS]\n        pred = ensemble_predict(models, Xte_t)\n        true = teY.values\n        fold_mses.append(float(np.mean((pred - true) ** 2)))\n        for i, p in enumerate(pred):\n            preds_rows.append({\"task\": 1, \"fold\": fold, \"row\": i, \"target_1\": p[0], \"target_2\": p[1], \"target_3\": p[2]})\n            true_rows.append({\"task\": 1, \"fold\": fold, \"row\": i, \"true_1\": true[i,0], \"true_2\": true[i,1], \"true_3\": true[i,2]})\n    return pd.DataFrame(preds_rows), pd.DataFrame(true_rows), float(np.mean(fold_mses))\n\n# -------------------\n# MAIN\n# -------------------\nif __name__ == \"__main__\":\n    print(f\"Detected {NUM_GPUS} GPU(s): {[torch.cuda.get_device_name(i) for i in range(NUM_GPUS)]}\")\n    print(f\"Using device: {DEVICE} | Batch size: {BATCH_SIZE}\")\n\n    sub_single, true_single, single_cv = run_single_solvent_cv('spange_descriptors')\n    sub_full, true_full, full_cv = run_full_ramp_cv('spange_descriptors')\n\n    submission = pd.concat([sub_single, sub_full]).reset_index(drop=True)\n    submission.index.name = \"id\"\n    submission.to_csv(\"submission.csv\", index=True)\n\n    merged_true = pd.concat([true_single, true_full]).reset_index(drop=True)\n    merged = submission.merge(merged_true, on=['task','fold','row'])\n\n    overall_mse = np.mean([\n        (merged['target_1'] - merged['true_1'])**2,\n        (merged['target_2'] - merged['true_2'])**2,\n        (merged['target_3'] - merged['true_3'])**2\n    ])\n    print(\"\\n\" + \"=\"*72)\n    print(\"FINAL RESULTS — Multi-GPU Upgraded CV\")\n    print(\"=\"*72)\n    print(f\"Overall CV MSE : {overall_mse:.6f}\")\n    print(f\"Overall CV RMSE: {overall_mse**0.5:.6f}\")\n    print(f\"Single Solvent : {single_cv:.6f}\")\n    print(f\"Full Data      : {full_cv:.6f}\")\n    print(\"=\"*72)\n    print(\"Saved: submission.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}