{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================\n# ULTIMATE CATECHOL PREDICTION SYSTEM\n# Advanced Hybrid Ensemble with Neural Networks\n# ==============================\n\nimport os\nimport sys\nimport gc\nimport copy\nimport warnings\nfrom pathlib import Path\nfrom typing import Tuple, List, Dict, Optional\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, log_loss\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer, OneHotEncoder\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Tree models\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\n# Kaggle utils\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\nfrom utils import (\n    INPUT_LABELS_NUMERIC, load_data, load_features,\n    generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n)\n\n# ============================\n# CONFIGURATION\n# ============================\n@dataclass\nclass Config:\n    \"\"\"Unified configuration for all models\"\"\"\n    # General\n    seed: int = 42\n    val_size: float = 0.12\n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Feature Engineering\n    use_robust_scaler: bool = True\n    use_quantile_transform: bool = False\n    add_poly_features: bool = True\n    add_interaction_features: bool = True\n    \n    # CatBoost\n    cb_iterations: int = 12000\n    cb_lr: float = 0.012\n    cb_depth: int = 9\n    cb_l2: float = 2.5\n    cb_subsample: float = 0.88\n    cb_early_stop: int = 250\n    \n    # XGBoost\n    xgb_rounds: int = 12000\n    xgb_eta: float = 0.02\n    xgb_depth: int = 9\n    xgb_subsample: float = 0.88\n    xgb_colsample: float = 0.78\n    xgb_early_stop: int = 250\n    \n    # LightGBM\n    lgb_rounds: int = 12000\n    lgb_lr: float = 0.015\n    lgb_leaves: int = 127\n    lgb_depth: int = 10\n    lgb_subsample: float = 0.88\n    lgb_early_stop: int = 250\n    \n    # Neural Network\n    nn_epochs: int = 800\n    nn_batch_size: int = 128\n    nn_lr: float = 0.001\n    nn_hidden: List[int] = None\n    nn_dropout: float = 0.35\n    nn_patience: int = 80\n    \n    # Ensemble\n    ensemble_strategy: str = 'adaptive'  # 'adaptive', 'stacking', 'geometric'\n    power_weights: float = 2.5\n    nn_weight_boost: float = 1.15  # Boost NN importance\n    \n    def __post_init__(self):\n        if self.nn_hidden is None:\n            self.nn_hidden = [768, 512, 384, 256, 128]\n\nCFG = Config()\n\n# ============================\n# ADVANCED FEATURE ENGINEERING\n# ============================\nclass UltraFeaturizer:\n    \"\"\"State-of-the-art feature engineering\"\"\"\n    \n    def __init__(self, features='spange_descriptors', mixed=False, config=CFG):\n        self.features_name = features\n        self.featurizer_df = load_features(features)\n        self.mixed = mixed\n        self.config = config\n        \n        # Multiple scalers for ensemble\n        self.scalers = {\n            'robust': RobustScaler(quantile_range=(3, 97)),\n            'quantile': QuantileTransformer(n_quantiles=1000, output_distribution='normal')\n        }\n        self.active_scaler = 'robust' if config.use_robust_scaler else 'quantile'\n        self._fitted = False\n        self._cache = {}\n        self._feature_importance = None\n\n    def _get_molecular(self, row):\n        \"\"\"Extract molecular features with caching\"\"\"\n        if not self.mixed:\n            key = row[\"SOLVENT NAME\"]\n            if key not in self._cache:\n                self._cache[key] = self.featurizer_df.loc[key].values\n            return self._cache[key]\n        else:\n            A_name = row[\"SOLVENT A NAME\"]\n            B_name = row[\"SOLVENT B NAME\"]\n            r = row[\"SolventB%\"]\n            \n            if A_name not in self._cache:\n                self._cache[A_name] = self.featurizer_df.loc[A_name].values\n            if B_name not in self._cache:\n                self._cache[B_name] = self.featurizer_df.loc[B_name].values\n            \n            A, B = self._cache[A_name], self._cache[B_name]\n            # Non-linear mixing for better representation\n            return A * (1 - r) + B * r + 0.05 * A * B * r * (1 - r)\n\n    def _create_advanced_features(self, numeric_feat, mol_feat):\n        \"\"\"Engineer advanced features\"\"\"\n        features = [numeric_feat, mol_feat]\n        \n        if self.config.add_poly_features and numeric_feat.shape[1] > 0:\n            # Polynomial features\n            features.append(numeric_feat ** 2)\n            features.append(np.sqrt(np.abs(numeric_feat) + 1e-8))\n            \n        if self.config.add_interaction_features and numeric_feat.shape[1] >= 2:\n            # Interaction terms\n            features.append((numeric_feat[:, 0] * numeric_feat[:, 1]).reshape(-1, 1))\n            if numeric_feat.shape[1] >= 3:\n                features.append((numeric_feat[:, 0] * numeric_feat[:, 2]).reshape(-1, 1))\n                features.append((numeric_feat[:, 1] * numeric_feat[:, 2]).reshape(-1, 1))\n        \n        # Statistical features from molecular descriptors\n        mol_stats = np.column_stack([\n            mol_feat.mean(axis=1),\n            mol_feat.std(axis=1),\n            mol_feat.max(axis=1),\n            mol_feat.min(axis=1)\n        ])\n        features.append(mol_stats)\n        \n        return np.concatenate(features, axis=1)\n\n    def featurize(self, X: pd.DataFrame, return_torch=False):\n        \"\"\"Convert DataFrame to feature matrix\"\"\"\n        numeric = X[INPUT_LABELS_NUMERIC].to_numpy(dtype=np.float32)\n        mol = np.vstack([self._get_molecular(X.iloc[i]) for i in range(len(X))]).astype(np.float32)\n        \n        # Advanced feature engineering\n        combined = self._create_advanced_features(numeric, mol)\n        combined = np.nan_to_num(combined, nan=0.0, posinf=1e6, neginf=-1e6)\n        \n        # Multi-scaler approach\n        if not self._fitted:\n            # Fit both scalers\n            for scaler in self.scalers.values():\n                scaler.fit(combined)\n            self._fitted = True\n        \n        # Use primary scaler\n        combined = self.scalers[self.active_scaler].transform(combined)\n        combined = combined.astype(np.float32)\n        \n        if return_torch:\n            return torch.tensor(combined, dtype=torch.float32)\n        return combined\n\n# ============================\n# NEURAL NETWORK ARCHITECTURE\n# ============================\nclass SEBlock(nn.Module):\n    \"\"\"Squeeze-and-Excitation block for feature recalibration\"\"\"\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels, bias=False),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return x * self.fc(x)\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Enhanced residual block with SE attention\"\"\"\n    def __init__(self, dim, dropout=0.3):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.LayerNorm(dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim, dim),\n            nn.LayerNorm(dim),\n        )\n        self.se = SEBlock(dim)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = nn.GELU()\n    \n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out = self.se(out)\n        out = self.activation(residual + out)\n        return self.dropout(out)\n\n\nclass UltimateNeuralNetwork(nn.Module):\n    \"\"\"State-of-the-art neural network for regression\"\"\"\n    def __init__(self, input_dim, hidden_dims=None, output_dim=3, dropout=0.35):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = [768, 512, 384, 256, 128]\n        \n        layers = []\n        \n        # Input projection with layer norm\n        layers.extend([\n            nn.Linear(input_dim, hidden_dims[0]),\n            nn.LayerNorm(hidden_dims[0]),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        ])\n        \n        # Deep residual layers\n        for i in range(len(hidden_dims) - 1):\n            layers.extend([\n                nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n                nn.LayerNorm(hidden_dims[i+1]),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                ResidualBlock(hidden_dims[i+1], dropout)\n            ])\n        \n        # Output head with multi-head approach\n        self.backbone = nn.Sequential(*layers)\n        self.output_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),\n                nn.GELU(),\n                nn.Dropout(dropout * 0.5),\n                nn.Linear(hidden_dims[-1] // 2, 1)\n            ) for _ in range(output_dim)\n        ])\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        outputs = [head(features) for head in self.output_heads]\n        return torch.cat(outputs, dim=1)\n\n# ============================\n# TRAINING UTILITIES\n# ============================\ndef prepare_targets(y_df, smoothing=3e-7):\n    \"\"\"Advanced target preparation with label smoothing\"\"\"\n    if y_df.shape[1] == 1:\n        labels = y_df.iloc[:, 0].to_numpy().reshape(-1)\n        ohe = OneHotEncoder(sparse_output=False, categories='auto')\n        onehot = ohe.fit_transform(labels.reshape(-1, 1)).astype(np.float32)\n        \n        # Adaptive label smoothing\n        n_classes = onehot.shape[1]\n        if smoothing > 0:\n            onehot = onehot * (1 - smoothing) + smoothing / n_classes\n        \n        if onehot.shape[1] < 3:\n            pad = np.zeros((onehot.shape[0], 3 - onehot.shape[1]), dtype=np.float32)\n            onehot = np.concatenate([onehot, pad], axis=1)\n        return onehot\n    else:\n        arr = y_df.to_numpy(dtype=np.float32)\n        if arr.ndim == 1:\n            arr = arr.reshape(-1, 1)\n        if arr.shape[1] < 3:\n            pad = np.zeros((arr.shape[0], 3 - arr.shape[1]), dtype=np.float32)\n            arr = np.concatenate([arr, pad], axis=1)\n        return arr\n\n\nclass FocalMSELoss(nn.Module):\n    \"\"\"Focal MSE Loss for hard example mining\"\"\"\n    def __init__(self, gamma=2.0):\n        super().__init__()\n        self.gamma = gamma\n        self.mse = nn.MSELoss(reduction='none')\n    \n    def forward(self, pred, target):\n        mse = self.mse(pred, target)\n        focal_weight = torch.pow(mse.detach(), self.gamma)\n        return (focal_weight * mse).mean()\n\n\ndef train_neural_network(model, X_train, y_train, X_val, y_val, config=CFG):\n    \"\"\"Advanced training with mixed precision and techniques\"\"\"\n    device = torch.device(config.device)\n    model.to(device)\n    \n    # Data preparation\n    train_dataset = TensorDataset(\n        torch.tensor(X_train, dtype=torch.float32),\n        torch.tensor(y_train, dtype=torch.float32)\n    )\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.nn_batch_size, \n        shuffle=True,\n        num_workers=0,\n        pin_memory=True\n    )\n    \n    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32).to(device)\n    \n    # Optimizer with gradient clipping\n    optimizer = optim.AdamW(\n        model.parameters(), \n        lr=config.nn_lr, \n        weight_decay=5e-5,\n        betas=(0.9, 0.999)\n    )\n    \n    # Advanced scheduler\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=config.nn_lr,\n        epochs=config.nn_epochs,\n        steps_per_epoch=len(train_loader),\n        pct_start=0.1,\n        anneal_strategy='cos'\n    )\n    \n    # Loss functions\n    criterion_mse = nn.MSELoss()\n    criterion_focal = FocalMSELoss(gamma=1.5)\n    \n    best_val_loss = float('inf')\n    best_model_state = None\n    patience_counter = 0\n    \n    for epoch in range(config.nn_epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        \n        for batch_X, batch_y in train_loader:\n            batch_X = batch_X.to(device, non_blocking=True)\n            batch_y = batch_y.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            \n            # Combined loss\n            loss = 0.7 * criterion_mse(outputs, batch_y) + 0.3 * criterion_focal(outputs, batch_y)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            train_loss += loss.item()\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(X_val_t)\n            val_loss = criterion_mse(val_outputs, y_val_t).item()\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = copy.deepcopy(model.state_dict())\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= config.nn_patience:\n            print(f\"  Early stop at epoch {epoch}\")\n            break\n        \n        if epoch % 200 == 0:\n            print(f\"  Epoch {epoch}: Train={train_loss/len(train_loader):.6f}, Val={val_loss:.6f}\")\n    \n    if best_model_state:\n        model.load_state_dict(best_model_state)\n    \n    return model, best_val_loss\n\n# ============================\n# TREE MODELS\n# ============================\ndef train_catboost(X_tr, y_tr, X_val, y_val, config=CFG, seed=SEED):\n    \"\"\"Optimized CatBoost training\"\"\"\n    models, preds = [], np.zeros_like(y_val, dtype=np.float32)\n    \n    for k in range(y_tr.shape[1]):\n        cb = CatBoostRegressor(\n            iterations=config.cb_iterations,\n            learning_rate=config.cb_lr,\n            depth=config.cb_depth,\n            l2_leaf_reg=config.cb_l2,\n            subsample=config.cb_subsample,\n            loss_function='RMSE',\n            bootstrap_type='Bayesian',\n            bagging_temperature=0.25,\n            border_count=254,\n            od_type='Iter',\n            od_wait=config.cb_early_stop,\n            random_seed=seed,\n            verbose=False,\n            thread_count=-1\n        )\n        cb.fit(X_tr, y_tr[:, k], eval_set=(X_val, y_val[:, k]), use_best_model=True)\n        preds[:, k] = cb.predict(X_val)\n        models.append(cb)\n    \n    return models, preds\n\n\ndef train_xgboost(X_tr, y_tr, X_val, y_val, config=CFG, seed=SEED):\n    \"\"\"Optimized XGBoost training\"\"\"\n    models, preds = [], np.zeros_like(y_val, dtype=np.float32)\n    \n    for k in range(y_tr.shape[1]):\n        params = {\n            \"objective\": \"reg:squarederror\",\n            \"tree_method\": \"hist\",\n            \"device\": \"cpu\",\n            \"eta\": config.xgb_eta,\n            \"max_depth\": config.xgb_depth,\n            \"subsample\": config.xgb_subsample,\n            \"colsample_bytree\": config.xgb_colsample,\n            \"gamma\": 0.03,\n            \"reg_alpha\": 0.03,\n            \"reg_lambda\": 2.0,\n            \"min_child_weight\": 2,\n            \"max_bin\": 256,\n            \"seed\": seed,\n            \"verbosity\": 0\n        }\n        dtrain = xgb.DMatrix(X_tr, label=y_tr[:, k])\n        dval = xgb.DMatrix(X_val, label=y_val[:, k])\n        \n        bst = xgb.train(\n            params, dtrain,\n            num_boost_round=config.xgb_rounds,\n            evals=[(dtrain, \"train\"), (dval, \"valid\")],\n            early_stopping_rounds=config.xgb_early_stop,\n            verbose_eval=False\n        )\n        preds[:, k] = bst.predict(dval)\n        models.append(bst)\n    \n    return models, preds\n\n\ndef train_lightgbm(X_tr, y_tr, X_val, y_val, config=CFG, seed=SEED):\n    \"\"\"Optimized LightGBM training\"\"\"\n    models, preds = [], np.zeros_like(y_val, dtype=np.float32)\n    \n    for k in range(y_tr.shape[1]):\n        train_data = lgb.Dataset(X_tr, label=y_tr[:, k])\n        val_data = lgb.Dataset(X_val, label=y_val[:, k], reference=train_data)\n        \n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"boosting\": \"gbdt\",\n            \"learning_rate\": config.lgb_lr,\n            \"num_leaves\": config.lgb_leaves,\n            \"max_depth\": config.lgb_depth,\n            \"bagging_fraction\": config.lgb_subsample,\n            \"bagging_freq\": 5,\n            \"feature_fraction\": 0.78,\n            \"lambda_l1\": 0.03,\n            \"lambda_l2\": 2.0,\n            \"min_data_in_leaf\": 15,\n            \"max_bin\": 255,\n            \"seed\": seed,\n            \"verbosity\": -1,\n            \"device\": \"cpu\"\n        }\n        \n        bst = lgb.train(\n            params, train_data,\n            num_boost_round=config.lgb_rounds,\n            valid_sets=[val_data],\n            callbacks=[\n                lgb.early_stopping(config.lgb_early_stop),\n                lgb.log_evaluation(0)\n            ]\n        )\n        preds[:, k] = bst.predict(X_val, num_iteration=bst.best_iteration)\n        models.append(bst)\n    \n    return models, preds\n\n# ============================\n# PREDICTION FUNCTIONS\n# ============================\ndef predict_models(models_dict, X, model_type):\n    \"\"\"Universal prediction function\"\"\"\n    if model_type == 'catboost':\n        return np.column_stack([m.predict(X) for m in models_dict])\n    elif model_type == 'xgboost':\n        dm = xgb.DMatrix(X)\n        return np.column_stack([m.predict(dm) for m in models_dict])\n    elif model_type == 'lightgbm':\n        return np.column_stack([m.predict(X, num_iteration=m.best_iteration) for m in models_dict])\n    elif model_type == 'neural':\n        device = torch.device(CFG.device)\n        models_dict.eval()\n        with torch.no_grad():\n            X_t = torch.tensor(X, dtype=torch.float32).to(device)\n            return models_dict(X_t).cpu().numpy()\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n# ============================\n# ADVANCED ENSEMBLE\n# ============================\ndef adaptive_ensemble(predictions, val_scores, config=CFG):\n    \"\"\"Advanced adaptive ensemble with boosted NN\"\"\"\n    # Calculate weights from validation scores\n    inv_scores = np.array([1.0 / (s + 1e-9) for s in val_scores])\n    inv_scores = inv_scores ** config.power_weights\n    \n    # Boost neural network weight\n    if len(inv_scores) == 4:  # CB, XGB, LGB, NN\n        inv_scores[-1] *= config.nn_weight_boost\n    \n    weights = inv_scores / inv_scores.sum()\n    \n    # Weighted ensemble\n    ensemble = np.zeros_like(predictions[0])\n    for pred, w in zip(predictions, weights):\n        ensemble += pred * w\n    \n    return ensemble, weights\n\n\ndef geometric_ensemble(predictions, epsilon=1e-9):\n    \"\"\"Geometric mean for probability distributions\"\"\"\n    predictions = [np.clip(p, epsilon, 1.0) for p in predictions]\n    product = np.ones_like(predictions[0])\n    for pred in predictions:\n        product *= pred\n    return np.power(product, 1.0 / len(predictions))\n\n# ============================\n# MAIN PIPELINE\n# ============================\ndef run_ultimate_pipeline(data_key, split_gen, featurizer_kwargs, task_id):\n    \"\"\"Ultimate training pipeline\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TASK {task_id}: {data_key.upper()}\")\n    print('='*70)\n    \n    all_preds = []\n    fold_stats = []\n    \n    for fold_idx, split in enumerate(split_gen):\n        print(f\"\\n--- Fold {fold_idx} ---\")\n        (train_X, train_Y), (test_X, test_Y) = split\n        \n        # Feature engineering\n        featurizer = UltraFeaturizer(**featurizer_kwargs)\n        X_train = featurizer.featurize(train_X)\n        X_test = featurizer.featurize(test_X)\n        print(f\"Features: {X_train.shape[1]}\")\n        \n        # Prepare targets\n        y_train = prepare_targets(train_Y)\n        y_test = prepare_targets(test_Y) if test_Y is not None else None\n        \n        # Train/Val split\n        X_tr, X_val, y_tr, y_val = train_test_split(\n            X_train, y_train,\n            test_size=CFG.val_size,\n            random_state=SEED + fold_idx,\n            shuffle=True\n        )\n        \n        print(f\"Train: {X_tr.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n        \n        # Train all models\n        print(\"[1/4] CatBoost...\")\n        cb_models, cb_val = train_catboost(X_tr, y_tr, X_val, y_val, seed=SEED+fold_idx)\n        cb_mse = mean_squared_error(y_val, cb_val)\n        print(f\"  MSE: {cb_mse:.7f}\")\n        \n        print(\"[2/4] XGBoost...\")\n        xgb_models, xgb_val = train_xgboost(X_tr, y_tr, X_val, y_val, seed=SEED+fold_idx)\n        xgb_mse = mean_squared_error(y_val, xgb_val)\n        print(f\"  MSE: {xgb_mse:.7f}\")\n        \n        print(\"[3/4] LightGBM...\")\n        lgb_models, lgb_val = train_lightgbm(X_tr, y_tr, X_val, y_val, seed=SEED+fold_idx)\n        lgb_mse = mean_squared_error(y_val, lgb_val)\n        print(f\"  MSE: {lgb_mse:.7f}\")\n        \n        print(\"[4/4] Neural Network...\")\n        nn_model = UltimateNeuralNetwork(\n            input_dim=X_tr.shape[1],\n            hidden_dims=CFG.nn_hidden,\n            dropout=CFG.nn_dropout\n        )\n        nn_model, nn_mse = train_neural_network(nn_model, X_tr, y_tr, X_val, y_val)\n        nn_val = predict_models(nn_model, X_val, 'neural')\n        print(f\"  MSE: {nn_mse:.7f}\")\n        \n        # Ensemble on validation\n        val_scores = [cb_mse, xgb_mse, lgb_mse, nn_mse]\n        val_preds = [cb_val, xgb_val, lgb_val, nn_val]\n        \n        if CFG.ensemble_strategy == 'adaptive':\n            val_ens, weights = adaptive_ensemble(val_preds, val_scores)\n        elif CFG.ensemble_strategy == 'geometric':\n            val_ens = geometric_ensemble(val_preds)\n            weights = [0.25] * 4\n        else:  # equal\n            val_ens = np.mean(val_preds, axis=0)\n            weights = [0.25] * 4\n        \n        ens_mse = mean_squared_error(y_val, val_ens)\n        print(f\"\\nEnsemble MSE: {ens_mse:.7f}\")\n        print(f\"Weights: CB={weights[0]:.4f}, XGB={weights[1]:.4f}, LGB={weights[2]:.4f}, NN={weights[3]:.4f}\")\n        \n        fold_stats.append({\n            'fold': fold_idx,\n            'cb_mse': cb_mse,\n            'xgb_mse': xgb_mse,\n            'lgb_mse': lgb_mse,\n            'nn_mse': nn_mse,\n            'ens_mse': ens_mse\n        })\n        \n        # Test predictions\n        cb_test = predict_models(cb_models, X_test, 'catboost')\n        xgb_test = predict_models(xgb_models, X_test, 'xgboost')\n        lgb_test = predict_models(lgb_models, X_test, 'lightgbm')\n        nn_test = predict_models(nn_model, X_test, 'neural')\n        \n        # Apply ensemble weights\n        test_ens = np.zeros_like(cb_test)\n        for pred, w in zip([cb_test, xgb_test, lgb_test, nn_test], weights):\n            test_ens += pred * w\n        \n        # Normalize to probabilities\n        test_ens = np.clip(test_ens, 1e-9, None)\n        test_ens = test_ens / test_ens.sum(axis=1, keepdims=True)\n        \n        all_preds.append((fold_idx, test_X.index.to_numpy(), test_ens))\n        \n        # Cleanup\n        del cb_models, xgb_models, lgb_models, nn_model\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    \n    # Summary statistics\n    print(f\"\\n{'='*70}\")\n    print(\"VALIDATION SUMMARY\")\n    df_stats = pd.DataFrame(fold_stats)\n    print(df_stats.to_string(index=False))\n    print(f\"\\nEnsemble: {df_stats['ens_mse'].mean():.7f} ± {df_stats['ens_mse'].std():.7f}\")\n    print('='*70)\n    \n    return all_preds\n\n\ndef assemble_submission(preds_list, task_id):\n    \"\"\"Create submission DataFrame\"\"\"\n    rows = []\n    for fold_idx, indices, preds in preds_list:\n        for row_idx, idx in enumerate(indices):\n            p = preds[row_idx]\n            rows.append({\n                \"task\": int(task_id),\n                \"fold\": int(fold_idx),\n                \"row\": int(row_idx),\n                \"target_1\": float(p[0]),\n                \"target_2\": float(p[1]),\n                \"target_3\": float(p[2])\n            })\n    return pd.DataFrame(rows)\n\n# ============================\n# MAIN EXECUTION\n# ============================\ndef main():\n    \"\"\"Execute complete pipeline\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"ULTIMATE CATECHOL PREDICTION SYSTEM\")\n    print(\"Hybrid Ensemble: Trees + Neural Networks\")\n    print(\"=\"*70)\n    print(f\"\\nDevice: {CFG.device}\")\n    print(f\"Ensemble: {CFG.ensemble_strategy}\")\n    print(f\"Neural Network: {' → '.join(map(str, CFG.nn_hidden))} → 3\")\n    \n    # TASK 0: Single Solvent\n    print(\"\\n\" + \"=\"*70)\n    print(\"STARTING TASK 0: SINGLE SOLVENT\")\n    print(\"=\"*70)\n    X_single, Y_single = load_data(\"single_solvent\")\n    split_gen_single = generate_leave_one_out_splits(X_single, Y_single)\n    \n    preds_single = run_ultimate_pipeline(\n        data_key=\"single_solvent\",\n        split_gen=split_gen_single,\n        featurizer_kwargs={\n            'features': 'spange_descriptors',\n            'mixed': False,\n            'config': CFG\n        },\n        task_id=0\n    )\n    \n    submission_single = assemble_submission(preds_single, task_id=0)\n    print(f\"\\n✓ Task 0 complete: {len(submission_single)} rows\")\n    \n    # TASK 1: Full Data (Mixed Solvents)\n    print(\"\\n\" + \"=\"*70)\n    print(\"STARTING TASK 1: MIXED SOLVENTS\")\n    print(\"=\"*70)\n    X_full, Y_full = load_data(\"full\")\n    split_gen_full = generate_leave_one_ramp_out_splits(X_full, Y_full)\n    \n    preds_full = run_ultimate_pipeline(\n        data_key=\"full\",\n        split_gen=split_gen_full,\n        featurizer_kwargs={\n            'features': 'spange_descriptors',\n            'mixed': True,\n            'config': CFG\n        },\n        task_id=1\n    )\n    \n    submission_full = assemble_submission(preds_full, task_id=1)\n    print(f\"\\n✓ Task 1 complete: {len(submission_full)} rows\")\n    \n    # COMBINE AND FINALIZE\n    print(\"\\n\" + \"=\"*70)\n    print(\"FINALIZING SUBMISSION\")\n    print(\"=\"*70)\n    \n    submission = pd.concat([submission_single, submission_full], ignore_index=True)\n    submission = submission.reset_index(drop=True)\n    submission.index.name = \"id\"\n    submission['id'] = submission.index\n    submission['index'] = submission.index\n    \n    # Reorder columns\n    submission = submission[['id', 'index', 'task', 'fold', 'row', \n                            'target_1', 'target_2', 'target_3']]\n    \n    # Final robust probability normalization\n    probs = submission[['target_1', 'target_2', 'target_3']].to_numpy(dtype=np.float64)\n    \n    # Triple normalization for maximum stability\n    probs = np.clip(probs, 1e-10, 1.0)\n    probs = probs / probs.sum(axis=1, keepdims=True)\n    probs = np.clip(probs, 1e-10, 1.0)\n    probs = probs / probs.sum(axis=1, keepdims=True)\n    \n    submission[['target_1', 'target_2', 'target_3']] = probs\n    \n    # Comprehensive validation\n    row_sums = probs.sum(axis=1)\n    print(f\"\\nSubmission Validation:\")\n    print(f\"  Total rows: {len(submission):,}\")\n    print(f\"  Tasks: {submission['task'].nunique()} (0: {(submission['task']==0).sum()}, 1: {(submission['task']==1).sum()})\")\n    print(f\"  Folds: {submission['fold'].nunique()}\")\n    print(f\"  Probability range:\")\n    print(f\"    target_1: [{probs[:, 0].min():.8f}, {probs[:, 0].max():.8f}]\")\n    print(f\"    target_2: [{probs[:, 1].min():.8f}, {probs[:, 1].max():.8f}]\")\n    print(f\"    target_3: [{probs[:, 2].min():.8f}, {probs[:, 2].max():.8f}]\")\n    print(f\"  Row sums: [{row_sums.min():.10f}, {row_sums.max():.10f}]\")\n    print(f\"  Max deviation from 1.0: {np.abs(row_sums - 1.0).max():.2e}\")\n    \n    # Validate assertions\n    assert len(submission) > 0, \"Empty submission!\"\n    assert np.all(probs >= 0), \"Negative probabilities detected!\"\n    assert np.allclose(row_sums, 1.0, atol=1e-6), \"Row sums don't equal 1.0!\"\n    assert np.all(np.isfinite(probs)), \"Non-finite values detected!\"\n    \n    # Save submission\n    submission.to_csv(\"submission.csv\", index=False)\n    print(f\"\\n✓ Submission saved: submission.csv\")\n    \n    # Display sample\n    print(\"\\n\" + \"=\"*70)\n    print(\"SAMPLE PREDICTIONS\")\n    print(\"=\"*70)\n    print(submission.head(10).to_string(index=False))\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"STATISTICS\")\n    print(\"=\"*70)\n    print(submission[['target_1', 'target_2', 'target_3']].describe())\n    \n    # Class distribution\n    print(\"\\n\" + \"=\"*70)\n    print(\"PREDICTED CLASS DISTRIBUTION\")\n    print(\"=\"*70)\n    predicted_classes = np.argmax(probs, axis=1)\n    for i in range(3):\n        count = (predicted_classes == i).sum()\n        pct = count / len(predicted_classes) * 100\n        print(f\"  Class {i+1}: {count:,} ({pct:.2f}%)\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"SUCCESS! Pipeline completed successfully.\")\n    print(\"=\"*70 + \"\\n\")\n    \n    return submission\n\n\nif __name__ == \"__main__\":\n    submission = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:28:55.891851Z","iopub.execute_input":"2025-11-06T13:28:55.892521Z","execution_failed":"2025-11-06T13:38:11.098Z"}},"outputs":[],"execution_count":null}]}