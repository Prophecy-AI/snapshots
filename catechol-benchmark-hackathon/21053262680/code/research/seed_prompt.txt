## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost+XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- **10 submissions pending LB feedback** (exp_049-079)

## CV-LB Relationship Analysis (CRITICAL - MUST READ)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept (0.0525) > Target (0.0347)
- **Required CV to hit target: -0.0041 (IMPOSSIBLE)**
- All 94 experiments (MLP, LGBM, XGB, CatBoost, GP, GNN, ChemBERTa) fall on the SAME LINE
- **CONCLUSION: Improving CV alone CANNOT reach target. Must change the CV-LB relationship.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_093 (Morgan fingerprints)
- Evaluator's top priority: **Investigate mixall kernel's GroupKFold approach** - may have different CV-LB relationship
- Key concerns raised: 
  1. CV-LB intercept (0.0525) > Target (0.0347) makes target mathematically unreachable
  2. Morgan fingerprints performed 277% worse than baseline
  3. GNN failures need root cause analysis
- **I AGREE with the evaluator's assessment.** The intercept problem is the fundamental blocker.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop94_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All tabular models converge to same CV-LB line regardless of features/architecture
  2. Pre-computed features (Spange, DRFP, ACS PCA) are already optimal
  3. Morgan fingerprints add noise, not signal
  4. GNNs trained from scratch fail on this small dataset

## MANDATORY NEXT EXPERIMENT: Implement Ens-Model Kernel Exactly

The ens-model kernel (matthewmaree/ens-model) is a top-voted kernel that uses:
1. **CatBoost + XGBoost ensemble** with optimized weights
2. **Feature correlation filtering** (removes redundant features)
3. **Per-target hyperparameters** (different params for single vs full data)
4. **Clipping and renormalization** (ensures yields sum to ~1)

**CRITICAL IMPLEMENTATION DETAILS:**
```python
# From ens-model kernel
class EnsembleModel(BaseModel):
    def __init__(self, data='single', verbose=False):
        self.data_mode = data
        # Optimised fixed weights per dataset
        if data == "single":
            self.weights = [0.6, 0.4]  # CatBoost, XGBoost
        else:
            self.weights = [0.5, 0.5]
        
        self.catboost = CatBoostModel(data=data)
        self.xgboost = XGBModel(data=data)
```

**Feature Selection:**
- Uses ALL features: spange, acs_pca, drfps, fragprints
- Filters by correlation (removes highly correlated features)
- Priority: spange > acs > drfps > fragprints

**Post-processing:**
- Clips predictions to [0, 1]
- Renormalizes so yields sum to ~1

## What NOT to Try
- ❌ More MLP variants (94 experiments, all on same line)
- ❌ More GNN variants trained from scratch (5 experiments, all worse than baseline)
- ❌ Morgan fingerprints (277% worse)
- ❌ Any approach that just improves CV without changing the CV-LB relationship

## Validation Notes
- **Current CV scheme**: Leave-One-Out (24 folds single, 13 folds full)
- **Alternative to test**: GroupKFold (5 folds each) - may have different intercept
- **Key insight**: The CV-LB relationship is the constraint, not the CV score itself

## Submission Strategy (4 remaining)
1. Wait for pending submissions to get LB feedback
2. If any pending submission breaks the CV-LB line → optimize that approach
3. If all pending submissions follow the same line → try fundamentally different approaches
4. Reserve 1-2 submissions for final ensemble

## CRITICAL REMINDER
The target IS reachable. The benchmark paper achieved MSE 0.0039. We need to find what they did differently. The key is NOT improving CV - it's changing the CV-LB relationship.

**IMMEDIATE ACTION:** Implement the ens-model kernel's approach EXACTLY as shown in the kernel, with proper feature correlation filtering and post-processing. This is a proven approach that achieved good LB scores.