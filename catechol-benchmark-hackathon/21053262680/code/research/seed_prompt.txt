## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 (exp_030 - GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 153%
- Experiments: 97 | Submissions: 22/5 used, 4 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.315 × CV + 0.0525 (R² = 0.95)
- Intercept (0.0525) > Target (0.0347)
- Required CV for target: -0.0041 (IMPOSSIBLE with current approaches)
- **ALL 97 experiments fall on the SAME CV-LB line**
- This is a STRUCTURAL DISTRIBUTION SHIFT problem, not model quality

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - exp_096 (Multi-Order GAT) was correctly implemented but achieved CV=0.0447, which is 452% worse than baseline.

**Evaluator's top priority**: Use PRE-TRAINED molecular embeddings instead of training GNNs from scratch.

**I AGREE with the evaluator's assessment:**
1. GNNs trained from scratch on small data (~600 samples) don't work
2. The benchmark paper likely used pre-trained embeddings
3. We need approaches that CHANGE the CV-LB relationship, not just improve CV
4. With only 4 submissions remaining, we must be strategic

**Key insight from exp_096**: The Multi-Order GAT with proper PyTorch Geometric implementation achieved CV=0.0447 - this is 452% WORSE than simple tabular models. This definitively proves that training GNNs from scratch on this small dataset doesn't work.

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop97_analysis.ipynb` - CV-LB relationship analysis

Key patterns:
1. **24 unique solvents** in single solvent data (656 samples)
2. **13 unique solvent pairs** in full data (1227 samples)
3. **Leave-One-Solvent-Out** validation means we predict for UNSEEN solvents
4. **The benchmark paper (arXiv:2512.19530) achieved MSE 0.0039** - this proves the target is reachable

## CRITICAL INSIGHT FROM PUBLIC KERNELS

The "mixall" kernel (9 votes) uses a DIFFERENT validation scheme:
- **GroupKFold (5 splits)** instead of Leave-One-Out (24 folds)
- Ensemble: MLP + XGBoost + RandomForest + LightGBM
- Claims "good CV/LB" with only 2m 15s runtime
- This might give a DIFFERENT CV-LB relationship!

## MANDATORY NEXT EXPERIMENT: Frozen ChemBERTa Embeddings + Ensemble

Based on web research, use frozen ChemBERTa embeddings as features:

```python
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np

class ChemBERTaFeaturizer:
    """Extract frozen ChemBERTa embeddings for SMILES strings."""
    
    def __init__(self, model_name="seyonec/ChemBERTa-zinc-base-v1"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()  # Freeze weights
        
    def featurize(self, smiles_list):
        """Extract embeddings for a list of SMILES strings."""
        embeddings = []
        with torch.no_grad():
            for smiles in smiles_list:
                inputs = self.tokenizer(smiles, return_tensors="pt", padding=True, truncation=True)
                outputs = self.model(**inputs)
                # Mean pool over token dimension
                embed = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
                embeddings.append(embed)
        return np.array(embeddings)

# Usage:
# 1. Pre-compute embeddings for all solvents
# 2. Use embeddings as features for MLP/LGBM/XGB ensemble
# 3. This leverages pre-trained knowledge without training from scratch
```

**Why this might work:**
- ChemBERTa was pre-trained on 10M+ SMILES strings
- Frozen embeddings capture molecular structure without overfitting
- Combines pre-trained knowledge with our best ensemble approach

### Implementation Steps:
1. Install transformers: `pip install transformers`
2. Load ChemBERTa model: `seyonec/ChemBERTa-zinc-base-v1`
3. Extract embeddings for all 24 solvents (768-dim vectors)
4. Create lookup table: solvent_name -> embedding
5. Use embeddings as features (instead of or in addition to Spange/DRFP)
6. Train MLP + LGBM + XGB ensemble
7. Verify submission cells use correct model class

### CRITICAL VERIFICATION:
- **BEFORE running CV**: Note the exact model class name
- **AFTER CV**: Verify submission cells use the SAME model class
- **If mismatch**: FIX submission cells before logging experiment

## ALTERNATIVE: Domain Constraint Enforcement (Mass Balance)

If ChemBERTa doesn't work, try enforcing mass balance:

```python
def enforce_mass_balance(predictions):
    """Post-process predictions to satisfy mass balance."""
    # predictions shape: [N, 3] for Product2, Product3, SM
    
    # Clip to [0, 1]
    predictions = np.clip(predictions, 0, 1)
    
    # Ensure sum doesn't exceed 1
    row_sums = predictions.sum(axis=1, keepdims=True)
    mask = row_sums > 1
    predictions[mask.squeeze()] = predictions[mask.squeeze()] / row_sums[mask]
    
    return predictions
```

## What NOT to Try

1. ❌ **More GNNs trained from scratch** - exp_096 proved this doesn't work (CV 452% worse)
2. ❌ **More tabular model variants** - All 97 experiments fall on same CV-LB line
3. ❌ **Hyperparameter tuning** - Won't change the intercept
4. ❌ **Multi-seed ensembles** - Gap is 153%, not 1-2%
5. ❌ **Submitting experiments with CV > 0.0081** - Only submit if better

## Validation Notes

1. **CV Scheme**: Leave-One-Solvent-Out for single solvent, Leave-One-Ramp-Out for full data
2. **Model Class Consistency**: ALWAYS verify submission cells use the EXACT model class from CV
3. **Only 4 submissions remaining** - use wisely
4. **Only submit if**:
   - CV is better than 0.0081 (baseline)
   - OR approach shows promise for changing CV-LB relationship

## REMEMBER

The benchmark paper achieved MSE 0.0039 (vs our best 0.0877) - a **25x improvement**. The target IS reachable. The key is:
1. **Pre-trained embeddings** - not training from scratch
2. **Distribution shift handling** - not just improving CV
3. **Domain constraints** - physics-based rules that generalize

The CV-LB intercept (0.0525) is the fundamental blocker. We need approaches that REDUCE this intercept, not just improve CV.