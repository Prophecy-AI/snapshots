## Current Status
- Best CV score: 0.0081 from exp_049/050/053
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 152.7%
- Experiments: 100 | Submissions: 22/5 used, 4 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 × CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES** (all 12 valid submissions)
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY**. The conservative blending experiment was correctly implemented.
- Evaluator's top priority: **Replicate public kernels exactly** to check if they have different CV-LB relationship. **AGREE** - this is the most important next step.
- Key concerns raised:
  1. Conservative blending HURTS CV - hypothesis invalidated. **Acknowledged** - we will NOT try more blending variants.
  2. CV-LB intercept problem is structural. **Acknowledged** - we need approaches that CHANGE the relationship.
  3. Public kernels may have different CV-LB relationships. **AGREE** - this is our best remaining hypothesis.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop100_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line
  2. GNN attempts failed (CV 0.018-0.068) - likely due to small dataset
  3. ChemBERTa attempts failed
  4. Conservative blending HURTS CV (not helps)
  5. Similarity weighting was a disaster (exp_073: CV 0.0084 → LB 0.1451)

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Exact Replication of ens-model Kernel
The ens-model kernel uses:
- CatBoost + XGBoost ensemble
- ALL feature sources (spange, acs_pca, drfps, fragprints, smiles)
- Correlation-based filtering with priority (spange > acs > drfps > frag > smiles)
- Different weights for single vs full data (7:6 vs 1:2)
- Leave-One-Out validation (same as us)

**CRITICAL**: Copy the EXACT code from `/home/code/research/kernels/matthewmaree_ens-model/ens-model.ipynb`:
1. Copy the `build_solvent_feature_table()` function with correlation filtering
2. Copy the `filter_correlated_features()` function with priority
3. Copy the `CatBoostModel` and `XGBModel` classes
4. Copy the `EnsembleModel` class with weights (7:6 for single, 1:2 for full)
5. Compute local CV score
6. Submit to get LB score
7. Check if it falls on the same CV-LB line

**WHY**: This kernel achieved good LB scores. If it has a DIFFERENT CV-LB relationship, we need to understand what makes it different.

### PRIORITY 2: Domain Constraints (Mass Balance)
The chemical reaction produces three outputs: Product 2, Product 3, and Starting Material (SM).
Mass balance constraint: P2 + P3 + SM ≈ 1 (conservation of mass)

**Implementation**:
```python
def enforce_mass_balance(predictions):
    """Enforce mass balance: P2 + P3 + SM = 1"""
    predictions = np.clip(predictions, 0, 1)  # Physical bounds
    row_sums = predictions.sum(axis=1, keepdims=True)
    predictions = predictions / np.maximum(row_sums, 1e-6)  # Normalize to sum to 1
    return predictions
```

**WHY**: This is a physical constraint that MUST hold for ALL solvents, including unseen ones. It may help reduce the intercept by enforcing domain knowledge.

### PRIORITY 3: Kinetic Constraints (Arrhenius)
The reaction follows Arrhenius kinetics: k = A × exp(-Ea/RT)
This means:
- Higher temperature → faster reaction → more products
- Longer residence time → more conversion

**Implementation**:
```python
def add_kinetic_features(X):
    """Add Arrhenius-inspired features"""
    T_kelvin = X['Temperature'] + 273.15
    t = X['Residence Time']
    
    # Arrhenius features
    inv_T = 1000.0 / T_kelvin  # 1/T scaled
    log_t = np.log(t + 1e-6)   # ln(time)
    interaction = inv_T * log_t  # Arrhenius-like interaction
    
    return np.column_stack([inv_T, log_t, interaction])
```

**WHY**: These features capture the underlying physics and should generalize to unseen solvents.

## What NOT to Try
- ❌ More conservative blending variants (hypothesis invalidated - HURTS CV)
- ❌ More similarity weighting approaches (exp_073 disaster)
- ❌ More GNN variants (all failed on small dataset)
- ❌ More ChemBERTa variants (all failed)
- ❌ More MLP/LGBM/XGB variants without changing the CV-LB relationship
- ❌ Multi-seed ensembles (we're 152% away from target - optimization is FORBIDDEN)

## Validation Notes
- Use Leave-One-Out validation for single solvent data (24 folds)
- Use Leave-One-Ramp-Out validation for full data (13 folds)
- Combined CV = (single_cv + full_cv) / 2
- **CRITICAL**: After getting LB score, check if it falls on the same CV-LB line
- If new approach falls on DIFFERENT line (lower intercept), that's the breakthrough

## Submission Strategy
- Only 4 submissions remaining
- ONLY submit if:
  1. The approach is fundamentally different (e.g., exact ens-model replication)
  2. We need to verify if it has a different CV-LB relationship
- DO NOT submit experiments that are just variations of existing approaches

## The Path Forward
The target IS reachable. The benchmark paper achieved MSE 0.0039. We need to find what they did differently.

Possible explanations for the gap:
1. The benchmark used a different evaluation methodology
2. The benchmark used domain-specific constraints we haven't implemented
3. The public kernels have a different CV-LB relationship we haven't discovered
4. There's a technique we haven't tried that changes the intercept

**NEXT EXPERIMENT**: Exact replication of ens-model kernel. This is our best remaining hypothesis.