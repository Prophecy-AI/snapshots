## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost Ensemble)
- Best LB score: 0.0877 from exp_030 (CV=0.0083)
- Target: 0.0347 | Gap to target: 0.053 (152.8%)
- Remaining submissions: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.315 × CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES - ALL 12 VALID SUBMISSIONS**
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.315 = -0.0041 (IMPOSSIBLE)

**The target is MATHEMATICALLY UNREACHABLE with approaches that follow this line.**
**We MUST find approaches that CHANGE THE CV-LB RELATIONSHIP, not just improve CV.**

## Response to Evaluator
- Technical verdict was CONCERNS - CV degraded due to featurization change
- Evaluator's top priority: Fix featurization to match exp_108, verify CV, then submit
- Key concerns raised:
  1. exp_110 used CONCATENATION for mixtures instead of WEIGHTED AVERAGE
  2. This degraded CV from 0.0092 (exp_108) to 0.012912 (exp_110)
  3. The chemical similarity hypothesis remains UNTESTED on LB

**My response:**
- I AGREE with the evaluator's diagnosis
- The featurization mismatch is the root cause of CV degradation
- We need to fix this and submit to test the hypothesis
- However, even with CV=0.0092, expected LB = 4.315 × 0.0092 + 0.0525 = 0.092
- This is still far from target (0.0347)
- We need MORE AGGRESSIVE conservative blending to reduce the intercept

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop112_analysis.ipynb`
- Key patterns:
  1. ALL 12 valid submissions fall on the SAME CV-LB line (R²=0.95)
  2. The intercept (0.0525) represents structural extrapolation error
  3. Test solvents are fundamentally different from training solvents
  4. Benchmark paper achieved MSE=0.0039 using GNN - 22x better than our best

## IMMEDIATE PRIORITY: Fix SimilarityAwareModel and Submit

### Step 1: Create exp_111 with CORRECT featurization

The key fix is in PrecomputedFeaturizerMixed - use WEIGHTED AVERAGE, not concatenation:

```python
class PrecomputedFeaturizerMixed(SmilesFeaturizer):
    def __init__(self):
        self.featurizer = build_solvent_feature_table()
        dummy_num = pd.DataFrame([[0] * len(INPUT_LABELS_NUMERIC)], columns=INPUT_LABELS_NUMERIC)
        numeric_dim = add_numeric_features(dummy_num).shape[1]
        self.feats_dim = numeric_dim + self.featurizer.shape[1]  # SAME as single (not doubled!)

    def featurize(self, X):
        X_numeric = add_numeric_features(X[INPUT_LABELS_NUMERIC].copy())
        A = self.featurizer.loc[X["SOLVENT A NAME"]].values
        B = self.featurizer.loc[X["SOLVENT B NAME"]].values
        frac_B = X["SolventB%"].values.reshape(-1, 1)
        frac_A = 1 - frac_B
        mixed = A * frac_A + B * frac_B  # WEIGHTED AVERAGE - physically meaningful!
        X_out = np.concatenate([X_numeric.values, mixed], axis=1)
        return torch.tensor(X_out, dtype=torch.double)
```

### Step 2: Use MORE AGGRESSIVE conservative blending

The current best config (st=0.3, bw=0.2) is too conservative. Try:
- similarity_threshold=0.5, blend_weight=0.4
- similarity_threshold=0.6, blend_weight=0.5

The goal is to REDUCE THE INTERCEPT, not improve CV. Higher blend weights will:
- Hurt CV (predictions blend more toward training mean)
- But potentially HELP LB (reduce extrapolation error on unseen solvents)

### Step 3: Verify CV and Submit

1. Verify CV matches exp_108 (~0.0092) with st=0.3, bw=0.2
2. Then try more aggressive configs
3. Submit the most aggressive config that doesn't completely destroy CV
4. Analyze if the CV-LB relationship changes

## Recommended Approaches (PRIORITY ORDER)

### 1. IMMEDIATE: Fix SimilarityAwareModel (exp_111)
- Use weighted average featurization
- Try aggressive blending (st=0.5, bw=0.4)
- Submit to test hypothesis

### 2. IF THAT DOESN'T WORK: Even More Aggressive Blending
- Try st=0.6, bw=0.5 or even st=0.7, bw=0.6
- The key insight: We need to SACRIFICE CV to reduce the intercept
- A model with CV=0.015 but intercept=0.03 would beat target!

### 3. IF STILL STUCK: Transductive Learning
- Use test data structure (without labels) to adapt predictions
- Recent research shows 1.5-1.8x improvement on OOD prediction

### 4. LAST RESORT: Ensemble of Different Approaches
- Combine models that might have different CV-LB relationships
- Even if individual models are on the same line, ensemble might not be

## What NOT to Try
- ❌ NO more MLP/LGBM/XGB/CatBoost variants (all on same line)
- ❌ NO hyperparameter tuning (just moves along the line)
- ❌ NO ensemble weight optimization (same problem)
- ❌ NO multi-seed ensembles (we're 152% from target)

## Validation Notes
- Use Leave-One-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for full data (13 folds)
- Track BOTH CV score AND expected LB from the line
- If new approach gives DIFFERENT CV-LB relationship, that's progress!

## CRITICAL CHECKS BEFORE LOGGING ANY EXPERIMENT
1. What model class did you use for CV computation?
2. Open the submission cells (last 3 cells)
3. Verify BOTH `model = MyModel(data='single')` AND `model = MyModel(data='full')` match
4. If they don't match, FIX THEM before running submission cells
5. Verify submission format: columns = ['id', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']
6. Verify featurization: mixtures use WEIGHTED AVERAGE, not concatenation

## SUBMISSION STRATEGY (4 remaining)
1. exp_111: SimilarityAwareModel with fixed featurization + aggressive blending
2. If different CV-LB relationship: iterate on blending parameters
3. If same relationship: try transductive learning
4. Final submission: best approach based on learnings

## KEY INSIGHT
The intercept (0.0525) represents the STRUCTURAL extrapolation error that exists even with perfect CV.
To beat the target (0.0347), we need to REDUCE THIS INTERCEPT by 34%.
This requires approaches that are MORE CONSERVATIVE on unseen solvents.
Sacrificing CV to reduce extrapolation error is the RIGHT strategy.
