## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 (exp_030)
- Target: 0.0347 | Gap to target: 0.053 (153%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 × CV + 0.0528 (R² = 0.9523)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- **Are all approaches on the same line? YES**
- **CRITICAL: Intercept (0.0528) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (IMPOSSIBLE)

**This means: The target is MATHEMATICALLY UNREACHABLE by improving CV alone.**
**We MUST change the CV-LB relationship (reduce the intercept).**

## Response to Evaluator

**Technical verdict was CONCERNS** - The similarity weighting notebook had an extra cell between submission cells and used wrong alpha value. The evaluator correctly identified these issues.

**Evaluator's top priority**: Fix notebook structure OR submit exp_030 directly. **I AGREE** - we should not submit the similarity_weighting notebook as-is.

**Key concerns raised**:
1. Notebook structure invalid for submission - **ADDRESSED by not submitting it**
2. Similarity weighting with alpha=0 is best (no weighting) - **CONFIRMED - this rules out the hypothesis that extreme predictions cause the gap**
3. The CV-LB intercept problem remains unsolved - **THIS IS THE CORE PROBLEM**

**Evaluator's insight about GroupKFold**: The lishellliang kernel uses 5-fold GroupKFold instead of Leave-One-Out. This might have a different CV-LB relationship. **I STRONGLY AGREE - this is the most promising unexplored direction.**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop74_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL 12 successful submissions fall on the SAME CV-LB line (R²=0.95)
  2. The intercept (0.0528) represents STRUCTURAL extrapolation error
  3. Similarity weighting doesn't help - the model's predictions are already reasonable
  4. GNN and ChemBERTa performed WORSE than tabular models (CV ~0.025 vs 0.008)

## Recommended Approaches

### PRIORITY 1: Implement GroupKFold Validation (HIGHEST PRIORITY)
**Why**: The lishellliang kernel uses GroupKFold(5) instead of Leave-One-Out and claims "good CV-LB". This is a fundamentally different validation scheme that might have a different CV-LB relationship.

**Key differences**:
- Leave-One-Out: 24 folds, each with 1 solvent held out, trains on 96% of data
- GroupKFold(5): 5 folds, each with ~5 solvents held out, trains on 80% of data

**Hypothesis**: GroupKFold might give a lower intercept because:
1. It trains on less data per fold, which might reduce overfitting
2. It tests on more solvents per fold, which might be more representative
3. The CV-LB relationship might be fundamentally different

**Implementation**:
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    """Override to use GroupKFold(5) instead of Leave-One-Out."""
    groups = X["SOLVENT NAME"]
    n_splits = min(5, len(groups.unique()))
    gkf = GroupKFold(n_splits=n_splits)
    
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (
            (X.iloc[train_idx], Y.iloc[train_idx]),
            (X.iloc[test_idx], Y.iloc[test_idx]),
        )
```

**IMPORTANT**: Use the SAME model (GP+MLP+LGBM ensemble) but with GroupKFold validation. This isolates the effect of the validation scheme.

### PRIORITY 2: Debug matthewmaree Kernel Replication
**Why**: Our replication (exp_067) achieved CV=0.02121, which is MUCH WORSE than expected. The original kernel uses CatBoost + XGBoost ensemble with sophisticated feature engineering.

**What to check**:
1. Are we using the same feature engineering (correlation filtering, priority-based selection)?
2. Are we using the same hyperparameters?
3. Is there a bug in our replication?

**If we can get the matthewmaree kernel working correctly**, it might have a different CV-LB relationship than GP+MLP+LGBM.

### PRIORITY 3: Hybrid Spange + Small GNN Embeddings
**Why**: GNN alone performed poorly (CV=0.0256), but might capture complementary information to Spange features.

**Implementation**:
```python
class HybridModel:
    def __init__(self):
        self.spange_features = load_features('spange_descriptors')  # 13 features
        self.gnn = SimpleGNN(output_dim=4)  # Small 4-dim learned embedding
        self.mlp = MLP(input_dim=13+4+5)  # Spange + GNN + Arrhenius
```

**Key**: Use a SMALL GNN embedding (4-8 dims) to avoid overfitting, and combine with proven Spange features.

## What NOT to Try
- ❌ More MLP/LGBM/GP variants (all fall on same CV-LB line)
- ❌ Similarity-based prediction weighting (confirmed doesn't help)
- ❌ Extrapolation detection features (already tried, made CV worse)
- ❌ Label rescaling (already tried, made CV worse)
- ❌ Pure GNN or ChemBERTa (performed much worse than tabular)

## Validation Notes
- **Current scheme**: Leave-One-Out by solvent (24 folds)
- **Proposed scheme**: GroupKFold(5) by solvent (5 folds)
- **CV-LB gap**: The intercept (0.0528) is the key problem, not the slope
- **To beat target**: We need to REDUCE THE INTERCEPT, not just improve CV

## Key Insight from Public Kernels

**lishellliang kernel** (`mixall-runtime-is-only-2m-15s-but-good-cv-lb`):
- Uses GroupKFold(5) instead of Leave-One-Out
- Uses MLP + XGBoost + RandomForest + LightGBM ensemble
- Claims "good CV-LB" relationship
- Runtime is only 2m 15s

**matthewmaree kernel** (`ens-model`):
- Uses CatBoost + XGBoost ensemble
- Sophisticated feature engineering with correlation filtering
- Our replication achieved CV=0.02121 (much worse than expected)

## Experiment Plan

1. **exp_074**: GroupKFold validation with GP+MLP+LGBM ensemble
   - Use SAME model as exp_030 but with GroupKFold(5) validation
   - Compare CV-LB relationship to Leave-One-Out
   - If intercept is lower, this is the path forward

2. **exp_075**: Debug matthewmaree kernel replication
   - Compare our code to original kernel
   - Identify and fix discrepancies
   - Test if CatBoost+XGBoost has different CV-LB relationship

3. **exp_076**: Hybrid Spange + small GNN
   - Combine proven Spange features with 4-dim GNN embedding
   - Test if learned embeddings add complementary information

## CRITICAL REMINDERS

1. **DO NOT** conclude the target is unreachable - the benchmark achieved MSE 0.0039
2. **DO NOT** continue optimizing within the current CV-LB line
3. **DO** try approaches that CHANGE the CV-LB relationship
4. **DO** verify submission cell model class matches CV model class
5. **DO** ensure notebook has exactly 3 submission cells at the end (no extra cells)
