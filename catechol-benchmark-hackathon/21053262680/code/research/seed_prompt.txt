## Current Status
- Best CV score: 0.0081 (exp_049, exp_050, exp_053 - CatBoost/XGBoost variants)
- Best LB score: 0.0877 (exp_030 - GP+MLP+LGBM)
- Target: 0.0347 | Gap to target: 0.0530 (152.8%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 × CV + 0.0525 (R² = 0.9505)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- **Are all approaches on the same line? YES**
- **This is a DISTRIBUTION SHIFT problem, not a modeling problem**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The GroupKFold experiment was well-executed.
- Evaluator's top priority: Submit the replicated matthewmaree kernel (067_exact_ens_model_copy) to verify CatBoost/XGBoost works. **AGREE - this is important to debug submission failures.**
- Key concerns raised: 
  1. CV-LB intercept (0.0525) > target (0.0347) - target is mathematically unreachable with current approaches
  2. 8 CatBoost/XGBoost submissions failed with errors
  3. Recent experiments are ruling out causes, not finding solutions
- How I'm addressing: **MANDATORY PIVOT to fundamentally different approaches**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop72_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 72 experiments (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line
  2. The intercept (0.0525) represents structural extrapolation error to unseen solvents
  3. The benchmark paper achieved MSE ~0.0039 using transfer learning and active learning
  4. Our best CV (0.0081) is 2x worse than the benchmark's best score

## Recommended Approaches (PRIORITY ORDER)

### IMMEDIATE: Submit exp_030 or 067_exact_ens_model_copy
Before trying new approaches, we should use one of our 5 remaining submissions to:
1. Verify the current best model (exp_030) still works
2. Or test if the matthewmaree kernel replication (067) can submit successfully

### PRIORITY 1: Proper GNN Implementation
The GNN experiment (exp_040) exists but may not have been submitted. Key requirements:
- Use PyTorch Geometric with AttentiveFP or GCNConv
- Process SMILES → molecular graph for each solvent
- Combine GNN embeddings with kinetics features
- **CRITICAL**: Verify submission cells use the SAME model class as CV
- The notebook has an extra cell after the final submission cell - this may cause issues

### PRIORITY 2: Transfer Learning from Related Reactions
Research shows this is the most effective approach for few-shot yield prediction:
- Pre-train on mechanistically related reactions (Diels-Alder, Cope, Claisen)
- Small, bespoke datasets work better than large diverse datasets
- Fine-tune on catechol data
- Use NERF algorithm or similar graph-based approach

### PRIORITY 3: Molecular Transformer (ChemBERTa)
- Use pre-trained ChemBERTa embeddings from SMILES
- These embeddings capture chemical knowledge from large corpora
- May generalize better to unseen solvents
- The exp_041 notebook exists but may have issues

### PRIORITY 4: Uncertainty-Weighted Predictions
If GNN/ChemBERTa still fall on the same CV-LB line:
- Use GP variance or ensemble disagreement to detect extrapolation
- Blend toward conservative predictions when uncertainty is high
- This could reduce the intercept without improving CV

## What NOT to Try
- ❌ More MLP/LGBM/XGBoost/CatBoost variants - all fall on the same CV-LB line
- ❌ More feature engineering on tabular features - doesn't change the intercept
- ❌ Label rescaling or conservative predictions - made CV worse (exp_068)
- ❌ GroupKFold validation - gives worse CV than LOO (exp_069)
- ❌ Multi-seed ensembles or hyperparameter sweeps - we're 152% from target

## Validation Notes
- Use Leave-One-Out by solvent (best validation scheme)
- Track BOTH single-solvent MSE and full-data MSE separately
- After submission, check if the new approach changes the CV-LB relationship
- If it falls on the same line, the approach didn't help

## Key Insight from Research
The benchmark paper (arxiv.org/abs/2506.07619) achieved MSE ~0.0039 using:
1. Transfer learning from related reactions
2. Active learning for efficient data selection
3. Proper molecular representations (not just tabular features)

Our best CV (0.0081) is 2x worse than this benchmark. The gap is NOT in model tuning - it's in the REPRESENTATION of solvents and the TRANSFER of knowledge from related chemistry.

## Submission Strategy
With 5 submissions remaining:
1. **Next submission**: exp_030 (best LB) or 067_exact_ens_model_copy (to debug CatBoost)
2. **After that**: Submit the best GNN or ChemBERTa experiment
3. **Reserve 2-3 submissions**: For final approaches that break the CV-LB line

## MANDATORY PIVOT ENFORCED
The last 5+ experiments have not improved best CV by >0.0001.
You MUST try a fundamentally different approach:
- GNN on molecular graphs
- Molecular transformers (ChemBERTa)
- Transfer learning from related reactions

DO NOT continue with tabular model variants.