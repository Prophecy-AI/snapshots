## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 (exp_030 - GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 153%
- Experiments: 97 | Submissions: 22/5 used, 4 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.315 × CV + 0.0525 (R² = 0.95)
- Intercept (0.0525) > Target (0.0347)
- Required CV for target: -0.0041 (IMPOSSIBLE with current approaches)
- **ALL 97 experiments fall on the SAME CV-LB line**
- This is a STRUCTURAL DISTRIBUTION SHIFT problem, not model quality

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - exp_096 (Multi-Order GAT) was correctly implemented but achieved CV=0.0447, which is 452% worse than baseline.

**Evaluator's top priority**: Use PRE-TRAINED molecular embeddings instead of training GNNs from scratch.

**I AGREE with the evaluator's assessment:**
1. GNNs trained from scratch on small data (~600 samples) don't work
2. The benchmark paper likely used pre-trained embeddings
3. We need approaches that CHANGE the CV-LB relationship, not just improve CV
4. With only 4 submissions remaining, we must be strategic

**Key insight from exp_096**: The Multi-Order GAT with proper PyTorch Geometric implementation achieved CV=0.0447 - this is 452% WORSE than simple tabular models. This definitively proves that training GNNs from scratch on this small dataset doesn't work.

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop97_analysis.ipynb` - CV-LB relationship analysis

Key patterns:
1. **24 unique solvents** in single solvent data (656 samples)
2. **13 unique solvent pairs** in full data (1227 samples)
3. **Leave-One-Solvent-Out** validation means we predict for UNSEEN solvents
4. **The benchmark paper (arXiv:2512.19530) achieved MSE 0.0039** - this proves the target is reachable

## CRITICAL INSIGHT FROM PUBLIC KERNELS

The "mixall" kernel (9 votes) uses a DIFFERENT validation scheme:
- **GroupKFold (5 splits)** instead of Leave-One-Out (24 folds)
- Ensemble: MLP + XGBoost + RandomForest + LightGBM
- Claims "good CV/LB" with only 2m 15s runtime
- This might give a DIFFERENT CV-LB relationship!

## MANDATORY NEXT EXPERIMENTS (Priority Order)

### PRIORITY 1: Frozen ChemBERTa Embeddings + Ensemble

Based on web research, use frozen ChemBERTa embeddings as features:

```python
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np

class ChemBERTaFeaturizer:
    """Extract frozen ChemBERTa embeddings for SMILES strings."""
    
    def __init__(self, model_name="seyonec/ChemBERTa-zinc-base-v1"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()  # Freeze weights
        
    def featurize(self, smiles_list):
        """Extract embeddings for a list of SMILES strings."""
        embeddings = []
        with torch.no_grad():
            for smiles in smiles_list:
                inputs = self.tokenizer(smiles, return_tensors="pt", padding=True, truncation=True)
                outputs = self.model(**inputs)
                # Mean pool over token dimension
                embed = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
                embeddings.append(embed)
        return np.array(embeddings)

# Usage:
# 1. Pre-compute embeddings for all solvents
# 2. Use embeddings as features for MLP/LGBM/XGB ensemble
# 3. This leverages pre-trained knowledge without training from scratch
```

**Why this might work:**
- ChemBERTa was pre-trained on 10M+ SMILES strings
- Frozen embeddings capture molecular structure without overfitting
- Combines pre-trained knowledge with our best ensemble approach

### PRIORITY 2: Domain Constraint Enforcement (Mass Balance)

The yields should sum to approximately 1 (mass balance):
- Product 1 + Product 2 + Product 3 + SM ≈ 1
- We predict Product 2, Product 3, SM
- Product 1 = 1 - Product2 - Product3 - SM

```python
def enforce_mass_balance(predictions, train_mean_sum=None):
    """Post-process predictions to satisfy mass balance."""
    # predictions shape: [N, 3] for Product2, Product3, SM
    
    # Clip to [0, 1]
    predictions = np.clip(predictions, 0, 1)
    
    # Ensure sum doesn't exceed 1
    row_sums = predictions.sum(axis=1, keepdims=True)
    mask = row_sums > 1
    predictions[mask.squeeze()] = predictions[mask.squeeze()] / row_sums[mask]
    
    return predictions
```

### PRIORITY 3: Conservative Extrapolation

Detect when test sample is far from training and blend toward mean:

```python
from sklearn.neighbors import NearestNeighbors

class ConservativePredictor:
    def __init__(self, base_model, blend_strength=0.3, n_neighbors=5):
        self.base_model = base_model
        self.blend_strength = blend_strength
        self.nn = NearestNeighbors(n_neighbors=n_neighbors)
        self.train_mean = None
        self.threshold = None
    
    def fit(self, X, y):
        self.base_model.fit(X, y)
        self.nn.fit(X)
        self.train_mean = y.mean(axis=0)
        # Set threshold as 95th percentile of training distances
        distances, _ = self.nn.kneighbors(X)
        self.threshold = np.percentile(distances.mean(axis=1), 95)
    
    def predict(self, X):
        base_pred = self.base_model.predict(X)
        distances, _ = self.nn.kneighbors(X)
        extrapolation_score = distances.mean(axis=1)
        
        # Blend toward mean when extrapolating
        weight = np.clip(extrapolation_score / self.threshold, 0, self.blend_strength)
        return (1 - weight[:, None]) * base_pred + weight[:, None] * self.train_mean
```

## What NOT to Try

1. ❌ **More GNNs trained from scratch** - exp_096 proved this doesn't work (CV 452% worse)
2. ❌ **More tabular model variants** - All 97 experiments fall on same CV-LB line
3. ❌ **Hyperparameter tuning** - Won't change the intercept
4. ❌ **Multi-seed ensembles** - Gap is 153%, not 1-2%
5. ❌ **Submitting experiments with CV > 0.0081** - Only submit if better

## Validation Notes

1. **CV Scheme**: Leave-One-Solvent-Out for single solvent, Leave-One-Ramp-Out for full data
2. **Model Class Consistency**: ALWAYS verify submission cells use the EXACT model class from CV
3. **Only 4 submissions remaining** - use wisely
4. **Only submit if**:
   - CV is better than 0.0081 (baseline)
   - OR approach shows promise for changing CV-LB relationship

## SUBMISSION STRATEGY (4 remaining)

Given only 4 submissions left, here's the recommended strategy:

1. **Submission 1**: Submit exp_049/050/053 (best CV 0.0081) to get LB feedback
   - These have pending LB scores
   - Will confirm if CV 0.0081 translates to better LB than 0.0877

2. **Submission 2**: ChemBERTa embeddings + ensemble
   - If CV improves, submit to test if it changes CV-LB relationship

3. **Submission 3**: Domain constraints + conservative extrapolation
   - Even if CV is similar, might reduce intercept

4. **Submission 4**: Best performing approach from above

## REMEMBER

The benchmark paper achieved MSE 0.0039 (vs our best 0.0877) - a **25x improvement**. The target IS reachable. The key is:
1. **Pre-trained embeddings** - not training from scratch
2. **Distribution shift handling** - not just improving CV
3. **Domain constraints** - physics-based rules that generalize

The CV-LB intercept (0.0525) is the fundamental blocker. We need approaches that REDUCE this intercept, not just improve CV.