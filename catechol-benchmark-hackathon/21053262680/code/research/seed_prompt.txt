## Current Status
- Best CV score: 0.0081 from exp_049/050/051/053 (CatBoost + XGBoost Ensemble)
- Best LB score: 0.0877 from exp_030 (CV=0.0083)
- Target: 0.0347 | Gap to target: 0.053 (152.8%)
- Remaining submissions: 4

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 × CV + 0.0528 (R² = 0.952)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- Are all approaches on the same line? **YES**
- **CRITICAL**: Intercept (0.0528) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (IMPOSSIBLE)

**The target is MATHEMATICALLY UNREACHABLE with approaches that follow this line.**
**We MUST find approaches that CHANGE THE CV-LB RELATIONSHIP, not just improve CV.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY CV, BUT MODEL MISMATCH
- Evaluator's top priority: Submit SimilarityAwareModel with correct format to test hypothesis
- Key concerns raised: 
  1. Current submission uses EnsembleModel (CV=0.011988), NOT SimilarityAwareModel (CV=0.0092)
  2. The chemical similarity hypothesis remains UNTESTED on LB
  3. GNN/ChemBERTa experiments had model class mismatch - results INVALID

**My response:**
- I AGREE with the evaluator that we need to test fundamentally different approaches
- However, the SimilarityAwareModel (CV=0.0092) is WORSE than our best CV (0.0081)
- Even if it changes the CV-LB relationship, it's unlikely to beat target
- We need approaches that BOTH improve CV AND change the intercept

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop111_analysis.ipynb`
- Key patterns:
  1. ALL 12 valid submissions fall on the SAME CV-LB line (R²=0.95)
  2. The intercept (0.0528) represents structural extrapolation error
  3. Test solvents are fundamentally different from training solvents
  4. Benchmark paper achieved MSE=0.0039 using GNN - 22x better than our best

## Recommended Approaches (PRIORITY ORDER)

### 1. TRANSDUCTIVE LEARNING (Highest Priority)
Based on web research, transductive approaches can improve OOD prediction by 1.5-1.8x.
The idea: Use test data (without labels) to adapt the model.

**Implementation:**
```python
# During inference, jointly embed test and train data
# Fine-tune on the combined representation
# This helps the model adapt to the test distribution
```

**Why this might work:**
- The benchmark paper likely used some form of transductive learning
- It directly addresses the distribution shift problem
- Can potentially reduce the intercept, not just improve CV

### 2. PSEUDO-LABELING WITH CONFIDENCE FILTERING
Use confident predictions on test data to augment training.

**Implementation:**
```python
# 1. Train initial model
# 2. Predict on test data
# 3. Select high-confidence predictions (low variance across ensemble)
# 4. Add pseudo-labels to training
# 5. Retrain
```

**Why this might work:**
- Adapts the model to test distribution
- Can reduce extrapolation error for similar test samples

### 3. PROPER GNN WITH CORRECT SUBMISSION FORMAT
Previous GNN attempts had model class mismatch. Fix this.

**Implementation:**
- Use PyTorch Geometric with GCNConv or GATConv
- VERIFY: submission cells use the EXACT SAME model class as CV
- Test if GNN falls on a DIFFERENT CV-LB line

**Why this might work:**
- Benchmark achieved 0.0039 with GNN
- GNN captures molecular structure directly
- May generalize better to unseen solvents

### 4. AGGRESSIVE CONSERVATIVE BLENDING
The SimilarityAwareModel used conservative parameters (st=0.3, bw=0.2).
Try more aggressive blending toward training mean.

**Implementation:**
```python
# For test samples with low similarity to training:
# Blend heavily toward training mean (bw=0.5 or higher)
# This may hurt CV but help LB
```

**Why this might work:**
- The CV-LB gap suggests we're overconfident on test data
- More conservative predictions may reduce extrapolation error

### 5. DOMAIN-SPECIFIC CONSTRAINTS
Use chemistry knowledge to constrain predictions.

**Implementation:**
- Yields must sum to ≤ 1 (already done with clipping)
- Arrhenius temperature dependence
- Mass-action kinetics constraints

## What NOT to Try
- ❌ NO more MLP/LGBM/XGB/CatBoost variants (all on same line)
- ❌ NO hyperparameter tuning (just moves along the line)
- ❌ NO ensemble weight optimization (same problem)
- ❌ NO multi-seed ensembles (we're 152% from target)

## Validation Notes
- Use Leave-One-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for full data (13 folds)
- Track BOTH CV score AND expected LB from the line
- If new approach gives DIFFERENT CV-LB relationship, that's progress!

## CRITICAL CHECKS BEFORE LOGGING ANY EXPERIMENT
1. What model class did you use for CV computation?
2. Open the submission cells (last 3 cells)
3. Verify BOTH `model = MyModel(data='single')` AND `model = MyModel(data='full')` match
4. If they don't match, FIX THEM before running submission cells
5. Verify submission format: columns = ['id', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']

## IMMEDIATE PRIORITY
Given 4 remaining submissions, we should:
1. **First**: Implement transductive learning approach
2. **Second**: If transductive doesn't work, try proper GNN with correct format
3. **Third**: Try aggressive conservative blending
4. **Fourth**: Submit best performing approach

The goal is to find an approach that CHANGES the CV-LB relationship, not just improves CV.
