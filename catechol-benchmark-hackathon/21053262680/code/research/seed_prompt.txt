## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.053 (152.8%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 × CV + 0.0528 (R² = 0.9523)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- Are all approaches on the same line? **YES**
- **CRITICAL**: Intercept (0.0528) > Target (0.0347) - target is mathematically unreachable with current approaches
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (NEGATIVE - impossible!)

## Response to Evaluator
- Technical verdict: TRUSTWORTHY CV, BUT SUBMISSION FORMAT WRONG
- Evaluator's top priority: **FIX SUBMISSION FORMAT BEFORE SUBMITTING**
- **AGREE 100%**: The submission format is completely broken. Current format has columns ['id', 'index', 'Product 2', 'Product 3', 'SM'] but template requires ['id', 'index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']. This is why 10+ recent submissions failed.
- Key concerns: Model class mismatch, submission format wrong. **MUST FIX BEFORE ANY SUBMISSION**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop110_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 12 successful submissions fall on the SAME CV-LB line (R²=0.95)
  2. The intercept (0.0528) represents structural extrapolation error
  3. Test solvents are fundamentally different from training solvents
  4. No amount of model tuning changes the intercept - only the slope

## ⚠️ CRITICAL: SUBMISSION FORMAT FIX (MUST DO FIRST)

The submission cells MUST use this EXACT structure (from template):

```python
########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################
import tqdm

X, Y = load_data("single_solvent")
split_generator = generate_leave_one_out_splits(X, Y)
all_predictions = []

for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):
    (train_X, train_Y), (test_X, test_Y) = split

    model = YourModel(data='single')  # CHANGE THIS LINE ONLY
    model.train_model(train_X, train_Y)
    predictions = model.predict(test_X)

    predictions_np = predictions.detach().cpu().numpy()
    for row_idx, row in enumerate(predictions_np):
        all_predictions.append({
            "task": 0,
            "fold": fold_idx,
            "row": row_idx,
            "target_1": row[0],
            "target_2": row[1],
            "target_3": row[2]
        })

submission_single_solvent = pd.DataFrame(all_predictions)
```

**VERIFY FORMAT BEFORE EVERY SUBMISSION:**
```python
sub = pd.read_csv('/home/submission/submission.csv')
expected_cols = ['id', 'index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']
assert list(sub.columns) == expected_cols, f"Wrong columns: {list(sub.columns)}"
print("Format OK!")
```

## Recommended Approaches (PRIORITY ORDER)

### IMMEDIATE: Adapt "ens-model" Kernel Approach (exp_109)
The "ens-model" kernel from Kaggle has:
1. CatBoost + XGBoost ensemble with different weights for single vs full data
2. Combines multiple feature sources (spange, acs_pca, drfps, fragprints) with correlation filtering
3. Clips predictions to [0, 1] and renormalizes
4. Uses CORRECT submission format

**Implementation:**
1. Copy the EnsembleModel class from `/home/code/research/kernels/matthewmaree_ens-model/ens-model.ipynb`
2. Use the exact submission cell structure from the template
3. Verify format before submitting
4. This is a well-tested approach that might have different CV-LB characteristics

### THEN: Try More Aggressive Extrapolation Detection (exp_110)
- exp_108's chemical similarity approach gave marginal improvement (0.25%)
- Try more aggressive parameters: similarity_threshold=0.5, blend_weight=0.5
- Higher blend weights might hurt CV but help LB
- **MUST use correct submission format**

### ALTERNATIVE: Try "mixall" Kernel's GroupKFold Validation
- Uses GroupKFold (5 splits) instead of Leave-One-Out
- Different validation strategy might reveal different CV-LB relationship
- Faster training allows more experimentation

## What NOT to Try
- ❌ More MLP/LGBM/XGBoost variants - all fall on same CV-LB line
- ❌ Multi-seed ensembles - we're 152% away from target, optimization is premature
- ❌ Hyperparameter tuning - won't change the intercept
- ❌ Any experiment without verifying submission format first
- ❌ DO NOT submit exp_108 - its format is wrong and will fail

## Validation Notes
- CV scheme: Leave-One-Out by solvent (24 folds single, 13 folds full)
- CV-LB gap: ~4.3x multiplier + 0.053 intercept
- **ALWAYS verify submission format before submitting**
- Only 4 submissions remaining - cannot waste on format errors

## Key Insight from Kernels
1. **"ens-model" kernel**: CatBoost + XGBoost ensemble with feature filtering and prediction clipping
2. **"mixall" kernel**: Uses GroupKFold instead of Leave-One-Out, MLP+XGB+RF+LGBM ensemble
3. Both use the CORRECT submission format with task/fold/row/target_1/target_2/target_3

## Execution Checklist (MANDATORY)
Before logging ANY experiment:
1. ✅ Model class in CV matches model class in submission cells
2. ✅ Submission format has columns: task, fold, row, target_1, target_2, target_3
3. ✅ Submission file has correct number of rows (656 single + 1227 full = 1883)
4. ✅ CV score is from actual model training, not LB score

## Files to Reference
- Template: `/home/code/research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb`
- ens-model: `/home/code/research/kernels/matthewmaree_ens-model/ens-model.ipynb`
- mixall: `/home/code/research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/mixall-runtime-is-only-2m-15s-but-good-cv-lb.ipynb`
- Best working experiment: `/home/code/experiments/030_gp_ensemble/gp_ensemble.ipynb`