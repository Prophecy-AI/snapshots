## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.8%)
- Experiments: 80 | Submissions: 21 (5 remaining)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES - ALL 80 experiments fall on the same line**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

**CRITICAL INSIGHT**: The intercept (0.0525) is HIGHER than the target (0.0347). This means:
- No amount of CV improvement can reach the target with current approaches
- We need approaches that CHANGE THE CV-LB RELATIONSHIP, not just improve CV
- The intercept represents STRUCTURAL DISTRIBUTION SHIFT

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The GroupKFold(5) experiment was correctly implemented.

**CRITICAL FINDING**: The submission FAILED because GroupKFold(5) produces only 5 folds, but the competition evaluation expects Leave-One-Out structure (24 folds for single solvent, 13 folds for full data). The "mixall" kernel approach is INCOMPATIBLE with this competition's evaluation.

**Evaluator's top priority**: Submit GroupKFold(5) to test CV-LB relationship. **CANNOT DO** - the submission format is incompatible.

**Key concerns raised**: 
1. GroupKFold(5) might have different CV-LB relationship → **MOOT** - can't submit
2. "ens-model" kernel techniques not fully replicated → **AGREE** - should try this

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop80_analysis.ipynb` for failure diagnosis
- Key patterns:
  1. All 80 experiments fall on same CV-LB line (LB = 4.31*CV + 0.0525)
  2. The intercept (0.0525) > target (0.0347) makes target IMPOSSIBLE with current approaches
  3. GroupKFold(5) is INCOMPATIBLE with competition evaluation
  4. Must use Leave-One-Out validation (24 folds single, 13 folds full)

## Recommended Approaches

### PRIORITY 1: Implement "ens-model" Kernel Feature Engineering (HIGH IMPACT)
The "ens-model" kernel has sophisticated techniques we haven't fully replicated:

```python
# 1. Combine ALL feature sources with priority-based filtering
sources = ["spange_descriptors", "acs_pca_descriptors", "drfps_catechol", "fragprints", "smiles"]

# 2. Filter correlated features (threshold=0.90)
# Priority: spange > acs > drfps > frag > smiles
# When two features are correlated, keep the higher priority one

# 3. Add numeric features
# T_inv = 1/T, T_x_RT = T*RT, RT_log = log(RT), RT_scaled = RT/mean(RT)

# 4. Use CatBoost + XGBoost ensemble with task-specific weights
# Single: CatBoost=7, XGBoost=6 (normalized: 0.538, 0.462)
# Full: CatBoost=1, XGBoost=2 (normalized: 0.333, 0.667)

# 5. Output normalization
# Clip to [0, ∞), renormalize so predictions sum to 1
```

**Why this might help**: The feature engineering and output normalization might change the CV-LB relationship by:
- Better capturing solvent chemistry through combined features
- Enforcing domain constraints (yields must be non-negative and sum to 1)
- Task-specific ensemble weights might better handle distribution shift

### PRIORITY 2: Extrapolation Detection + Conservative Predictions
Since the problem is DISTRIBUTION SHIFT (predicting for unseen solvents), add features that detect extrapolation:

```python
from sklearn.neighbors import NearestNeighbors

# Compute distance to nearest training solvents
nn = NearestNeighbors(n_neighbors=5).fit(train_solvent_features)
distances, _ = nn.kneighbors(test_solvent_features)
extrapolation_score = distances.mean(axis=1)

# When extrapolating, blend toward training mean
weight = np.clip(extrapolation_score / threshold, 0, 1)
final_pred = (1 - weight) * model_pred + weight * train_mean
```

**Why this might help**: Reduces extreme predictions for unseen solvents, potentially lowering the intercept.

### PRIORITY 3: Pseudo-labeling with High-Confidence Predictions
Use confident predictions on test-like samples to augment training:

```python
# 1. Train initial model
# 2. Make predictions on all data
# 3. Identify high-confidence predictions (low variance across ensemble)
# 4. Add these to training data with pseudo-labels
# 5. Retrain model
```

### PRIORITY 4: Adversarial Validation Features
Add features that distinguish train/test distributions:

```python
# Train a classifier to distinguish train vs test solvents
# Use the classifier's probability as a feature
# This helps the model know when it's extrapolating
```

## What NOT to Try
- ❌ GroupKFold(5) - INCOMPATIBLE with competition evaluation
- ❌ More MLP/LGBM/XGB variants without feature changes - all fall on same CV-LB line
- ❌ Multi-seed ensembles - too far from target (152.8% gap)
- ❌ Hyperparameter tuning - won't change the intercept

## Validation Notes
- MUST use Leave-One-Out validation (24 folds single, 13 folds full)
- Submission must have correct fold structure or evaluation will fail
- Track both CV and expected LB (using the linear fit) to monitor progress
- If new approach gives DIFFERENT CV-LB relationship, that's a breakthrough!

## Implementation Checklist
1. [ ] Implement "ens-model" feature engineering (combine all sources, priority filtering)
2. [ ] Add numeric features (T_inv, T_x_RT, RT_log, RT_scaled)
3. [ ] Use CatBoost + XGBoost with task-specific weights
4. [ ] Add output normalization (clip + renormalize)
5. [ ] VERIFY submission cells use correct model class
6. [ ] VERIFY submission has 24 folds for single, 13 folds for full

## Key Insight
The target IS reachable - the benchmark achieved MSE 0.0039. But we need to:
1. Change the CV-LB RELATIONSHIP, not just improve CV
2. Reduce the INTERCEPT (0.0525), not just the slope
3. Try approaches that handle DISTRIBUTION SHIFT to unseen solvents