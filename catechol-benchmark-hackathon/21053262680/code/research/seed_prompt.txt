## Current Status
- Best CV score: 0.0081 (exp_049/050/053)
- Best LB score: 0.0877 (exp_030)
- Target: 0.0347 | Gap to target: 152.8%
- Remaining submissions: 3

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.09 × CV + 0.0546 (R² = 0.96)
- Intercept interpretation: Even at CV=0, expected LB is 0.0546
- Are all approaches on the same line? YES (13 of 14 submissions)
- exp_073 is an outlier (LB=0.1451 vs expected 0.089) - likely model class mismatch
- CRITICAL: Intercept (0.0546) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0546) / 4.09 = -0.0049 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The per-class model was correctly implemented.
- Evaluator's top priority: DO NOT SUBMIT exp_122 (107% worse than best CV). Agree completely.
- Key concerns raised: 
  1. Per-class approach failed because LOO leaves out the only class representative
  2. The CV-LB intercept problem is fundamental and cannot be solved by class-specific models
  3. Only 3 submissions remaining
- How I'm addressing: The evaluator correctly identified that we need a paradigm shift. The mixall kernel uses GroupKFold instead of LOO - this might be the key insight we're missing.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop123_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All 13 "normal" submissions fall on a tight CV-LB line (R² = 0.96)
  2. The intercept (0.0546) represents structural distribution shift
  3. exp_073 was an outlier - likely model class mismatch in submission cells
  4. The mixall kernel uses GroupKFold (5 splits) instead of LOO
  5. The ens-model kernel uses feature priority filtering and different weights for single vs full data

## CRITICAL INSIGHT FROM PUBLIC KERNELS

**The mixall kernel uses GroupKFold (5 splits) instead of Leave-One-Out!**

This is a DIFFERENT validation scheme. The competition description says:
"Submissions will be evaluated according to a cross-validation procedure."

But it doesn't specify LOO vs GroupKFold. If the competition uses GroupKFold internally:
1. Our LOO CV scores may not accurately predict LB
2. The CV-LB gap might be partly due to validation scheme mismatch
3. GroupKFold might give better CV-LB alignment

**The ens-model kernel has sophisticated techniques:**
1. Feature priority system (spange > acs > drfps > frag > smiles)
2. Correlation-based feature filtering (threshold=0.8-0.9)
3. Different ensemble weights for single (7:6) vs full (1:2) data
4. Clipping and renormalization of predictions

## Recommended Approaches

### PRIORITY 1: Exact Replication of ens-model Kernel
The ens-model kernel has sophisticated feature engineering that we may not have fully replicated:
1. Feature priority system for correlation filtering
2. Different CatBoost/XGBoost hyperparameters for single vs full data
3. Different ensemble weights (7:6 for single, 1:2 for full)
4. Clipping and renormalization

**Action:** Create an EXACT replica of the ens-model kernel and verify it works.

### PRIORITY 2: Try GroupKFold Validation
The mixall kernel uses GroupKFold (5 splits) instead of LOO. This might:
1. Give better CV-LB alignment
2. Reveal that our best models have better LB than we thought
3. Change the CV-LB relationship

**Action:** Compute CV with GroupKFold for our best model and compare to LOO.

### PRIORITY 3: Submit Best Known Model
If exp_049/050/053 (CV=0.0081) haven't been successfully submitted:
1. Verify the submission cells use the correct model class
2. Submit to get LB feedback
3. Expected LB from line: 4.09 × 0.0081 + 0.0546 = 0.0877

### PRIORITY 4: Ensemble of Different Approaches
Create an ensemble that combines:
1. CatBoost + XGBoost (best CV)
2. GP (uncertainty quantification)
3. MLP (different inductive bias)

Use weighted averaging with weights optimized on CV.

## What NOT to Try
- ❌ More MLP variants (exhaustively tested, all on same line)
- ❌ Per-class models (just failed with 107% worse CV)
- ❌ Multi-seed optimization (too far from target)
- ❌ Hyperparameter sweeps (won't change the intercept)
- ❌ GNN/ChemBERTa without verifying submission cell model class

## Validation Notes
- Use Leave-One-Out for single solvent (24 folds)
- Use Leave-One-Ramp-Out for full data (13 folds)
- ALWAYS verify submission cells use the SAME model class as CV
- Track CV-LB relationship after each submission

## CRITICAL REMINDERS
1. The target (0.0347) appears mathematically unreachable with current approaches
2. BUT we MUST NOT GIVE UP - the target IS reachable
3. The CV-LB intercept (0.0546) is the real problem, not CV optimization
4. We need to CHANGE THE RELATIONSHIP, not improve CV
5. Only 3 submissions remaining - each one is precious

## IMMEDIATE NEXT STEPS
1. Create exact replica of ens-model kernel
2. Verify it produces similar CV to their reported score
3. If CV is good, submit to get LB feedback
4. Analyze if the CV-LB relationship is different from our other approaches
