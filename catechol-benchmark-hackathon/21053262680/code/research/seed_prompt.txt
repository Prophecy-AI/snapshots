## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (pending LB)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 152.7%

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.9505)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES - ALL 80 EXPERIMENTS**
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- This means the target is MATHEMATICALLY UNREACHABLE with current approaches!
- We MUST find approaches that CHANGE THE CV-LB RELATIONSHIP (reduce intercept)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The GroupKFold(5) experiment was well-executed.
- Evaluator's top priority: Submit GroupKFold(5) to test if it changes CV-LB relationship.
- **I AGREE**: We need to verify if GroupKFold(5) has a different CV-LB relationship.
- Key concern: GroupKFold(5) gives 12.84% worse CV (0.011030 vs 0.009775).
- **BUT**: Worse CV doesn't mean worse LB if the relationship is different!

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop80_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All models (MLP, LGBM, XGB, CatBoost, GP, RF, GNN, ChemBERTa) fall on SAME line
  2. The intercept (0.0525) represents STRUCTURAL extrapolation error
  3. Test solvents are fundamentally different from training solvents
  4. No amount of model tuning can fix the intercept problem

## Recommended Approaches (PRIORITY ORDER)

### IMMEDIATE: Submit to Test CV-LB Relationship
1. **Submit exp_077 (GroupKFold)** - CV=0.011030
   - Hypothesis: GroupKFold(5) might have LOWER intercept
   - If LB ≈ 0.100 → Same line, GroupKFold doesn't help
   - If LB < 0.095 → Different line, potential breakthrough!

### IF GroupKFold DOESN'T HELP: Try "ens-model" Kernel Approach
2. **Replicate "ens-model" kernel exactly**
   - Uses CatBoost + XGBoost ensemble with DIFFERENT weights:
     - Single solvent: CatBoost=7, XGBoost=6 (normalized)
     - Full data: CatBoost=1, XGBoost=2 (normalized)
   - Combines ALL feature sources with correlation-based filtering:
     - spange_descriptors (priority 5)
     - acs_pca_descriptors (priority 4)
     - drfps_catechol (priority 3)
     - fragprints (priority 2)
     - smiles (priority 1)
   - Uses `filter_correlated_features()` with threshold=0.90
   - Adds numeric features: T_inv, T_x_RT, RT_log, RT_scaled
   - Output normalization: clip to [0,1], renormalize to sum=1

### ALTERNATIVE: Try "best-work-here" Kernel Approach
3. **Deep Neural Network with SE Blocks**
   - Architecture: [768, 512, 384, 256, 128] with residual connections
   - Squeeze-and-Excitation blocks for feature recalibration
   - Ensemble with CatBoost, XGBoost, LightGBM
   - Adaptive ensemble weighting based on validation MSE
   - Output normalization to probabilities (sum to 1)

### DISTRIBUTION SHIFT STRATEGIES (If above don't work)
4. **Extrapolation Detection + Conservative Predictions**
   ```python
   # Compute distance to nearest training solvents
   from sklearn.neighbors import NearestNeighbors
   nn = NearestNeighbors(n_neighbors=5).fit(X_train_features)
   distances, _ = nn.kneighbors(X_test_features)
   extrapolation_score = distances.mean(axis=1)
   
   # Blend toward mean when extrapolating
   weight = np.clip(extrapolation_score / threshold, 0, 1)
   final_pred = (1 - weight) * model_pred + weight * train_mean
   ```

5. **Domain Constraints**
   - Force predictions to sum to 1 (yield constraint)
   - Clip predictions to [0, 1] range
   - Use softmax output layer instead of sigmoid

## What NOT to Try
- ❌ More MLP/LGBM/XGB/CatBoost variants - ALL fall on same CV-LB line
- ❌ More feature engineering within same framework - doesn't change intercept
- ❌ Multi-seed ensembles - we're 152% from target, not 2%
- ❌ Hyperparameter tuning - doesn't change the CV-LB relationship

## Validation Notes
- Use Leave-One-Out for single solvent (24 folds)
- Use Leave-One-Ramp-Out for full data (13 folds)
- Track BOTH CV and LB to monitor the relationship
- If new approach gives same CV but different LB → potential breakthrough!

## CRITICAL REMINDERS
1. **VERIFY submission cells match CV model class** - This has caused issues before
2. **The target IS reachable** - The benchmark achieved MSE 0.0039
3. **Focus on CHANGING THE RELATIONSHIP, not improving CV**
4. **Only 5 submissions remaining** - Use them strategically

## Submission Strategy
1. FIRST: Submit exp_077 (GroupKFold) to test if it changes CV-LB relationship
2. IF same line: Implement "ens-model" kernel approach and submit
3. IF still same line: Try extrapolation detection + conservative predictions
4. NEVER give up - the solution exists, we just need to find it!
