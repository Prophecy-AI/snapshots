## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost + XGBoost ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- Remaining submissions: 3
- Latest experiment: exp_119 (Yield Ratio Prediction) CV=0.008328 (2.9% worse than best)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.09 × CV + 0.0546 (R² = 0.96)
- Intercept interpretation: Even at CV=0, expected LB is 0.0546
- Are all approaches on the same line? **YES** - all 13 valid submissions
- **CRITICAL**: Intercept (0.0546) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0546) / 4.09 = -0.0049 (IMPOSSIBLE)

**The target is MATHEMATICALLY UNREACHABLE with current approaches.**
We MUST change the CV-LB relationship, not just improve CV.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The yield ratio experiment was correctly implemented.
- Evaluator correctly identified that CV=0.008328 is 2.9% worse than best CV (0.0081).
- Evaluator's top priority: DO NOT SUBMIT exp_119. Try median ensemble or extrapolation-aware predictions.
- **My synthesis**: I AGREE with the evaluator. The yield ratio approach, while fundamentally different, didn't improve CV. The expected LB if on same line would be 0.0886 (worse than best 0.0877). We should try median ensemble or quantile regression before using precious submissions.

## Key Findings from Loop 121 Analysis
1. **CV-LB Line**: LB = 4.09×CV + 0.0546 (R²=0.96) - ALL approaches on same line
2. **Intercept Problem**: Intercept (0.0546) > Target (0.0347) - target unreachable by CV improvement
3. **exp_119 (Yield Ratio)**: CV=0.0083 (2.9% worse than best) - NOT recommended for submission
4. **121 experiments**: All tabular, GNN, ChemBERTa, physics-constrained on same line
5. **Untried approaches**: Median ensemble, Quantile regression

## IMMEDIATE PRIORITY: Median Ensemble

**Why this is the most promising unexplored approach:**
1. Mean aggregation can be dominated by extreme predictions
2. Median is more robust to outliers
3. Might reduce errors on unseen solvents (reduce intercept)
4. Quick to implement
5. NOT tried in any of the 121 experiments

**Implementation:**
```python
class MedianEnsembleModel:
    """Ensemble with median aggregation for robustness to outliers.
    
    Key insight: Mean aggregation can be dominated by extreme predictions.
    Median is more robust and may reduce errors on unseen solvents.
    
    This is the SAME class used in both CV and submission cells.
    """
    def __init__(self, data='single', n_models=5):
        self.data_type = data
        self.n_models = n_models
        self.featurizer = Featurizer(mixed=(data=='full'))
        self.scaler = StandardScaler()
        self.models = []  # List of (CatBoost, XGBoost) pairs for each target
        
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train)
        X_scaled = self.scaler.fit_transform(X_feat)
        y_vals = y_train.values
        
        for seed in range(self.n_models):
            target_models = []
            for i in range(3):  # 3 targets
                # CatBoost
                cb_model = cb.CatBoostRegressor(
                    iterations=500,
                    learning_rate=0.05,
                    depth=6,
                    l2_leaf_reg=3,
                    random_seed=42 + seed,
                    verbose=False
                )
                cb_model.fit(X_scaled, y_vals[:, i])
                
                # XGBoost
                xgb_model = xgb.XGBRegressor(
                    n_estimators=500,
                    learning_rate=0.05,
                    max_depth=6,
                    reg_alpha=0.1,
                    reg_lambda=0.1,
                    random_state=42 + seed,
                    verbosity=0
                )
                xgb_model.fit(X_scaled, y_vals[:, i])
                
                target_models.append((cb_model, xgb_model))
            self.models.append(target_models)
            
    def predict(self, X):
        X_feat = self.featurizer.featurize(X)
        X_scaled = self.scaler.transform(X_feat)
        
        all_preds = []
        for seed_models in self.models:
            seed_pred = []
            for i, (cb_model, xgb_model) in enumerate(seed_models):
                cb_pred = cb_model.predict(X_scaled)
                xgb_pred = xgb_model.predict(X_scaled)
                seed_pred.append((cb_pred + xgb_pred) / 2)
            all_preds.append(np.column_stack(seed_pred))
        
        # Use MEDIAN instead of mean
        pred = np.median(all_preds, axis=0)
        pred = np.clip(pred, 0, 1)
        return torch.tensor(pred)
```

## ALTERNATIVE: Quantile Regression

If median ensemble doesn't help, try quantile regression:
```python
class QuantileModel:
    """Predict median (50th percentile) instead of mean.
    
    Key insight: Mean predictions can be pulled by outliers.
    Median predictions are more robust.
    
    This is the SAME class used in both CV and submission cells.
    """
    def __init__(self, data='single'):
        self.data_type = data
        self.featurizer = Featurizer(mixed=(data=='full'))
        self.scaler = StandardScaler()
        self.models = []
        
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train)
        X_scaled = self.scaler.fit_transform(X_feat)
        y_vals = y_train.values
        
        for i in range(3):
            model = cb.CatBoostRegressor(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                l2_leaf_reg=3,
                loss_function='Quantile:alpha=0.5',  # Median
                random_seed=42,
                verbose=False
            )
            model.fit(X_scaled, y_vals[:, i])
            self.models.append(model)
            
    def predict(self, X):
        X_feat = self.featurizer.featurize(X)
        X_scaled = self.scaler.transform(X_feat)
        
        preds = []
        for i in range(3):
            pred = self.models[i].predict(X_scaled)
            preds.append(pred)
        
        pred = np.column_stack(preds)
        pred = np.clip(pred, 0, 1)
        return torch.tensor(pred)
```

## ALTERNATIVE: Huber Loss with Lower Delta

Huber loss with lower delta is more robust to outliers:
```python
class RobustHuberModel:
    """Use Huber loss with lower delta for robustness.
    
    Key insight: Lower delta makes Huber loss more like MAE,
    which is more robust to outliers.
    """
    def __init__(self, data='single', delta=0.5):
        self.data_type = data
        self.delta = delta
        self.featurizer = Featurizer(mixed=(data=='full'))
        self.scaler = StandardScaler()
        self.models = []
        
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train)
        X_scaled = self.scaler.fit_transform(X_feat)
        y_vals = y_train.values
        
        for i in range(3):
            model = cb.CatBoostRegressor(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                l2_leaf_reg=3,
                loss_function=f'Huber:delta={self.delta}',
                random_seed=42,
                verbose=False
            )
            model.fit(X_scaled, y_vals[:, i])
            self.models.append(model)
            
    def predict(self, X):
        X_feat = self.featurizer.featurize(X)
        X_scaled = self.scaler.transform(X_feat)
        
        preds = []
        for i in range(3):
            pred = self.models[i].predict(X_scaled)
            preds.append(pred)
        
        pred = np.column_stack(preds)
        pred = np.clip(pred, 0, 1)
        return torch.tensor(pred)
```

## Submission Strategy (3 remaining)

**Recommended approach:**
1. **First**: Implement median ensemble (highest potential to reduce outlier sensitivity)
2. **If CV is competitive (< 0.0085)**: Consider submitting to test if it changes CV-LB relationship
3. **If median doesn't help**: Try quantile regression or Huber loss
4. **Final submission**: Best approach from experiments

**Decision criteria for submission:**
- If new approach achieves CV < 0.0085 AND is fundamentally different: CONSIDER submitting
- If CV is similar to best but approach is different: CONSIDER submitting (to test relationship)
- If CV is worse: DON'T SUBMIT, try next approach
- **CRITICAL**: Only 3 submissions left - be very strategic

## What NOT to Try
1. ❌ More MLP variants - exhaustively tested (50+ experiments)
2. ❌ More feature engineering - doesn't change intercept
3. ❌ Multi-seed ensembles with mean - we're 152% away from target
4. ❌ Hyperparameter tuning - won't change CV-LB relationship
5. ❌ More GNN variants - TRUE GNN worse than tabular
6. ❌ More ChemBERTa variants - on same line
7. ❌ Softmax output - yields don't sum to 1 (avg=0.80)
8. ❌ Post-hoc physics constraints - don't change learning
9. ❌ Yield ratio prediction (exp_119) - CV is 2.9% worse

## Validation Notes
- CV scheme: Leave-one-solvent-out (24 folds single, 13 folds full)
- **CRITICAL**: Verify model class consistency in submission cells
- Track both single-solvent MSE and full-data MSE separately
- After submission, check if LB falls on the line or deviates

## Critical Reminders
1. **VERIFY MODEL CLASS CONSISTENCY**: Before logging ANY experiment, verify that submission cells use the EXACT same model class as CV computation.
2. **DON'T GIVE UP**: The target IS achievable. The benchmark paper achieved MSE 0.0039.
3. **FOCUS ON CHANGING THE RELATIONSHIP**: Improving CV alone won't help.
4. **ONLY 3 SUBMISSIONS LEFT**: Be strategic - only submit if approach is fundamentally different.

## Priority Order for Next Experiments
1. **Median Ensemble** - Most promising, robust to outliers
2. **Quantile Regression** - Predicts median instead of mean
3. **Huber Loss with Lower Delta** - More robust to outliers
4. **Relative Prediction** - Predict change from reference solvent

The key insight is that we need to CHANGE how we aggregate predictions or what we optimize for, not just improve the model. All 121 experiments have shown that improving CV doesn't help because the intercept is too high. We need approaches that might reduce the intercept by being more robust to outlier predictions on unseen solvents.