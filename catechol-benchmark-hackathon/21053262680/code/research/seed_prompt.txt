## Current Status
- Best CV score: 0.0081 (exp_049, exp_050, exp_053)
- Best LB score: 0.0877 (exp_030)
- Target: 0.0347 | Gap to target: 152.8%

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? YES (except exp_073 which is WORSE)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE - IMPOSSIBLE!)

**CRITICAL FINDING FROM LOOP 81:**
- exp_073 (similarity weighting) got LB 0.1451 - 63% WORSE than expected!
- Distribution shift handling via conservative predictions HURTS performance
- Test solvents are DIFFERENT, not OUTLIERS - blending toward mean introduces BIAS
- DO NOT try more conservative prediction strategies

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_080 (ens-model replica)
- Evaluator's top priority: Implement a PROPER GNN or submit GroupKFold(5)
- Key concerns: All tabular approaches fall on same CV-LB line (R²=0.95)
- I AGREE with the evaluator - we need to BREAK the CV-LB line, not optimize along it

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop81_lb_feedback.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge, RF) fall on same CV-LB line
  2. The intercept (0.0525) > target (0.0347) - tabular optimization CANNOT reach target
  3. Similarity weighting HURT performance (LB 0.1451 vs expected 0.089)
  4. Test solvents are DIFFERENT, not OUTLIERS

## Recommended Approaches

### PRIORITY 1: Implement a PROPER Graph Neural Network
The benchmark paper achieved MSE 0.0039 with GNN. Our GNN attempts (exp_072, exp_077) achieved CV 0.025+ (3x worse than tabular). This suggests implementation issues.

**Key implementation using PyTorch Geometric:**
```python
from torch_geometric.utils import from_smiles
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, Batch

class MolGNN(torch.nn.Module):
    def __init__(self, hidden_dim=64, out_dim=3):
        super().__init__()
        # from_smiles gives 9 atom features
        self.conv1 = GCNConv(9, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)
        # +5 for process features (T, RT, 1/T, ln(RT), interaction)
        self.fc = torch.nn.Linear(hidden_dim + 5, out_dim)
        
    def forward(self, data, process_features):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = global_mean_pool(x, batch)  # Graph-level readout
        x = torch.cat([x, process_features], dim=1)
        return torch.sigmoid(self.fc(x))
```

**CRITICAL: Verify submission cells use the SAME model class as CV!**

### PRIORITY 2: Try Graph Attention Networks (GAT)
The benchmark paper used GAT + DRFP. GAT may capture more nuanced molecular relationships:
```python
from torch_geometric.nn import GATConv

class MolGAT(torch.nn.Module):
    def __init__(self, hidden_dim=64, heads=4, out_dim=3):
        super().__init__()
        self.conv1 = GATConv(9, hidden_dim, heads=heads)
        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1)
        self.fc = torch.nn.Linear(hidden_dim + 5, out_dim)
```

### PRIORITY 3: Pseudo-labeling / Domain Adaptation
Since test solvents are DIFFERENT (not outliers), we could:
1. Train initial model on training data
2. Make predictions on test data
3. Use confident predictions (low variance across ensemble) to augment training
4. Retrain with augmented data

### PRIORITY 4: Submit exp_049 or exp_053 to verify CV-LB line
These have best CV (0.0081) and should give LB ~0.0875 if they follow the line.
This would confirm whether our CV-LB relationship is stable.

## What NOT to Try

### FORBIDDEN - Distribution Shift Handling via Conservative Predictions
- exp_073 (similarity weighting) got LB 0.1451 - 63% WORSE than expected
- Blending toward training mean introduces BIAS
- Test solvents are DIFFERENT, not OUTLIERS
- DO NOT try: mean reversion, uncertainty-weighted blending, extrapolation detection

### EXHAUSTED - Tabular Model Optimization
- MLP, LightGBM, XGBoost, CatBoost, GP, Ridge, RF - ALL on same CV-LB line
- Feature engineering (Spange, DRFP, ACS PCA, fragprints) - ALL on same line
- Ensemble strategies - ALL on same line
- No more tabular experiments unless they fundamentally change representation

### EXHAUSTED - Ens-model Kernel Approaches
- exp_080 replicated ens-model kernel exactly - CV 0.009217 (11% worse than best)
- Sophisticated feature engineering doesn't help

## Validation Notes
- Use Leave-One-Out for single solvent, Leave-One-Ramp-Out for full data
- Track CV-LB relationship after EVERY submission
- If new approach falls on same CV-LB line, it's NOT a breakthrough
- A breakthrough = DIFFERENT CV-LB relationship (lower intercept or slope)

## Remaining Submissions: 4
Use wisely:
1. Submit exp_049 or exp_053 (best CV) to verify CV-LB line
2. If GNN is implemented properly, submit that
3. Save 1-2 for final attempts

## THE TARGET IS REACHABLE
- Benchmark achieved MSE 0.0039 with GNN
- Our best LB is 0.0877 (2.5x worse than target)
- The gap is large but NOT insurmountable
- We need the RIGHT approach (GNN), not more optimization
- The solution exists - we just haven't implemented it correctly yet