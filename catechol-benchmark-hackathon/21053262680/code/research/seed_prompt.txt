## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- Experiments completed: 79
- Submissions used: 20/25 (5 remaining)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept: 0.0525 > Target: 0.0347
- **CRITICAL**: Even at CV=0, expected LB is 0.0525 - ABOVE the target!
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (NEGATIVE = IMPOSSIBLE)
- **ALL 79 experiments fall on the SAME CV-LB line** - this is STRUCTURAL DISTRIBUTION SHIFT

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. ChemBERTa frozen embeddings experiment was well-executed.
- Evaluator's top priority: Test GroupKFold(5) validation from "mixall" kernel. **AGREE STRONGLY**.
- Key concerns raised: CV-LB intercept (0.0528) > target (0.0347). **This is THE problem**.
- ChemBERTa frozen embeddings failed (77% worse than baseline) - valuable negative result.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop79_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL approaches (MLP, LGBM, XGB, CatBoost, GP, GNN, GAT, ChemBERTa) fall on SAME CV-LB line
  2. The intercept (0.0525) represents STRUCTURAL extrapolation error
  3. Improving CV just moves along the line, doesn't change the intercept
  4. The "mixall" kernel uses GroupKFold(5) - a DIFFERENT validation strategy

## Recommended Approaches (PRIORITY ORDER)

### PRIORITY 1: Implement "mixall" Kernel Approach with GroupKFold(5)
**Hypothesis**: GroupKFold(5) validation may give a different CV-LB relationship.

**Implementation**:
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    groups = X["SOLVENT NAME"]
    n_splits = min(5, len(groups.unique()))
    gkf = GroupKFold(n_splits=n_splits)
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx])
```

**Key features from "mixall" kernel**:
- Ensemble: MLP + XGB + RF + LGBM with weighted average
- Spange descriptors only (simpler features)
- Runtime: 2m 15s (fast iteration)

### PRIORITY 2: Implement "ens-model" Kernel Approach
**Hypothesis**: CatBoost + XGBoost ensemble with feature filtering may improve LB.

**Key features**:
1. **Correlation-based feature filtering** (threshold=0.90):
   - Priority: spange > acs > drfps > frag > smiles
   - Drop highly correlated features
   
2. **Numeric feature engineering**:
   - Temperature to Kelvin: T_K = T + 273.15
   - Interaction: T_x_RT = T_K * RT
   - Log time: RT_log = log(RT + 1e-6)
   - Inverse temp: T_inv = 1 / T_K
   - Scaled time: RT_scaled = RT / mean(RT)

3. **Prediction clipping and renormalization**:
   ```python
   out = np.clip(out, a_min=0.0, a_max=None)
   totals = out.sum(axis=1, keepdims=True)
   divisor = np.maximum(totals, 1.0)
   out = out / divisor
   ```

4. **Different weights for single vs full data**:
   - Single: CatBoost 7:6 XGBoost
   - Full: CatBoost 1:2 XGBoost

### PRIORITY 3: Combine Best of Both Kernels
If individual approaches don't work, combine:
- GroupKFold(5) validation from "mixall"
- CatBoost + XGBoost ensemble from "ens-model"
- Feature filtering and engineering from "ens-model"
- Prediction clipping and renormalization

## What NOT to Try
- ❌ More ChemBERTa variants (frozen embeddings failed by 77%)
- ❌ More MLP/LGBM/XGB variants without changing validation strategy (all on same line)
- ❌ GNN/GAT from scratch (not enough data, already failed)
- ❌ Multi-seed ensembles (too far from target for optimization)
- ❌ Hyperparameter sweeps (won't change the CV-LB intercept)

## Validation Notes
- Current validation: Leave-One-Out (24 folds for single, 13 folds for full)
- "mixall" kernel uses: GroupKFold(5) - this is DIFFERENT
- The validation strategy may be causing the CV-LB gap
- Test GroupKFold(5) FIRST to see if it changes the CV-LB relationship

## CRITICAL REMINDERS
1. **VERIFY submission cell model class matches CV model class** before logging
2. **DO NOT log "summary" or "final decision" as experiments**
3. **DO NOT use LB score as CV score when logging**
4. **The target IS reachable** - the benchmark achieved MSE 0.0039

## Expected Outcomes
- If GroupKFold(5) gives a DIFFERENT CV-LB relationship → breakthrough potential
- If "ens-model" approach improves LB even slightly → worth submitting
- If both fail → need to investigate what the benchmark paper actually did
