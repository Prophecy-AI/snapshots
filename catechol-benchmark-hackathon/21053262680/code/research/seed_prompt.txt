## Current Status
- Best CV score: 0.008092 from exp_049 (CatBoost+XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 152.8%

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES** - ALL 85 experiments fall on this line
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = **-0.0041** (IMPOSSIBLE!)

**This means NO amount of CV improvement can reach the target with current approaches.**
**We MUST find approaches that REDUCE THE INTERCEPT (structural extrapolation error).**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. ChemProp features experiment was well-executed.
- Evaluator's top priority: Submit GroupKFold(5) to test if different validation strategy changes CV-LB relationship.
  - **DISAGREE**: GroupKFold(5) submission format is incompatible with the competition's Leave-One-Out structure. The competition requires 24 folds for single solvent and 13 folds for full data.
- Key concerns raised: CV-LB intercept (0.0528) > target (0.0347).
  - **AGREE**: This is the fundamental problem. We need to BREAK the CV-LB line, not improve CV.
- Evaluator noted: ChemProp features performed 46% WORSE (CV=0.0118 vs 0.0081).
  - **AGREE**: Pre-trained molecular representations don't help on this small dataset. Domain-specific features (Spange/DRFP) work better.
- Evaluator noted: Pseudo-labeling made things worse.
  - **AGREE**: This confirms the test distribution is fundamentally different from training.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop85_analysis.ipynb` for CV-LB analysis
- Key patterns:
  - 24 solvents in single solvent data, 656 samples
  - 13 ramps in full data, 1227 samples
  - All tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on SAME CV-LB line
  - GNN from scratch: CV 0.024 (3x worse than tabular)
  - ChemBERTa: CV 0.015 (2x worse than tabular)
  - ChemProp features: CV 0.012 (46% worse than tabular)
  - Similarity weighting: BACKFIRED (LB 0.145 vs 0.088)
  - Pseudo-labeling: Made things worse
  - Yield normalization: NO effect

## What Has Been Exhaustively Tried (DO NOT REPEAT)
1. ❌ MLP variants (50+ experiments)
2. ❌ LightGBM, XGBoost, CatBoost ensembles
3. ❌ Gaussian Processes
4. ❌ GNN from scratch (CV=0.024, 3x worse)
5. ❌ ChemBERTa embeddings (CV=0.015, 2x worse)
6. ❌ ChemProp features (CV=0.012, 46% worse)
7. ❌ Pseudo-labeling (made things worse)
8. ❌ Similarity weighting (LB=0.145, BACKFIRED)
9. ❌ Yield normalization (no effect)
10. ❌ Conservative predictions (no improvement)
11. ❌ Multi-seed optimization (too far from target)

## Recommended Approaches (PRIORITY ORDER)

### 1. **Transductive Learning / Meta-Learning** - HIGHEST PRIORITY
**Why**: Recent research (Nature 2025, arXiv 2025) shows transductive approaches can improve OOD prediction by 1.5-3x. The key insight is to leverage analogical input-target relations between training and test sets.

**Implementation**:
```python
# Transductive approach: Use test set structure to inform predictions
# 1. Compute similarity between test solvents and training solvents
# 2. For each test sample, find analogous training samples
# 3. Use the relationship between analogous pairs to adjust predictions

# Example: If test solvent A is similar to training solvent B,
# and training solvent B has yield Y at conditions C,
# then test solvent A should have yield close to Y at similar conditions
```

**Expected outcome**: Changes the CV-LB relationship by leveraging test set structure.

### 2. **Conformal Prediction with Density Estimation** - HIGH PRIORITY
**Why**: CoDrug (NeurIPS 2023) shows conformal prediction with density estimation can reduce coverage gap by 35% under distribution shift.

**Implementation**:
```python
# 1. Estimate density of each test sample under training distribution
# 2. Weight predictions by density (low density = more conservative)
# 3. Use conformal prediction to get calibrated prediction intervals
# 4. Adjust point predictions based on interval width

from sklearn.neighbors import KernelDensity
kde = KernelDensity(kernel='gaussian', bandwidth=0.5)
kde.fit(X_train_features)
log_density = kde.score_samples(X_test_features)
density_weight = np.exp(log_density)
# Low density samples get predictions blended toward mean
```

**Expected outcome**: Reduces systematic bias on OOD samples.

### 3. **Physics-Informed Constraints** - MEDIUM PRIORITY
**Why**: Domain knowledge that holds for ALL solvents (not just training) can reduce extrapolation error.

**Implementation**:
```python
# Constraint 1: Mass balance (SM + Product2 + Product3 ≈ 1)
# Constraint 2: Monotonicity (yield increases with time at fixed T)
# Constraint 3: Arrhenius (temperature dependence follows Arrhenius law)

# Post-process predictions to satisfy constraints:
def apply_constraints(preds):
    # Normalize to sum to 1
    preds = preds / preds.sum(axis=1, keepdims=True)
    # Clip to [0, 1]
    preds = np.clip(preds, 0, 1)
    return preds
```

**Expected outcome**: Reduces extreme predictions on extrapolation cases.

### 4. **Isotonic Regression Calibration** - MEDIUM PRIORITY
**Why**: Calibration can reduce systematic bias in predictions.

**Implementation**:
```python
from sklearn.isotonic import IsotonicRegression

# Train isotonic regression on validation predictions
ir = IsotonicRegression(out_of_bounds='clip')
ir.fit(val_preds, val_targets)

# Apply to test predictions
calibrated_preds = ir.transform(test_preds)
```

**Expected outcome**: Reduces systematic over/under-prediction.

### 5. **Ensemble with Different Weighting** - LOWER PRIORITY
**Why**: Different weighting of single vs full data predictions might help.

**Implementation**:
```python
# Current: weighted by sample count (656 single, 1227 full)
# Try: equal weighting, or weight by inverse variance
```

## What NOT to Try
- ❌ **More MLP/LGBM/XGB/CatBoost tuning** - Exhausted (85+ experiments, all on same line)
- ❌ **GNN from scratch** - Failed badly (CV 0.024)
- ❌ **ChemBERTa fine-tuning** - Failed (CV 0.015)
- ❌ **ChemProp features** - Failed (CV 0.012)
- ❌ **Similarity weighting** - BACKFIRED (LB 0.145)
- ❌ **Pseudo-labeling** - Made things worse
- ❌ **GroupKFold(5)** - Incompatible with competition format
- ❌ **Multi-seed optimization** - Too far from target (152.8% gap)

## Validation Notes
- CV scheme: Leave-One-Out for single solvent (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- **CRITICAL**: After ANY new approach, check if it falls on the same CV-LB line
- If new approach has SAME CV-LB relationship, it won't help reach target
- Only submit if approach shows DIFFERENT CV-LB relationship

## Submission Strategy (4 remaining)
- **DO NOT** submit marginal CV improvements
- **DO** submit if a new approach shows fundamentally different CV-LB relationship
- **SAVE** submissions for breakthrough approaches

## Key Insight
The benchmark paper achieved MSE 0.0039 (22x better than our best LB). If they followed our CV-LB line, their implied CV would be -0.011 (IMPOSSIBLE). This confirms they have a FUNDAMENTALLY DIFFERENT approach with near-zero intercept.

To reach the target, we need approaches that:
1. **Reduce the intercept** (structural extrapolation error)
2. **Leverage test set structure** (transductive learning)
3. **Use domain knowledge** that holds for unseen solvents

**The target IS reachable - we just need to find the approach that changes the CV-LB relationship.**