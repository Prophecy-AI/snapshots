## Current Status
- Best CV score: 0.0081 from exp_049, exp_050, exp_053
- Best LB score: 0.0877 from exp_030 (GP + MLP + LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (152.8%)
- Submissions used: 22/5 (4 remaining)
- Experiments run: 89

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES** (except exp_073 which was WORSE)
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = **-0.0041 (IMPOSSIBLE)**

**This means NO amount of CV improvement on the current line can reach the target.**

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The Hybrid GNN implementation was correct with proper validation.

**Evaluator's top priority: STOP GNN experiments.** I AGREE. Five consecutive GNN experiments have all performed 2-3x worse than tabular baselines:
- exp_077 (GAT + DRFP): +136% worse
- exp_081 (GCN + MLP): +216% worse
- exp_082 (Dual-encoder GNN): +195% worse
- exp_088 (Hybrid GAT + DRFP + Spange): +120% worse

**Key concerns raised:**
1. GNN approaches consistently fail on this small dataset - AGREED, stopping GNN experiments
2. Pre-training may be required for benchmark-level performance - AGREED, but we don't have resources for this
3. exp_088 should NOT be submitted - AGREED, CV is 120% worse than baseline

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop89_analysis.ipynb` for CV-LB analysis
- Key patterns:
  - ALL tabular approaches (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line
  - GNN from scratch (5 experiments) all 2-3x worse than tabular
  - Similarity weighting (exp_073) made LB 63% WORSE (0.0877 → 0.1451)
  - ChemBERTa and ChemProp features did not help

## Recommended Approaches

**CRITICAL INSIGHT**: The CV-LB intercept (0.0525) is HIGHER than the target (0.0347). This means we need to CHANGE THE RELATIONSHIP, not just improve CV.

### Priority 1: Replicate "ens-model" Kernel EXACTLY
The "ens-model" kernel by matthewmaree uses a specific approach we haven't tried exactly:
- CatBoost + XGBoost ensemble (we tried this but maybe not with exact features)
- Combined features: spange + acs_pca + drfps + fragprints + smiles (ALL features)
- Correlation-based feature filtering (threshold 0.90)
- Different weights for single vs full data:
  - Single: CatBoost 7, XGB 6 (normalized)
  - Full: CatBoost 1, XGB 2 (normalized)

**Implementation:**
```python
# Build combined feature table from ALL sources
sources = ["spange_descriptors", "acs_pca_descriptors", "drfps_catechol", "fragprints", "smiles"]
# Apply correlation filtering at threshold 0.90
# Use CatBoost + XGBoost ensemble with different weights per data type
```

### Priority 2: Stacking with Meta-Learner
Instead of simple weighted averaging, try proper stacking:
- Train diverse base models (GP, MLP, LGBM, CatBoost, XGBoost)
- Use out-of-fold predictions as features for a meta-learner
- Meta-learner can be Ridge or simple linear model

### Priority 3: Pseudo-Labeling with Confidence Filtering
- Use best model to predict on test data
- Add high-confidence predictions to training data
- Retrain model on augmented data
- This can help with distribution shift

### Priority 4: Test-Time Adaptation
From Kaggle winners' techniques:
- Adjust predictions based on test data statistics
- Use adversarial validation to identify distribution shift features
- Weight training samples by similarity to test distribution

## What NOT to Try
- ❌ More GNN variants (5 experiments, all failed)
- ❌ ChemBERTa embeddings (already failed)
- ❌ ChemProp features (already failed)
- ❌ Similarity weighting / conservative predictions (made LB 63% worse)
- ❌ Multi-seed optimization (too far from target)
- ❌ Any approach that just improves CV without changing the CV-LB relationship

## Validation Notes
- CV scheme: Leave-One-Out by solvent (single) / Leave-One-Ramp-Out (full)
- CV-LB relationship: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- **WARNING**: Improving CV alone will NOT reach target due to high intercept
- Need approaches that REDUCE THE INTERCEPT, not just improve CV

## Submission Strategy
- Only 4 submissions remaining
- DO NOT submit experiments that are clearly worse than baseline (CV > 0.0083)
- Only submit if there's theoretical reason to expect different CV-LB relationship
- Best current LB is 0.0877 - any submission should aim to beat this

## Key Insight from Research
Kaggle winners narrow CV-LB gap by:
1. Forcing validation to look like test set (adversarial validation)
2. Test-time adaptation (adjusting predictions at inference)
3. Stacking/blending diverse models
4. Pseudo-labeling test data
5. Label-shift re-weighting

We should try these approaches to CHANGE the CV-LB relationship, not just improve CV.