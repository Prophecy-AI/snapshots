## Current Status
- Best CV score: 0.0081 from exp_049/exp_050/exp_053
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM Ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (152.8%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 × CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES**
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE!)

**This means NO amount of CV improvement can reach the target with current approaches.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_090 implementation is correct.
- Evaluator's top priority: DO NOT submit exp_090 (CV 14.93% worse than baseline). **AGREED.**
- Key concerns raised:
  1. Aggressive feature filtering (4199→85) hurt performance - **AGREED, will not repeat**
  2. Yield renormalization has negligible effect (0.17%) - **NOTED**
  3. The intercept problem is the fundamental blocker - **AGREED, this is THE problem**
- Evaluator recommends understanding and addressing the intercept problem - **FULLY AGREE**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop91_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. 24 solvents in single-solvent data, 13 unique solvents in mixture data
  2. Solvents with highest variability (hardest to predict): IPA, Decanol, Ethylene Glycol
  3. All 26 solvents in spange descriptors are covered in the data
  4. The benchmark paper achieved MSE 0.0039 - our best CV is 0.0081 (2.1x worse)

## THE FUNDAMENTAL PROBLEM

After 91 experiments and 22 submissions (12 with valid LB scores), we have discovered:

1. **ALL tabular approaches fall on the SAME CV-LB line** (R² = 0.95)
2. **The intercept (0.0525) is ABOVE the target (0.0347)**
3. **This is STRUCTURAL DISTRIBUTION SHIFT** - test solvents are systematically harder
4. **Improving CV just moves along the line, NOT toward the target**

The intercept represents the "extrapolation error" - the error when predicting for solvents very different from training. To reach the target, we need to REDUCE THE INTERCEPT, not improve CV.

## Recommended Approaches (PRIORITY ORDER)

### HIGHEST PRIORITY: Approaches that might change the CV-LB relationship

1. **Implement the mixall kernel ensemble approach** - Because:
   - The mixall kernel claims "good CV/LB" correlation
   - Uses MLP + XGBoost + RF + LightGBM ensemble with learned weights
   - Uses GroupKFold (5-fold) instead of Leave-One-Out
   - This is a DIFFERENT validation strategy that might have different CV-LB relationship
   - Reference: `research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb/`

2. **Extrapolation detection with conservative predictions** - Because:
   - When predicting for "far" solvents, blend toward training mean
   - This directly addresses the intercept problem
   - Implementation:
   ```python
   from sklearn.neighbors import NearestNeighbors
   nn = NearestNeighbors(n_neighbors=5).fit(X_train_features)
   distances, _ = nn.kneighbors(X_test_features)
   extrapolation_score = distances.mean(axis=1)
   weight = np.clip(extrapolation_score / threshold, 0, 1)
   final_pred = (1 - weight) * model_pred + weight * train_mean
   ```

3. **Per-solvent difficulty weighting** - Because:
   - Analysis shows IPA, Decanol, Ethylene Glycol have highest variability
   - These solvents are "harder" to predict
   - Weight predictions toward mean for high-variability solvents

### MEDIUM PRIORITY: Representation changes

4. **Proper GNN implementation** - Because:
   - Previous GNN attempts had model class mismatch issues
   - The benchmark paper used GNN with attention mechanisms
   - MUST verify submission cells use the SAME model class as CV

5. **Pre-trained molecular embeddings** - Because:
   - ChemBERTa/MolBERT embeddings capture chemical knowledge
   - Previous attempts may have had implementation issues
   - Use frozen embeddings as features, not fine-tuning

### LOW PRIORITY (only if above fail)

6. **Domain constraints** - Because:
   - Yields must be non-negative
   - Yields should sum to ≤ 1 (mass balance)
   - These constraints generalize to unseen solvents

## What NOT to Try
- ❌ More MLP/LGBM/XGBoost/CatBoost variants (all on same CV-LB line)
- ❌ Aggressive feature filtering (just demonstrated to fail)
- ❌ Yield renormalization (negligible effect)
- ❌ Multi-seed ensembles (optimization forbidden when >5% from target)
- ❌ Hyperparameter sweeps (won't change the intercept)

## Validation Notes
- Use Leave-One-Out for single solvent (24 folds)
- Use Leave-One-Ramp-Out for full data (13 folds)
- Track BOTH single-solvent MSE and full-data MSE separately
- **CRITICAL**: Verify submission cells use the EXACT model class from CV

## Submission Strategy (4 remaining)
- Only submit if approach is FUNDAMENTALLY DIFFERENT
- Only submit if CV is competitive (< 0.009) AND approach might change CV-LB relationship
- Do NOT submit experiments that are worse than baseline CV (0.0083)

## IMMEDIATE NEXT EXPERIMENT

**Implement the mixall kernel ensemble approach:**
1. Use MLP + XGBoost + RF + LightGBM ensemble
2. Use spange_descriptors features (13 features)
3. Use the official Leave-One-Out validation (not GroupKFold)
4. Compute CV and compare to baseline
5. If CV is competitive, this might have different CV-LB relationship

The mixall kernel is interesting because:
- It combines 4 different model types (diversity)
- It uses learned ensemble weights
- It claims good CV/LB correlation
- It's fast (2m 15s runtime)

**CRITICAL VERIFICATION:**
Before logging ANY experiment, verify:
1. The model class in submission cells MATCHES the CV computation
2. The CV score is from actual model training, not LB
3. The experiment is fundamentally different from previous attempts