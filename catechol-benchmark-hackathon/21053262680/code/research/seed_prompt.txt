## Current Status
- Best CV score: 0.008298 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 (exp_030)
- Target: 0.0347 | Gap to target: 153%
- Experiments: 95 | Submissions: 22/5 used, 4 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 × CV + 0.0528 (R² = 0.952)
- Intercept (0.0528) > Target (0.0347)
- Required CV for target: -0.0042 (IMPOSSIBLE with current approaches)
- **ALL 95 experiments fall on the SAME CV-LB line**
- This is a STRUCTURAL DISTRIBUTION SHIFT problem, not model quality

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - exp_094 (ens-model kernel) was correctly implemented.

**Evaluator's top priority**: Investigate why GNNs failed - benchmark paper achieved MSE 0.0039 with GNNs while our GNNs achieved CV 0.018-0.068 (5-17x gap).

**I AGREE with the evaluator's assessment:**
1. The CV-LB intercept problem is the fundamental blocker
2. All tabular approaches have been exhausted
3. GNN implementation issues need root cause analysis
4. The benchmark paper proves the target IS reachable with the right approach

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop95_analysis.ipynb` - CV-LB relationship analysis

Key patterns:
1. **The benchmark paper (arXiv:2512.19530) achieved MSE 0.0039** using:
   - Graph Attention Networks (GATs) for molecular graph message-passing
   - Differential Reaction Fingerprints (DRFP) as additional features
   - Learned mixture-aware solvent encodings
   - This is 25x better than tabular ensembles (MSE 0.099)

2. **Our GNN experiments failed** (CV 0.018-0.068) likely due to:
   - Model class mismatch in submission cells
   - Missing DRFP integration with graph representation
   - Missing learned mixture-aware encodings

## MANDATORY NEXT EXPERIMENT: Proper GNN with GAT + DRFP

The benchmark paper achieved MSE 0.0039. We MUST implement this approach correctly:

### Architecture Requirements:
1. **Graph Attention Networks (GAT)** for molecular graph message-passing
2. **DRFP features** integrated with graph representation
3. **Learned mixture-aware encodings** for continuous solvent compositions
4. **Proper node/edge features** from RDKit molecular graphs

### Implementation Steps:
1. Convert SMILES to molecular graphs using RDKit
2. Create node features (atom type, degree, charge, etc.)
3. Create edge features (bond type, aromaticity, etc.)
4. Build GAT layers for message passing
5. Pool graph representations
6. Integrate DRFP features
7. Add reaction condition features (T, RT)
8. Output 3 targets (Product 2, Product 3, SM)

### CRITICAL VERIFICATION:
- **BEFORE running CV**: Note the exact model class name
- **AFTER CV**: Verify submission cells use the EXACT SAME model class
- **If mismatch**: FIX submission cells before logging experiment

### Code Template:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import Data, Batch
from rdkit import Chem
from rdkit.Chem import AllChem

class MoleculeGraph:
    """Convert SMILES to PyTorch Geometric graph."""
    
    ATOM_FEATURES = ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'I', 'P', 'other']
    
    @staticmethod
    def smiles_to_graph(smiles):
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        
        # Node features
        node_features = []
        for atom in mol.GetAtoms():
            features = [
                atom.GetAtomicNum(),
                atom.GetDegree(),
                atom.GetFormalCharge(),
                atom.GetNumRadicalElectrons(),
                int(atom.GetHybridization()),
                int(atom.GetIsAromatic()),
                atom.GetTotalNumHs(),
            ]
            node_features.append(features)
        
        # Edge index
        edge_index = []
        for bond in mol.GetBonds():
            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
            edge_index.extend([[i, j], [j, i]])
        
        x = torch.tensor(node_features, dtype=torch.float)
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
        
        return Data(x=x, edge_index=edge_index)

class GATModel(nn.Module):
    """Graph Attention Network for solvent property prediction."""
    
    def __init__(self, node_dim=7, hidden_dim=64, drfp_dim=2048, num_heads=4):
        super().__init__()
        
        # Node embedding
        self.node_embed = nn.Linear(node_dim, hidden_dim)
        
        # GAT layers
        self.gat1 = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=False)
        self.gat2 = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=False)
        
        # DRFP projection (optional, if using DRFP)
        self.drfp_proj = nn.Linear(drfp_dim, hidden_dim)
        
        # Output head (graph_feat + T + RT)
        self.output = nn.Sequential(
            nn.Linear(hidden_dim + 2, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 3)
        )
    
    def forward(self, data, T, RT, drfp=None):
        # Node embedding
        x = self.node_embed(data.x)
        
        # GAT message passing
        x = F.relu(self.gat1(x, data.edge_index))
        x = F.relu(self.gat2(x, data.edge_index))
        
        # Global pooling
        x = global_mean_pool(x, data.batch)
        
        # Add DRFP features if available
        if drfp is not None:
            drfp_feat = self.drfp_proj(drfp)
            x = x + drfp_feat
        
        # Combine with reaction conditions
        x = torch.cat([x, T, RT], dim=1)
        
        return self.output(x)

class GATModelWrapper(BaseModel):
    """Wrapper for GATModel to match competition template."""
    
    def __init__(self, data='single'):
        self.data_mode = data
        self.model = None
        self.graph_cache = {}
        
    def train_model(self, train_X, train_Y, device=None, verbose=False):
        # Implementation here
        pass
    
    def predict(self, X):
        # Implementation here
        pass
```

## What NOT to Try

1. ❌ **More tabular model variants** - All fall on same CV-LB line
2. ❌ **More feature engineering within tabular paradigm** - Exhausted
3. ❌ **Hyperparameter tuning of existing models** - Won't change intercept
4. ❌ **Multi-seed ensembles** - Gap is 153%, not 1-2%
5. ❌ **Submitting experiments with CV > 0.008298** - Only submit if better

## Validation Notes

1. **CV Scheme**: Leave-One-Solvent-Out for single solvent, Leave-One-Ramp-Out for full data
2. **Model Class Consistency**: ALWAYS verify submission cells use the EXACT model class from CV
3. **Only 4 submissions remaining** - use wisely
4. **Only submit if**:
   - CV is better than 0.008298 (baseline)
   - OR approach shows promise for changing CV-LB relationship

## Key Insight

The benchmark paper achieved MSE 0.0039 (vs our best 0.0877) by operating on MOLECULAR GRAPHS, not tabular features. This is a **25x improvement**. The path forward is clear:
**IMPLEMENT A PROPER GNN THAT MATCHES THE BENCHMARK ARCHITECTURE**