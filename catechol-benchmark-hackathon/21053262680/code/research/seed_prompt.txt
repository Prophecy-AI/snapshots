## Current Status
- Best CV score: 0.0081 from exp_049 (CatBoost+XGBoost)
- Best LB score: 0.0877 from exp_030 (GP Ensemble)
- Target: 0.0347 | Gap to target: 152.8%

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.29 × CV + 0.0528 (R² = 0.9523)
- Intercept interpretation: Even at CV=0, expected LB is 0.0528
- Are all approaches on the same line? **YES** (12/13 submissions)
- **CRITICAL**: Intercept (0.0528) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0528) / 4.29 = -0.0042 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY**. The SolventB% bug fix was correct.
- Evaluator's top priority: **SUBMIT exp_103 to check CV-LB relationship**. I agree this is valuable to verify if GroupKFold gives a different CV-LB relationship.
- Key concerns raised: CV (0.01124) is higher than best CV (0.0083), predicted LB ≈ 0.10 (worse than best). This is expected since GroupKFold (5 splits) is an easier validation task than leave-one-out (24 folds).
- The evaluator correctly identifies that we need to check if GroupKFold changes the CV-LB relationship.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop104_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line
  2. The intercept (0.0528) represents STRUCTURAL extrapolation error
  3. Test solvents are fundamentally different from training solvents
  4. No amount of CV improvement can reach target if intercept > target

## Recommended Approaches

### PRIORITY 1: Post-hoc Bias Correction (NEW - from web research)
**Rationale**: Web research revealed that post-hoc intercept-bias correction is a standard technique for reducing systematic offset when deploying models on unseen chemicals.

**Implementation**:
```python
# After training, compute average residual on validation set
preds = model.predict(X_val)
bias = preds.mean() - y_val.mean()

# Apply correction to test predictions
corrected_pred = model.predict(X_test) - bias
```

This directly addresses the intercept problem by calibrating predictions based on validation residuals.

### PRIORITY 2: Submit exp_049 (Best Pending CV)
**Rationale**: exp_049 has CV=0.0081 (best pending). If it falls on the same CV-LB line, predicted LB ≈ 0.0875. This would confirm the line relationship holds for CatBoost+XGBoost.

**Expected outcome**: LB ≈ 0.0875 (on the line) or potentially slightly better if CatBoost generalizes differently.

### PRIORITY 3: Ensemble with Bias Correction
**Rationale**: Combine the best models (MLP, XGBoost, CatBoost, LightGBM) with per-fold bias correction.

**Implementation**:
```python
class BiasCorrectingEnsemble:
    def __init__(self, models):
        self.models = models
        self.biases = []
    
    def train_model(self, X_train, Y_train):
        # Train all models
        for model in self.models:
            model.train_model(X_train, Y_train)
        
        # Compute bias on training data (OOB or validation)
        preds = self.predict_raw(X_train)
        self.bias = preds.mean(axis=0) - Y_train.values.mean(axis=0)
    
    def predict(self, X_test):
        preds = self.predict_raw(X_test)
        return preds - self.bias  # Apply bias correction
```

### PRIORITY 4: Target-Specific Bias Correction
**Rationale**: Different targets (Product 2, Product 3, SM) may have different biases.

**Implementation**:
```python
# Compute per-target bias
for target_idx in range(3):
    bias[target_idx] = preds[:, target_idx].mean() - y_val[:, target_idx].mean()
```

### PRIORITY 5: Fold-Specific Bias Correction
**Rationale**: Different folds (solvents) may have different biases. Apply fold-specific correction.

**Implementation**:
```python
# During CV, track per-fold bias
fold_biases = []
for fold, ((train_X, train_Y), (test_X, test_Y)) in enumerate(splits):
    model.train_model(train_X, train_Y)
    preds = model.predict(test_X)
    fold_bias = preds.mean() - test_Y.values.mean()
    fold_biases.append(fold_bias)

# Use average fold bias for test predictions
avg_bias = np.mean(fold_biases)
```

## What NOT to Try
- ❌ More MLP/LGBM/XGB variants without bias correction (all fall on same line)
- ❌ GNN/ChemBERTa without proper model class verification (previous attempts had issues)
- ❌ Multi-seed ensembles (optimization is forbidden when gap > 5%)
- ❌ Hyperparameter tuning (doesn't change the intercept)

## Validation Notes
- CV scheme: Leave-One-Out (24 folds for single, 13 for full) or GroupKFold (5 splits)
- Key insight: GroupKFold gives higher CV but might have different CV-LB relationship
- After submission, plot (CV, LB) point to check if it's on the line or below

## Immediate Action Plan
1. **First**: Implement bias correction on the best model (exp_030 or exp_049)
2. **Second**: Submit to check if bias correction reduces the intercept
3. **Third**: If bias correction works, apply to ensemble

## Key Insight from Web Research
> "Apply a post‑hoc intercept‑bias correction – after fitting, compute the average residual on a small validation set of known compounds and subtract that mean bias from all future predictions."

This is the key to reducing the intercept! The intercept represents systematic bias that can be corrected post-hoc.
