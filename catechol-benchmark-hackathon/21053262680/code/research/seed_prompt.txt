## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost+XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept (0.0525) > Target (0.0347)
- **Required CV to hit target: -0.0041 (IMPOSSIBLE)**
- All 94 experiments (MLP, LGBM, XGB, CatBoost, GP, GNN, ChemBERTa) fall on the SAME LINE
- **CONCLUSION: Improving CV alone CANNOT reach target. Must change the CV-LB relationship.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_093 (Morgan fingerprints)
- Evaluator's top priority: **Investigate mixall kernel's GroupKFold approach** - may have different CV-LB relationship
- Key concerns raised: 
  1. CV-LB intercept (0.0525) > Target (0.0347) makes target mathematically unreachable
  2. Morgan fingerprints performed 277% worse than baseline
  3. GNN failures need root cause analysis
- **I AGREE with the evaluator's assessment.** The intercept problem is the fundamental blocker.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop94_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All tabular models converge to same CV-LB line regardless of features/architecture
  2. Pre-computed features (Spange, DRFP, ACS PCA) are already optimal
  3. Morgan fingerprints add noise, not signal
  4. GNNs trained from scratch fail on this small dataset

## Recommended Approaches (PIVOT REQUIRED)

### PRIORITY 1: Test GroupKFold Validation (Mixall Kernel Approach)
The mixall kernel uses GroupKFold(5) instead of Leave-One-Out. This is a fundamentally different validation scheme that may have a different CV-LB relationship.

**Why this matters:**
- GroupKFold has more training data per fold → less overfitting
- More stable CV estimates → potentially lower intercept
- The kernel claims "good CV/LB correlation"

**Implementation:**
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    groups = X["SOLVENT NAME"]
    gkf = GroupKFold(n_splits=5)
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx])
```

**CRITICAL:** We need to submit a GroupKFold-based model to see if it has a different CV-LB relationship. This is the ONLY way to know if this approach can break the intercept barrier.

### PRIORITY 2: Implement the Ens-Model Kernel's CatBoost+XGBoost Ensemble
The ens-model kernel (matthewmaree/ens-model) uses:
- CatBoost + XGBoost ensemble with optimized weights
- Feature correlation filtering
- Per-target hyperparameters
- Clipping and renormalization

This is a proven approach that achieved good LB scores on Kaggle.

### PRIORITY 3: If GroupKFold Doesn't Help, Try Domain Adaptation
If GroupKFold still falls on the same CV-LB line, we need more aggressive approaches:
1. **Test-time training**: Fine-tune on test predictions
2. **Pseudo-labeling**: Use confident predictions to augment training
3. **Uncertainty-weighted predictions**: Blend toward mean when extrapolating

## What NOT to Try
- ❌ More MLP variants (94 experiments, all on same line)
- ❌ More GNN variants trained from scratch (5 experiments, all worse than baseline)
- ❌ Morgan fingerprints (277% worse)
- ❌ Any approach that just improves CV without changing the CV-LB relationship

## Validation Notes
- **Current CV scheme**: Leave-One-Out (24 folds single, 13 folds full)
- **Alternative to test**: GroupKFold (5 folds each)
- **Key insight**: The CV-LB relationship is the constraint, not the CV score itself

## Submission Strategy (4 remaining)
1. **SUBMIT exp_055 (Mixall GroupKFold)** - Already computed, CV=0.008504, need LB to check if it breaks the line
2. If GroupKFold has different intercept → optimize within that approach
3. If GroupKFold has same intercept → try domain adaptation approaches
4. Reserve 1-2 submissions for final ensemble

## CRITICAL REMINDER
The target IS reachable. The benchmark paper achieved MSE 0.0039. We need to find what they did differently. The key is NOT improving CV - it's changing the CV-LB relationship.
