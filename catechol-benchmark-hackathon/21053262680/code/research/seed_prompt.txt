## Current Status
- Best CV score: 0.0081 from exp_049/exp_050/exp_053
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 152.7%
- Submissions: 23/5 used, 4 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 × CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES**
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The bias correction experiment was correctly implemented.
- Evaluator's top priority: **DO NOT submit exp_106** (expected LB ~0.0924, worse than best 0.0877). **AGREED.**
- Key concerns raised: 
  1. Submitting a worse model wastes a submission
  2. GNN experiments remain unexplored on LB
  3. The intercept problem remains unsolved
- How I'm addressing: 
  1. NOT submitting exp_106
  2. Investigating why GNN experiments have no submission files
  3. Pivoting to approaches that might change the CV-LB relationship

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop107_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL 12 valid submissions fall on the SAME CV-LB line (R² = 0.95)
  2. exp_073 (RF ensemble) was an outlier with WORSE LB than expected
  3. GNN experiments (exp_086, exp_095, exp_096) achieved good CV but NO submission files
  4. The problem is DISTRIBUTION SHIFT, not model architecture

## Recommended Approaches

### PRIORITY 1: Debug and Submit a GNN Experiment
**Rationale**: GNN is a fundamentally different representation (graph-based vs tabular). If it works, it might give a different CV-LB relationship.

**Steps**:
1. Check exp_095 (Simple GAT, CV=0.00955) - why no submission file?
2. If the notebook ran but didn't generate submission, fix the submission cells
3. Verify model class consistency (same class in CV and submission cells)
4. Generate submission and submit to test CV-LB relationship

**Expected outcome**: If GNN gives a different CV-LB relationship (different slope or intercept), we have a path to the target.

### PRIORITY 2: Uncertainty-Weighted Conservative Predictions
**Rationale**: When the model is uncertain (high ensemble disagreement), blend toward a conservative prediction.

**Implementation**:
```python
class UncertaintyAwareModel(BaseModel):
    def __init__(self, data="single", n_seeds=5, blend_threshold=0.1):
        self.data_mode = data
        self.n_seeds = n_seeds
        self.blend_threshold = blend_threshold
        self.models = []
        
    def train_model(self, train_X, train_Y, device=None, verbose=False):
        self.train_mean = train_Y.values.mean(axis=0)
        for i in range(self.n_seeds):
            np.random.seed(42 + i)
            torch.manual_seed(42 + i)
            model = EnsembleModel(data=self.data_mode)
            model.train_model(train_X, train_Y, device, verbose)
            self.models.append(model)
        
    def predict(self, X):
        preds = np.array([m.predict(X).numpy() for m in self.models])
        mean_pred = preds.mean(axis=0)
        std_pred = preds.std(axis=0)
        
        # Blend toward training mean when uncertainty is high
        weight = np.clip(std_pred / self.blend_threshold, 0, 0.5)
        final_pred = (1 - weight) * mean_pred + weight * self.train_mean
        
        # Clip and renormalize
        final_pred = np.clip(final_pred, 0, 1)
        totals = final_pred.sum(axis=1, keepdims=True)
        final_pred = final_pred / np.maximum(totals, 1.0)
        
        return torch.tensor(final_pred, dtype=torch.double)
```

**Expected outcome**: Conservative predictions for uncertain samples might reduce the intercept.

### PRIORITY 3: Pseudo-Labeling for Distribution Adaptation
**Rationale**: Use confident predictions on test-like samples to adapt to the test distribution.

**Implementation**:
1. Train initial model on training data
2. Generate predictions on all training data
3. Identify samples where model is confident (low variance across ensemble)
4. Use these confident predictions as pseudo-labels for retraining
5. This helps the model adapt to the distribution it will see at test time

### PRIORITY 4: Domain-Specific Constraints
**Rationale**: Physics-based constraints that hold even on unseen data.

**Ideas**:
1. **Arrhenius temperature dependence**: Enforce that yield changes with temperature follow Arrhenius kinetics
2. **Mass balance**: Ensure Product 2 + Product 3 + SM ≤ 1 (already done)
3. **Monotonicity**: Yield should change monotonically with residence time (for fixed temperature)

## What NOT to Try
- ❌ More MLP/LGBM/XGB/CatBoost variants - all fall on the same CV-LB line
- ❌ More feature engineering with tabular models - doesn't change the intercept
- ❌ Bias correction with constant shift - already tested, doesn't help
- ❌ Submitting exp_106 - expected to be worse than best LB

## Validation Notes
- CV scheme: Leave-One-Out for single solvent (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- CV-LB gap: ~4.3x multiplier + 0.0525 intercept
- The intercept is the STRUCTURAL problem - no amount of CV improvement can fix it
- We need approaches that CHANGE the CV-LB relationship, not just improve CV

## IMMEDIATE ACTION
1. **DO NOT submit exp_106** - it will waste a submission
2. **Debug GNN experiments** - check why exp_095 has no submission file
3. **If GNN is fixable, submit it** - test if it gives a different CV-LB relationship
4. **If GNN fails, try uncertainty-weighted predictions** - might reduce the intercept

## Key Insight from Public Kernels
- The "ens-model" kernel uses CatBoost + XGBoost ensemble with optimized weights
- The "mixall" kernel uses GroupKFold (5-fold) instead of Leave-One-Out
- Both kernels use the same tabular approach - they likely fall on the same CV-LB line
- We need something FUNDAMENTALLY DIFFERENT to break the line