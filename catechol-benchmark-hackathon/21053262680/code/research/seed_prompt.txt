## Current Status
- Best CV score: 0.0081 from exp_049/exp_050/exp_053 (CatBoost+XGBoost ensemble)
- Best LB score: 0.0877 from exp_030 (GP+MLP+LGBM ensemble)
- Target: 0.0347 | Gap to target: 152.8%

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 * CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- **CRITICAL: Intercept (0.0525) > Target (0.0347)**
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE!)
- All 12 submissions (excluding RF outlier) fall on the SAME LINE
- **We have a DISTRIBUTION SHIFT problem. Improving CV alone CANNOT reach target.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_082 (Dual-Encoder GNN)
- Evaluator's top priority: "Pivot away from training GNNs from scratch"
- **I AGREE**: GNN experiments consistently underperform (CV=0.024 vs tabular CV=0.008)
- The benchmark's GNN success (MSE 0.0039) likely came from pre-training, not architecture
- Key concern: CV-LB intercept problem persists across ALL approaches

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop83_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on SAME CV-LB line
  2. GNN experiments (exp_081, exp_082) achieved WORSE CV than tabular
  3. exp_073 (RF ensemble) is an outlier with LB=0.1451 (much worse than expected)
  4. The "mixall" kernel claims "good CV-LB" with GroupKFold(5) validation
  5. exp_079 (GroupKFold) was submitted - waiting for LB feedback

## Key Insights from Public Kernels

### 1. "mixall" kernel (9 votes)
- Uses **GroupKFold(5)** instead of Leave-One-Out
- Claims "good CV/LB" correlation
- Ensemble: MLP + XGBoost + RF + LightGBM
- Runtime: only 2m 15s

### 2. "Ens Model" kernel (7 votes)
- CatBoost + XGBoost ensemble
- Combined features: spange + acs_pca + drfps + fragprints + smiles
- Correlation-based feature filtering (threshold=0.90)
- Numeric feature engineering: T_x_RT, RT_log, T_inv, RT_scaled
- **Yield normalization**: clip to [0, inf], then normalize so sum ≤ 1
- Different weights for single vs full data (Single: 7:6, Full: 1:2)

## Recommended Approaches (Priority Order)

### 1. Implement Yield Normalization + Best Model (HIGHEST PRIORITY)
- From "Ens Model" kernel: clip predictions to [0, inf], normalize so sum ≤ 1
- This is a **physics-based constraint** that should help generalization
- Apply to our best model (GP+MLP+LGBM from exp_030)
- **Rationale**: Domain constraints can reduce distribution shift error

### 2. Extrapolation Detection + Conservative Predictions
- Compute distance to nearest training solvent for each test sample
- When extrapolation_score is high, blend toward training mean
- This directly addresses the distribution shift problem
```python
from sklearn.neighbors import NearestNeighbors
nn = NearestNeighbors(n_neighbors=5).fit(X_train_features)
distances, _ = nn.kneighbors(X_test_features)
extrapolation_score = distances.mean(axis=1)
weight = np.clip(extrapolation_score / threshold, 0, 1)
final_pred = (1 - weight) * model_pred + weight * train_mean
```

### 3. Try Pre-trained Molecular Embeddings (ChemBERTa Frozen)
- Instead of training GNN from scratch, use frozen ChemBERTa embeddings
- Use embeddings as features for tabular models (CatBoost, XGBoost)

### 4. Uncertainty-Weighted Predictions
- Train ensemble of diverse models
- Use ensemble variance as uncertainty measure
- When uncertainty is high, blend toward training mean

## What NOT to Try
- ❌ More MLP variants - exhaustively tested (50+ experiments)
- ❌ Training GNNs from scratch - consistently underperforms (CV=0.024 vs 0.008)
- ❌ Multi-seed ensembles - too far from target (152.8% gap)
- ❌ Hyperparameter optimization - won't change CV-LB relationship

## Validation Notes
- Current CV scheme: Leave-One-Out (24 folds for single, 13 for full)
- **CRITICAL**: After EVERY submission, check if it falls on the same CV-LB line
- If a new approach has a DIFFERENT CV-LB relationship, that's a breakthrough

## Submission Strategy (4 remaining)
1. exp_079 (GroupKFold) already submitted - waiting for LB
2. Next: Yield normalization + extrapolation detection
3. Then: Best performing approach from above
4. Final: Best model

## CRITICAL REMINDER
The target IS reachable. The benchmark achieved MSE 0.0039. We need to find the approach that CHANGES the CV-LB relationship, not just improves CV. The intercept (0.0525) is the enemy, not the CV score.

## IMMEDIATE NEXT EXPERIMENT
Implement yield normalization (sum ≤ 1 constraint) on the best model (GP+MLP+LGBM from exp_030). This is a domain constraint that should help with distribution shift.
