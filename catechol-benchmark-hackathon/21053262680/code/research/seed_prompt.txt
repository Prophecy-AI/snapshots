## Current Status
- Best CV score: 0.0081 from exp_049/050/053 (CatBoost/XGBoost variants)
- Best LB score: 0.0877 from exp_030 (GP ensemble)
- Target: 0.0347 | Gap to target: 0.0530 (153%)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.31 × CV + 0.0525 (R² = 0.95)
- Intercept interpretation: Even at CV=0, expected LB is 0.0525
- Are all approaches on the same line? **YES** - All 12 valid submissions fall on this line
- **CRITICAL**: Intercept (0.0525) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0525) / 4.31 = -0.0041 (IMPOSSIBLE - negative)

**THE TARGET IS MATHEMATICALLY UNREACHABLE WITH CURRENT APPROACHES.**
We MUST find approaches that CHANGE the CV-LB relationship (reduce intercept or change slope).

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_098 implementation was correct.
- Evaluator's top priority: Try conservative blending or domain constraints to reduce intercept. **AGREE** - this is the right direction.
- Key concerns raised: 
  1. ChemBERTa embeddings don't help (even with PCA). **AGREE** - stop trying ChemBERTa variants.
  2. The CV-LB intercept (0.0525) is the fundamental blocker. **AGREE** - this is the core problem.
  3. Similarity-based approaches fail catastrophically (exp_073). **AGREE** - avoid similarity weighting.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop99_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. All tabular models (MLP, LGBM, XGB, CatBoost, GP, Ridge) fall on the SAME CV-LB line
  2. GNNs and ChemBERTa fail worse than tabular models (CV 2-5x worse)
  3. Similarity-based approaches cause catastrophic LB failure (exp_073: CV=0.0084, LB=0.1451)
  4. The test solvents are fundamentally different from training solvents

## Recommended Approaches (PRIORITY ORDER)

### 1. CONSERVATIVE BLENDING TOWARD MEAN (HIGHEST PRIORITY)
**Rationale**: If test solvents are fundamentally different, extreme predictions hurt LB. Blending toward training mean may reduce the intercept.

```python
class ConservativeModel:
    def __init__(self, base_model, blend_factor=0.3):
        self.base_model = base_model
        self.blend_factor = blend_factor
        self.train_mean = None
    
    def fit(self, X, y):
        self.base_model.fit(X, y)
        self.train_mean = y.mean(axis=0)
    
    def predict(self, X):
        base_pred = self.base_model.predict(X)
        # Blend toward training mean
        return (1 - self.blend_factor) * base_pred + self.blend_factor * self.train_mean
```

**Try blend factors**: 0.1, 0.2, 0.3, 0.4, 0.5
**Use with**: Best model (GP ensemble from exp_030)

### 2. STRICT PREDICTION CLIPPING
**Rationale**: Extreme predictions (near 0 or 1) are likely wrong for unseen solvents.

```python
def clip_predictions(predictions, lower=0.05, upper=0.95):
    return np.clip(predictions, lower, upper)
```

**Try clipping ranges**: [0.05, 0.95], [0.10, 0.90], [0.15, 0.85]

### 3. GROUPKFOLD VALIDATION (MIXALL KERNEL APPROACH)
**Rationale**: The mixall kernel uses GroupKFold (5 splits) instead of Leave-One-Out. This may give a DIFFERENT CV-LB relationship.

**Key insight from mixall kernel**:
- Uses GroupKFold with 5 splits instead of Leave-One-Out
- Uses ensemble of MLP + XGBoost + RF + LightGBM
- Runtime is only 2m 15s but claims "good CV/LB"

**Implementation**:
```python
from sklearn.model_selection import GroupKFold

def generate_leave_one_out_splits(X, Y):
    groups = X["SOLVENT NAME"]
    n_splits = min(5, len(groups.unique()))
    gkf = GroupKFold(n_splits=n_splits)
    for train_idx, test_idx in gkf.split(X, Y, groups):
        yield (X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx])
```

**IMPORTANT**: This changes the validation scheme, so CV scores won't be directly comparable. But if it gives a DIFFERENT CV-LB relationship with lower intercept, it's worth trying.

### 4. UNCERTAINTY-WEIGHTED PREDICTIONS
**Rationale**: Use GP or ensemble variance to detect high-uncertainty predictions and blend them toward mean.

```python
# Train GP ensemble
gp_preds, gp_stds = gp_model.predict(X_test, return_std=True)

# Blend high-uncertainty predictions toward mean
uncertainty_threshold = np.percentile(gp_stds, 75)
blend_weights = np.clip(gp_stds / uncertainty_threshold, 0, 1)
final_preds = (1 - blend_weights) * gp_preds + blend_weights * train_mean
```

## What NOT to Try
- ❌ More ChemBERTa variants (exp_097, exp_098 both failed)
- ❌ More GNN variants (7+ attempts, all failed with CV 2-5x worse)
- ❌ Similarity-based approaches (exp_073 catastrophic failure)
- ❌ More tabular model variants without changing the CV-LB relationship
- ❌ Multi-seed ensembles (we're 153% away from target - optimization is premature)
- ❌ Hyperparameter tuning (won't change the intercept)

## Validation Notes
- CV scheme: Leave-One-Out for single solvent (24 folds), Leave-One-Ramp-Out for full data (13 folds)
- CV-LB gap: LB ≈ 4.31 × CV + 0.0525
- **CRITICAL**: Any approach that doesn't change this relationship is USELESS for reaching target
- After implementing conservative blending, SUBMIT to check if intercept changes

## Experiment Priority
1. **exp_099**: Conservative blending (blend_factor=0.3) with GP ensemble
2. **exp_100**: Strict prediction clipping [0.05, 0.95] with GP ensemble
3. **exp_101**: GroupKFold validation with MLP+XGB+RF+LGBM ensemble (mixall approach)
4. **exp_102**: Uncertainty-weighted predictions with GP variance

## CRITICAL REMINDER
- We have 4 submissions remaining
- Only submit experiments that show promise for CHANGING the CV-LB relationship
- If CV is similar but approach is fundamentally different, submit to check intercept
- The goal is to REDUCE THE INTERCEPT, not improve CV
