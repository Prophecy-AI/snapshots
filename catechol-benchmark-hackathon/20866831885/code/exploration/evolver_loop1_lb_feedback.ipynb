{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72b2015",
   "metadata": {},
   "source": [
    "# Loop 1 LB Feedback Analysis\n",
    "\n",
    "**Critical Issue:** CV score 0.0111 vs LB score 0.0982 - a 9x gap!\n",
    "\n",
    "Possible causes:\n",
    "1. Notebook structure non-compliance (evaluator's concern)\n",
    "2. CV methodology mismatch with LB evaluation\n",
    "3. Data leakage in local CV\n",
    "4. Different evaluation metric on LB\n",
    "5. Distribution shift between train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff791ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.406746Z",
     "iopub.status.busy": "2026-01-07T23:20:47.406088Z",
     "iopub.status.idle": "2026-01-07T23:20:47.909878Z",
     "shell.execute_reply": "2026-01-07T23:20:47.909010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (1883, 8)\n",
      "\n",
      "Columns: ['id', 'index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']\n",
      "\n",
      "First 10 rows:\n",
      "   id  index  task  fold  row  target_1  target_2  target_3\n",
      "0   0      0     0     0    0  0.011080  0.010218  0.897487\n",
      "1   1      1     0     0    1  0.028685  0.025615  0.821114\n",
      "2   2      2     0     0    2  0.067066  0.059342  0.693974\n",
      "3   3      3     0     0    3  0.104709  0.081861  0.584283\n",
      "4   4      4     0     0    4  0.125673  0.089247  0.525820\n",
      "5   5      5     0     0    5  0.132799  0.090096  0.504345\n",
      "6   6      6     0     0    6  0.132781  0.090095  0.504405\n",
      "7   7      7     0     0    7  0.132757  0.090094  0.504482\n",
      "8   8      8     0     0    8  0.132820  0.090097  0.504278\n",
      "9   9      9     0     0    9  0.132820  0.090097  0.504278\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load our submission\n",
    "submission = pd.read_csv('/home/submission/submission.csv')\n",
    "print('Submission shape:', submission.shape)\n",
    "print('\\nColumns:', submission.columns.tolist())\n",
    "print('\\nFirst 10 rows:')\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d2e460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.912557Z",
     "iopub.status.busy": "2026-01-07T23:20:47.911923Z",
     "iopub.status.idle": "2026-01-07T23:20:47.921843Z",
     "shell.execute_reply": "2026-01-07T23:20:47.921201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task distribution:\n",
      "task\n",
      "1    1227\n",
      "0     656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold distribution for task 0 (single solvent):\n",
      "fold\n",
      "0     37\n",
      "1     37\n",
      "2     58\n",
      "3     59\n",
      "4     22\n",
      "5     18\n",
      "6     34\n",
      "7     41\n",
      "8     20\n",
      "9     22\n",
      "10    18\n",
      "11    18\n",
      "12    42\n",
      "13    18\n",
      "14    17\n",
      "15    22\n",
      "16     5\n",
      "17    16\n",
      "18    36\n",
      "19    18\n",
      "20    21\n",
      "21    22\n",
      "22    37\n",
      "23    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold distribution for task 1 (full data):\n",
      "fold\n",
      "0     122\n",
      "1     124\n",
      "2     104\n",
      "3     125\n",
      "4     125\n",
      "5     124\n",
      "6     125\n",
      "7     110\n",
      "8     127\n",
      "9      36\n",
      "10     34\n",
      "11     36\n",
      "12     35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the submission format\n",
    "print('Task distribution:')\n",
    "print(submission['task'].value_counts())\n",
    "\n",
    "print('\\nFold distribution for task 0 (single solvent):')\n",
    "print(submission[submission['task']==0]['fold'].value_counts().sort_index())\n",
    "\n",
    "print('\\nFold distribution for task 1 (full data):')\n",
    "print(submission[submission['task']==1]['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffd86ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.924055Z",
     "iopub.status.busy": "2026-01-07T23:20:47.923594Z",
     "iopub.status.idle": "2026-01-07T23:20:47.929800Z",
     "shell.execute_reply": "2026-01-07T23:20:47.929220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction statistics:\n",
      "\n",
      "target_1:\n",
      "  Min: 0.0001\n",
      "  Max: 0.4517\n",
      "  Mean: 0.1678\n",
      "  Std: 0.1435\n",
      "\n",
      "target_2:\n",
      "  Min: 0.0001\n",
      "  Max: 0.4423\n",
      "  Mean: 0.1434\n",
      "  Std: 0.1325\n",
      "\n",
      "target_3:\n",
      "  Min: 0.0000\n",
      "  Max: 0.9970\n",
      "  Mean: 0.5138\n",
      "  Std: 0.3497\n"
     ]
    }
   ],
   "source": [
    "# Check prediction ranges\n",
    "print('Prediction statistics:')\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f'\\n{col}:')\n",
    "    print(f'  Min: {submission[col].min():.4f}')\n",
    "    print(f'  Max: {submission[col].max():.4f}')\n",
    "    print(f'  Mean: {submission[col].mean():.4f}')\n",
    "    print(f'  Std: {submission[col].std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e921bc2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.931558Z",
     "iopub.status.busy": "2026-01-07T23:20:47.931316Z",
     "iopub.status.idle": "2026-01-07T23:20:47.952559Z",
     "shell.execute_reply": "2026-01-07T23:20:47.951953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data shape: (656, 13)\n",
      "Full data shape: (1227, 19)\n",
      "\n",
      "Target columns in single data: ['SM', 'Product 2', 'Product 3']\n",
      "Target columns in full data: ['SM', 'Product 2', 'Product 3']\n"
     ]
    }
   ],
   "source": [
    "# Load actual data to compare\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "single_data = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "full_data = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('Single solvent data shape:', single_data.shape)\n",
    "print('Full data shape:', full_data.shape)\n",
    "\n",
    "# Target columns\n",
    "print('\\nTarget columns in single data:', [c for c in single_data.columns if c in ['SM', 'Product 2', 'Product 3']])\n",
    "print('Target columns in full data:', [c for c in full_data.columns if c in ['SM', 'Product 2', 'Product 3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c978f7f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.954427Z",
     "iopub.status.busy": "2026-01-07T23:20:47.954200Z",
     "iopub.status.idle": "2026-01-07T23:20:47.959936Z",
     "shell.execute_reply": "2026-01-07T23:20:47.959367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target order in our submission:\n",
      "target_1 = Product 2\n",
      "target_2 = Product 3\n",
      "target_3 = SM\n",
      "\n",
      "Actual target statistics from single solvent data:\n",
      "Product 2: mean=0.1499, std=0.1431\n",
      "Product 3: mean=0.1234, std=0.1315\n",
      "SM: mean=0.5222, std=0.3602\n"
     ]
    }
   ],
   "source": [
    "# Check target order - this is critical!\n",
    "# Our submission has target_1, target_2, target_3\n",
    "# The template uses Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "\n",
    "print('Target order in our submission:')\n",
    "print('target_1 = Product 2')\n",
    "print('target_2 = Product 3')\n",
    "print('target_3 = SM')\n",
    "\n",
    "# Let's verify by checking the actual values\n",
    "print('\\nActual target statistics from single solvent data:')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'{col}: mean={single_data[col].mean():.4f}, std={single_data[col].std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452ce1a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.961859Z",
     "iopub.status.busy": "2026-01-07T23:20:47.961636Z",
     "iopub.status.idle": "2026-01-07T23:20:47.968152Z",
     "shell.execute_reply": "2026-01-07T23:20:47.967517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if our predictions make sense...\n",
      "\n",
      "Our predictions (target_3 = SM):\n",
      "  Mean: 0.5138\n",
      "  Actual SM mean: 0.5222\n",
      "\n",
      "Our predictions (target_1 = Product 2):\n",
      "  Mean: 0.1678\n",
      "  Actual Product 2 mean: 0.1499\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Check if our target order matches the expected order\n",
    "# The template loads Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "# So target_1 = Product 2, target_2 = Product 3, target_3 = SM\n",
    "\n",
    "# But wait - let me check what order we used in our model\n",
    "# In our baseline, we used: Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "# This should be correct!\n",
    "\n",
    "# Let's verify by checking the submission predictions vs actuals\n",
    "print('Checking if our predictions make sense...')\n",
    "print('\\nOur predictions (target_3 = SM):')\n",
    "print(f'  Mean: {submission[\"target_3\"].mean():.4f}')\n",
    "print(f'  Actual SM mean: {single_data[\"SM\"].mean():.4f}')\n",
    "\n",
    "print('\\nOur predictions (target_1 = Product 2):')\n",
    "print(f'  Mean: {submission[\"target_1\"].mean():.4f}')\n",
    "print(f'  Actual Product 2 mean: {single_data[\"Product 2\"].mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526f693b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.970279Z",
     "iopub.status.busy": "2026-01-07T23:20:47.970032Z",
     "iopub.status.idle": "2026-01-07T23:20:47.975911Z",
     "shell.execute_reply": "2026-01-07T23:20:47.975272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for TARGET_LABELS in utils.py...\n",
      "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
      "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
      "    Y = df[TARGET_LABELS]\n",
      "INPUT_LABELS_FULL_SOLVENT = [\n",
      "INPUT_LABELS_SINGLE_SOLVENT = [\n",
      "INPUT_LABELS_NUMERIC = [\n",
      "INPUT_LABELS_SINGLE_FEATURES = [\n",
      "INPUT_LABELS_FULL_FEATURES = [\n",
      "TARGET_LABELS = [\n"
     ]
    }
   ],
   "source": [
    "# Let me check the template's expected submission format more carefully\n",
    "# The template saves predictions in order: target_1, target_2, target_3\n",
    "# And the model predicts in the order of Y columns\n",
    "\n",
    "# In utils.py, TARGET_LABELS is defined - let me check\n",
    "import sys\n",
    "sys.path.insert(0, '/home/data')\n",
    "\n",
    "# Read utils.py to find TARGET_LABELS\n",
    "with open('/home/data/utils.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    print('Looking for TARGET_LABELS in utils.py...')\n",
    "    for line in content.split('\\n'):\n",
    "        if 'TARGET' in line or 'LABEL' in line:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8b78d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:47.977756Z",
     "iopub.status.busy": "2026-01-07T23:20:47.977522Z",
     "iopub.status.idle": "2026-01-07T23:20:47.983585Z",
     "shell.execute_reply": "2026-01-07T23:20:47.982974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalculating MSE in different ways...\n",
      "\n",
      "Method 1: Overall MSE (our method)\n",
      "Single: 0.010429, Full: 0.011429, Overall: 0.011081\n",
      "\n",
      "Method 2: Would need to recalculate per-fold MSE and average\n"
     ]
    }
   ],
   "source": [
    "# The key insight: The LB evaluation likely uses a different CV procedure\n",
    "# or the submission format is different from what we're producing\n",
    "\n",
    "# Let me check if the issue is with how we're computing MSE locally\n",
    "# vs how the LB computes it\n",
    "\n",
    "# Our local MSE calculation:\n",
    "# MSE = mean((actuals - predictions)^2)\n",
    "\n",
    "# But the LB might be computing it differently\n",
    "# For example, it might be computing MSE per fold and then averaging\n",
    "\n",
    "# Let's recalculate our MSE in different ways\n",
    "print('Recalculating MSE in different ways...')\n",
    "\n",
    "# Method 1: Overall MSE (what we did)\n",
    "print('\\nMethod 1: Overall MSE (our method)')\n",
    "print('Single: 0.010429, Full: 0.011429, Overall: 0.011081')\n",
    "\n",
    "# Method 2: Average MSE per fold\n",
    "print('\\nMethod 2: Would need to recalculate per-fold MSE and average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469321c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.009199Z",
     "iopub.status.busy": "2026-01-07T23:20:53.008491Z",
     "iopub.status.idle": "2026-01-07T23:20:53.013774Z",
     "shell.execute_reply": "2026-01-07T23:20:53.012988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL INSIGHT:\n",
      "LB score 0.0982 ≈ Reference kernel score 0.09831\n",
      "This suggests our model is working correctly!\n",
      "The issue is our LOCAL CV calculation is too optimistic.\n",
      "\n",
      "Possible causes:\n",
      "1. We might be computing MSE on training data instead of test data\n",
      "2. We might have data leakage in our CV\n",
      "3. The CV methodology might be different\n"
     ]
    }
   ],
   "source": [
    "# HYPOTHESIS: The LB score of 0.0982 is suspiciously close to the reference kernel's score of 0.09831\n",
    "# This suggests our submission might have been evaluated correctly, but our LOCAL CV is wrong!\n",
    "\n",
    "# The reference kernel (arrhenius-kinetics-tta) achieved LB 0.09831\n",
    "# Our submission got LB 0.0982 - almost identical!\n",
    "\n",
    "# This means:\n",
    "# 1. Our model is working correctly on the LB\n",
    "# 2. Our LOCAL CV calculation is WRONG - it's too optimistic\n",
    "\n",
    "# The issue is likely that we're computing MSE on the wrong data or in the wrong way\n",
    "\n",
    "print('CRITICAL INSIGHT:')\n",
    "print('LB score 0.0982 ≈ Reference kernel score 0.09831')\n",
    "print('This suggests our model is working correctly!')\n",
    "print('The issue is our LOCAL CV calculation is too optimistic.')\n",
    "print('')\n",
    "print('Possible causes:')\n",
    "print('1. We might be computing MSE on training data instead of test data')\n",
    "print('2. We might have data leakage in our CV')\n",
    "print('3. The CV methodology might be different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1448e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.016030Z",
     "iopub.status.busy": "2026-01-07T23:20:53.015555Z",
     "iopub.status.idle": "2026-01-07T23:20:53.024206Z",
     "shell.execute_reply": "2026-01-07T23:20:53.023489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY INSIGHT:\n",
      "The competition evaluates by RUNNING the CV procedure on Kaggle!\n",
      "This means our local CV predictions are not directly comparable.\n",
      "\n",
      "The LB score of 0.0982 is the ACTUAL CV score from running our model.\n",
      "Our local CV score of 0.0111 might be computed incorrectly.\n"
     ]
    }
   ],
   "source": [
    "# Let me check our baseline notebook to see if there's a bug\n",
    "# Looking at the code, I see we store actuals and predictions correctly\n",
    "# But let me verify the MSE calculation\n",
    "\n",
    "# The issue might be that we're computing MSE differently\n",
    "# Let's check if the LB uses a different metric\n",
    "\n",
    "# Actually, looking at the competition description, it says:\n",
    "# \"Submissions will be evaluated according to a cross-validation procedure\"\n",
    "# This means the LB runs the ENTIRE CV procedure, not just evaluates predictions\n",
    "\n",
    "print('KEY INSIGHT:')\n",
    "print('The competition evaluates by RUNNING the CV procedure on Kaggle!')\n",
    "print('This means our local CV predictions are not directly comparable.')\n",
    "print('')\n",
    "print('The LB score of 0.0982 is the ACTUAL CV score from running our model.')\n",
    "print('Our local CV score of 0.0111 might be computed incorrectly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a585d149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.026542Z",
     "iopub.status.busy": "2026-01-07T23:20:53.026273Z",
     "iopub.status.idle": "2026-01-07T23:20:53.034924Z",
     "shell.execute_reply": "2026-01-07T23:20:53.034226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent actuals shape: (656, 3)\n",
      "Single solvent predictions shape: (656, 3)\n",
      "\n",
      "Recalculated Single Solvent MSE: 0.109481\n"
     ]
    }
   ],
   "source": [
    "# Let me verify by checking if our predictions are reasonable\n",
    "# If our model is predicting well, the predictions should be close to actuals\n",
    "\n",
    "# Load actuals for single solvent\n",
    "single_actuals = single_data[['Product 2', 'Product 3', 'SM']].values\n",
    "print('Single solvent actuals shape:', single_actuals.shape)\n",
    "\n",
    "# Our predictions for single solvent (task 0)\n",
    "single_preds = submission[submission['task']==0][['target_1', 'target_2', 'target_3']].values\n",
    "print('Single solvent predictions shape:', single_preds.shape)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = np.mean((single_actuals - single_preds) ** 2)\n",
    "print(f'\\nRecalculated Single Solvent MSE: {mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cfc3806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.037539Z",
     "iopub.status.busy": "2026-01-07T23:20:53.036891Z",
     "iopub.status.idle": "2026-01-07T23:20:53.045433Z",
     "shell.execute_reply": "2026-01-07T23:20:53.044760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data actuals shape: (1227, 3)\n",
      "Full data predictions shape: (1227, 3)\n",
      "\n",
      "Recalculated Full Data MSE: 0.011429\n",
      "\n",
      "Overall MSE: 0.045588\n"
     ]
    }
   ],
   "source": [
    "# Same for full data\n",
    "full_actuals = full_data[['Product 2', 'Product 3', 'SM']].values\n",
    "print('Full data actuals shape:', full_actuals.shape)\n",
    "\n",
    "# Our predictions for full data (task 1)\n",
    "full_preds = submission[submission['task']==1][['target_1', 'target_2', 'target_3']].values\n",
    "print('Full data predictions shape:', full_preds.shape)\n",
    "\n",
    "# Calculate MSE\n",
    "mse_full = np.mean((full_actuals - full_preds) ** 2)\n",
    "print(f'\\nRecalculated Full Data MSE: {mse_full:.6f}')\n",
    "\n",
    "# Overall\n",
    "n_single = len(single_actuals)\n",
    "n_full = len(full_actuals)\n",
    "overall_mse = (mse * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "print(f'\\nOverall MSE: {overall_mse:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4cbf32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.048309Z",
     "iopub.status.busy": "2026-01-07T23:20:53.047683Z",
     "iopub.status.idle": "2026-01-07T23:20:53.063604Z",
     "shell.execute_reply": "2026-01-07T23:20:53.062945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking prediction order...\n",
      "\n",
      "Task 0 (single solvent):\n",
      "  Fold 0: 37 rows\n",
      "  Fold 1: 37 rows\n",
      "  Fold 2: 58 rows\n",
      "  Fold 3: 59 rows\n",
      "  Fold 4: 22 rows\n",
      "  Fold 5: 18 rows\n",
      "  Fold 6: 34 rows\n",
      "  Fold 7: 41 rows\n",
      "  Fold 8: 20 rows\n",
      "  Fold 9: 22 rows\n",
      "  Fold 10: 18 rows\n",
      "  Fold 11: 18 rows\n",
      "  Fold 12: 42 rows\n",
      "  Fold 13: 18 rows\n",
      "  Fold 14: 17 rows\n",
      "  Fold 15: 22 rows\n",
      "  Fold 16: 5 rows\n",
      "  Fold 17: 16 rows\n",
      "  Fold 18: 36 rows\n",
      "  Fold 19: 18 rows\n",
      "  Fold 20: 21 rows\n",
      "  Fold 21: 22 rows\n",
      "  Fold 22: 37 rows\n",
      "  Fold 23: 18 rows\n"
     ]
    }
   ],
   "source": [
    "# The recalculated MSE should match our original calculation\n",
    "# If it doesn't, there's a bug in our original code\n",
    "\n",
    "# Let me also check if the predictions are in the right order\n",
    "# The submission should have predictions for each fold in order\n",
    "\n",
    "print('Checking prediction order...')\n",
    "print('\\nTask 0 (single solvent):')\n",
    "for fold in range(24):\n",
    "    fold_data = submission[(submission['task']==0) & (submission['fold']==fold)]\n",
    "    print(f'  Fold {fold}: {len(fold_data)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e48a553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.065909Z",
     "iopub.status.busy": "2026-01-07T23:20:53.065300Z",
     "iopub.status.idle": "2026-01-07T23:20:53.083696Z",
     "shell.execute_reply": "2026-01-07T23:20:53.083015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying fold structure...\n",
      "\n",
      "Single solvent data by solvent:\n",
      "SOLVENT NAME\n",
      "1,1,1,3,3,3-Hexafluoropropan-2-ol     37\n",
      "2,2,2-Trifluoroethanol                37\n",
      "2-Methyltetrahydrofuran [2-MeTHF]     58\n",
      "Acetonitrile                          59\n",
      "Acetonitrile.Acetic Acid              22\n",
      "Butanone [MEK]                        18\n",
      "Cyclohexane                           34\n",
      "DMA [N,N-Dimethylacetamide]           41\n",
      "Decanol                               20\n",
      "Diethyl Ether [Ether]                 22\n",
      "Dihydrolevoglucosenone (Cyrene)       18\n",
      "Dimethyl Carbonate                    18\n",
      "Ethanol                               42\n",
      "Ethyl Acetate                         18\n",
      "Ethyl Lactate                         17\n",
      "Ethylene Glycol [1,2-Ethanediol]      22\n",
      "IPA [Propan-2-ol]                      5\n",
      "MTBE [tert-Butylmethylether]          16\n",
      "Methanol                              36\n",
      "Methyl Propionate                     18\n",
      "THF [Tetrahydrofuran]                 21\n",
      "Water.2,2,2-Trifluoroethanol          22\n",
      "Water.Acetonitrile                    37\n",
      "tert-Butanol [2-Methylpropan-2-ol]    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Submission rows per fold (task 0):\n",
      "  Fold 0: 37 rows\n",
      "  Fold 1: 37 rows\n",
      "  Fold 2: 58 rows\n",
      "  Fold 3: 59 rows\n",
      "  Fold 4: 22 rows\n",
      "  Fold 5: 18 rows\n",
      "  Fold 6: 34 rows\n",
      "  Fold 7: 41 rows\n",
      "  Fold 8: 20 rows\n",
      "  Fold 9: 22 rows\n",
      "  Fold 10: 18 rows\n",
      "  Fold 11: 18 rows\n",
      "  Fold 12: 42 rows\n",
      "  Fold 13: 18 rows\n",
      "  Fold 14: 17 rows\n",
      "  Fold 15: 22 rows\n",
      "  Fold 16: 5 rows\n",
      "  Fold 17: 16 rows\n",
      "  Fold 18: 36 rows\n",
      "  Fold 19: 18 rows\n",
      "  Fold 20: 21 rows\n",
      "  Fold 21: 22 rows\n",
      "  Fold 22: 37 rows\n",
      "  Fold 23: 18 rows\n"
     ]
    }
   ],
   "source": [
    "# Now I understand the issue!\n",
    "# The submission file contains predictions for EACH FOLD of the CV\n",
    "# But the actuals are the FULL dataset\n",
    "\n",
    "# The correct way to compute MSE is to match predictions to actuals BY FOLD\n",
    "# Each fold's predictions correspond to the test set for that fold\n",
    "\n",
    "# For single solvent CV (leave-one-solvent-out):\n",
    "# - Fold 0 predictions are for solvent 0's data\n",
    "# - Fold 1 predictions are for solvent 1's data\n",
    "# etc.\n",
    "\n",
    "# Let me verify this by checking the number of rows per fold\n",
    "print('Verifying fold structure...')\n",
    "print('\\nSingle solvent data by solvent:')\n",
    "solvent_counts = single_data['SOLVENT NAME'].value_counts().sort_index()\n",
    "print(solvent_counts)\n",
    "\n",
    "print('\\nSubmission rows per fold (task 0):')\n",
    "for fold in range(24):\n",
    "    fold_data = submission[(submission['task']==0) & (submission['fold']==fold)]\n",
    "    print(f'  Fold {fold}: {len(fold_data)} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2a184c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.085655Z",
     "iopub.status.busy": "2026-01-07T23:20:53.085374Z",
     "iopub.status.idle": "2026-01-07T23:20:53.111854Z",
     "shell.execute_reply": "2026-01-07T23:20:53.111194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solvents in order: ['1,1,1,3,3,3-Hexafluoropropan-2-ol', '2,2,2-Trifluoroethanol', '2-Methyltetrahydrofuran [2-MeTHF]', 'Acetonitrile', 'Acetonitrile.Acetic Acid'] ...\n",
      "Fold 0 (1,1,1,3,3,3-Hexafluoropropan-2-ol): solvent=37, submission=37 ✓\n",
      "Fold 1 (2,2,2-Trifluoroethanol): solvent=37, submission=37 ✓\n",
      "Fold 2 (2-Methyltetrahydrofuran [2-MeTHF]): solvent=58, submission=58 ✓\n",
      "Fold 3 (Acetonitrile): solvent=59, submission=59 ✓\n",
      "Fold 4 (Acetonitrile.Acetic Acid): solvent=22, submission=22 ✓\n",
      "Fold 5 (Butanone [MEK]): solvent=18, submission=18 ✓\n",
      "Fold 6 (Cyclohexane): solvent=34, submission=34 ✓\n",
      "Fold 7 (DMA [N,N-Dimethylacetamide]): solvent=41, submission=41 ✓\n",
      "Fold 8 (Decanol): solvent=20, submission=20 ✓\n",
      "Fold 9 (Diethyl Ether [Ether]): solvent=22, submission=22 ✓\n",
      "Fold 10 (Dihydrolevoglucosenone (Cyrene)): solvent=18, submission=18 ✓\n",
      "Fold 11 (Dimethyl Carbonate): solvent=18, submission=18 ✓\n",
      "Fold 12 (Ethanol): solvent=42, submission=42 ✓\n",
      "Fold 13 (Ethyl Acetate): solvent=18, submission=18 ✓\n",
      "Fold 14 (Ethyl Lactate): solvent=17, submission=17 ✓\n",
      "Fold 15 (Ethylene Glycol [1,2-Ethanediol]): solvent=22, submission=22 ✓\n",
      "Fold 16 (IPA [Propan-2-ol]): solvent=5, submission=5 ✓\n",
      "Fold 17 (MTBE [tert-Butylmethylether]): solvent=16, submission=16 ✓\n",
      "Fold 18 (Methanol): solvent=36, submission=36 ✓\n",
      "Fold 19 (Methyl Propionate): solvent=18, submission=18 ✓\n",
      "Fold 20 (THF [Tetrahydrofuran]): solvent=21, submission=21 ✓\n",
      "Fold 21 (Water.2,2,2-Trifluoroethanol): solvent=22, submission=22 ✓\n",
      "Fold 22 (Water.Acetonitrile): solvent=37, submission=37 ✓\n",
      "Fold 23 (tert-Butanol [2-Methylpropan-2-ol]): solvent=18, submission=18 ✓\n"
     ]
    }
   ],
   "source": [
    "# The fold sizes should match the solvent counts!\n",
    "# Let me verify this more carefully\n",
    "\n",
    "solvents = sorted(single_data['SOLVENT NAME'].unique())\n",
    "print('Solvents in order:', solvents[:5], '...')\n",
    "\n",
    "for i, solvent in enumerate(solvents):\n",
    "    solvent_count = len(single_data[single_data['SOLVENT NAME'] == solvent])\n",
    "    fold_count = len(submission[(submission['task']==0) & (submission['fold']==i)])\n",
    "    match = '✓' if solvent_count == fold_count else '✗'\n",
    "    print(f'Fold {i} ({solvent}): solvent={solvent_count}, submission={fold_count} {match}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c602a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.114354Z",
     "iopub.status.busy": "2026-01-07T23:20:53.113741Z",
     "iopub.status.idle": "2026-01-07T23:20:53.156127Z",
     "shell.execute_reply": "2026-01-07T23:20:53.155513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properly computed Single Solvent MSE: 0.010429\n"
     ]
    }
   ],
   "source": [
    "# Now let me properly compute the MSE by matching predictions to actuals\n",
    "# For each fold, the predictions correspond to the test set (one solvent)\n",
    "\n",
    "def compute_cv_mse_single():\n",
    "    solvents = sorted(single_data['SOLVENT NAME'].unique())\n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, solvent in enumerate(solvents):\n",
    "        # Get actuals for this solvent\n",
    "        mask = single_data['SOLVENT NAME'] == solvent\n",
    "        actuals = single_data[mask][['Product 2', 'Product 3', 'SM']].values\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        fold_preds = submission[(submission['task']==0) & (submission['fold']==fold_idx)]\n",
    "        preds = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(actuals)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    mse = np.mean((all_actuals - all_preds) ** 2)\n",
    "    return mse, all_preds, all_actuals\n",
    "\n",
    "mse_single, preds_single, actuals_single = compute_cv_mse_single()\n",
    "print(f'Properly computed Single Solvent MSE: {mse_single:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec4e0610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:53.158283Z",
     "iopub.status.busy": "2026-01-07T23:20:53.157694Z",
     "iopub.status.idle": "2026-01-07T23:20:53.193236Z",
     "shell.execute_reply": "2026-01-07T23:20:53.192382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Fold 0 mismatch: actuals=124, preds=122\n",
      "WARNING: Fold 1 mismatch: actuals=125, preds=124\n",
      "WARNING: Fold 2 mismatch: actuals=124, preds=104\n",
      "WARNING: Fold 4 mismatch: actuals=104, preds=125\n",
      "WARNING: Fold 5 mismatch: actuals=110, preds=124\n",
      "WARNING: Fold 6 mismatch: actuals=36, preds=125\n",
      "WARNING: Fold 7 mismatch: actuals=127, preds=110\n",
      "WARNING: Fold 8 mismatch: actuals=34, preds=127\n",
      "WARNING: Fold 9 mismatch: actuals=122, preds=36\n",
      "WARNING: Fold 10 mismatch: actuals=35, preds=34\n",
      "WARNING: Fold 11 mismatch: actuals=125, preds=36\n",
      "WARNING: Fold 12 mismatch: actuals=36, preds=35\n",
      "Properly computed Full Data MSE: 0.089905\n"
     ]
    }
   ],
   "source": [
    "# Now for full data (leave-one-ramp-out)\n",
    "def compute_cv_mse_full():\n",
    "    # Get unique ramps\n",
    "    ramps = full_data[['SOLVENT A NAME', 'SOLVENT B NAME']].drop_duplicates()\n",
    "    ramps = ramps.sort_values(['SOLVENT A NAME', 'SOLVENT B NAME']).reset_index(drop=True)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, (_, row) in enumerate(ramps.iterrows()):\n",
    "        # Get actuals for this ramp\n",
    "        mask = (full_data['SOLVENT A NAME'] == row['SOLVENT A NAME']) & \\\n",
    "               (full_data['SOLVENT B NAME'] == row['SOLVENT B NAME'])\n",
    "        actuals = full_data[mask][['Product 2', 'Product 3', 'SM']].values\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        fold_preds = submission[(submission['task']==1) & (submission['fold']==fold_idx)]\n",
    "        preds = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "        \n",
    "        if len(actuals) != len(preds):\n",
    "            print(f'WARNING: Fold {fold_idx} mismatch: actuals={len(actuals)}, preds={len(preds)}')\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_actuals.append(actuals)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    mse = np.mean((all_actuals - all_preds) ** 2)\n",
    "    return mse, all_preds, all_actuals\n",
    "\n",
    "mse_full, preds_full, actuals_full = compute_cv_mse_full()\n",
    "print(f'Properly computed Full Data MSE: {mse_full:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41fe9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:59.503330Z",
     "iopub.status.busy": "2026-01-07T23:20:59.503005Z",
     "iopub.status.idle": "2026-01-07T23:20:59.508494Z",
     "shell.execute_reply": "2026-01-07T23:20:59.507837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL VERIFICATION ===\n",
      "Single Solvent MSE: 0.010429 (n=656)\n",
      "Full Data MSE: 0.089905 (n=1227)\n",
      "Overall MSE: 0.062217\n",
      "\n",
      "LB Score: 0.0982\n",
      "Gap: 0.035983\n"
     ]
    }
   ],
   "source": [
    "# Overall MSE\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n=== FINAL VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nLB Score: 0.0982')\n",
    "print(f'Gap: {abs(overall_mse - 0.0982):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186b6d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:20:59.510579Z",
     "iopub.status.busy": "2026-01-07T23:20:59.510304Z",
     "iopub.status.idle": "2026-01-07T23:20:59.517876Z",
     "shell.execute_reply": "2026-01-07T23:20:59.516930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONCLUSION:\n",
      "MSE is 0.062217 - need to investigate further\n"
     ]
    }
   ],
   "source": [
    "# CONCLUSION:\n",
    "# If the properly computed MSE matches our original calculation (~0.011),\n",
    "# then the issue is that the LB evaluates differently.\n",
    "\n",
    "# If the properly computed MSE is closer to 0.0982,\n",
    "# then our original calculation was wrong.\n",
    "\n",
    "print('\\nCONCLUSION:')\n",
    "if abs(overall_mse - 0.0982) < 0.01:\n",
    "    print('Our properly computed MSE matches LB - original calculation was wrong!')\n",
    "elif abs(overall_mse - 0.011) < 0.01:\n",
    "    print('Our MSE calculation is correct - LB evaluates differently!')\n",
    "    print('This could be due to:')\n",
    "    print('1. Different random seeds on Kaggle')\n",
    "    print('2. Different PyTorch/NumPy versions')\n",
    "    print('3. Different GPU behavior')\n",
    "else:\n",
    "    print(f'MSE is {overall_mse:.6f} - need to investigate further')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc5ea71f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:21:26.748845Z",
     "iopub.status.busy": "2026-01-07T23:21:26.748082Z",
     "iopub.status.idle": "2026-01-07T23:21:26.773210Z",
     "shell.execute_reply": "2026-01-07T23:21:26.772488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramps in our submission (by fold size):\n",
      "  Fold 0: 122 rows\n",
      "  Fold 1: 124 rows\n",
      "  Fold 2: 104 rows\n",
      "  Fold 3: 125 rows\n",
      "  Fold 4: 125 rows\n",
      "  Fold 5: 124 rows\n",
      "  Fold 6: 125 rows\n",
      "  Fold 7: 110 rows\n",
      "  Fold 8: 127 rows\n",
      "  Fold 9: 36 rows\n",
      "  Fold 10: 34 rows\n",
      "  Fold 11: 36 rows\n",
      "  Fold 12: 35 rows\n",
      "\n",
      "Ramps in data (sorted by SOLVENT A NAME, SOLVENT B NAME):\n",
      "  Ramp 0 (1,1,1,3,3,3-Hexafluoropropan-2-ol + 2-Methyltetrahydrofuran [2-MeTHF]): 124 rows\n",
      "  Ramp 1 (2,2,2-Trifluoroethanol + Water.2,2,2-Trifluoroethanol): 125 rows\n",
      "  Ramp 2 (2-Methyltetrahydrofuran [2-MeTHF] + Diethyl Ether [Ether]): 124 rows\n",
      "  Ramp 3 (Acetonitrile + Acetonitrile.Acetic Acid): 125 rows\n",
      "  Ramp 4 (Cyclohexane + IPA [Propan-2-ol]): 104 rows\n",
      "  Ramp 5 (DMA [N,N-Dimethylacetamide] + Decanol): 110 rows\n",
      "  Ramp 6 (Dihydrolevoglucosenone (Cyrene) + Ethyl Acetate): 36 rows\n",
      "  Ramp 7 (Ethanol + THF [Tetrahydrofuran]): 127 rows\n",
      "  Ramp 8 (MTBE [tert-Butylmethylether] + Butanone [MEK]): 34 rows\n",
      "  Ramp 9 (Methanol + Ethylene Glycol [1,2-Ethanediol]): 122 rows\n",
      "  Ramp 10 (Methyl Propionate + Ethyl Lactate): 35 rows\n",
      "  Ramp 11 (Water.Acetonitrile + Acetonitrile): 125 rows\n",
      "  Ramp 12 (tert-Butanol [2-Methylpropan-2-ol] + Dimethyl Carbonate): 36 rows\n"
     ]
    }
   ],
   "source": [
    "# The issue is clear: fold ordering mismatch in full data!\n",
    "# Let me check how the ramps are ordered in our submission vs the data\n",
    "\n",
    "print('Ramps in our submission (by fold size):')\n",
    "for fold in range(13):\n",
    "    fold_size = len(submission[(submission['task']==1) & (submission['fold']==fold)])\n",
    "    print(f'  Fold {fold}: {fold_size} rows')\n",
    "\n",
    "print('\\nRamps in data (sorted by SOLVENT A NAME, SOLVENT B NAME):')\n",
    "ramps = full_data[['SOLVENT A NAME', 'SOLVENT B NAME']].drop_duplicates()\n",
    "ramps = ramps.sort_values(['SOLVENT A NAME', 'SOLVENT B NAME']).reset_index(drop=True)\n",
    "for i, (_, row) in enumerate(ramps.iterrows()):\n",
    "    mask = (full_data['SOLVENT A NAME'] == row['SOLVENT A NAME']) & \\\n",
    "           (full_data['SOLVENT B NAME'] == row['SOLVENT B NAME'])\n",
    "    count = mask.sum()\n",
    "    print(f'  Ramp {i} ({row[\"SOLVENT A NAME\"]} + {row[\"SOLVENT B NAME\"]}): {count} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d08948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:21:26.775224Z",
     "iopub.status.busy": "2026-01-07T23:21:26.774826Z",
     "iopub.status.idle": "2026-01-07T23:21:26.835835Z",
     "shell.execute_reply": "2026-01-07T23:21:26.835108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramps in order of appearance in full_data:\n",
      "  Ramp 0 (Methanol + Ethylene Glycol [1,2-Ethanediol]): 122 rows\n",
      "  Ramp 1 (1,1,1,3,3,3-Hexafluoropropan-2-ol + 2-Methyltetrahydrofuran [2-MeTHF]): 124 rows\n",
      "  Ramp 2 (Cyclohexane + IPA [Propan-2-ol]): 104 rows\n",
      "  Ramp 3 (Water.Acetonitrile + Acetonitrile): 125 rows\n",
      "  Ramp 4 (Acetonitrile + Acetonitrile.Acetic Acid): 125 rows\n",
      "  Ramp 5 (2-Methyltetrahydrofuran [2-MeTHF] + Diethyl Ether [Ether]): 124 rows\n",
      "  Ramp 6 (2,2,2-Trifluoroethanol + Water.2,2,2-Trifluoroethanol): 125 rows\n",
      "  Ramp 7 (DMA [N,N-Dimethylacetamide] + Decanol): 110 rows\n",
      "  Ramp 8 (Ethanol + THF [Tetrahydrofuran]): 127 rows\n",
      "  Ramp 9 (Dihydrolevoglucosenone (Cyrene) + Ethyl Acetate): 36 rows\n",
      "  Ramp 10 (MTBE [tert-Butylmethylether] + Butanone [MEK]): 34 rows\n",
      "  Ramp 11 (tert-Butanol [2-Methylpropan-2-ol] + Dimethyl Carbonate): 36 rows\n",
      "  Ramp 12 (Methyl Propionate + Ethyl Lactate): 35 rows\n"
     ]
    }
   ],
   "source": [
    "# The key insight: Our generate_leave_one_ramp_out_splits function might be ordering differently\n",
    "# Let me check how the competition's utils.py orders the ramps\n",
    "\n",
    "# Looking at our baseline code, we used:\n",
    "# ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "# for _, row in ramps.iterrows():  # This iterates in the ORDER they appear in the dataframe!\n",
    "\n",
    "# But the competition's utils.py might sort them differently\n",
    "# Let me check the order in which ramps appear in the data\n",
    "\n",
    "print('Ramps in order of appearance in full_data:')\n",
    "seen_ramps = []\n",
    "for i, row in full_data.iterrows():\n",
    "    ramp = (row['SOLVENT A NAME'], row['SOLVENT B NAME'])\n",
    "    if ramp not in seen_ramps:\n",
    "        seen_ramps.append(ramp)\n",
    "        mask = (full_data['SOLVENT A NAME'] == ramp[0]) & (full_data['SOLVENT B NAME'] == ramp[1])\n",
    "        count = mask.sum()\n",
    "        print(f'  Ramp {len(seen_ramps)-1} ({ramp[0]} + {ramp[1]}): {count} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2fa251c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:21:44.972043Z",
     "iopub.status.busy": "2026-01-07T23:21:44.971169Z",
     "iopub.status.idle": "2026-01-07T23:21:45.050241Z",
     "shell.execute_reply": "2026-01-07T23:21:45.049387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly computed Full Data MSE: 0.011429\n"
     ]
    }
   ],
   "source": [
    "# Now let me properly compute the MSE using the ORDER OF APPEARANCE\n",
    "def compute_cv_mse_full_correct():\n",
    "    # Get ramps in order of appearance\n",
    "    seen_ramps = []\n",
    "    for i, row in full_data.iterrows():\n",
    "        ramp = (row['SOLVENT A NAME'], row['SOLVENT B NAME'])\n",
    "        if ramp not in seen_ramps:\n",
    "            seen_ramps.append(ramp)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_actuals = []\n",
    "    \n",
    "    for fold_idx, ramp in enumerate(seen_ramps):\n",
    "        # Get actuals for this ramp\n",
    "        mask = (full_data['SOLVENT A NAME'] == ramp[0]) & (full_data['SOLVENT B NAME'] == ramp[1])\n",
    "        actuals = full_data[mask][['Product 2', 'Product 3', 'SM']].values\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        fold_preds = submission[(submission['task']==1) & (submission['fold']==fold_idx)]\n",
    "        preds = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "        \n",
    "        if len(actuals) != len(preds):\n",
    "            print(f'WARNING: Fold {fold_idx} mismatch: actuals={len(actuals)}, preds={len(preds)}')\n",
    "        else:\n",
    "            all_preds.append(preds)\n",
    "            all_actuals.append(actuals)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_actuals = np.vstack(all_actuals)\n",
    "    \n",
    "    mse = np.mean((all_actuals - all_preds) ** 2)\n",
    "    return mse, all_preds, all_actuals\n",
    "\n",
    "mse_full_correct, preds_full_correct, actuals_full_correct = compute_cv_mse_full_correct()\n",
    "print(f'Correctly computed Full Data MSE: {mse_full_correct:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8734f18c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:22:01.424860Z",
     "iopub.status.busy": "2026-01-07T23:22:01.424047Z",
     "iopub.status.idle": "2026-01-07T23:22:01.431683Z",
     "shell.execute_reply": "2026-01-07T23:22:01.430714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL SUMMARY ===\n",
      "Single Solvent MSE: 0.010429\n",
      "Full Data MSE: 0.011429\n",
      "Overall MSE: 0.011081\n",
      "\n",
      "LB Score: 0.0982\n",
      "CV-LB Gap: 0.087119\n",
      "\n",
      "=== DIAGNOSIS ===\n",
      "Our local CV calculation is CORRECT (0.0111).\n",
      "The LB score (0.0982) is MUCH WORSE.\n",
      "\n",
      "This means the LB is evaluating our model differently!\n",
      "Possible causes:\n",
      "1. The LB runs the model from scratch with different random seeds\n",
      "2. The LB uses different PyTorch/NumPy versions\n",
      "3. The LB has different GPU behavior\n",
      "4. Our model has high variance between runs\n",
      "\n",
      "The reference kernel (arrhenius-kinetics-tta) also got ~0.098 on LB.\n",
      "This suggests the LB evaluation is consistent, but our model\n",
      "performs differently when run on Kaggle vs locally.\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY\n",
    "print('=== FINAL SUMMARY ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f}')\n",
    "print(f'Full Data MSE: {mse_full_correct:.6f}')\n",
    "\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full_correct)\n",
    "overall_mse = (mse_single * n_single + mse_full_correct * n_full) / (n_single + n_full)\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "\n",
    "print(f'\\nLB Score: 0.0982')\n",
    "print(f'CV-LB Gap: {0.0982 - overall_mse:.6f}')\n",
    "\n",
    "print('\\n=== DIAGNOSIS ===')\n",
    "print('Our local CV calculation is CORRECT (0.0111).')\n",
    "print('The LB score (0.0982) is MUCH WORSE.')\n",
    "print('')\n",
    "print('This means the LB is evaluating our model differently!')\n",
    "print('Possible causes:')\n",
    "print('1. The LB runs the model from scratch with different random seeds')\n",
    "print('2. The LB uses different PyTorch/NumPy versions')\n",
    "print('3. The LB has different GPU behavior')\n",
    "print('4. Our model has high variance between runs')\n",
    "print('')\n",
    "print('The reference kernel (arrhenius-kinetics-tta) also got ~0.098 on LB.')\n",
    "print('This suggests the LB evaluation is consistent, but our model')\n",
    "print('performs differently when run on Kaggle vs locally.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
