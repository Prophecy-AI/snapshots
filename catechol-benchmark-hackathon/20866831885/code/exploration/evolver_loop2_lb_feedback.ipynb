{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2af822",
   "metadata": {},
   "source": [
    "# Loop 2 LB Feedback Analysis\n",
    "\n",
    "## Submission Results\n",
    "- **exp_000 (MLP)**: CV 0.0111 → LB 0.0982 (gap: -0.0871)\n",
    "- **exp_001 (LightGBM)**: CV 0.0123 → LB 0.1065 (gap: -0.0942)\n",
    "\n",
    "## Key Insight\n",
    "LightGBM performed WORSE on LB despite being deterministic. This disproves the hypothesis that model variance was the cause of the CV-LB gap.\n",
    "\n",
    "## What's Really Happening?\n",
    "The competition runs notebooks from scratch on Kaggle. The LB score IS the CV score computed on Kaggle. So why is there such a large gap?\n",
    "\n",
    "Possible causes:\n",
    "1. **Different random seeds** - Kaggle environment may use different seeds\n",
    "2. **Different data splits** - The CV splits might be computed differently\n",
    "3. **Notebook structure non-compliance** - Our notebooks may not follow the exact template\n",
    "4. **Environment differences** - Library versions, GPU vs CPU, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f671e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.243999Z",
     "iopub.status.busy": "2026-01-07T23:30:14.243695Z",
     "iopub.status.idle": "2026-01-07T23:30:14.648081Z",
     "shell.execute_reply": "2026-01-07T23:30:14.647486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing competition requirements...\n",
      "\n",
      "The competition states:\n",
      "- Last 3 cells must be EXACTLY as in template\n",
      "- Only model definition line can be changed\n",
      "- Same hyperparameters must be used across all folds\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the competition template structure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the template notebook to understand the exact structure required\n",
    "print(\"Analyzing competition requirements...\")\n",
    "print(\"\\nThe competition states:\")\n",
    "print(\"- Last 3 cells must be EXACTLY as in template\")\n",
    "print(\"- Only model definition line can be changed\")\n",
    "print(\"- Same hyperparameters must be used across all folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e48fe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.650428Z",
     "iopub.status.busy": "2026-01-07T23:30:14.649918Z",
     "iopub.status.idle": "2026-01-07T23:30:14.654212Z",
     "shell.execute_reply": "2026-01-07T23:30:14.653662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference kernel (arrhenius-kinetics-tta) LB: 0.09831\n",
      "Our MLP LB: 0.0982\n",
      "Our LightGBM LB: 0.1065\n",
      "\n",
      "Our MLP matches the reference kernel almost exactly!\n",
      "This suggests our MLP implementation is correct.\n",
      "\n",
      "LightGBM is worse - tree models may not generalize as well to unseen solvents.\n"
     ]
    }
   ],
   "source": [
    "# Let's check what the reference kernel achieved\n",
    "print(\"Reference kernel (arrhenius-kinetics-tta) LB: 0.09831\")\n",
    "print(\"Our MLP LB: 0.0982\")\n",
    "print(\"Our LightGBM LB: 0.1065\")\n",
    "print(\"\\nOur MLP matches the reference kernel almost exactly!\")\n",
    "print(\"This suggests our MLP implementation is correct.\")\n",
    "print(\"\\nLightGBM is worse - tree models may not generalize as well to unseen solvents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c910d12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.656103Z",
     "iopub.status.busy": "2026-01-07T23:30:14.655851Z",
     "iopub.status.idle": "2026-01-07T23:30:14.662180Z",
     "shell.execute_reply": "2026-01-07T23:30:14.661657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL REALIZATION:\n",
      "==================================================\n",
      "The LB score ~0.098 is the ACTUAL CV score when run on Kaggle.\n",
      "Our local CV of 0.011 is computed differently.\n",
      "\n",
      "Possible explanations:\n",
      "1. The competition may weight folds differently\n",
      "2. The competition may use a different metric\n",
      "3. Our local CV may have a bug (data leakage?)\n",
      "4. The competition may evaluate on held-out test data, not CV\n"
     ]
    }
   ],
   "source": [
    "# Key realization: The LB score IS the CV score on Kaggle\n",
    "# The gap is not due to variance - it's due to the evaluation metric\n",
    "# or the way the competition computes the score\n",
    "\n",
    "print(\"CRITICAL REALIZATION:\")\n",
    "print(\"=\"*50)\n",
    "print(\"The LB score ~0.098 is the ACTUAL CV score when run on Kaggle.\")\n",
    "print(\"Our local CV of 0.011 is computed differently.\")\n",
    "print(\"\\nPossible explanations:\")\n",
    "print(\"1. The competition may weight folds differently\")\n",
    "print(\"2. The competition may use a different metric\")\n",
    "print(\"3. Our local CV may have a bug (data leakage?)\")\n",
    "print(\"4. The competition may evaluate on held-out test data, not CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e281232b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.663948Z",
     "iopub.status.busy": "2026-01-07T23:30:14.663544Z",
     "iopub.status.idle": "2026-01-07T23:30:14.694718Z",
     "shell.execute_reply": "2026-01-07T23:30:14.694181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data: 656 samples, 24 solvents\n",
      "Full data: 1227 samples\n",
      "Unique ramps in full data: 13\n"
     ]
    }
   ],
   "source": [
    "# Let's re-examine our CV methodology\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "# Load data\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print(f\"Single solvent data: {len(df_single)} samples, {df_single['SOLVENT NAME'].nunique()} solvents\")\n",
    "print(f\"Full data: {len(df_full)} samples\")\n",
    "\n",
    "# Check unique ramps in full data\n",
    "ramps = df_full[['SOLVENT A NAME', 'SOLVENT B NAME']].drop_duplicates()\n",
    "print(f\"Unique ramps in full data: {len(ramps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4787555",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.696433Z",
     "iopub.status.busy": "2026-01-07T23:30:14.696243Z",
     "iopub.status.idle": "2026-01-07T23:30:14.700525Z",
     "shell.execute_reply": "2026-01-07T23:30:14.699974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Analysis:\n",
      "Target: 0.0333\n",
      "Best LB: 0.0982\n",
      "Gap to close: 0.0649\n",
      "\n",
      "To beat the target, we need ~66% improvement from current best.\n",
      "\n",
      "Strategies to explore:\n",
      "1. DRFP features (2048-dim) - reported MSE 0.0039 in GNN benchmarks\n",
      "2. Better model architecture (GNN with attention)\n",
      "3. Ensemble MLP + other models\n",
      "4. Feature engineering improvements\n"
     ]
    }
   ],
   "source": [
    "# The target is 0.0333 - this is achievable if we can get our local CV to match LB\n",
    "# Current best LB: 0.0982 (MLP)\n",
    "# Target: 0.0333\n",
    "# Gap to close: 0.0649\n",
    "\n",
    "print(\"Target Analysis:\")\n",
    "print(f\"Target: 0.0333\")\n",
    "print(f\"Best LB: 0.0982\")\n",
    "print(f\"Gap to close: {0.0982 - 0.0333:.4f}\")\n",
    "print(\"\\nTo beat the target, we need ~66% improvement from current best.\")\n",
    "print(\"\\nStrategies to explore:\")\n",
    "print(\"1. DRFP features (2048-dim) - reported MSE 0.0039 in GNN benchmarks\")\n",
    "print(\"2. Better model architecture (GNN with attention)\")\n",
    "print(\"3. Ensemble MLP + other models\")\n",
    "print(\"4. Feature engineering improvements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e132786f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.702236Z",
     "iopub.status.busy": "2026-01-07T23:30:14.702050Z",
     "iopub.status.idle": "2026-01-07T23:30:14.714146Z",
     "shell.execute_reply": "2026-01-07T23:30:14.713642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (1883, 8)\n",
      "   id  index  task  fold  row  target_1  target_2  target_3\n",
      "0   0      0     0     0    0  0.006372  0.031107  0.860582\n",
      "1   1      1     0     0    1  0.022224  0.035327  0.823031\n",
      "2   2      2     0     0    2  0.027948  0.040687  0.816167\n",
      "3   3      3     0     0    3  0.051174  0.062100  0.720941\n",
      "4   4      4     0     0    4  0.064178  0.090988  0.613215\n"
     ]
    }
   ],
   "source": [
    "# Let's check if there's something wrong with our CV calculation\n",
    "# by examining the actual predictions vs actuals\n",
    "\n",
    "# Load our submission\n",
    "submission = pd.read_csv('/home/submission/submission.csv')\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(submission.head())\n",
    "\n",
    "# The submission has predictions but not actuals\n",
    "# We need to verify our CV is computed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8279a722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.715783Z",
     "iopub.status.busy": "2026-01-07T23:30:14.715585Z",
     "iopub.status.idle": "2026-01-07T23:30:14.719343Z",
     "shell.execute_reply": "2026-01-07T23:30:14.718838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition Evaluation Structure:\n",
      "==================================================\n",
      "1. Task 0: Single solvent - 24 folds (leave-one-solvent-out)\n",
      "2. Task 1: Full data - 13 folds (leave-one-ramp-out)\n",
      "\n",
      "The submission.csv contains predictions for all folds.\n",
      "The competition computes MSE by comparing to actual values.\n",
      "\n",
      "Our local CV computes MSE during training.\n",
      "If these match, the scores should be identical.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: The competition template shows the exact evaluation structure\n",
    "# Let's understand what the competition actually evaluates\n",
    "\n",
    "print(\"Competition Evaluation Structure:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Task 0: Single solvent - 24 folds (leave-one-solvent-out)\")\n",
    "print(\"2. Task 1: Full data - 13 folds (leave-one-ramp-out)\")\n",
    "print(\"\\nThe submission.csv contains predictions for all folds.\")\n",
    "print(\"The competition computes MSE by comparing to actual values.\")\n",
    "print(\"\\nOur local CV computes MSE during training.\")\n",
    "print(\"If these match, the scores should be identical.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eac4312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.721105Z",
     "iopub.status.busy": "2026-01-07T23:30:14.720914Z",
     "iopub.status.idle": "2026-01-07T23:30:14.725760Z",
     "shell.execute_reply": "2026-01-07T23:30:14.725235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Comparison:\n",
      "Reference baseline LB: 0.098\n",
      "Target: 0.0333 (3x better than baseline)\n",
      "GNN benchmark: 0.0039 (25x better than baseline)\n",
      "\n",
      "The GNN benchmark used:\n",
      "- DRFP features (2048-dim)\n",
      "- Graph Attention Networks\n",
      "- Mixture-aware encodings\n",
      "\n",
      "We should try DRFP features with our models.\n"
     ]
    }
   ],
   "source": [
    "# Key insight: The reference kernel achieves 0.098 on LB\n",
    "# This is the baseline performance for this problem\n",
    "# \n",
    "# The target of 0.0333 is ~3x better than baseline\n",
    "# This suggests significant improvements are possible\n",
    "#\n",
    "# The GNN benchmark achieved 0.0039 - that's ~25x better than baseline!\n",
    "# This confirms that better approaches exist\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Reference baseline LB: 0.098\")\n",
    "print(f\"Target: 0.0333 (3x better than baseline)\")\n",
    "print(f\"GNN benchmark: 0.0039 (25x better than baseline)\")\n",
    "print(\"\\nThe GNN benchmark used:\")\n",
    "print(\"- DRFP features (2048-dim)\")\n",
    "print(\"- Graph Attention Networks\")\n",
    "print(\"- Mixture-aware encodings\")\n",
    "print(\"\\nWe should try DRFP features with our models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b1ad79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.727689Z",
     "iopub.status.busy": "2026-01-07T23:30:14.727313Z",
     "iopub.status.idle": "2026-01-07T23:30:14.767644Z",
     "shell.execute_reply": "2026-01-07T23:30:14.767090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP shape: (24, 2048)\n",
      "Sparsity: 97.43%\n",
      "\n",
      "Solvents with DRFP: ['Methanol', 'Ethylene Glycol [1,2-Ethanediol]', '1,1,1,3,3,3-Hexafluoropropan-2-ol', '2-Methyltetrahydrofuran [2-MeTHF]', 'Cyclohexane', 'IPA [Propan-2-ol]', 'Water.Acetonitrile', 'Acetonitrile', 'Acetonitrile.Acetic Acid', 'Diethyl Ether [Ether]', '2,2,2-Trifluoroethanol', 'Water.2,2,2-Trifluoroethanol', 'DMA [N,N-Dimethylacetamide]', 'Decanol', 'Ethanol', 'THF [Tetrahydrofuran]', 'Dihydrolevoglucosenone (Cyrene)', 'Ethyl Acetate', 'MTBE [tert-Butylmethylether]', 'Butanone [MEK]', 'tert-Butanol [2-Methylpropan-2-ol]', 'Dimethyl Carbonate', 'Methyl Propionate', 'Ethyl Lactate']\n"
     ]
    }
   ],
   "source": [
    "# Load DRFP features to understand them\n",
    "drfp = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "print(f\"DRFP shape: {drfp.shape}\")\n",
    "print(f\"Sparsity: {(drfp.values == 0).mean()*100:.2f}%\")\n",
    "print(f\"\\nSolvents with DRFP: {list(drfp.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef2fc89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T23:30:14.769444Z",
     "iopub.status.busy": "2026-01-07T23:30:14.768929Z",
     "iopub.status.idle": "2026-01-07T23:30:14.774037Z",
     "shell.execute_reply": "2026-01-07T23:30:14.773386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solvents in data: 24\n",
      "Solvents in DRFP: 24\n",
      "\n",
      "Missing from DRFP: set()\n",
      "Extra in DRFP: set()\n"
     ]
    }
   ],
   "source": [
    "# Check which solvents are in our data vs DRFP lookup\n",
    "single_solvents = set(df_single['SOLVENT NAME'].unique())\n",
    "full_solvents_a = set(df_full['SOLVENT A NAME'].unique())\n",
    "full_solvents_b = set(df_full['SOLVENT B NAME'].unique())\n",
    "all_data_solvents = single_solvents | full_solvents_a | full_solvents_b\n",
    "\n",
    "drfp_solvents = set(drfp.index)\n",
    "\n",
    "print(f\"Solvents in data: {len(all_data_solvents)}\")\n",
    "print(f\"Solvents in DRFP: {len(drfp_solvents)}\")\n",
    "print(f\"\\nMissing from DRFP: {all_data_solvents - drfp_solvents}\")\n",
    "print(f\"Extra in DRFP: {drfp_solvents - all_data_solvents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736cdc0",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1. **LightGBM hypothesis failed**: Determinism didn't help. LightGBM (0.1065) performed worse than MLP (0.0982).\n",
    "\n",
    "2. **MLP matches reference**: Our MLP achieves same LB as the reference kernel (0.098), confirming correct implementation.\n",
    "\n",
    "3. **Target is achievable**: GNN benchmark achieved 0.0039, which is far better than target 0.0333.\n",
    "\n",
    "4. **DRFP features are key**: The GNN benchmark used DRFP (2048-dim) features. We should try these.\n",
    "\n",
    "5. **Next steps**:\n",
    "   - Try DRFP features with MLP\n",
    "   - Try DRFP features with LightGBM\n",
    "   - Consider dimensionality reduction (PCA) for DRFP\n",
    "   - Explore ensemble approaches"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
