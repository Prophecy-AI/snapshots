{
  "query": "What techniques improve generalization for leave-one-out cross-validation when predicting chemical reaction yields?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Leave\u2011one\u2011out cross\u2011validation (LOOCV) can give overly optimistic error estimates when the underlying reaction\u2011yield model is sensitive to small changes in the training set.  Recent work therefore combines **data\u2011augmentation** with **test\u2011time augmentation** to enlarge the effective training distribution and to generate multiple perturbed versions of each reaction at inference time.  By fine\u2011tuning transformer\u2011based reaction models on augmented reaction strings, researchers showed that even with as little as\u202f2.5\u202f% of the original data the models outperform physics\u2011based baselines, and the variance across the augmented test predictions provides a reliable uncertainty estimate that correlates with actual errors\u3010https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101\u3011\u3010https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty\u3011.  \n\nAnother family of techniques focuses on **model simplicity and transfer learning**.  Random\u2011forest classifiers with a limited number of shallow trees were found to retain interpretability while improving out\u2011of\u2011sample generalization for yield prediction, especially when combined with **active transfer learning** that iteratively selects the most informative new experiments from a related reaction space\u3010https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b\u3011.  Complementary to this, the **honest LOOCV** framework separates the data used for hyper\u2011parameter tuning from the data used for error estimation, yielding a nearly unbiased post\u2011tuning generalization error and outperforming the traditional .632+ estimator for kernel\u2011based models\u3010https://myweb.uiowa.edu/boxwang/papers/honestCV.pdf\u3011.  \n\nFinally, **standardized CV protocols** and **probabilistic calibration** have been advocated for low\u2011data chemical datasets.  By applying the DIONYSUS calibration pipeline, probabilistic models achieve better calibrated predictions and more robust generalization across different LOOCV splits\u3010https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b\u3011, while the MatFold study highlights that overly simplistic CV (including na\u00efve LOOCV) can bias performance estimates and recommends systematic, stratified splitting schemes to obtain realistic generalization metrics\u3010https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d\u3011.  \n\nTogether, these strategies\u2014augmented training and test\u2011time data, simple yet expressive models with active transfer learning, honest error estimation, and rigorously designed CV splits\u2014are the most effective ways reported for improving generalization when using LOOCV to predict chemical reaction yields.",
      "url": ""
    },
    {
      "title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty",
      "text": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Data augmentation strategies to improve reaction yield predictions and estimate uncertainty\n26 November 2020, Version 1\nWorking Paper\n## Authors\n* [Philippe Schwaller](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Philippe%20Schwaller)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-3046-6576),\n* [Alain C. Vaucher](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Alain%20C.%20Vaucher)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0001-7554-0288),\n* [Teodoro Laino](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Teodoro%20Laino),\n* [Jean-Louis Reymond](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Jean-Louis%20Reymond)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-2724-2942)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nChemical reactions describe how precursor molecules react together and transform into products. The reaction yield describes the percentage of the precursors successfully transformed into products relative to the theoretical maximum. The prediction of reaction yields can help chemists navigate reaction space and accelerate the design of more effective routes. Here, we investigate the best-studied high-throughput experiment data set and show how data augmentation on chemical reactions can improve yield predictions' accuracy, even when only small data sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In this manuscript, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the random training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Keywords\n[SMILES](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=SMILES)\n[SMILES-Encoded](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=SMILES-Encoded)\n[chemical reactions](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=chemical%20reactions)\n[reaction yields](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=reaction%20yields)\n[data augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=data%20augmentation)\n[BERT](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=BERT)\n[Transformers](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Transformers)\n[Deep learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Deep%20learning)\n[regression](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=regression)\n[test-time augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=test-time%20augmentation)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\nNov 26, 2020 Version 1\n## Version Notes\nAccepted to NeurIPS 2020 Machine Learning for Molecules workshop.\n## Metrics\n15,686\n2,814\n22\nViews\nDownloads\nView article\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\nNC\n![ND logo](https://chemrxiv.org/engage/_nuxt/img/nd.7966b83.svg)\nND\nThe content is available under[CC BY NC ND 4.0[opens in a new tab]](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n## DOI\n[\n10.26434/chemrxiv.13286741.v1\nD O I: 10.26434/chemrxiv.13286741.v1 [opens in a new tab]](https://doi.org/10.26434/chemrxiv.13286741.v1)\n## Author\u2019s competing interest statement\nNo conflict of interest.\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101"
    },
    {
      "title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020",
      "text": "Authors Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020 https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty\nData augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020\nAuthors\n2020-12-06T00:00:00Z\n## Abstract\nChemical reactions describe how precursor molecules react together and transform into products. The reaction yield describes the percentage of the precursors successfully transformed into products relative to the theoretical maximum. The prediction of reaction yields can help chemists navigate reaction space and accelerate the design of more effective routes. Here, we investigate the best-studied high-throughput experiment data set and show how data augmentation on chemical reactions can improve yield predictions' accuracy, even when only small data sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In this manuscript, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the random training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Related\nWorkshop paper\n### [Monitoring the Impact of Wildfires on Tree Species with Deep Learning](https://research.ibm.com/publications/monitoring-the-impact-of-wildfires-on-tree-species-with-deep-learning)\nWang Zhou, Levente Klein\nNeurIPS 2020\nWorkshop paper\n### [Structure Discovery in (Causal) Proximal Graphical Event Models](https://research.ibm.com/publications/structure-discovery-in-causal-proximal-graphical-event-models)\nDebarun Bhattacharjya, Karthikeyan Shanmugam, et al.\nNeurIPS 2020\nWorkshop paper\n### [Differentially Private Stochastic Coordinate Descent](https://research.ibm.com/publications/differentially-private-stochastic-coordinate-descent--1)\nGeorgios Damaskinos, Celestine Mendler-D\u00fcnner, et al.\nNeurIPS 2020\nWorkshop paper\n### [Long-Range Seasonal Forecasting of 2m Temperature with Machine Learning](https://research.ibm.com/publications/long-range-seasonal-forecasting-of-2m-temperature-with-machine-learning)\nEtienne Eben Vos, Ashley Daniel Gritzman, et al.\nNeurIPS 2020\n[View all publications](https://research.ibm.com/publications)",
      "url": "https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "Predicting reaction conditions from limited data\nthrough active transfer learning\u2020\nEunjae Shim, a Joshua A. Kammeraad, ab Ziping Xu, b Ambuj Tewari, bc\nTim Cernak *ad and Paul M. Zimmerman *a\nTransfer and active learning have the potential to accelerate the development of new chemical reactions,\nusing prior data and new experiments to inform models that adapt to the target area of interest. This article\nshows how specifically tuned machine learning models, based on random forest classifiers, can expand the\napplicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First,\nmodel transfer is shown to be effective when reaction mechanisms and substrates are closely related, even\nwhen models are trained on relatively small numbers of data points. Then, a model simplification scheme is\ntested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen\nreagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit\nover random selection, an active transfer learning strategy is introduced to improve model predictions.\nSimple models, composed of a small number of decision trees with limited depths, are crucial for\nsecuring generalizability, interpretability, and performance of active transfer learning.\nIntroduction\nComputers are becoming increasingly capable of performing\nhigh-level chemical tasks.1\u20134 Machine learning approaches have\ndemonstrated viable retrosynthetic analyses,5\u20137 product predic\u0002tion,8\u201311 reaction condition suggestion,12\u201316 prediction of ster\u0002eoselectivity,17\u201320 regioselectivity,19,21\u201324 and reaction yield25,26\nand optimization of reaction conditions.27\u201330 These advances\nallow computers to assist synthesis planning for functional\nmolecules using well-established chemistry. For machine\nlearning to aid the development of new reactions, a model\nbased on established chemical knowledge must be able to\ngeneralize its predictions to reactivity that lies outside of the\ndataset. However, because most supervised learning algorithms\nlearn how features (e.g. reaction conditions) within a particular\ndomain relate to an outcome (e.g. yield), the model is not ex\u0002pected to be accurate outside its domain. This situation\nrequires chemists to consider other machine learning methods\nfor navigating new reactivity.\nExpert knowledge based on known reactions plays a central\nrole in the design of new reactions. The assumption that\nsubstrates with chemically similar reaction centers have trans\u0002ferable performance provides a plausible starting point for\nexperimental exploration. This concept of chemical similarity,\ntogether with literature data, guides expert chemists in the\ndevelopment of new reactions. Transfer learning, which\nassumes that data from a nearby domain, called the source\ndomain, can be leveraged to model the problem of interest in\na new domain, called the target domain,31 emulates a tactic\ncommonly employed by human chemists.\nTransfer learning is a promising strategy when limited data\nis available in the domain of interest, but a sizeable dataset is\navailable in a related domain.31,32 Models are \ue103rst created using\nthe source data, then transferred to the target domain using\nvarious algorithms.19,33\u201335 For new chemical targets where no\nlabeled data is available, the head start in predictivity a source\nmodel can provide becomes important. However, when a shi\ue09d\nin distribution of descriptor values occurs (e.g., descriptors\noutside of the original model ranges) in the target data, making\npredictions becomes challenging. For such a situation, the\nobjective of transfer learning becomes training a model that is\nas predictive in the target domain as possible.31,36 Toward this\nend, cross-validation is known to improve generalizability by\nproviding a procedure to avoid over\ue103tting on the training data.37\nThe reduction of generalization error, however, may not be\nsufficient outside the source domain. Accordingly, new\nmethods that enhance the applicability of a transferred model\nto new targets would be bene\ue103cial for reaction condition\nprediction.\nAnother machine learning method that can help tackle data\nscarcity is active learning. By making iterative queries of\na\nDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail:\npaulzim@umich.edu\nb\nDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\nc\nDepartment of Electrical Engineering and Computer Science, University of Michigan,\nAnn Arbor, MI, USA\nd\nDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA.\nE-mail: tcernak@med.umich.edu\n\u2020 Electronic supplementary information (ESI) available: Additional results. See\nhttps://doi.org/10.1039/d1sc06932b.\nCite this: Chem. Sci., 2022, 13, 6655\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 10th December 2021\nAccepted 10th May 2022\nDOI: 10.1039/d1sc06932b\nrsc.li/chemical-science\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2022, 13, 6655\u20136668 | 6655\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 11 May 2022. Downloaded on 8/27/2025 5:29:13 AM. This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence. View Article Online View Journal | View Issue\nlabeling a small number of datapoints, active learning updates\nmodels with knowledge from newly labeled data. As a result,\nexploration is guided into the most informative areas and\navoids collection of unnecessary data.38,39 Active learning is\ntherefore well-suited for reaction development, which greatly\nbene\ue103ts from efficient exploration and where chemists conduct\nthe next batch of reactions based on previous experimental\nresults. Based on this analogy, reaction optimization27,28 and\nreaction condition identi\ue103cation40 have been demonstrated to\nbene\ue103t from active learning. However, these prior works initiate\nexploration with randomly selected data points (Fig. 1A) which\ndoes not leverage prior knowledge, and therefore does not\nre\ue104ect how expert chemists initiate exploration. Initial search\ndirected by transfer learning could identify productive regions\nearly on, which in turn will help build more useful models for\nsubsequent active learning steps.\nTo align transfer and active learning closer to how expert\nchemists develop new reactions, appropriate chemical reaction\ndata is necessary.41 Available datasets42 that are o\ue09den used for\nmachine learning are overrepresented by positive reactions,\nfailing to re\ue104ect reactions with negative outcomes. On the other\nhand, reaction condition screening data of methodology\nreports\u2014which chemists o\ue09den refer to\u2014only constitute\na sparse subset of possible reagent combinations, making it\nhard for machine learning algorithms to extract meaningful\nknowledge.43\nHigh-throughput experimentation44\u201346 (HTE) data can \ue103ll\nthis gap. HTE provides reaction data16,25,27,47,48 with reduced\nvariations in outcome due to systematic experimentation. Pd\u0002catalyzed coupling data was therefore collected from reported\nwork using nanomole scale HTE in 1536 well plates.49\u201351 In the\ncurrent work, subsets of this data, classi\ue103ed by nucleophile type\nas shown in Fig. 2A, were selected to a dataset size of approxi\u0002mately 100 datapoints, which captured both positive and\nnegative reaction performance.\nReaction condition exploration could be made more efficient\nif algorithmic strategies could leverage prior knowledge. Toward\nthis goal, model transfer and its combination with active\nlearning were evaluated. Taking advantage of diverse campaigns,\nthis study will show that transferred models can be effective in\napplying prior reaction conditions to a new substrate type under\ncertain conditions. Next, the source model's ability to predict\nreaction conditions with new combinations of reagents will also\nbe evaluated. Lastly, challenging scenarios are considered where\nproductive reaction conditions for one class of substrate...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "",
      "text": "ORIGINAL ARTICLE\nHonest leave-one-out cross-validation for estimating post\u0002tuning generalization error\nBoxiang Wang1 | Hui Zou2\n1\nDepartment of Statistics and Actuarial\nScience, University of Iowa, Iowa City, IA,\n52242, USA\n2\nSchool of Statistics, University of Minnesota,\nMinneapolis, MN, 55455, USA\nCorrespondence\nHui Zou, School of Statistics, University of\nMinnesota, Minneapolis, MN 55455, USA.\nEmail: zouxx019@umn.edu\nFunding information\nNSF, Grant/Award Numbers: 1915-842,\n2015-120\nMany machine learning models have tuning parameters to be determined by the\ntraining data, and cross-validation (CV) is perhaps the most commonly used method\nfor selecting tuning parameters. This work concerns the problem of estimating the\ngeneralization error of a CV-tuned predictive model. We propose to use an honest\nleave-one-out cross-validation framework to produce a nearly unbiased estimator of\nthe post-tuning generalization error. By using the kernel support vector machine and\nthe kernel logistic regression as examples, we demonstrate that the honest leave\u0002one-out cross-validation has very competitive performance even when competing\nwith the state-of-the-art .632+ estimator.\nKEYWORDS\nbootstrap, prediction, resampling methods, statistical learning\n1 | INTRODUCTION\nMany modern machine learning models involve important tuning parameters to be determined by the data. For example, the number of boosting\niterations and the size of each tree are some of the tuning parameters in tree-based gradient boosting, and the penalization parameter is a tuning\nparameter in the kernel support vector machine (Cortes & Vapnik, 1995; Vapnik, 1995), the lasso regression model and ridge regression. Selecting\nthe right tuning parameter is referred to as tuning in the literature, and many statistical methods and theory have been devoted to the topic of\ntuning. Some popular approaches include information criterion-based methods such as Akaike information criterion (Akaike, 1973), Bayesian\ninformation criterion (Stein, 1981a), Stein's unbiased risk estimation (SURE, Stein, 1981b) or some resampling methods like cross-validation (CV,\ne.g., Allen, 1974; Arlot & Celisse, 2010; Lachenbruch & Mickey, 1968; Stone, 1974; Wahba & Wold, 1975) and bootstrap (Efron &\nTibshirani, 1994). Among those proposals, CV is perhaps the most popular technique for tuning in practice. In a standard implementation of CV,\none segments the training data into V roughly equal-sized subsets, trains the model on each (V \u0001 1)-fold ensemble and computes the prediction\nerror of the trained model on the remaining fold. Typical choices of V are the sample size n, 10, 5, corresponding to leave-one-out cross-validation\n(LOOCV), 10-fold and 5-fold CV, respectively. In practice, CV tuning selects the parameter that incurs the least CV error. The basic idea of CV is\nto obtain a natural estimate of the generalization error of a given model via data splitting. Actually, the LOOCV error is nearly unbiased in terms\nof estimation bias (Luntz & Brailovsky, 1969). In practice, 10-fold and 5-fold CV are much more popular due to reduced computational efforts,\nalthough they tend to have larger bias in terms of estimating the generalization error. As pointed out by Yang (2006), estimating the generalization\nerror and tuning the model are two related but very different tasks. In some settings, estimating the generalization error is more demanding while\nbetter estimation of the error does not necessarily lead to a better tuning parameter.\nOne objection of the LOOCV error comes from the high variance claim: there is a wide-spreading argument that LOOCV has a much larger\nvariance than 10-fold or 5-fold CV such that the LOOCV error is a statistically inferior estimator of the generalization error. The argument is more\nor less heuristic. Actually, some papers (e.g., Bengio & Grandvalet, 2004; Burman, 1989; Kohavi, 1995; Molinaro et al. 2005) have theoretically\nand numerically demonstrated that the variance of the LOOCV error tends to be similar to the variance of other V-fold CV errors in many applica\u0002tions; Zhang and Yang (2015) even claimed \u201cLOOCV in fact has the smallest variability\u201d for fixed models in the regression setting. Despite these\npapers, the misconception still widely exists; for example, see discussions in chapter 7.10 in the famous book, Hastie et al. (2009). In fact, Wang\nReceived: 9 June 2021 Revised: 6 August 2021 Accepted: 17 August 2021\nDOI: 10.1002/sta4.413\nStat. 2021;10:e413. wileyonlinelibrary.com/journal/sta4 \u00a9 2021 John Wiley & Sons, Ltd. 1 of 8\nhttps://doi.org/10.1002/sta4.413\nand Zou (2021) have shown that LOOCV does not have higher variance than 10-fold CV and 5-fold CV in the context of binary classification.\nHere, we show a graphical illustration. We fit the kernel support vector machine (SVM) on a data set drawn from a mixture Gaussian distribution\nwhose Bayes decision boundary is non-linear. The details of the data generating model are given in the caption of Figure 1, from which we\nobserve that (i) LOOCV has almost no bias in estimating the generalization error, while the bias largely increases as V decreases; (ii) the variance\nof the CV error exhibits very little difference among V \u00bc 5,10,n, but the variance is much larger in two-fold CV.\nA legitimate complaint of LOOCV arises from its expensive computation. The vanilla implementation of LOOCV requires n repeats of model\nfitting, which seems to be too expensive. Thus, many people prefer to use 10-fold or 5-fold CV over LOOCV. This claim can be traced back to the\ninvention of V-fold CV at the outset as a computational remedy of LOOCV (Geisser, 1975). However, the CV error depends on the way one splits\nthe data and hence would vary even if the same CV procedure is deployed with a different data split. Rodr\u00edguez et al. (2010) studied repeated V\u0002fold CV to stabilize the CV procedure, but the total computational cost becomes much more expensive. In contrast, LOOCV enjoys the advantage\nof having a deterministic data split scheme. Moreover, a smart implementation of LOOCV can make the total computation time much less than\nthat by the vanilla implementation. Let us take the ridge regression as an example. Golub et al. (1979) introduced a neat formula to allow for effi\u0002cient computation of the exact LOOCV error with basically the same cost as fitting a single ridge regression model. This nice result can be general\u0002ized to a class of linear smoothers that possess the self-stable property (Fan et al. 2020). As such, computational cost will not become a deciding\nfactor that prevents users from using LOOCV to estimate the generalization error in real applications. In Wang and Zou (2021), the neat formula\nfor ridge regression is extended to the large-margin classifiers and a new algorithm has been proposed for fast and exact LOOCV computation of\na kernel SVM or related kernel classifiers.\nThe above discussion focuses on a single given model and how to estimate its generalization error. In practice, we select a final model from a list\nof candidate models. Now, suppose that the final model is already selected and one is willing to use this model for predicting future unseen data. Nat\u0002urally, we would like to know its true accuracy. Statistically speaking, we aim to find a good estimator of the generalization error of the selected\nmodel. In particular, consider the final model chosen by CV (10-fold CV or LOOCV), how to estimate the generalization error of this CV-tuned model?\nIn a series of papers (e.g., Efron, 1983, 1986, 2004; Efron & Tibshirani, 1997), Efron studied the problem of estimating the generalization error\nof any given model and applied his methods to tuning. Conceptually, his results can be applied to a tuned model to obtain a good estimate of the\ngeneralization error of the tuned model. The caveat is to consider the tuning process as a part of the selected model. According to Efron, the\nstate-of-the-art method is the .632+ bootstrap estimator (Efron & Tibshirani, ...",
      "url": "https://myweb.uiowa.edu/boxwang/papers/honestCV.pdf"
    },
    {
      "title": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models",
      "text": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n# BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nEvan R. AntoniukShehtab ZamanTal Ben-NunLawrence Livermore National LaboratoryPeggy LiLawrence Livermore National LaboratoryJames DiffenderferLawrence Livermore National LaboratoryBusra SahinBinghamton University, School of ComputingObadiah SmolenskiBinghamton University, School of ComputingTim HsuLawrence Livermore National LaboratoryAnna M. HiszpanskiLawrence Livermore National LaboratoryKenneth ChiuBinghamton University, School of ComputingBhavya KailkhuraLawrence Livermore National LaboratoryBrian Van EssenLawrence Livermore National Laboratory\n(January 2025)\n###### Abstract\nAdvances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations.\nAlthough the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks.\nWe present BOOM,benchmarks forout-of-distributionmolecular property predictions\u2014a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.\n## 1Introduction\nMolecular discovery is the process by which novel molecular structures with desirable application-specific properties are identified. Given the immense total search space of hypothetical small molecules enumerating approximately106010^{60}hypothetical molecules and 100 billion enumerated molecules, molecule discovery has increasingly relied upon machine learning models to efficiently navigate this space.> [\n[> 1\n](https://arxiv.org/html/2505.01912v1#bib.bib1)> , [> 2\n](https://arxiv.org/html/2505.01912v1#bib.bib2)> , [> 3\n](https://arxiv.org/html/2505.01912v1#bib.bib3)> ]\nTypically, molecule discovery is performed by first training a machine learning (ML) model on a molecule property dataset to learn the property-structure relationship. Then, this trained ML model is used to discover new molecules either by screening a list of enumerated molecules or by guiding a generative model towards molecules of interest> [\n[> 4\n](https://arxiv.org/html/2505.01912v1#bib.bib4)> ]\n.\nMolecule discovery is inherently an out-of-distribution (OOD) prediction problem. For the discovered molecules to constitute an exciting chemical discovery, the molecules need to either i) exhibit properties that extrapolate beyond those of the known molecules in the training dataset, or ii) possess a new chemical substructure that was previously not considered for the application of interest. In either case, the success of the molecule discovery campaign is dependent on the machine learning model\u2019s ability to make accurate predictions on samples that do not follow the same distribution as the known molecules (training data).\nDespite the importance of OOD performance to the problem of molecule discovery, the OOD performance of commonly used ML models for molecular property prediction has yet to be systematically explored. The majority of the standardized benchmarks used to assess the performance of chemical property prediction models do not include evaluations of model performance in the case where the test set is drawn from a different distribution than the training data. As a result of the lack of OOD chemistry benchmarks, the development of chemistry ML models are currently driven primarily by maximizing in-distribution performance, which may be hurting model generalization. The lack of OOD chemistry benchmarks has also hindered our understanding of how to develop generalizable chemistry foundation models. Currently, there is little empirical knowledge about how choices regarding the pretraining task, model architecture, and/or dataset diversity impact the generalization performance of chemistry foundation models that are expected to generalize across all chemical systems.\nIn this work, we develop BOOM,benchmarks forout-of-distributionmolecular property predictions, a standardized benchmark for assessing the OOD generalization performance of molecule property prediction models.\n### 1.1Main Findings\nOur work consists of the following main contributions:\n* \u2022We develop a robust methodology for evaluating the performance of chemical property prediction models to extrapolate to property values beyond their training distribution. Notably, this methodology is developed in a general manner, allowing it to apply to any material property dataset regardless of the specific model architecture, material property, or chemical system.\n* \u2022We perform the first large-scale benchmarking of the OOD performance of state-of-the-art ML chemical property prediction models. Across 10 diverse OOD tasks and 12 ML models, we do not find any existing models that show strong OOD generalization across all tasks. We therefore put forth BOOM OOD property prediction as a frontier challenge for chemical foundation models.\n* \u2022Our work highlights insights into how pretraining strategies, model architecture, molecule representation, and data augmentation impact OOD performance. These findings point towards strategies for the chemistry community to achieve chemical foundation models with strong OOD generalization across all chemical systems.\n## 2BOOM Overview\nIn general, one can define OOD with respect to either the model inputs (holding out a region of chemical space as the OOD test split) or with respect to the model outputs (holding out a range of chemical property values). In this work, we adopt the latter approach of benchmarking the performance of the models to extrapolate to property values not seen in training. Following the OOD definitions outlined by Farquhar et al., we here define OOD as a complement distribution with respect to the targets> [\n[> 5\n](https://arxiv.org/html/2505.01912v1#bib.bib5)> , [> 6\n](https://arxiv.org/html/2505.01912v1#bib.bib6)> ]\n. Specifically, given a molecule property dataset of chemical structures and their numerical property values, we create our OOD test set to consist of numerical values on the tail ends of the numerical property distribution (see Figure[1](https://arxiv.org/html/2505.01912v1#S2.F1)). In this way, our OOD benchmarking is directly aligned with the molecule discovery task in that it allows us to evaluate the consistency of ML models to discover molecules with state-of-the-art properties that extrapolate beyond the training data.\n### 2.1Datasets\nOverall, BOOM consists of 10 unique molecular property datasets. We collect 8 molecular property datasets from the QM9 Dataset: isotropic polarizability...",
      "url": "https://arxiv.org/html/2505.01912v1"
    },
    {
      "title": "Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane",
      "text": "[Skip navigation](https://livrepository.liverpool.ac.uk/3170088/#main-content)\n# Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane\n* * *\nSiritanaratkul, Bhavin\n(2022)\n_Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane._\nDigital Chemical Engineering, 2.\np. 100013.\nAccess the full-text of this item by clicking on the [Open Access link.](https://doi.org/10.1016/j.dche.2022.100013)\nOfficial URL: [http://dx.doi.org/10.1016/j.dche.2022.100013](http://dx.doi.org/10.1016/j.dche.2022.100013)\n## Abstract\nProduct yields of catalytic reaction networks are dependent on many factors, encompassing both catalyst properties and reaction conditions. The oxidative coupling of methane (OCM) is a complex heterogeneous-homogeneous process, and the yield of the desired C2 products is non-linear with respect to reaction conditions. Herein, using two published datasets of OCM catalytic experimental results, I show that various machine learning (ML) algorithms can predict C2 yields from reaction conditions with a mean absolute error (MAE) of 0.5 \u2013 1.0 percentage points in the best case. However, complications arising from real-world applications should be anticipated, therefore I investigated the effects of training set size, added noise, and out-of-sample partitions on the performance of ML algorithms. These results provide insights into the generalizability of the algorithms as well as caveats into the applicability of ML to reaction yield prediction\n| Item Type: | Article |\n| Uncontrolled Keywords: | 46 Information and Computing Sciences, 4611 Machine Learning, Machine Learning and Artificial Intelligence |\n| Divisions: | Faculty of Science and Engineering > School of Physical Sciences |\n| Depositing User: | [Symplectic Admin](https://livrepository.liverpool.ac.uk/cgi/users/home?screen=User::View&userid=8684) |\n| Date Deposited: | 02 May 2023 10:54 |\n| Last Modified: | 21 Jun 2024 13:37 |\n| DOI: | [10.1016/j.dche.2022.100013](https://doi.org/10.1016/j.dche.2022.100013) |\n| Open Access URL: | [https://doi.org/10.1016/j.dche.2022.100013](https://doi.org/10.1016/j.dche.2022.100013) |\n| Related URLs: | - [Publisher](http://dx.doi.org/10.1016/j.dche.2022.100013) |\n| URI: | [https://livrepository.liverpool.ac.uk/id/eprint/3170088](https://livrepository.liverpool.ac.uk/id/eprint/3170088) |\n[Repository Staff](http://livrepository.liverpool.ac.uk/cgi/users/home?screen=EPrint%3A%3AView&eprintid=3170088)\n[Statistics](http://livrepository.liverpool.ac.uk/cgi/stats/report/eprint/3170088)\nAltmetric\n[![-](https://livrepository.liverpool.ac.uk/style/images/minus.png) Altmetric](https://livrepository.liverpool.ac.uk/3170088/)\n[![+](https://livrepository.liverpool.ac.uk/style/images/plus.png) Altmetric](https://livrepository.liverpool.ac.uk/3170088/)\nShare\n[![-](https://livrepository.liverpool.ac.uk/style/images/minus.png) Share](https://livrepository.liverpool.ac.uk/3170088/)\n[![+](https://livrepository.liverpool.ac.uk/style/images/plus.png) Share](https://livrepository.liverpool.ac.uk/3170088/)\n[![](https://livrepository.liverpool.ac.uk/images/Twitter_Social_Icon_Circle_Color.png)](https://twitter.com/share?url=https://livrepository.liverpool.ac.uk/3170088/&text=Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane)\n[![](https://livrepository.liverpool.ac.uk/images/iconfinder_social-facebook-2019-circle_4696483.png)](https://facebook.com/sharer/sharer.php?u=https://livrepository.liverpool.ac.uk/3170088&quote=Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane)\n[![](https://livrepository.liverpool.ac.uk/images/iconfinder_mail-icon_456095.png)](mailto://https:%2F%2Flivrepository.liverpool.ac.uk%2F3170088%2F?subject=Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane&body=Generalizability and limitations of machine learning for yield prediction of oxidative coupling of methane%0A%0Ahttps://livrepository.liverpool.ac.uk/3170088)\n[![](https://livrepository.liverpool.ac.uk/images/iconfinder_whatsapp_1632535.png)](whatsapp://send?text=https://livrepository.liverpool.ac.uk/3170088)\nCORE (COnnecting REpositories)\n[![-](https://livrepository.liverpool.ac.uk/style/images/minus.png) CORE (COnnecting REpositories)](https://livrepository.liverpool.ac.uk/3170088/)\n[![+](https://livrepository.liverpool.ac.uk/style/images/plus.png) CORE (COnnecting REpositories)](https://livrepository.liverpool.ac.uk/3170088/)\n* * *\n- [Repository Staff Access](https://livrepository.liverpool.ac.uk/cgi/users/home)\n* * *\nResearch Support,\u00a0University of Liverpool\nSydney Jones Library,\u00a0Abercromby SquareLiverpoolL69 3DA,\u00a0UK\n+44 (0)151 794 0000",
      "url": "https://livrepository.liverpool.ac.uk/3170088"
    }
  ]
}