{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:08.338005Z","iopub.execute_input":"2025-12-12T21:58:08.33825Z","iopub.status.idle":"2025-12-12T21:58:08.584181Z","shell.execute_reply.started":"2025-12-12T21:58:08.338229Z","shell.execute_reply":"2025-12-12T21:58:08.583361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES, load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:08.642261Z","iopub.execute_input":"2025-12-12T21:58:08.642638Z","iopub.status.idle":"2025-12-12T21:58:08.650721Z","shell.execute_reply.started":"2025-12-12T21:58:08.642621Z","shell.execute_reply":"2025-12-12T21:58:08.650115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        raise NotImplementedError\n\n    def featurize(X, Y):\n        raise NotImplementedError\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n\n    def train_model(self, X_train, y_train):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:08.806489Z","iopub.execute_input":"2025-12-12T21:58:08.806769Z","iopub.status.idle":"2025-12-12T21:58:08.8122Z","shell.execute_reply.started":"2025-12-12T21:58:08.806751Z","shell.execute_reply":"2025-12-12T21:58:08.810968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the cells below we create the base classes of the two main objects you must write for the competition. \n\nThe first thing to write is a SmilesFeaturizer, which will take the solvent molecules and create a machine-learning ready featurization of the molecule. Finding better ways of featurizing solvents is one of the goals of the hackathon, however, you can also skip this step and use the pre-computed featurizations given in the utils file. Further down, you can see a SmilesFeaturizer that loads all the precomputed representations. A **featurizer** object simply consists of:\n- An initialization function\n- A featurize function that takes \n\nThe second one being a **model** which has:\n- An initialization function, where the model internally defines which featurizer to use\n- A \"train_model\" which lets the model train on data given by X_train, y_train as pandas data-frames. \n- A \"predict\" which takes a data frame of test inputs and makes a prediction","metadata":{}},{"cell_type":"markdown","source":"In the next cell we define two featurizers, which allow you to use the pre-computed featurizations from the original benchmark paper. These are:\n\n- drfps\n- fragprints\n- acs_pca_descriptors\n- spange_descriptors\n\nYou can refer to the paper for more details on them. We also include the simple SMILES string featurization which can be chained into more complicated representations.\n\nThe first featurizer simply uses the features directly. The second one is expanded to featurize *mixed* solvents too, which is done by taking a weighted average of the two single-solvent features.\n\nWe also show how to write code for a simple multi-layer perceptron on the data.","metadata":{}},{"cell_type":"code","source":"class PerTargetEnsembleModel:\n    def __init__(self):\n        self.targets = [\"Product 2\", \"Product 3\", \"SM\"]\n        self.models = {}\n\n        for t in self.targets:\n            if t == \"SM\":\n                self.models[t] = [\n                    BetterCatecholModel(feature_table=\"acs_pca_descriptors\", base_type=\"hgb\"),\n                    BetterCatecholModel(feature_table=\"spange_descriptors\", base_type=\"hgb\"),\n                ]\n            else:\n                self.models[t] = [\n                    BetterCatecholModel(feature_table=\"acs_pca_descriptors\", base_type=\"etr\"),\n                    BetterCatecholModel(feature_table=\"spange_descriptors\", base_type=\"etr\"),\n                ]\n\n    def train_model(self, X, Y):\n        for t in self.targets:\n            y_single = Y[[t]]\n            for m in self.models[t]:\n                m.train_model(X, y_single)\n\n    def predict(self, X):\n        preds = []\n\n        for t in self.targets:\n            preds_t = []\n            for m in self.models[t]:\n                p = m.model.predict(m._build_X(X))\n                preds_t.append(p.reshape(-1, 1))\n\n            # average ensemble\n            pred_t = 0.65 * preds_t[0] + 0.35 * preds_t[1]\n            preds.append(pred_t)\n\n        pred = np.hstack(preds)\n        pred = np.clip(pred, 0, 1)\n\n        return torch.tensor(pred, dtype=torch.double)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:09.296498Z","iopub.execute_input":"2025-12-12T21:58:09.296772Z","iopub.status.idle":"2025-12-12T21:58:09.304081Z","shell.execute_reply.started":"2025-12-12T21:58:09.29675Z","shell.execute_reply":"2025-12-12T21:58:09.30314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntorch.set_default_dtype(torch.double)\n\n# ============================================================\n# Featurizers (TEMPLATE – DO NOT CHANGE LOGIC)\n# ============================================================\n\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    def __init__(self, features=\"spange_descriptors\"):\n        self.featurizer = load_features(features)\n        self.feats_dim = self.featurizer.shape[1] + 2\n\n    def featurize(self, X):\n        X_num = torch.tensor(X[INPUT_LABELS_NUMERIC].values)\n        X_sol = torch.tensor(self.featurizer.loc[X[\"SOLVENT NAME\"]].values)\n        return torch.cat([X_num, X_sol], dim=1)\n\n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    def __init__(self, features=\"spange_descriptors\"):\n        self.featurizer = load_features(features)\n        self.feats_dim = self.featurizer.shape[1] + 2\n\n    def featurize(self, X):\n        X_num = torch.tensor(X[INPUT_LABELS_NUMERIC].values)\n\n        A = self.featurizer.loc[X[\"SOLVENT A NAME\"]].values\n        B = self.featurizer.loc[X[\"SOLVENT B NAME\"]].values\n        frac_b = X[\"SolventB%\"].values.reshape(-1, 1)\n\n        mix = A * (1 - frac_b) + B * frac_b\n        X_mix = torch.tensor(mix)\n\n        return torch.cat([X_num, X_mix], dim=1)\n\n\n# ============================================================\n# TEMPLATE BASELINE MODEL (KEEP UNCHANGED)\n# ============================================================\n\nclass MLPModel(nn.Module, BaseModel):\n    def __init__(self, features=\"spange_descriptors\", hidden_dims=[64, 64], output_dim=3, data=\"single\"):\n        super().__init__()\n\n        self.featurizer = (\n            PrecomputedFeaturizer(features)\n            if data == \"single\"\n            else PrecomputedFeaturizerMixed(features)\n        )\n\n        layers = []\n        dim = self.featurizer.feats_dim\n        for h in hidden_dims:\n            layers += [nn.Linear(dim, h), nn.ReLU()]\n            dim = h\n\n        layers.append(nn.Linear(dim, output_dim))\n        self.net = nn.Sequential(*layers)\n\n    def train_model(self, X, Y, **kwargs):\n        X = self.featurizer.featurize(X)\n        Y = torch.tensor(Y.values)\n\n        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n        loss_fn = nn.MSELoss()\n\n        for _ in range(100):\n            opt.zero_grad()\n            loss = loss_fn(self.net(X), Y)\n            loss.backward()\n            opt.step()\n\n    def predict(self, X):\n        return self.net(self.featurizer.featurize(X))\n\n\n# ============================================================\n# CUSTOM SKLEARN MODEL (FIXED)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nclass BetterCatecholModel:\n    def __init__(self, feature_table=\"spange_descriptors\", base_type=\"hgb\"):\n        self.base_type = base_type\n        self.lookup = (\n            pd.read_csv(\n                f\"/kaggle/input/catechol-benchmark-hackathon/{feature_table}_lookup.csv\",\n                index_col=0,\n            )\n            .apply(pd.to_numeric, errors=\"coerce\")\n            .fillna(0)\n        )\n        self.model = None\n\n    def _vec(self, s):\n        return self.lookup.loc[s].values if s in self.lookup.index else np.zeros(self.lookup.shape[1])\n\n    def _build_X(self, X):\n        rt = X[\"Residence Time\"].values.reshape(-1, 1)\n        temp = X[\"Temperature\"].values.reshape(-1, 1)\n\n        if \"SOLVENT NAME\" in X.columns:\n            S = np.vstack([self._vec(s) for s in X[\"SOLVENT NAME\"]])\n            return np.hstack([rt, temp, S])\n\n        frac_b = X[\"SolventB%\"].values.reshape(-1, 1) / 100.0\n        A = np.vstack([self._vec(s) for s in X[\"SOLVENT A NAME\"]])\n        B = np.vstack([self._vec(s) for s in X[\"SOLVENT B NAME\"]])\n        mix = (1 - frac_b) * A + frac_b * B\n\n        return np.hstack([rt, temp, frac_b, mix])\n\n    def train_model(self, X, Y):\n        Xf = self._build_X(X)\n        y = Y.values\n\n        if self.base_type == \"hgb\":\n            base = HistGradientBoostingRegressor(\n                max_depth=7, max_iter=700, learning_rate=0.04\n            )\n        else:\n            base = ExtraTreesRegressor(\n                n_estimators=900,\n                min_samples_leaf=2,\n                random_state=42,\n                n_jobs=-1,\n            )\n\n        self.model = Pipeline(\n            [(\"scaler\", StandardScaler()), (\"reg\", MultiOutputRegressor(base))]\n        )\n        self.model.fit(Xf, y)\n\n    def predict(self, X):\n        pred = np.clip(self.model.predict(self._build_X(X)), 0, 1)\n        return torch.tensor(pred, dtype=torch.double)\n\n\n# ============================================================\n# PER-TARGET + HETEROGENEOUS ENSEMBLE\n# ============================================================\n\nclass PerTargetEnsembleModel:\n    def __init__(self):\n        self.targets = [\"Product 2\", \"Product 3\", \"SM\"]\n        self.models = {}\n\n        for t in self.targets:\n            if t == \"SM\":\n                self.models[t] = [\n                    BetterCatecholModel(\"acs_pca_descriptors\", \"hgb\"),\n                    BetterCatecholModel(\"spange_descriptors\", \"hgb\"),\n                ]\n            else:\n                self.models[t] = [\n                    BetterCatecholModel(\"acs_pca_descriptors\", \"etr\"),\n                    BetterCatecholModel(\"spange_descriptors\", \"etr\"),\n                ]\n\n    def train_model(self, X, Y):\n        for t in self.targets:\n            y_single = Y[[t]]\n            for m in self.models[t]:\n                m.train_model(X, y_single)\n\n    def predict(self, X):\n        preds = []\n\n        for t in self.targets:\n            p1 = self.models[t][0].model.predict(self.models[t][0]._build_X(X))\n            p2 = self.models[t][1].model.predict(self.models[t][1]._build_X(X))\n\n            pred_t = 0.65 * p1 + 0.35 * p2\n            preds.append(pred_t.reshape(-1, 1))\n\n        pred = np.clip(np.hstack(preds), 0, 1)\n        return torch.tensor(pred, dtype=torch.double)\n\n\n# ============================================================\n# TWO MODELS FOR CV\n# ============================================================\n\nmodel_single = PerTargetEnsembleModel()\nmodel_full = PerTargetEnsembleModel()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:09.46773Z","iopub.execute_input":"2025-12-12T21:58:09.467998Z","iopub.status.idle":"2025-12-12T21:58:13.997361Z","shell.execute_reply.started":"2025-12-12T21:58:09.467978Z","shell.execute_reply":"2025-12-12T21:58:13.996413Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From this point onward the cross-validation procedure is calculated. **For a submission to be valid the next three cells must be the final three of your submission, and you can only modify the lines where the models are defined.**","metadata":{}},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nimport tqdm\n\nX, Y = load_data(\"single_solvent\")\n\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = model_single # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_single_solvent = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T21:58:13.9984Z","iopub.execute_input":"2025-12-12T21:58:13.998593Z","iopub.status.idle":"2025-12-12T22:00:44.789047Z","shell.execute_reply.started":"2025-12-12T21:58:13.998579Z","shell.execute_reply":"2025-12-12T22:00:44.788331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nX, Y = load_data(\"full\")\n\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = model_full # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_full_data = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T22:00:44.789857Z","iopub.execute_input":"2025-12-12T22:00:44.790122Z","iopub.status.idle":"2025-12-12T22:02:17.488392Z","shell.execute_reply.started":"2025-12-12T22:00:44.790106Z","shell.execute_reply":"2025-12-12T22:02:17.48777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T22:02:17.489385Z","iopub.execute_input":"2025-12-12T22:02:17.489609Z","iopub.status.idle":"2025-12-12T22:02:17.508202Z","shell.execute_reply.started":"2025-12-12T22:02:17.489588Z","shell.execute_reply":"2025-12-12T22:02:17.50745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}