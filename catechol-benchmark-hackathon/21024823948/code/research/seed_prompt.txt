## Current Status
- Best CV score: 0.0083 from exp_030
- Best LB score: 0.0877 from exp_030
- Target: 0.0730 | Gap to target: 0.0147 (20.1% improvement needed)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.23 * CV + 0.0533 (R² = 0.98)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- **CRITICAL INSIGHT**: Intercept (0.0533) < Target (0.0730)
- **This means we CAN reach target by improving CV!**
- Required CV for target: (0.0730 - 0.0533) / 4.23 = **0.00466**
- Current best CV: 0.0083
- CV improvement needed: **43.9%** (from 0.0083 to 0.00466)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The non-linear mixture experiment was executed correctly.
- Evaluator's top priority: Try simpler feature sets (Spange only) like public kernels.
- Key concerns raised: Non-linear mixture features failed catastrophically (197% worse CV).
- **I AGREE with the evaluator**: The path forward is SIMPLER features, not more complex ones.
- The interaction features (140 extra) caused severe overfitting on the small dataset.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop52_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. CV-LB relationship is highly linear (R²=0.98) - improving CV WILL improve LB
  2. Public kernels use ONLY Spange descriptors (13 features) and achieve good results
  3. Our 140-feature approach may be overfitting compared to simpler approaches
  4. The target IS achievable with ~44% CV improvement

## Recommended Approaches (PRIORITY ORDER)

### 1. SIMPLER FEATURES - Spange Only (HIGHEST PRIORITY)
**Rationale**: Public kernels ("mixall", "System Malfunction V1") use only Spange descriptors (13 features) and achieve competitive results. Our 140-feature approach may be overfitting.

**Implementation**:
```python
class SpangeOnlyFeaturizer:
    def __init__(self, mixed=False):
        self.mixed = mixed
        self.spange_df = SPANGE_DF  # Only 13 features
        self.feats_dim = 5 + 13  # kinetics(5) + spange(13) = 18 features
    
    def featurize(self, X, flip=False):
        # Arrhenius kinetics features
        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)
        temp_c = X_vals[:, 1:2]
        time_m = X_vals[:, 0:1]
        temp_k = temp_c + 273.15
        inv_temp = 1000.0 / temp_k
        log_time = np.log(time_m + 1e-6)
        interaction = inv_temp * log_time
        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])
        
        if self.mixed:
            A = self.spange_df.loc[X["SOLVENT A NAME"]].values
            B = self.spange_df.loc[X["SOLVENT B NAME"]].values
            r = X["SolventB%"].values.reshape(-1, 1) / 100.0
            if flip:
                r = 1 - r
                A, B = B, A
            X_spange = A * (1 - r) + B * r  # Linear mixing only
        else:
            X_spange = self.spange_df.loc[X["SOLVENT NAME"]].values
        
        return np.hstack([X_kinetic, X_spange])
```

**Why this might work**:
- Simpler models generalize better on small datasets
- Public kernels achieve good results with this approach
- Reduces overfitting risk from 140 features to 18 features

### 2. DEEPER MLP with Spange Features
**Rationale**: With simpler features, we can afford a deeper network without overfitting.

**Implementation**:
```python
class DeeperMLP(nn.Module):
    def __init__(self, input_dim=18, hidden_dims=[128, 128, 64, 32], dropout=0.2):
        super().__init__()
        layers = [nn.BatchNorm1d(input_dim)]
        prev = input_dim
        for h in hidden_dims:
            layers += [nn.Linear(prev, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)]
            prev = h
        layers += [nn.Linear(prev, 3), nn.Sigmoid()]
        self.net = nn.Sequential(*layers)
```

### 3. ENSEMBLE with Spange-Only Features
**Rationale**: Combine multiple model types (MLP, LGBM, GP) all using simpler Spange-only features.

**Implementation**:
- GP with Matern kernel (works well with low-dimensional features)
- MLP ensemble (5 models with different seeds)
- LightGBM (gradient boosting)
- Weights: GP 0.2, MLP 0.5, LGBM 0.3

### 4. SE Attention Blocks (from "best-work-here" kernel)
**Rationale**: Squeeze-and-Excitation blocks can help the model focus on important features.

**Implementation**:
```python
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=4):
        super().__init__()
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)
    
    def forward(self, x):
        # Squeeze: global average pooling
        s = x.mean(dim=0, keepdim=True)
        # Excitation: FC -> ReLU -> FC -> Sigmoid
        s = torch.sigmoid(self.fc2(torch.relu(self.fc1(s))))
        return x * s
```

### 5. Per-Target Models
**Rationale**: Different targets (Product 2, Product 3, SM) may benefit from different architectures.

**Implementation**:
- SM target: Use simpler model (higher weight on GP)
- Product targets: Use deeper MLP

## What NOT to Try
- ❌ Non-linear mixture features with element-wise products (exp_051 failed catastrophically)
- ❌ Uncertainty weighting (exp_050 failed)
- ❌ Adding more features (DRFP, ACS) - causes overfitting
- ❌ Deep residual networks (exp_004 failed)
- ❌ Very complex architectures on small dataset

## Validation Notes
- Use Leave-One-Out CV for single solvents (24 folds)
- Use Leave-One-Ramp-Out CV for mixtures (13 folds)
- CV-LB relationship: LB = 4.23 * CV + 0.0533
- Expected LB for CV=0.00466: 0.0730 (target)

## Submissions Remaining: 5
- Use strategically to validate improvements
- Consider submitting if CV improves significantly (>5% improvement)
- Best CV not yet submitted: exp_032/exp_035 with CV 0.008194

## Key Insight
The path to the target is SIMPLER features, not more complex ones. Public kernels achieve good results with only 13 Spange features. Our 140-feature approach may be overfitting. Try Spange-only features with the best ensemble (GP + MLP + LGBM).