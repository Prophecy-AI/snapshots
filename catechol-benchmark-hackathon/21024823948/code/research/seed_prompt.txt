## Current Status
- Best CV score: 0.0083 from exp_030
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)
- Submissions remaining: 5

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.23 * CV + 0.0533 (R² = 0.98)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- **CRITICAL**: Intercept (0.0533) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0533) / 4.23 = **NEGATIVE** (-0.0044)
- **CONCLUSION**: We CANNOT reach target by improving CV alone. We MUST change the CV-LB relationship.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The uncertainty-weighted experiment was executed correctly.
- Evaluator's top priority: Implement adaptive ensemble with non-linear mixture features. **AGREE** - this is a promising direction.
- Key concerns raised:
  1. Uncertainty weighting FAILED (46% worse CV) - **CONFIRMED, will not retry**
  2. Only 5 submissions remaining - **AGREE, must be strategic**
  3. Public kernels use different techniques - **AGREE, will implement**
- Evaluator correctly identified that we need to learn from top public kernels.

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop51_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL model types (MLP, LGBM, XGB, GP, Ridge, k-NN, CatBoost) fall on the SAME CV-LB line
  2. The intercept (0.0533) represents extrapolation error to unseen solvents
  3. The test set likely contains solvents that are MORE different from training
  4. SM target is hardest to predict (highest variance)

## Recommended Approaches (Priority Order)

### PRIORITY 1: Non-linear Mixture Features
**Why**: The "best-work-here" kernel uses non-linear mixture features that capture solvent interaction effects. This may generalize better to unseen mixtures.

```python
# Non-linear mixture features (from "best-work-here" kernel)
def featurize_mixture(A, B, r):
    # Linear mixing + interaction term
    linear = A * (1 - r) + B * r
    interaction = 0.05 * A * B * r * (1 - r)  # Captures non-linear effects
    return linear + interaction
```

**Implementation**:
1. Add interaction term to mixture featurization
2. Test with best ensemble (GP 0.15 + MLP 0.55 + LGBM 0.3)
3. Compare CV to exp_030 (0.0083)

### PRIORITY 2: Adaptive Ensemble Weighting (Per-Fold)
**Why**: The "mixall" kernel uses adaptive ensemble weighting based on validation performance per fold. This may find better weight combinations.

```python
# Adaptive ensemble weighting (from "mixall" kernel)
def adaptive_ensemble(val_preds, val_scores, power=2.5):
    # Inverse MSE weighting with power scaling
    inv_scores = 1.0 / (np.array(val_scores) + 1e-8)
    weights = inv_scores ** power
    weights = weights / weights.sum()
    return weights
```

**Implementation**:
1. Train each model in ensemble
2. Compute validation MSE for each model on held-out fold
3. Use inverse MSE weighting with power scaling
4. Apply per-fold weights (not global weights)

### PRIORITY 3: Per-Target Ensemble Weights
**Why**: SM target is hardest to predict. Different targets may benefit from different model combinations.

```python
# Per-target weights
weights_per_target = {
    'Product 2': [0.2, 0.5, 0.3],  # GP, MLP, LGBM
    'Product 3': [0.2, 0.5, 0.3],
    'SM': [0.3, 0.4, 0.3],  # More GP weight for SM (uncertainty-aware)
}
```

### PRIORITY 4: Optuna Hyperparameter Optimization
**Why**: The "mixall" kernel uses Optuna for systematic hyperparameter search. May find configurations that generalize better.

**Implementation**:
1. Define search space for ensemble weights, hidden dims, dropout, etc.
2. Use GroupKFold (5 splits) for faster iteration
3. Optimize for validation MSE
4. Apply best params to Leave-One-Out CV

### PRIORITY 5: Solvent Clustering + Class-Specific Models
**Why**: Solvents can be grouped by chemical class (alcohols, ethers, esters, etc.). Class-specific models may generalize better within chemical families.

**Implementation**:
1. Cluster solvents by Spange descriptors or chemical class
2. Train separate models for each cluster
3. Use cluster membership to select model at inference

## What NOT to Try
- **Uncertainty weighting**: FAILED (46% worse CV). The approach of blending toward population mean hurts CV.
- **Deep residual networks**: FAILED (5x worse CV). Too complex for small dataset.
- **More model types on same features**: All fall on same CV-LB line. Won't change the intercept.
- **Pure GP**: FAILED (77% worse CV, same CV-LB line as other models).

## Validation Notes
- Use Leave-One-Out CV (24 folds for single solvent, 13 folds for full data)
- CV-LB relationship: LB = 4.23 * CV + 0.0533
- If CV improves but LB doesn't improve proportionally, the approach is NOT changing the intercept
- **KEY**: We need approaches that REDUCE THE INTERCEPT, not just improve CV

## Submission Strategy
With 5 submissions remaining:
1. **DO NOT submit exp_052** (uncertainty-weighted) - CV is 46% worse
2. Focus on approaches that might change the CV-LB relationship
3. Submit only if:
   - CV improves significantly (>5% better than 0.0083)
   - OR approach is fundamentally different (might have different CV-LB relationship)
4. Save at least 2 submissions for final attempts

## Key Insight from Analysis
The CV-LB relationship has R² = 0.98, meaning ALL our approaches fall on the SAME line. The intercept (0.0533) represents extrapolation error that no amount of model tuning can fix. We need approaches that:
1. Capture non-linear solvent interactions (mixture features)
2. Adapt to different solvent characteristics (per-fold weighting)
3. Use physics-based constraints that hold for all solvents

## Competition-Specific Notes
- Must follow template structure (last 3 cells unchanged)
- Only change model definition line
- Leave-One-Out CV is required (not GroupKFold)
- Same hyperparameters across all folds (no per-fold tuning)