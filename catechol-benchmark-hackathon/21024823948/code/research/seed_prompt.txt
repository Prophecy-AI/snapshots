## Current Status
- Best CV score: 0.008194 from exp_032 (GP 0.15 + MLP 0.55 + LGBM 0.3)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7%)
- Experiments completed: 51
- Submissions used: 13/5 remaining

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.23 × CV + 0.0533 (R² = 0.9807)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- **CRITICAL**: Intercept (0.0533) > Target (0.0347)
- Required CV for target: (0.0347 - 0.0533) / 4.23 = **-0.0044** (NEGATIVE = impossible!)
- **CONCLUSION**: The current paradigm CANNOT reach the target. We need a fundamentally different approach.

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The GroupKFold experiment was executed correctly.
- Evaluator's top priority: "Try GroupKFold (5 splits)" - **TESTED AND FAILED**. CV was 7.5% WORSE (0.008807 vs 0.008194).
- Key concerns raised:
  1. GroupKFold has less training data per fold → worse predictions - **CONFIRMED**
  2. The validation scheme is NOT the cause of the CV-LB gap - **CONFIRMED**
  3. The gap is STRUCTURAL due to distribution shift - **CONFIRMED**

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop50_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. ALL 13 submissions follow the SAME CV-LB line (R²=0.98)
  2. Mean absolute residual from line is only 0.0008 - no model deviates
  3. The intercept (0.0533) represents STRUCTURAL extrapolation error
  4. GroupKFold does NOT change the CV-LB relationship

## What We've Learned After 51 Experiments

### CONFIRMED: All approaches fall on the same CV-LB line
- MLP (various architectures): ON THE LINE
- LightGBM, XGBoost, CatBoost, RF: ON THE LINE
- Gaussian Process: ON THE LINE
- Ridge Regression: ON THE LINE
- GP+MLP+LGBM Ensemble: ON THE LINE
- Aggressive regularization: ON THE LINE
- GroupKFold validation: ON THE LINE (and WORSE CV)

### CONFIRMED: The problem is STRUCTURAL distribution shift
- The hidden test set contains solvents/conditions that are systematically harder
- No amount of model tuning can fix extrapolation error
- The intercept (0.0533) > target (0.0347) means current paradigm CANNOT reach target

## Recommended Approaches (PRIORITY ORDER)

### 1. **HIGHEST PRIORITY: Uncertainty-Weighted Predictions**
Use GP uncertainty to blend predictions toward population mean when extrapolating.

**Implementation:**
```python
class UncertaintyWeightedEnsemble:
    def __init__(self, data='single'):
        self.gp = GPWrapper(data=data)
        self.mlp = WeightedMLPEnsemble(data=data)
        self.lgbm = LGBMWrapper(data=data)
        # Population means from training data
        self.population_mean = None
    
    def train_model(self, X_train, y_train):
        self.gp.train_model(X_train, y_train)
        self.mlp.train_model(X_train, y_train)
        self.lgbm.train_model(X_train, y_train)
        # Store population mean for blending
        self.population_mean = y_train.mean().values
    
    def predict(self, X_test):
        # Get GP predictions with uncertainty
        gp_pred, gp_std = self.gp.predict_with_uncertainty(X_test)
        mlp_pred = self.mlp.predict(X_test)
        lgbm_pred = self.lgbm.predict(X_test)
        
        # Ensemble prediction
        ensemble_pred = 0.15 * gp_pred + 0.55 * mlp_pred + 0.3 * lgbm_pred
        
        # Normalize uncertainty to [0, 0.5] - max 50% blend toward mean
        uncertainty = np.clip(gp_std / gp_std.max(), 0, 0.5)
        
        # Blend toward population mean when uncertain
        final_pred = (1 - uncertainty.reshape(-1, 1)) * ensemble_pred + \
                     uncertainty.reshape(-1, 1) * self.population_mean
        
        return torch.clamp(torch.tensor(final_pred), 0, 1)
```

**Why this might work:**
- High uncertainty indicates extrapolation to unseen solvents
- Blending toward mean reduces extreme predictions
- This might reduce the intercept in CV-LB relationship
- GP naturally provides uncertainty estimates

### 2. **MEDIUM PRIORITY: Extrapolation Detection Features**
Add features measuring distance to training distribution.

**Implementation:**
```python
def add_extrapolation_features(X_train, X_test, featurizer):
    """Add features measuring distance to training distribution"""
    train_feats = featurizer.featurize(X_train)
    test_feats = featurizer.featurize(X_test)
    
    # Compute distance to nearest training sample
    from scipy.spatial.distance import cdist
    distances = cdist(test_feats, train_feats, metric='euclidean')
    min_distance = distances.min(axis=1)
    mean_distance = distances.mean(axis=1)
    
    # Compute distance to training centroid
    centroid = train_feats.mean(axis=0)
    centroid_distance = np.linalg.norm(test_feats - centroid, axis=1)
    
    # Add as features
    extra_feats = np.column_stack([min_distance, mean_distance, centroid_distance])
    return np.hstack([test_feats, extra_feats])
```

**Why this might work:**
- Model can learn to be conservative when extrapolating
- Features explicitly encode "how different is this from training"

### 3. **MEDIUM PRIORITY: Solvent Clustering / Class-Specific Models**
Group solvents by chemical class and use class-specific models.

**Implementation:**
```python
# Solvent classes based on chemical structure
SOLVENT_CLASSES = {
    'alcohols': ['Methanol', 'Ethanol', 'Propanol', 'Butanol', 'Pentanol', 'Hexanol', 'Heptanol', 'Octanol'],
    'ethers': ['THF', 'Dioxane', 'DME'],
    'esters': ['Ethyl Acetate', 'Methyl Acetate'],
    'amides': ['DMF', 'NMP', 'DMAc'],
    'nitriles': ['Acetonitrile', 'Propionitrile'],
    'ketones': ['Acetone', 'MEK'],
    'other': ['DMSO', 'Water', 'Toluene', 'DCM']
}

class ClassSpecificEnsemble:
    def __init__(self):
        self.class_models = {}
        self.default_model = None
    
    def train_model(self, X_train, y_train):
        # Train class-specific models
        for class_name, solvents in SOLVENT_CLASSES.items():
            mask = X_train['SOLVENT NAME'].isin(solvents)
            if mask.sum() > 0:
                model = GPMLPLGBMEnsemble(data='single')
                model.train_model(X_train[mask], y_train[mask])
                self.class_models[class_name] = model
        
        # Train default model on all data
        self.default_model = GPMLPLGBMEnsemble(data='single')
        self.default_model.train_model(X_train, y_train)
    
    def predict(self, X_test):
        # Predict using class-specific model if available
        predictions = []
        for idx, row in X_test.iterrows():
            solvent = row['SOLVENT NAME']
            model = self.default_model
            for class_name, solvents in SOLVENT_CLASSES.items():
                if solvent in solvents and class_name in self.class_models:
                    model = self.class_models[class_name]
                    break
            pred = model.predict(X_test.iloc[[idx]])
            predictions.append(pred)
        return torch.cat(predictions, dim=0)
```

### 4. **LOW PRIORITY: Optuna Hyperparameter Optimization**
The "mixall" kernel uses Optuna for hyperparameter optimization.

**Implementation:**
```python
import optuna

def objective(trial):
    # Hyperparameters to optimize
    gp_weight = trial.suggest_float('gp_weight', 0.05, 0.3)
    mlp_weight = trial.suggest_float('mlp_weight', 0.3, 0.7)
    lgbm_weight = 1 - gp_weight - mlp_weight
    
    hidden_dim = trial.suggest_int('hidden_dim', 16, 64)
    dropout = trial.suggest_float('dropout', 0.01, 0.2)
    
    # Train and evaluate
    model = GPMLPLGBMEnsemble(
        gp_weight=gp_weight,
        mlp_weight=mlp_weight,
        lgbm_weight=lgbm_weight,
        hidden_dims=[hidden_dim, hidden_dim//2],
        dropout=dropout
    )
    
    # Cross-validation
    cv_score = cross_validate(model, X, Y)
    return cv_score

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
```

## What NOT to Try
1. **GroupKFold validation** - TESTED, 7.5% worse CV
2. **More model types in ensemble** - exp_050 showed this doesn't help
3. **Deeper networks** - exp_004 showed this fails
4. **More regularization** - exp_041 showed this doesn't change CV-LB relationship
5. **Pure GP** - exp_042 showed this is on the same CV-LB line
6. **Stacking** - exp_045 showed this doesn't help
7. **Any approach that just improves CV** - The intercept problem means CV improvement alone won't reach target

## Validation Notes
- Current CV scheme: Leave-One-Solvent-Out (24 folds for single, 13 folds for full)
- The CV-LB relationship is highly predictable (R²=0.98)
- **CRITICAL**: The intercept (0.0533) > target (0.0347) means we CANNOT reach target with current approach
- **RECOMMENDATION**: Focus on approaches that might CHANGE the CV-LB relationship (reduce intercept)

## Submission Strategy (5 remaining)
1. **DO NOT SUBMIT exp_051 (GroupKFold)** - CV is 7.5% worse, no benefit expected
2. **Save submissions for approaches with DIFFERENT CV-LB relationship**
3. **Each submission should test a fundamentally different hypothesis**
4. **Priority order for submissions:**
   - Uncertainty-weighted predictions (if CV improves AND shows different pattern)
   - Extrapolation detection features (if shows promise)
   - Best CV model (exp_032) only if no better approach found

## Strategic Summary

After 51 experiments, we've definitively proven:
1. **ALL model types follow the SAME CV-LB line** (R²=0.98)
2. **The intercept (0.0533) > target (0.0347)** → current paradigm CANNOT reach target
3. **GroupKFold validation does NOT help** - it's actually worse
4. **The problem is STRUCTURAL distribution shift**, not model choice

**The only way to reach the target is to find an approach with a DIFFERENT CV-LB relationship.**

The most promising unexplored direction is **Uncertainty-Weighted Predictions** - using GP uncertainty to blend toward population mean when extrapolating. This might reduce the intercept by making conservative predictions on unseen solvents.

**NEXT EXPERIMENT**: Implement uncertainty-weighted predictions with GP uncertainty. If CV improves AND the CV-LB relationship changes, submit to test the hypothesis.
