## What I Understood

The junior researcher followed my suggestion to try physics-constrained normalization (SM + P2 + P3 = 1), hypothesizing that enforcing mass balance might improve generalization and reduce the CV-LB gap. They implemented post-processing normalization that divides predictions by their sum. The experiment discovered a **critical insight**: the actual targets do NOT sum to 1.0 (mean ~0.80, range 0.03-1.12), invalidating the mass balance assumption. The normalization made CV 91% worse (0.016180 vs 0.008465 baseline).

## Technical Execution Assessment

**Validation**: SOUND ✓
- Leave-one-solvent-out CV for single solvents (24 folds) correctly implemented
- Leave-one-ramp-out CV for mixtures (13 folds) correctly implemented
- TTA for mixtures properly implemented
- Scalers fitted only on training data per fold

**Leakage Risk**: None detected ✓
- Feature lookups are static (no target leakage)
- No data contamination between folds
- Proper train/test separation

**Score Integrity**: VERIFIED ✓
- Single Solvent MSE: 0.016141 (n=656)
- Full Data MSE: 0.016201 (n=1227)
- Overall MSE: 0.016180
- Normalization constraint verified (all predictions sum to exactly 1.0)

**Code Quality**: GOOD ✓
- Clean implementation of normalization post-processing
- Proper verification of both predictions and actual target sums
- Template compliance maintained
- Training time ~1 hour (reasonable)

Verdict: **TRUSTWORTHY** - Results are reliable, experiment was well-executed, and the negative result is informative.

## Strategic Assessment

**Approach Fit**: HYPOTHESIS DISPROVEN - VALUABLE LEARNING

This was a reasonable hypothesis to test. The discovery that actual targets do NOT sum to 1.0 is extremely valuable:
- Single Solvent: target sums range [0.0288, 1.0000], mean=0.7955, std=0.1942
- Full Data: target sums range [0.0112, 1.1233], mean=0.8035, std=0.2091

This makes chemical sense - there could be:
1. Other products not measured (side reactions)
2. Measurement error
3. Loss of material during reaction

**Why Normalization Failed**: Forcing predictions to sum to 1.0 when actuals average ~0.80 introduces systematic bias. The model is being penalized for predicting the correct total yield.

**Effort Allocation**: APPROPRIATE
This was a quick experiment (~1 hour) that tested a specific hypothesis. The negative result eliminates a potential approach and provides domain insight.

**What This Experiment Tells Us**:
1. Mass balance constraint (SM + P2 + P3 = 1) is NOT valid for this data
2. The actual yields average ~80% with significant variation
3. Post-processing normalization is NOT the solution
4. The CV-LB gap is NOT caused by violating mass balance

## What's Working

1. **Systematic hypothesis testing**: The researcher tested a specific hypothesis and learned from the negative result
2. **Verification of assumptions**: Checking actual target sums was excellent scientific practice
3. **Clean implementation**: The code is well-structured and template-compliant
4. **Documentation**: Clear explanation of hypothesis, implementation, and results

## Key Concerns

### CRITICAL: The Normalization Approach is Fundamentally Wrong

**Observation**: The actual targets do NOT sum to 1.0 (mean ~0.80, range 0.03-1.12).

**Why it matters**: This invalidates the mass balance assumption. Forcing predictions to sum to 1.0 introduces systematic error of ~20% on average.

**Implication**: DO NOT SUBMIT this experiment. It will perform significantly worse than exp_026.

### HIGH: The CV-LB Gap Remains Unsolved

**Observation**: After 29 experiments, the CV-LB relationship is still ~10x (LB = 4.22*CV + 0.0533).

**Why it matters**: The intercept (0.0533) is 3x higher than the target (0.01727). Even with perfect CV=0, the predicted LB would be 0.0533.

**What this suggests**: We need an approach that fundamentally changes the CV-LB relationship, not just improves CV.

### MEDIUM: Only 3 Submissions Remaining

**Observation**: 3 submissions left, best LB is 0.0888, target is 0.01727 (5.14x gap).

**Why it matters**: Each submission is precious. We need to be strategic.

**Suggestion**: Only submit if a new approach shows fundamentally different behavior.

## Unexplored Directions Worth Considering

Given the discovery that targets don't sum to 1.0, here are refined suggestions:

1. **Gaussian Process Regression**: The competition description mentions "imputing any missing values using a multi-task GP" - GPs might be expected or have different generalization properties. GPs naturally handle uncertainty and work well with small datasets.

2. **Soft constraint instead of hard normalization**: Instead of forcing sum=1, add a soft penalty term to the loss: `loss += lambda * (pred.sum() - 0.80)^2`. This regularizes toward the observed mean without forcing exact constraint.

3. **Predict total yield separately**: Train a model to predict total yield (SM+P2+P3), then predict ratios. This separates the two tasks.

4. **Ridge/Lasso regression**: Pure linear models might have lower variance and different CV-LB relationship. Worth trying as a baseline.

5. **Simpler ensemble**: Given that adding more models (exp_028) didn't help, try removing models. Maybe MLP alone or LightGBM alone performs better on LB.

6. **Different feature subsets**: The 145 features might be causing overfitting. Try aggressive feature selection (top 20-30 features by importance).

## Current State Summary

| Metric | Value |
|--------|-------|
| Best CV Score | 0.008465 (exp_026) |
| Best LB Score | 0.0888 (exp_026) |
| This Experiment CV | 0.016180 (91% WORSE) |
| Target | 0.01727 |
| Gap to Target | 5.14x |
| Submissions Remaining | 3 |

## Top Priority for Next Experiment

**DO NOT SUBMIT exp_029** - it's 91% worse than exp_026.

**RECOMMENDED: Try Gaussian Process Regression**

Rationale:
1. The competition description explicitly mentions GPs ("imputing any missing values using a multi-task GP")
2. GPs have fundamentally different generalization properties than neural networks
3. GPs naturally handle uncertainty and small datasets
4. GPs work well with leave-one-out CV (they can compute LOO predictions analytically)
5. This is a qualitatively different approach that might break the current CV-LB pattern

Implementation suggestion:
```python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern

# Use ARD kernel to learn feature relevance
kernel = 1.0 * Matern(length_scale=np.ones(n_features), nu=2.5) + WhiteKernel()

# Multi-output GP (one per target, or use GPyTorch for true multi-output)
gp_models = [GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5) 
             for _ in range(3)]
```

**Alternative: Aggressive feature selection**
- Use LightGBM feature importance to select top 20-30 features
- Simpler feature space might reduce overfitting and improve generalization

**THE TARGET IS REACHABLE.** The normalization experiment was a valuable negative result that eliminated one hypothesis. The CV-LB gap suggests we need a fundamentally different approach - GPs are a strong candidate because they're explicitly mentioned in the competition description and have different inductive biases than neural networks.
