{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3653a8d",
   "metadata": {},
   "source": [
    "# Loop 31 Analysis: Feature Selection with CatBoost\n",
    "\n",
    "**Objective**: Identify the most important features to reduce dimensionality and improve generalization (reduce CV-LB gap).\n",
    "\n",
    "**Hypothesis**: The current model (145+ features) is overfitting the small dataset (600-1200 samples). A compact feature set (10-20 features) will generalize better.\n",
    "\n",
    "**Method**:\n",
    "1. Load data (Single and Full).\n",
    "2. Train CatBoostRegressor.\n",
    "3. Analyze Feature Importance.\n",
    "4. Perform Recursive Feature Elimination (RFE).\n",
    "5. Compare CV scores of Full vs Reduced feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Loading\n",
    "DATA_PATH = '/home/data'\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "# Load Lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "# Load DRFP if available (simulating what was done in exp_003/026)\n",
    "# For this analysis, let's reconstruct the features used in exp_026 (Spange + Arrhenius + DRFP)\n",
    "# We need to check if we can easily generate DRFP here. \n",
    "# If not, we'll stick to Spange + Arrhenius first, as that's the core.\n",
    "# Actually, let's check if we have the DRFP file.\n",
    "import os\n",
    "if os.path.exists(f'{DATA_PATH}/drfp_descriptors.csv'):\n",
    "    DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfp_descriptors.csv', index_col=0)\n",
    "else:\n",
    "    DRFP_DF = None\n",
    "    print(\"DRFP file not found, skipping DRFP features for analysis\")\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (reusing logic from exp_026)\n",
    "class Featurizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        \n",
    "    def featurize(self, X):\n",
    "        # 1. Kinetics (Arrhenius)\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        feature_names = [\"Time\", \"Temp\", \"1000/T\", \"ln(t)\", \"1000/T*ln(t)\"]\n",
    "        \n",
    "        # 2. Spange\n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        spange_names = [f\"Spange_{i}\" for i in range(X_spange.shape[1])]\n",
    "            \n",
    "        # Combine\n",
    "        X_all = np.hstack([X_kinetic, X_spange])\n",
    "        names = feature_names + spange_names\n",
    "        \n",
    "        return X_all, names\n",
    "\n",
    "print(\"Featurizer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38be1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "featurizer = Featurizer(mixed=False)\n",
    "X_feat, feature_names = featurizer.featurize(X_single)\n",
    "y = Y_single.values\n",
    "\n",
    "print(f\"Features shape: {X_feat.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "\n",
    "# Train CatBoost to get feature importance\n",
    "# We'll train on Product 2 (Yield) as a proxy, or average over all targets\n",
    "model = CatBoostRegressor(iterations=500, depth=6, learning_rate=0.1, loss_function='RMSE', verbose=0, random_seed=42)\n",
    "\n",
    "# Fit on all data to get global importance\n",
    "model.fit(X_feat, y[:, 0]) # Target 1: Product 2\n",
    "\n",
    "importance = model.get_feature_importance()\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "print(\"\\nFeature Importance (Product 2):\")\n",
    "for f in range(X_feat.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feature_names[indices[f]], importance[indices[f]]))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance (CatBoost)\")\n",
    "plt.bar(range(X_feat.shape[1]), importance[indices], align=\"center\")\n",
    "plt.xticks(range(X_feat.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with CatBoost\n",
    "# We'll use sklearn's RFE with CatBoost as the estimator\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "print(\"\\nRunning RFE to find top 10 features...\")\n",
    "selector = RFE(estimator=CatBoostRegressor(iterations=100, verbose=0, random_seed=42), n_features_to_select=10, step=1)\n",
    "selector = selector.fit(X_feat, y[:, 0])\n",
    "\n",
    "print(\"Selected features:\")\n",
    "selected_indices = np.where(selector.support_)[0]\n",
    "for i in selected_indices:\n",
    "    print(f\"- {feature_names[i]}\")\n",
    "    \n",
    "# Evaluate CV with Top 10 vs All\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def evaluate_cv(X, y, name):\n",
    "    model = CatBoostRegressor(iterations=200, verbose=0, random_seed=42)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    mse = -scores.mean()\n",
    "    print(f\"{name} CV MSE: {mse:.6f}\")\n",
    "    return mse\n",
    "\n",
    "print(\"\\nComparing CV scores:\")\n",
    "mse_all = evaluate_cv(X_feat, y[:, 0], \"All Features (18)\")\n",
    "mse_top10 = evaluate_cv(X_feat[:, selected_indices], y[:, 0], \"Top 10 Features\")\n",
    "\n",
    "diff = (mse_top10 - mse_all) / mse_all * 100\n",
    "print(f\"Difference: {diff:.2f}% (Positive means Top 10 is worse)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
