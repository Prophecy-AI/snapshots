{
  "query": "How can I improve out-of-distribution generalization when my model has a systematic bias between cross-validation and test performance?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Out\u2011of\u2011distribution (OOD) generalization refers to a model\u2019s ability to maintain performance when the test data come from a different distribution than the data used for training and cross\u2011validation.  Empirically, many recent studies have shown that OOD accuracy is often tightly linked to in\u2011distribution (cross\u2011validation) accuracy, so a systematic gap between CV and test scores usually signals that the model is over\u2011fitting to spurious patterns that do not survive the shift\u202f\u2014\u202fa phenomenon documented across image, satellite\u2011imagery, and wildlife\u2011classification benchmarks\u202f([arxiv.org](https://arxiv.org/abs/2107.04649)).  Recognizing this correlation is the first step: if improving CV performance alone does not close the gap, you need methods that explicitly address distribution shift rather than merely scaling model capacity.\n\nA practical toolbox for reducing the CV\u2011test bias includes several complementary techniques that have been validated in the last few years.  **Calibrated ensembles** combine diverse predictors and adjust their confidence, which has been shown to trade off accuracy and robustness more favorably under shift\u202f([arxiv.org](https://arxiv.org/abs/2207.08977)).  **Uniformity\u2011guided fine\u2011tuning and rebalancing** encourage the feature space to be evenly spread across classes and domains, mitigating reliance on narrow cues\u202f([arxiv.org](https://arxiv.org/abs/2410.05980)).  **Selective augmentation (LISA)** uses a mix\u2011up style interpolation that pairs samples either from the same label but different domains or the same domain with different labels, directly learning invariant predictors without constraining internal representations\u202f([mlr.press](https://proceedings.mlr.press/v162/yao22b/yao22b.pdf)).  More conceptually, the recent \u201cBias as a Virtue\u201d perspective argues that a controlled amount of bias can be beneficial if it aligns with the target shift, suggesting you deliberately re\u2011weight or augment data to reflect anticipated test\u2011time distributions\u202f([arxiv.org](https://arxiv.org/abs/2506.00407)).  Finally, the comprehensive OOD survey recommends combining these ideas\u2014ensemble calibration, domain\u2011aware rebalancing, and targeted augmentations\u2014while also validating on held\u2011out shift\u2011simulating splits to detect residual bias early\u202f([arxiv.org](https://arxiv.org/abs/2108.13624)).  Applying these strategies together typically narrows the CV\u2011test performance gap and yields more reliable OOD generalization.",
      "url": ""
    },
    {
      "title": "Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2410.05980"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2506.00407] Bias as a Virtue: Rethinking Generalization under Distribution Shifts\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2506.00407\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2506.00407**(cs)\n[Submitted on 31 May 2025]\n# Title:Bias as a Virtue: Rethinking Generalization under Distribution Shifts\nAuthors:[Ruixuan Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R),[Wentao Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+W),[Jiahui Xiao](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+J),[Yuchen Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Yimin Tang](https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+Y),[Xiaonan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X)\nView a PDF of the paper titled Bias as a Virtue: Rethinking Generalization under Distribution Shifts, by Ruixuan Chen and 5 other authors\n[View PDF](https://arxiv.org/pdf/2506.00407)[HTML (experimental)](https://arxiv.org/html/2506.00407v1)> > Abstract:\n> Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error--a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 74.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning. Comments:|14 pages|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nCite as:|[arXiv:2506.00407](https://arxiv.org/abs/2506.00407)[cs.LG]|\n|(or[arXiv:2506.00407v1](https://arxiv.org/abs/2506.00407v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2506.00407](https://doi.org/10.48550/arXiv.2506.00407)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Wentao Li [[view email](https://arxiv.org/show-email/25ecdd09/2506.00407)]\n**[v1]**Sat, 31 May 2025 05:54:49 UTC (343 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Bias as a Virtue: Rethinking Generalization under Distribution Shifts, by Ruixuan Chen and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2506.00407)\n* [HTML (experimental)](https://arxiv.org/html/2506.00407v1)\n* [TeX Source](https://arxiv.org/src/2506.00407)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2506.00407&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2506.00407&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-06](https://arxiv.org/list/cs.LG/2025-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2506.00407?context=cs)\n[cs.AI](https://arxiv.org/abs/2506.00407?context=cs.AI)\n[stat](https://arxiv.org/abs/2506.00407?context=stat)\n[stat.ML](https://arxiv.org/abs/2506.00407?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.00407)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.00407)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.00407)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2506.00407&amp;description=Bias as a Virtue: Rethinking Generalization under Distribution Shifts>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2506.00407&amp;title=Bias as a Virtue: Rethinking Generalization under Distribution Shifts>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?...",
      "url": "https://arxiv.org/abs/2506.00407"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2207.08977] Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2207.08977\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2207.08977**(cs)\n[Submitted on 18 Jul 2022]\n# Title:Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift\nAuthors:[Ananya Kumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A),[Tengyu Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+T),[Percy Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+P),[Aditi Raghunathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Raghunathan,+A)\nView a PDF of the paper titled Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift, by Ananya Kumar and Tengyu Ma and Percy Liang and Aditi Raghunathan\n[View PDF](https://arxiv.org/pdf/2207.08977)> > Abstract:\n> We often see undesirable tradeoffs in robust machine learning where out-of-distribution (OOD) accuracy is at odds with in-distribution (ID) accuracy: a robust classifier obtained via specialized techniques such as removing spurious features often has better OOD but worse ID accuracy compared to a standard classifier trained via ERM. In this paper, we find that ID-calibrated ensembles -- where we simply ensemble the standard and robust models after calibrating on only ID data -- outperforms prior state-of-the-art (based on self-training) on both ID and OOD accuracy. On eleven natural distribution shift datasets, ID-calibrated ensembles obtain the best of both worlds: strong ID accuracy and OOD accuracy. We analyze this method in stylized settings, and identify two important conditions for ensembles to perform well both ID and OOD: (1) we need to calibrate the standard and robust models (on ID data, because OOD data is unavailable), (2) OOD has no anticorrelated spurious features. Comments:|Accepted to UAI 2022|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2207.08977](https://arxiv.org/abs/2207.08977)[cs.LG]|\n|(or[arXiv:2207.08977v1](https://arxiv.org/abs/2207.08977v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2207.08977](https://doi.org/10.48550/arXiv.2207.08977)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ananya Kumar [[view email](https://arxiv.org/show-email/44155113/2207.08977)]\n**[v1]**Mon, 18 Jul 2022 23:14:44 UTC (334 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift, by Ananya Kumar and Tengyu Ma and Percy Liang and Aditi Raghunathan\n* [View PDF](https://arxiv.org/pdf/2207.08977)\n* [TeX Source](https://arxiv.org/src/2207.08977)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2207.08977&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2207.08977&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-07](https://arxiv.org/list/cs.LG/2022-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2207.08977?context=cs)\n[stat](https://arxiv.org/abs/2207.08977?context=stat)\n[stat.ML](https://arxiv.org/abs/2207.08977?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.08977)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.08977)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.08977)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2207.08977&amp;description=Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2207.08977&amp;title=Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2207.08977)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2207.08977"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2107.04649] Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2107.04649\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2107.04649**(cs)\n[Submitted on 9 Jul 2021 ([v1](https://arxiv.org/abs/2107.04649v1)), last revised 7 Oct 2021 (this version, v2)]\n# Title:Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\nAuthors:[John Miller](https://arxiv.org/search/cs?searchtype=author&amp;query=Miller,+J),[Rohan Taori](https://arxiv.org/search/cs?searchtype=author&amp;query=Taori,+R),[Aditi Raghunathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Raghunathan,+A),[Shiori Sagawa](https://arxiv.org/search/cs?searchtype=author&amp;query=Sagawa,+S),[Pang Wei Koh](https://arxiv.org/search/cs?searchtype=author&amp;query=Koh,+P+W),[Vaishaal Shankar](https://arxiv.org/search/cs?searchtype=author&amp;query=Shankar,+V),[Percy Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+P),[Yair Carmon](https://arxiv.org/search/cs?searchtype=author&amp;query=Carmon,+Y),[Ludwig Schmidt](https://arxiv.org/search/cs?searchtype=author&amp;query=Schmidt,+L)\nView a PDF of the paper titled Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization, by John Miller and 8 other authors\n[View PDF](https://arxiv.org/pdf/2107.04649)> > Abstract:\n> For machine learning systems to be reliable, we must understand their performance in unseen, out-of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of-distribution performance on variants of CIFAR-10 &amp; ImageNet, a synthetic pose estimation task derived from YCB objects, satellite imagery classification in FMoW-WILDS, and wildlife classification in iWildCam-WILDS. The strong correlations hold across model architectures, hyperparameters, training set size, and training duration, and are more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2107.04649](https://arxiv.org/abs/2107.04649)[cs.LG]|\n|(or[arXiv:2107.04649v2](https://arxiv.org/abs/2107.04649v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2107.04649](https://doi.org/10.48550/arXiv.2107.04649)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: John Miller [[view email](https://arxiv.org/show-email/8b446f5b/2107.04649)]\n**[[v1]](https://arxiv.org/abs/2107.04649v1)**Fri, 9 Jul 2021 19:48:23 UTC (46,790 KB)\n**[v2]**Thu, 7 Oct 2021 23:59:19 UTC (46,791 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization, by John Miller and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2107.04649)\n* [TeX Source](https://arxiv.org/src/2107.04649)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2107.04649&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2107.04649&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-07](https://arxiv.org/list/cs.LG/2021-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2107.04649?context=cs)\n[stat](https://arxiv.org/abs/2107.04649?context=stat)\n[stat.ML](https://arxiv.org/abs/2107.04649?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.04649)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.04649)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.04649)\n### [1 blog link](https://arxiv.org/tb/2107.04649)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-04649)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-04649)\n[John Miller](<https://dblp.uni-trier.de/search/author?author=John Miller>)\n[Rohan Taori](<https://dblp.uni-trier.de/search/author?author=Rohan Taori>)\n[Aditi Raghunathan](<https://dblp.uni-trier.de/search/author?author=Aditi Raghunathan>)\n[Pang Wei Koh](<https://dblp.uni-trier.de/search/author?author=Pang Wei Koh>)\n[Vaishaal Shankar](<https://dblp.uni-trier.de/search/author?author=Vaishaal Shankar>)\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2107.04649&amp;description=Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2107.04649&amp;title=Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/...",
      "url": "https://arxiv.org/abs/2107.04649"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2108.13624] Towards Out-Of-Distribution Generalization: A Survey\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2108.13624\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2108.13624**(cs)\n[Submitted on 31 Aug 2021 ([v1](https://arxiv.org/abs/2108.13624v1)), last revised 27 Jul 2023 (this version, v2)]\n# Title:Towards Out-Of-Distribution Generalization: A Survey\nAuthors:[Jiashuo Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J),[Zheyan Shen](https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Z),[Yue He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Y),[Xingxuan Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Renzhe Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+R),[Han Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+H),[Peng Cui](https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+P)\nView a PDF of the paper titled Towards Out-Of-Distribution Generalization: A Survey, by Jiashuo Liu and 6 other authors\n[View PDF](https://arxiv.org/pdf/2108.13624)> > Abstract:\n> Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed ($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at [> this http URL\n](http://out-of-distribution-generalization.com)> . Comments:|51 pages|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2108.13624](https://arxiv.org/abs/2108.13624)[cs.LG]|\n|(or[arXiv:2108.13624v2](https://arxiv.org/abs/2108.13624v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2108.13624](https://doi.org/10.48550/arXiv.2108.13624)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jiashuo Liu [[view email](https://arxiv.org/show-email/b1e61827/2108.13624)]\n**[[v1]](https://arxiv.org/abs/2108.13624v1)**Tue, 31 Aug 2021 05:28:42 UTC (4,533 KB)\n**[v2]**Thu, 27 Jul 2023 13:13:11 UTC (360 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Towards Out-Of-Distribution Generalization: A Survey, by Jiashuo Liu and 6 other authors\n* [View PDF](https://arxiv.org/pdf/2108.13624)\n* [TeX Source](https://arxiv.org/src/2108.13624)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2108.13624&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2108.13624&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-08](https://arxiv.org/list/cs.LG/2021-08)\nChange to browse by:\n[cs](https://arxiv.org/abs/2108.13624?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2108.13624)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2108.13624)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2108.13624)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2108.html#abs-2108-13624)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2108-13624)\n[Zheyan Shen](<https://dblp.uni-trier.de/search/author?author=Zheyan Shen>)\n[Yue He](<https://dblp.uni-trier.de/search/author?author=Yue He>)\n[Han Yu](<https://dblp.uni-trier.de/search/author?author=Han Yu>)\n[Peng Cui](<https://dblp.uni-trier.de/search/author?author=Peng Cui>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2108.13624&amp;description=Towards Out-Of-Distribution Generalization: A Survey>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2108.13624&amp;title=Towards Out-Of-Distribution Generalization: A Survey>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggl...",
      "url": "https://arxiv.org/abs/2108.13624"
    },
    {
      "title": "",
      "text": "Improving Out-of-Distribution Robustness via Selective Augmentation\nHuaxiu Yao * 1 Yu Wang * 2 Sai Li 3 Linjun Zhang 4 Weixin Liang 1\nJames Zou 1 Chelsea Finn 1\nAbstract\nMachine learning algorithms typically assume\nthat training and test examples are drawn from the\nsame distribution. However, distribution shift is a\ncommon problem in real-world applications and\ncan cause models to perform dramatically worse\nat test time. In this paper, we specifically consider\nthe problems of subpopulation shifts (e.g., imbal\u0002anced data) and domain shifts. While prior works\noften seek to explicitly regularize internal repre\u0002sentations or predictors of the model to be domain\ninvariant, we instead aim to learn invariant pre\u0002dictors without restricting the model\u2019s internal\nrepresentations or predictors. This leads to a sim\u0002ple mixup-based technique which learns invariant\npredictors via selective augmentation called LISA.\nLISA selectively interpolates samples either with\nthe same labels but different domains or with the\nsame domain but different labels. Empirically, we\nstudy the effectiveness of LISA on nine bench\u0002marks ranging from subpopulation shifts to do\u0002main shifts, and we find that LISA consistently\noutperforms other state-of-the-art methods and\nleads to more invariant predictors. We further an\u0002alyze a linear setting and theoretically show how\nLISA leads to a smaller worst-group error. Code\nis released in https://github.com/huaxiuyao/LISA\n1. Introduction\nTo deploy machine learning algorithms in real-world appli\u0002cations, we must pay attention to distribution shift, i.e. when\nthe test distribution is different from the training distribution,\nwhich substantially degrades model performance. In this\n*Equal contribution . This work was done when Yu Wang was\nmentored by Huaxiu Yao remotely. 1Stanford University, CA,\nUSA 2University of California San Diego, CA, USA 3Renmin\nUniversity of China, Beijing, China 4Rutgers University, NJ, USA.\nCorrespondence to: Huaxiu Yao <huaxiu@cs.stanford.edu>, Sai\nLi <saili@ruc.edu.cn>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy\u0002right 2022 by the author(s).\npaper, we refer this problem as out-of-distribution (OOD)\ngeneralization and specifically consider performance gaps\ncaused by two kinds of distribution shifts: subpopulation\nshifts and domain shifts. In subpopulation shifts, the test\ndomains (or subpopulations) are seen but underrepresented\nin the training data. When subpopulation shift occurs, mod\u0002els may perform poorly when they falsely rely on spurious\ncorrelations between the particular subpopulation and the\nlabel. For example, in health risk prediction, a machine\nlearning model trained on the entire population may asso\u0002ciate the labels with demographic features (e.g., gender and\nage), making the model fail on the test set when such an\nassociation does not hold in reality. In domain shifts, the\ntest data is from new domains, which requires the trained\nmodel to generalize well to test domains without seeing the\ndata from those domains at training time. In the health risk\nexample, we may want to train a model on patients from\na few sampled hospitals and then deploy the model to a\nbroader set of hospitals (Koh et al., 2021).\nTo improve model robustness under these two kinds of distri\u0002bution shifts, prior works have proposed various regularizers\nto learn representations or predictors that are invariant to\ndifferent domains while still containing sufficient informa\u0002tion to fulfill the task (Li et al., 2018; Sun & Saenko, 2016;\nArjovsky et al., 2019; Krueger et al., 2021; Rosenfeld et al.,\n2021). However, designing regularizers that are widely suit\u0002able to datasets from diverse domains is challenging, and\nunsuitable regularizers may adversely limit the model\u2019s ex\u0002pressive power or yield a difficult optimization problem,\nleading to inconsistent performance among various real\u0002world datasets. For example, on the WILDS datasets, invari\u0002ant risk minimization (IRM) (Arjovsky et al., 2019) with\nreweighting \u2013 a representative method for learning invariant\npredictor \u2013 outperforms empirical risk minimization (ERM)\non CivilComments, but fails to improve robustness on a\nvariety of other datasets like Camelyon17 and RxRx1 (Koh\net al., 2021).\nInstead of explicitly imposing regularization, we propose to\nlearn invariant predictors through data interpolation, leading\nto a simple algorithm called LISA (Learning Invariant Pre\u0002dictors with Selective Augmentation). Concretely, inspired\nby mixup (Zhang et al., 2018), LISA linearly interpolates\nthe features for a pair of samples and applies the same\nImproving Out-of-Distribution Robustness via Selective Augmentation\n = 0.0  = 0.25  = 0.5  = 0.75  = 1.0\n = 0.0  = 0.25  = 0.5  = 0.75  = 1.0\nDomain information is not the reason for the label change\ny = [1, 0] y = [0.25, 0.75] y = [0.5, 0.5] y = [0.75, 0.25] y = [0, 1]\nAll y = [0, 1] \n!: Green\n\n\": Red\n!: [1, 0] \": [0, 1]\n40% of train data\n10% of train data 40% of train data\n10% of train data\nOne-hot label\nDomain\n(b) Intra-label LISA: interpolates samples with the same label but different domains\n(c) Intra-domain LISA: interpolates samples with the same domain but different labels\n(a) Colored MNIST dataset\nFigure 1. Illustration of the variants of LISA (Intra-label LISA and Intra-domain LISA) on Colored MNIST dataset. \u03bb represents the\ninterpolation ratio, which is sampled from a Beta distribution. (a) Colored MNIST (CMNIST). We classify MNIST digits as two classes,\nand original digits (0,1,2,3,4) and (5,6,7,8,9) are labeled as class 0 and 1, respectively. Digit color is used as domain information, which is\nspuriously correlated with labels in training data; (b) Intra-label LISA (LISA-L) cancels out spurious correlation by interpolating samples\nwith the same label; (c) Intra-domain LISA (LISA-D) interpolates samples with the same domain but different labels to encourage the\nmodel to learn specific features within a domain.\ninterpolation strategy on the corresponding labels. Crit\u0002ically, the pairs are selectively chosen according to two\nselective augmentation strategies \u2013 intra-label LISA (LISA\u0002L) and intra-domain LISA (LISA-D), which are described\nbelow and illustrated on Colored MNIST dataset in Figure 1.\nIntra-label LISA (Figure 1(b)) interpolates samples with the\nsame label but from different domains, aiming to eliminate\ndomain-related spurious correlations. Intra-domain LISA\n(Figure 1(c)) interpolates samples with the same domain but\ndifferent labels, such that the model should learn to ignore\nthe domain information and generate different predicted\nvalues as the interpolation ratio changes. In this way, LISA\nencourages the model to learn domain-invariant predictors\nwithout any explicit constraints or regularizers.\nThe primary contributions of this paper are as follows:\n(1) We propose a simple yet widely-applicable method for\nlearning domain invariant predictors that is shown to be\nrobust to subpopulation shifts and domain shifts. (2) We\nconduct broad experiments to evaluate LISA on nine bench\u0002mark datasets from diverse domains. In these experiments,\nwe make the following key observations. First, we observe\nthat LISA consistently outperforms seven prior methods to\naddress subpopulation and domain shifts. Second, we find\nthat LISA produces predictors that are consistently more\ndomain invariant than prior approaches. Third, we identify\nthat the performance gains of LISA are from canceling out\ndomain-specific information or spurious correlations and\nlearning invariant predictors, rather than simply involving\nmore data via interpolation. Finally, when the degree of\ndistribution shift increases, LISA achieves more significant\nperformance gains. (3) We provide a theoretical analysis of\nthe phenomena distilled from the empirical studies, where\nwe provably demonstrate that LISA can mitigate spurious\ncorrelations and therefore lead to smaller worst-domain er\u0002ror compared with ERM and vanilla mixup...",
      "url": "https://proceedings.mlr.press/v162/yao22b/yao22b.pdf"
    },
    {
      "title": "",
      "text": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
      "url": "https://arxiv.org/pdf/2207.09239"
    },
    {
      "title": "",
      "text": "Accuracy on the Line: On the Strong Correlation\nBetween Out-of-Distribution and In-Distribution Generalization\nJohn Miller 1 Rohan Taori 2 Aditi Raghunathan 2 Shiori Sagawa 2 Pang Wei Koh 2 Vaishaal Shankar 1\nPercy Liang 2 Yair Carmon 3 Ludwig Schmidt 4\nAbstract\nFor machine learning systems to be reliable, we\nmust understand their performance in unseen, out\u0002of-distribution environments. In this paper, we\nempirically show that out-of-distribution perfor\u0002mance is strongly correlated with in-distribution\nperformance for a wide range of models and distri\u0002bution shifts. Specifically, we demonstrate strong\ncorrelations between in-distribution and out-of\u0002distribution performance on variants of CIFAR\u000210 & ImageNet, a synthetic pose estimation task\nderived from YCB objects, FMoW-WILDS satel\u0002lite imagery classification, and wildlife classi\u0002fication in iWildCam-WILDS. The correlation\nholds across model architectures, hyperparame\u0002ters, training set size, and training duration, and\nis more precise than what is expected from exist\u0002ing domain adaptation theory. To complete the\npicture, we also investigate cases where the cor\u0002relation is weaker, for instance some synthetic\ndistribution shifts from CIFAR-10-C and the tis\u0002sue classification dataset Camelyon17-WILDS.\nFinally, we provide a candidate theory based on a\nGaussian data model that shows how changes in\nthe data covariance arising from distribution shift\ncan affect the observed correlations.\n1. Introduction\nMachine learning models often need to generalize from\ntraining data to new environments. A kitchen robot should\nwork reliably in different homes, autonomous vehicles\nshould drive reliably in different cities, and analysis software\nfor satellite imagery should still perform well next year. The\n1Department of Computer Science, UC Berkeley, CA, USA\n2Department of Computer Science, Stanford University, Stanford,\nCA, USA 3School of Computer Science, Tel Aviv University, Tel\nAviv, Israel 4Toyota Research Institute, Cambridge, MA, USA.\nCorrespondence to: John Miller <miller john@berkeley.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nstandard paradigm to measure generalization is to evaluate a\nmodel on a single test set drawn from the same distribution\nas the training set. But this paradigm provides only a narrow\nin-distribution performance guarantee: a small test error\ncertifies future performance on new samples from exactly\nthe same distribution as the training set. In many scenarios,\nit is hard or impossible to train a model on precisely the dis\u0002tribution it will be applied to. Hence a model will inevitably\nencounter out-of-distribution data on which its performance\ncould vary widely compared to in-distribution performance.\nUnderstanding the performance of models beyond the train\u0002ing distribution therefore raises the following fundamental\nquestion: how does out-of-distribution performance relate\nto in-distribution performance?\nClassical theory for generalization across different distri\u0002butions provides a partial answer (Mansour et al., 2009;\nBen-David et al., 2010). For a model f trained on a distribu\u0002tion D, known guarantees typically relate the in-distribution\ntest accuracy on D to the out-of-distribution test accuracy\non a new distribution D1 via inequalities of the form\n|accDpfq \u00b4 accD1pfq| \u010f dpD, D1q\nwhere d is a distance between the distributions D and D1\nsuch as the total variation distance. Qualitatively, these\nbounds suggest that out-of-distribution accuracy may vary\nwidely as a function of in-distribution accuracy unless the\ndistribution distance d is small and the accuracies are there\u0002fore close (see Figure 1 (top-left) for an illustration). More\nrecently, empirical studies have shown that in some set\u0002tings, models with similar in-distribution performance can\nindeed have different out-of-distribution performance (Mc\u0002Coy et al., 2019; Zhou et al., 2020; D\u2019Amour et al., 2020).\nIn contrast to the aforementioned results, recent dataset re\u0002constructions of the popular CIFAR-10, ImageNet, MNIST,\nand SQuAD benchmarks showed a much more regular pat\u0002tern (Recht et al., 2019; Miller et al., 2020; Yadav & Bottou,\n2019; Lu et al., 2020). The reconstructions closely followed\nthe original dataset creation processes to assemble new test\nsets, but small differences were still enough to cause substan\u0002tial changes in the resulting model accuracies. Nevertheless,\nthe new out-of-distribution accuracies are almost perfectly\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\n10 30 50 70 90\nIn-distribution accuracy\n10\n30\n50\n70\n90\nOut-of-distribution accuracy\nSketch of theoretical bounds\nPotential model\nOOD / ID discrepancy\nmeasure\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCIFAR-10.2 test accuracy\nCIFAR-10.2\n10 20 30 40 50 65\nIn-distribution test accuracy\n5\n10\n20\n30\n40\n50\nOOD test worst region accuracy\nfMoW-WILDS\n5 20 35 50 65 80\nImageNet accuracy\n5\n20\n35\n50\n65\n80\nImageNetV2 accuracy\nImageNetV2\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nFog accuracy\nCIFAR-10-C fog\n10 20 30 40 50 60 70\nID test macro F1\n5\n10\n20\n30\n40\n50\nOOD test macro F1\niWildCam-WILDS\n5 20 35 50 65 80\nYCB in-distribution accuracy\n5\n20\n35\n50\n65\n80\nYCB OOD accuracy\nYCB-Objects\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCINIC-10 test accuracy\nCINIC-10\ny = x\nLinear Fit\nNeural Network\nImageNet Pretrained Network\nRandom Features\nRandom Forest\nKNN\nSVM\nLinear Model\nAdaBoost\nFigure 1. Out-of-distribution accuracies vs. in-distribution accuracies for a wide range of models, datasets, and distribution shifts. Top left:\nA sketch of the current bounds from domain adaptation theory. These bounds depend on distributional distances between in-distribution\nand out-of-distribution data, and they are loose in that they limit the deviation away from the y = x diagonal but do not prescribe a\nspecific trend within these wide bounds (see Section 7). Remaining panels: In contrast, we show that for a wide range of models and\ndatasets, there is a precise linear trend between out-of-distribution accuracy and in-distribution accuracy. Unlike what we might expect\nfrom theory, the linear trend does not follow the y \u201c x diagonal. The different panels represent different pairs of in-distribution and\nout-of-distribution datasets. Within each panel, we plot the performances of many different models, with different model architectures\nand hyperparameters. These datasets capture a variety of distribution shifts from dataset reproduction (CIFAR-10.2, ImageNet-V2); a\nreal-world spatiotemporal distribution shift on satellite imagery (FMoW-WILDS); using a different benchmark test dataset (CINIC-10);\nsynthetic perturbations (CIFAR-10-C and YCB-Objects); and a real-world geographic shift in wildlife monitoring (iWildCam-WILDS).\nInterestingly, for iWildCam-WILDS, models pretrained on ImageNet follow a different linear trend than models trained from scratch\nin-distribution, and we plot a separate trend line for ImageNet pretrained models in the iWildCam-WILDS panel. We explore this\nphenomenon more in Section 5.\nlinearly correlated with the original in-distribution accura\u0002cies for a range of deep neural networks. Importantly, this\ncorrelation holds despite the substantial gap between in\u0002distribution and out-of-distribution accuracies (see Figure\n1 (top-middle) for an example). However, it is currently\nunclear how widely these linear trends apply since they\nhave been mainly observed for dataset reproductions and\ncommon variations of convolutional neural networks.\nIn this paper, we conduct a broad empirical investigation to\ncharacterize when precise linear trends such as in Figure 1\n(top-middle) may be expected, and when out-of-distribution\nperformance is less predictable as in Figure 1 (top-left).\nConcretely, we make the following contributions:\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribu...",
      "url": "https://proceedings.mlr.press/v139/miller21b/miller21b.pdf"
    }
  ]
}