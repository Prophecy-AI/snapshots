{
  "query": "How can I improve machine learning predictions for outlier or hard-to-predict samples that dominate the error?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving predictions on the few samples that dominate the error can be done by first **identifying** those hard\u2011to\u2011predict points and then **training the model to treat them differently**\u2014either by down\u2011weighting true outliers, up\u2011weighting informative hard examples, or augmenting them with additional data. The following concise workflow combines the most effective techniques reported in recent research\u202f([1](https://arxiv.org/html/2501.00265v1), [2](https://arxiv.org/pdf/1803.09050), [3](https://arxiv.org/abs/2405.03869), [4](https://arxiv.org/pdf/2107.13098), [5](https://arxiv.org/pdf/1910.13427)):\n\n1. **Detect outlier / hard samples**  \n   - Compute an *outlier\u2011score* for each training example using a robust metric such as the **outlier gradient magnitude** (samples whose gradients are unusually large are likely detrimental)\u202f([3](https://arxiv.org/abs/2405.03869)).  \n   - Optionally complement this with **distribution\u2011density scores** (e.g., the five metrics from Carlini\u202fet\u202fal.) to distinguish true outliers from rare but well\u2011represented sub\u2011modes\u202f([5](https://arxiv.org/pdf/1910.13427)).  \n\n2. **Separate true outliers from hard\u2011but\u2011informative examples**  \n   - Set a threshold on the combined scores:  \n     *Low\u2011score* \u2192 probable outlier (noise, label error).  \n     *High\u2011score* \u2192 hard but valid example (long\u2011tail).  \n\n3. **Apply a robust training scheme**  \n   - **For probable outliers:** use a *robust loss kernel* (e.g., Huber or Tukey loss) or simply **down\u2011weight** their contribution. The Adaptive Alternation Algorithm (AAA) treats the weights as inlier probabilities and updates them each iteration, removing the need for manual tuning\u202f([1](https://arxiv.org/html/2501.00265v1)).  \n   - **For hard but informative examples:** **up\u2011weight** them using a learned re\u2011weighting scheme (e.g., \u201cLearning to Reweight Examples for Robust Deep Learning\u201d) that optimizes a meta\u2011objective on a clean validation set\u202f([2](https://arxiv.org/pdf/1803.09050)).  \n\n4. **Targeted data augmentation**  \n   - For the up\u2011weighted long\u2011tail samples, apply **targeted augmentations** (noise injection, style transfer, synthetic variations) during training. The \u201cTale of Two Long Tails\u201d study shows that such interventions accelerate learning on atypical examples and improve overall calibration\u202f([4](https://arxiv.org/pdf/2107.13098)).  \n\n5. **Iterate with adaptive weighting**  \n   - Run the training loop:  \n     a) Compute current weights (inlier probabilities) \u2192 weighted loss.  \n     b) Update model parameters.  \n     c) Re\u2011estimate outlier scores and adjust weights.  \n   - This alternating process converges to an *outlier\u2011free optimum* and reduces the dominance of a few bad samples\u202f([1](https://arxiv.org/html/2501.00265v1)).  \n\n6. **Validate and monitor**  \n   - Track validation error on both the full set and the subset of previously hard examples.  \n   - If residual error remains high on a subset, consider **outlier exposure**: augment training with an auxiliary dataset of generic outliers to improve the model\u2019s ability to recognize and ignore anomalous inputs\u202f([10](https://web.engr.oregonstate.edu/~tgd/publications/hendrycks-mazeika-dietterich-deep-anomaly-detection-with-outlier-exposure-arxiv2019.pdf)).  \n\nBy systematically **detecting**, **differentiating**, and **re\u2011training** on outlier versus hard\u2011but\u2011useful samples\u2014using robust losses, adaptive weighting, and targeted augmentation\u2014you can substantially lower the error contributed by the few dominant outliers and achieve more reliable overall predictions.",
      "url": ""
    },
    {
      "title": "Outlier-Robust Training of Machine Learning Models",
      "text": "\\\\xpatchcmd\\\\proof\n\nRajat Talak talak@mit.edu\n\nLaboratory of Information & Decision Systems\n\nMassachusetts Institute of Technology\n\nCambridge, MA 02139, USA\nCharis Georgiou cgeo@mit.edu\n\nDepartment of Electrical Engineering and Computer Science\n\nMassachusetts Institute of Technology\n\nCambridge, MA 02139, USA\nJingnan Shi jnshi@mit.edu\n\nLaboratory of Information & Decision Systems\n\nMassachusetts Institute of Technology\n\nCambridge, MA 02139, USA\nLuca Carlone lcarlone@mit.edu\n\nLaboratory of Information & Decision Systems\n\nMassachusetts Institute of Technology\n\nCambridge, MA 02139, USA\n\n###### Abstract\n\nRobust training of machine learning models in the presence of outliers has garnered attention across various domains.\nThe use of robust losses is a popular approach and is known to mitigate the impact of outliers.\nWe bring to light two literatures that have diverged in their ways of designing robust losses: one using M-estimation, which is popular in robotics and computer vision, and another using a risk-minimization framework, which is popular in deep learning.\nWe first show that a simple modification of the Black-Rangarajan duality provides a unifying view.\nThe modified duality brings out a definition of a _robust loss kernel_ that is satisfied by robust losses in both the literatures.\nSecondly, using the modified duality, we propose an\n_Adaptive Alternation Algorithm_ (AAA) for training machine learning models with outliers.\nThe algorithm iteratively trains the model by using a weighted version of the non-robust loss, while updating the weights at each iteration.\nThe algorithm\nis\naugmented with a novel\nparameter update rule by interpreting the weights as inlier probabilities, and obviates the need for complex parameter tuning.\nThirdly,\nwe investigate convergence\nof the\nadaptive alternation algorithm to outlier-free optima.\nConsidering arbitrary outliers ( _i.e.,_ with no distributional assumption on the outliers), we show that the use of robust loss kernels increases the region of convergence.\nWe experimentally show the efficacy of our algorithm on regression, classification, and neural scene reconstruction problems.111We release our implementation code: [https://github.com/MIT-SPARK/ORT](https://github.com/MIT-SPARK/ORT).\n\n## 1 Introduction\n\nHumans are good at detecting and isolating outliers\u00a0(Chai et\u00a0al., [2020](https://arxiv.org/html/2501.00265v1#bib.bib16)).\nThis is not the case when it comes to training machine learning models\u00a0(Sukhbaatar et\u00a0al., [2015](https://arxiv.org/html/2501.00265v1#bib.bib78); Wang et\u00a0al., [2024a](https://arxiv.org/html/2501.00265v1#bib.bib83); Sabour et\u00a0al., [2023](https://arxiv.org/html/2501.00265v1#bib.bib71)).\nRobustly training deep learning models in the presence of outliers is an important challenge. In particular, it can offset the high cost of obtaining accurate annotations. Many works now implement automatic or semi-automatic annotation pipelines which can be leveraged to train models (Armeni et\u00a0al., [2016](https://arxiv.org/html/2501.00265v1#bib.bib6); Chang et\u00a0al., [2017](https://arxiv.org/html/2501.00265v1#bib.bib17); Tkachenko et\u00a0al., [2020](https://arxiv.org/html/2501.00265v1#bib.bib82); Yang et\u00a0al., [2021](https://arxiv.org/html/2501.00265v1#bib.bib88); Gadre et\u00a0al., [2023](https://arxiv.org/html/2501.00265v1#bib.bib37)). Recent efforts in robotics envision robots that\ncan self-train their models by collecting and self-annotating data (Schmidt & Fox, [2020](https://arxiv.org/html/2501.00265v1#bib.bib72); Deng et\u00a0al., [2020](https://arxiv.org/html/2501.00265v1#bib.bib24); Lu et\u00a0al., [2022](https://arxiv.org/html/2501.00265v1#bib.bib56); Talak et\u00a0al., [2023](https://arxiv.org/html/2501.00265v1#bib.bib79); Shi et\u00a0al., [2023](https://arxiv.org/html/2501.00265v1#bib.bib76); Jawaid et\u00a0al., [2024](https://arxiv.org/html/2501.00265v1#bib.bib44); Wang et\u00a0al., [2024b](https://arxiv.org/html/2501.00265v1#bib.bib85)).\n\nFigure 1: Nerfacto\u00a0(Tancik et\u00a0al., [2023](https://arxiv.org/html/2501.00265v1#bib.bib80)) reconstruction results after of the training pixels have been perturbed by outliers.\n(left) Training with the original Adam optimizer. (middle) Training with our Adaptive Alternation Algorithm with Truncated Loss. (right) Ground truth.\n\nIn typical learning problems one computes the unknowns ( _e.g.,_ network weights) by optimizing a loss function for each training sample :\n\n|     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  | (1) |\n\nwhere W is the set of allowed parameters.\nFor instance, may be the cross-entropy loss or the norm squared measuring the mismatch between the -th training label and the corresponding network prediction.\n\nM-estimation\u00a0(Huber, [1981](https://arxiv.org/html/2501.00265v1#bib.bib43)) suggests that in the presence of outliers, one needs to wrap typical losses into a robust loss function :\n\n|     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  | (2) |\n\nwhere is responsible for mitigating the impact of terms with high loss ( _i.e.,_ high ).\nMany robust losses have been proposed in the literature to mitigate the effect of outliers. Recent works in robust estimation in robotics have shown that using a parameterized robust loss , with adaptive parameter tuning during training, yields better outlier mitigation (see Section\u00a0[8.2](https://arxiv.org/html/2501.00265v1#S8.SS2)).\nMany robust losses have also been proposed in training deep learning models for the task of multi-label classification (see Section\u00a0[8.1](https://arxiv.org/html/2501.00265v1#S8.SS1)). However, we observe a divergence in the principles that govern the design of robust losses in (a) robotics and computer vision, where works mostly use robust estimation frameworks, and in (b) training deep learning models, which mostly relies on risk-minimization frameworks (see Section\u00a0[2](https://arxiv.org/html/2501.00265v1#S2)).\n\nRobust estimation as applied in robotics and computer vision often relies on the insight that problem\u00a0(LABEL:eq:intro-m-est) can be written down as a weighted least squares problem\n\n|     |     |     |     |     |     |     |\n| --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  | (3) |\n\nwhere is an outlier process that is determined by the Black-Rangarajan duality\u00a0(Black & Rangarajan, [1996](https://arxiv.org/html/2501.00265v1#bib.bib11)). The equivalence between\u00a0(LABEL:eq:intro-m-est) and\u00a0(LABEL:eq:intro-m-est-dual) is useful for robotics and computer vision applications as common robust estimation problems can be re-written as weighted non-linear least squares, which are typically easier to solve.\nHowever, this framework cannot be applied directly to machine learning problems. For example, if is the cross-entropy loss, minimizing the squared cross-entropy loss does not make an equal sense.\n\nOn the other hand, when we consider classification problems in machine learning,\nthe literature uses a risk-minimization framework to develop the notion of noise-tolerant loss\u00a0(Ghosh et\u00a0al., [2015](https://arxiv.org/html/2501.00265v1#bib.bib39); [2017](https://arxiv.org/html/2501.00265v1#bib.bib40)).\nLet model weight minimize risk when there are fraction of outliers. Ghosh et\u00a0al. ( [2015](https://arxiv.org/html/2501.00265v1#bib.bib39); [2017](https://arxiv.org/html/2501.00265v1#bib.bib40)) define a loss to be noise-tolerant when ( _i.e.,_ equal to the optimal weights when there are no outliers).\nSeveral noise-tolerant losses have been proposed since then that have shown improved performance at mitigating the presence of outliers in the training data. These losses include generalized cross entropy, symmetric cross entropy, reverse cross entropy, Taylor cross entropy, among others (see Section\u00a0[8.1](https://arxiv.org/html/2501.00265v1#S8.SS1)). While the setup has some advantages, it suffers from some limitations; for instance, one has to assume an outlier distribution to derive the noise-tolerant loss.\nAs an instance of t...",
      "url": "https://arxiv.org/html/2501.00265v1"
    },
    {
      "title": "",
      "text": "A Tale Of Two Long Tails\nDaniel D\u2019souza 1 2 Zach Nussbaum 1 Chirag Agarwal 3 Sara Hooker 4\nAbstract\nAs machine learning models are increasingly\nemployed to assist human decision-makers, it\nbecomes critical to communicate the uncertainty\nassociated with these model predictions. How\u0002ever, the majority of work on uncertainty has\nfocused on traditional probabilistic or ranking\napproaches \u2013 where the model assigns low prob\u0002abilities or scores to uncertain examples. While\nthis captures what examples are challenging for\nthe model, it does not capture the underlying\nsource of the uncertainty. In this work, we seek\nto identify examples the model is uncertain about\nand characterize the source of said uncertainty.\nWe explore the benefits of designing a targeted\nintervention \u2013 targeted data augmentation of the\nexamples where the model is uncertain over the\ncourse of training. We investigate whether the\nrate of learning in the presence of additional\ninformation differs between atypical and noisy\nexamples? Our results show that this is indeed the\ncase, suggesting that well-designed interventions\nover the course of training can be an effective\nway to characterize and distinguish between\ndifferent sources of uncertainty.\n1. Introduction\nAs machine learning models are increasingly implemented\nin real-world applications, it becomes critical to understand\nwhere model predictions are uncertain and ensure that model\nbehavior is safe and trustworthy. Traditional approaches to\nuncertainty estimation use a probabilistic approach \u2013 where\nexamples a model is uncertain about are assigned low prob\u0002abilities or scores (Agarwal and Hooker, 2020; Baldock\net al., 2021; Denker and LeCun, 1990; Hendrycks and Gim\u0002pel, 2016; Erfani et al., 2016; Ruff et al., 2018; Parzen,\n1962; Rosenblatt, 1956; Hawkins, 1974; Vandeginste, 1988).\n*Equal contribution 1ML Collective 2\nProQuest LLC 3Harvard\nUniversity 4Google Research, Brain Team. Correspondence\nto: Daniel D\u2019souza, Sara Hooker <ddsouza@umich.edu,\nshooker@google.com>.\nPresented at the ICML 2021 Workshop on Uncertainty and Robust\u0002ness in Deep Learning., Copyright 2021 by the author(s).\nhorse donkey\n(a) Reducible Error (b) Irreducible Error\nFigure 1. Examples of different predictive uncertainties. Left: An\ninstance of the horse class representing error reducible using\nmore data examples. Right: A horse image mislabelled as a\ndonkey, representing irreducible error as the model cannot learn\nthis class distribution even with more examples because of the\ncorrupted label.\nWhile probabilities and other scores are an effective way\nto isolate a subset of examples that are high uncertainty,\nthis estimate of uncertainty is fundamentally limited as it\ncaptures what predictions are challenging for the model but\nnot the underlying source of the uncertainty.\nMost vision and language datasets exhibit a long-tail\ndistribution with an unequal frequency of attributes in\ntraining data (Zipf, 1999; Feldman, 2020). However, the\nnature of these low-frequency attributes differs considerably.\nAtypical examples are rare or unusual attributes \u2013 data\npoints sampled from sparsely populated regions of the\ninput space. Poor model performance on atypical examples\nreflects epistemic uncertainty, where there is insufficient\nevidence for the model to learn the respective distribution.\nIdeally, a model spends more time learning these atypical\nexamples than redundant high frequency attributes which\nare learned early during the training and can subsequently\nbe safely ignored (Agarwal and Hooker, 2020; Paul et al.,\n2021; Mangalam and Prabhu, 2019).\nUnlike atypical examples, noisy examples are due to influ\u0002ences on the data-generating process, such as label corrup\u0002tion or input data perturbation, which impairs the learnabil\u0002ity of the instance. These noisy examples are dominated\nby aleatoric uncertainty or irreducible error because the\nmapping between the input and output space is entirely\narXiv:2107.13098v1 [cs.CV] 27 Jul 2021\nA Tale Of Two Long Tails\nstochastic. Recent works have suggested that labeling noise\nis widespread in widely used datasets, and can constitute\na large fraction of the training set (Hooker et al., 2020a;\nNorthcutt et al., 2021; Beyer et al., 2020).\nThe need for a framework to estimate both the level and\nsource of uncertainty is driven by the very different down\u0002stream remedies for different sources of uncertainty. For\nsources of high epistemic uncertainty, such as low-frequency\natypical attributes or challenging fine-grained samples, a\npractitioner can improve model performance by either col\u0002lecting more data that are similar or re-weighting examples\nto improve model learning of this instance (or other ac\u0002tive learning techniques) (Budd et al., 2021; Zhang et al.,\n2019; Liu et al., 2020; Rosenberg et al., 2005; Shen et al.,\n2018). In contrast, for causes of aleatoric uncertainty, such\nas noisy examples, solutions like down-weighting or elim\u0002ination through data cleaning are advocated (Zhang et al.,\n2020; Li et al., 2019; Thulasidasan et al., 2019b; Liu and\nGuo, 2020; Schroder and Niekler, 2020; Pleiss et al., 2020).\nDespite the importance of identifying the sources of\npredictive uncertainty, it has been relatively under-treated\nin machine learning literature. Probabilistic frameworks\nwill accord high uncertainty to both atypical and noisy\nexamples, failing to distinguish between the two. Moreover,\noptimization techniques that aim to modify training\nto prioritize instances with high uncertainty \u2013 such as\nloss-based prioritization (Loshchilov and Hutter, 2016;\nKawaguchi and Lu, 2020) and importance sampling meth\u0002ods (Katharopoulos and Fleuret, 2019) \u2013 fail to distinguish\nbetween the different sources of uncertainty that dominate\nchallenging examples. In large-scale training settings,\nmodels are often trained on datasets with unknown quality\nand degrees of input or label corruption (Hooker et al.,\n2020a; Beyer et al., 2020; Tsipras et al., 2020; Dehghani\net al., 2021). Recent work empirically demonstrates that\nloss-based acceleration methods degrade in scenarios with\neven a limited amount of noisy and corrupted instances\nin the dataset (Hu et al., 2021; Paul et al., 2021). While\nsub-fields have evolved separately around the treatment\nof low-frequency (Hooker et al., 2020b; Buolamwini and\nGebru, 2018; Hashimoto et al., 2018; S\u0142owik and Bottou,\n2021) and noisy distributions (Wu et al., 2020; Yi and Wu,\n2019; Thulasidasan et al., 2019a), only limited work to date\nhas focused on the sources of uncertainty within a unified\nframework (Kendall and Gal, 2017; Depeweg et al., 2018).\nPresent Work. In this work, we seek to identify examples\nthe model is uncertain about and characterize the source\nof said uncertainty. We leverage the key difference between\nEpistemic and Aleatoric uncertainty \u2013 one error is reducible\nin the presence of additional data, and the other is not. We\npropose targeted data augmentation throughout training to\namplify the difference in learning rate between atypical and\nnoisy examples. Our results show well-designed interven\u0002tions throughout training can be an effective way to cluster\nand distinguish between different sources of uncertainty.\n2. Methodology\n2.1. Sources of Uncertainty\nWe consider a supervised learning setting where we denote\nthe training dataset D as:\nD\ndef =\n\b\n(x1, y1), . . . ,(xN , yN )\n\t\n\u2282 X \u00d7 Y , (1)\nwhere X represents the data space and Y the set of outcomes\nassociated with the respective instances. We consider a\nneural network as a function fw : X 7\u2192 Y with trainable\nweights w. Given the training dataset, fw optimizes a set of\nweights w\n\u2217 by minimizing an objective function L,\nw\n\u2217 = arg min\nw\nL(w) (2)\nHere, we aim to quantify the uncertainty associated with\na model prediction and to subsequently identify the source\nof the uncertainty by classifying examples contributing\ndisproportionately to aleatoric or epistemic uncertainty.\nThus, we firstly would like to obtain a good measure of\npredictive uncertainty related to the predic...",
      "url": "https://arxiv.org/pdf/2107.13098"
    },
    {
      "title": "",
      "text": "Distribution Density, Tails, and Outliers in Machine Learning:\nMetrics and Applications\nNicholas Carlini Ulfar Erlingsson Nicolas Papernot \u00b4\nGoogle Research\nAbstract\nWe develop techniques to quantify the degree to which a given (training or testing) example is an\noutlier in the underlying distribution. We evaluate five methods to score examples in a dataset by how\nwell-represented the examples are, for different plausible definitions of \u201cwell-represented\u201d, and apply these\nto four common datasets: MNIST, Fashion-MNIST, CIFAR-10, and ImageNet. Despite being independent\napproaches, we find all five are highly correlated, suggesting that the notion of being well-represented can be\nquantified. Among other uses, we find these methods can be combined to identify (a) prototypical examples\n(that match human expectations); (b) memorized training examples; and, (c) uncommon submodes of the\ndataset. Further, we show how we can utilize our metrics to determine an improved ordering for curriculum\nlearning, and impact adversarial robustness. We release all metric values on training and test sets we studied.\nFigure 1: Sorting images sampled from the MNIST \u201c3\u201d class, Fashion-MNIST \u201cshirt\u201d class, and CIFAR-10\n\u201cdog\u201d class using our five metrics. Outliers are shown on the left, and well represented examples on the right.\nNotice, for example, the mislabeled \u201c9\u201d for MNIST as a digit that is an outlier, or how many of the poorly\nrepresented Fashion-MNIST \u201cshirts\u201d in fact belong in the different \u201ct-shirt\u201d or \u201cdress\u201d class.\n1 Introduction\nMachine learning (ML) is now applied to problems with sufficiently large datasets that it is difficult to manually\ninspect each training and test point. This drives interest in research that seeks to understand the dataset and the\nunderlying data distribution. Potential uses of these techniques are numerous. On the one hand, they contribute\nto improving how ML is perceived by end users (e.g., one of the motivations behind interpretability efforts).\nOn the other hand, they also help ML practitioners glean insights into the learning procedure. This surfaces\nthe need for tools that enable one to (1) measure and characterize the contribution of each training point to the\nlearning procedure and (2) explain the different failure modes observed on individual test points when the\nmodel infers.\n1\narXiv:1910.13427v1 [cs.LG] 29 Oct 2019\nTowards this goal, prior work has investigated model interpretability, identifying training and test points\nthat are prototypical (Kim et al., 2014), or applying influence functions to measure the contribution of individual\ntraining points to the final model (Koh & Liang, 2017). While defining precisely a prototype remains an open\nproblem, a common intuitive definition of the notion is that prototypes should be \u201ca relatively small number\nof samples from a data set which, if well chosen, can serve as a summary of the original data set\u201d (Bien &\nTibshirani, 2011). In addition to these two examples, there is a wealth of related efforts discussed below.\nIn this work, we take an orthogonal direction and show that rather than trying to identify a single metric or\ntechnique to identify \u201cprototypes\u201d, simultaneously considering a variety of metrics can be more effective to\ndiscover properties of the training data. In particular, we introduce five metrics for measuring to what extent a\nspecific point is well represented or an outlier in a dataset.\nWe explicitly do not define what we mean by well-represented or outlier specifically because we are\ninterested in the interplay between different metrics that may fall under that definition. Indeed, we find that\nwhile the different metrics are highly correlated for most training and test inputs, their disagreement are highly\ninformative.\nIn more detail, our metrics are based on adversarial robustness, retraining stability, ensemble agreement,\nand differentially-private learning. We demonstrate that in addition to supporting use cases previously studied\nin the literature (e.g., identifying prototypes), studying the interplay between these five metrics allows us to\nidentify other types of examples that help form an understanding of the training and inference procedures. They\nprovide a more complete picture of a model\u2019s training and test performance than can be captured by accuracy\nalone. For instance, disagreements between our metrics distinguish memorized training examples\u2014that models\noverfit on to in order to learn, or uncommon submodes\u2014not sufficiently well-represented in the training data\nfor a privacy-preserving model to recognize them at test time. These results hold for all the datasets we\nconsider: MNIST, Fashion-MNIST, CIFAR-10, and ImageNet. We release the results of running our metrics\non these datasets to help other researchers interested in building on our results.\nUsefully, there are advantages to training models using only the well-represented examples: the models\nlearn much faster, their accuracy loss is not great and occurs almost entirely on outlier test examples, and the\nmodels are both easier to interpret and more adversarially robust. Conversely, at the same sample complexity,\nsignificantly higher overall accuracy can be achieved by training models exclusively on outliers\u2014once\nerroneous and misleading examples have been eliminated from the dataset automatically through an analysis\nof the disagreement between our metrics.\nAs an independent result, we show that predictive stability under retraining strongly correlates with\nadversarial distance, and may be used as an approximation. This is particularly interesting for tasks where\ndefining the adversary\u2019s goal when creating an adversarial example (Biggio et al., 2013; Szegedy et al., 2013)\ncan be difficult (e.g., in sequence-to-sequence language modeling).\n2 Identifying Outlier Examples\nIt is important to understand the underlying datasets (both training and testing) used for machine learning\nmodels. In the following, we introduce the five metrics that underly our approach for interpreting datasets.\nEach metric we develop scores examples on a continuum where in one direction the examples are somehow\nmore well-represented in the dataset, and the other direction they are less represented\u2014more of an outlier\u2014in\nthe dataset.\nWe do not define a priori what we mean by well-represented: Rather, we define the term with respect to\nour different algorithms for computing this. As we will demonstrate, our rankings agree with the definition of\nprototypes in many ways. However, their disagreement are useful to identify training and test points that are\n2\nimportant for forming an understanding of the training and inference procedures. Indeed, in Section 3.3 we\ndemonstrate how the metrics allow us to identify memorized exceptions or uncommon submodes at scale in\nthe data.\n2.1 Metrics for Identifying Representitive Examples\nEach of the metrics below begins corresponds to an definition for what one might mean by saying an example\nis representative or an outlier. For each, we provide a concrete method for measuring this informally-specified\nquantity.\nWe study five metrics that we found generalizable and useful; clearly these are not the only possible\nmetrics, and we encourage future work to study other metrics. However, we believe these metrics to cover\na wide range of what one might mean by representative. Other definitions which we considered were either\nunstable 1 or model-specific 2. All of the algorithms we give below are both stable and appear to be consistent\nproperties of the training data, and not the model (e.g., architecture).\nAdversarial Robustness (adv): Examples that well represent the dataset should be more adversarially\nrobust, i.e., more difficult to find an input perturbation which makes them change classification. Indeed,\nas a measure of prototypicality, this exact measure (the distance to the decision boundary measured by an\nadversarial-example attack was) was recently proposed and utilized by Stock & Cisse (2017). Spec...",
      "url": "https://arxiv.org/pdf/1910.13427"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2405.03869] Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2405.03869\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2405.03869**(cs)\n[Submitted on 6 May 2024 ([v1](https://arxiv.org/abs/2405.03869v1)), last revised 1 Nov 2025 (this version, v7)]\n# Title:Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models\nAuthors:[Anshuman Chhabra](https://arxiv.org/search/cs?searchtype=author&amp;query=Chhabra,+A),[Bo Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+B),[Jian Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Prasant Mohapatra](https://arxiv.org/search/cs?searchtype=author&amp;query=Mohapatra,+P),[Hongfu Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H)\nView a PDF of the paper titled Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models, by Anshuman Chhabra and 4 other authors\n[View PDF](https://arxiv.org/pdf/2405.03869)[HTML (experimental)](https://arxiv.org/html/2405.03869v7)> > Abstract:\n> A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models. Comments:|Accepted to ICML 2025 (Oral)|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2405.03869](https://arxiv.org/abs/2405.03869)[cs.LG]|\n|(or[arXiv:2405.03869v7](https://arxiv.org/abs/2405.03869v7)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2405.03869](https://doi.org/10.48550/arXiv.2405.03869)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Anshuman Chhabra [[view email](https://arxiv.org/show-email/fd6498c5/2405.03869)]\n**[[v1]](https://arxiv.org/abs/2405.03869v1)**Mon, 6 May 2024 21:34:46 UTC (20,403 KB)\n**[[v2]](https://arxiv.org/abs/2405.03869v2)**Sun, 12 May 2024 20:20:57 UTC (20,400 KB)\n**[[v3]](https://arxiv.org/abs/2405.03869v3)**Tue, 1 Oct 2024 15:07:09 UTC (20,396 KB)\n**[[v4]](https://arxiv.org/abs/2405.03869v4)**Wed, 2 Oct 2024 01:38:15 UTC (20,395 KB)\n**[[v5]](https://arxiv.org/abs/2405.03869v5)**Sat, 3 May 2025 14:54:17 UTC (5,229 KB)\n**[[v6]](https://arxiv.org/abs/2405.03869v6)**Thu, 22 May 2025 22:58:04 UTC (5,229 KB)\n**[v7]**Sat, 1 Nov 2025 14:54:52 UTC (4,951 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models, by Anshuman Chhabra and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2405.03869)\n* [HTML (experimental)](https://arxiv.org/html/2405.03869v7)\n* [TeX Source](https://arxiv.org/src/2405.03869)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.03869&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.03869&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-05](https://arxiv.org/list/cs.LG/2024-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2405.03869?context=cs)\n[cs.AI](https://arxiv.org/abs/2405.03869?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.03869)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.03869)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.03869)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2405.03869&amp;description=Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2405.03869&amp;title=Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is ...",
      "url": "https://arxiv.org/abs/2405.03869"
    },
    {
      "title": "",
      "text": "Published as a conference paper at ICLR 2019\nDEEP ANOMALY DETECTION WITH\nOUTLIER EXPOSURE\nDan Hendrycks\nUniversity of California, Berkeley\nhendrycks@berkeley.edu\nMantas Mazeika\nUniversity of Chicago\nmantas@ttic.edu\nThomas Dietterich\nOregon State University\ntgd@oregonstate.edu\nABSTRACT\nIt is important to detect anomalous inputs when deploying machine learning\nsystems. The use of larger and more complex inputs in deep learning magnifies\nthe difficulty of distinguishing between anomalous and in-distribution examples.\nAt the same time, diverse image and text data are available in enormous quantities.\nWe propose leveraging these data to improve deep anomaly detection by training\nanomaly detectors against an auxiliary dataset of outliers, an approach we call\nOutlier Exposure (OE). This enables anomaly detectors to generalize and detect\nunseen anomalies. In extensive experiments on natural language processing and\nsmall- and large-scale vision tasks, we find that Outlier Exposure significantly\nimproves detection performance. We also observe that cutting-edge generative\nmodels trained on CIFAR-10 may assign higher likelihoods to SVHN images\nthan to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the\nflexibility and robustness of Outlier Exposure, and identify characteristics of the\nauxiliary dataset that improve performance.\n1 INTRODUCTION\nMachine Learning systems in deployment often encounter data that is unlike the model\u2019s training\ndata. This can occur in discovering novel astronomical phenomena, finding unknown diseases, or\ndetecting sensor failure. In these situations, models that can detect anomalies (Liu et al., 2018;\nEmmott et al., 2013) are capable of correctly flagging unusual examples for human intervention, or\ncarefully proceeding with a more conservative fallback policy.\nBehind many machine learning systems are deep learning models (Krizhevsky et al., 2012) which\ncan provide high performance in a variety of applications, so long as the data seen at test time is\nsimilar to the training data. However, when there is a distribution mismatch, deep neural network\nclassifiers tend to give high confidence predictions on anomalous test examples (Nguyen et al.,\n2015). This can invalidate the use of prediction probabilities as calibrated confidence estimates\n(Guo et al., 2017), and makes detecting anomalous examples doubly important.\nSeveral previous works seek to address these problems by giving deep neural network classifiers\na means of assigning anomaly scores to inputs. These scores can then be used for detecting out\u0002of-distribution (OOD) examples (Hendrycks & Gimpel, 2017; Lee et al., 2018; Liu et al., 2018).\nThese approaches have been demonstrated to work surprisingly well for complex input spaces, such\nas images, text, and speech. Moreover, they do not require modeling the full data distribution, but\ninstead can use heuristics for detecting unmodeled phenomena. Several of these methods detect\nunmodeled phenomena by using representations from only in-distribution data.\nIn this paper, we investigate a complementary method where we train models to detect unmodeled\ndata by learning cues for whether an input is unmodeled. While it is difficult to model the full data\ndistribution, we can learn effective heuristics for detecting out-of-distribution inputs by exposing\nthe model to OOD examples, thus learning a more conservative concept of the inliers and enabling\nthe detection of novel forms of anomalies. We propose leveraging diverse, realistic datasets for this\npurpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to\nconsistently improve existing methods for OOD detection.\nThrough numerous experiments, we extensively evaluate the broad applicability of Outlier Expo\u0002sure. For multiclass neural networks, we provide thorough results on Computer Vision and Natural\n1\narXiv:1812.04606v3 [cs.LG] 28 Jan 2019\nPublished as a conference paper at ICLR 2019\nLanguage Processing tasks which show that Outlier Exposure can help anomaly detectors generalize\nto and perform well on unseen distributions of outliers, even on large-scale images. We also demon\u0002strate that Outlier Exposure provides gains over several existing approaches to out-of-distribution\ndetection. Our results also show the flexibility of Outlier Exposure, as we can train various mod\u0002els with different sources of outlier distributions. Additionally, we establish that Outlier Exposure\ncan make density estimates of OOD samples significantly more useful for OOD detection. Fi\u0002nally, we demonstrate that Outlier Exposure improves the calibration of neural network classifiers\nin the realistic setting where a fraction of the data is OOD. Our code is made publicly available at\nhttps://github.com/hendrycks/outlier-exposure.\n2 RELATED WORK\nOut-of-Distribution Detection with Deep Networks. Hendrycks & Gimpel (2017) demonstrate\nthat a deep, pre-trained classifier has a lower maximum softmax probability on anomalous examples\nthan in-distribution examples, so a classifier can conveniently double as a consistently useful out\u0002of-distribution detector. Building on this work, DeVries & Taylor (2018) attach an auxiliary branch\nonto a pre-trained classifier and derive a new OOD score from this branch. Liang et al. (2018) present\na method which can improve performance of OOD detectors that use a softmax distribution. In\nparticular, they make the maximum softmax probability more discriminative between anomalies and\nin-distribution examples by pre-processing input data with adversarial perturbations (Goodfellow\net al., 2015). Unlike in our work, their parameters are tailored to each source of anomalies.\nLee et al. (2018) train a classifier concurrently with a GAN (Radford et al., 2016; Goodfellow et al.,\n2014), and the classifier is trained to have lower confidence on GAN samples. For each testing\ndistribution of anomalies, they tune the classifier and GAN using samples from that out-distribution,\nas discussed in Appendix B of their work. Unlike Liang et al. (2018); Lee et al. (2018), in this work\nwe train our method without tuning parameters to fit specific types of anomaly test distributions, so\nour results are not directly comparable with their results. Many other works (de Vries et al., 2016;\nSubramanya et al., 2017; Malinin & Gales, 2018; Bevandic et al., 2018) also encourage the model\nto have lower confidence on anomalous examples. Recently, Liu et al. (2018) provide theoretical\nguarantees for detecting out-of-distribution examples under the assumption that a suitably powerful\nanomaly detector is available.\nUtilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from\ntest-time data in order to teach the network better representations for anomaly detection. Goodfel\u0002low et al. (2015) train on adversarial examples to increased robustness. Salakhutdinov et al. (2011)\npre-train unsupervised deep models on a database of web images for stronger features. Radford\net al. (2017) train an unsupervised network on a corpus of Amazon reviews for a month in order\nto obtain quality sentiment representations. Zeiler & Fergus (2014) find that pre-training a network\non the large ImageNet database (Russakovsky et al., 2015) endows the network with general\nrepresentations that are useful in many fine-tuning applications. Chen & Gupta (2015); Mahajan\net al. (2018) show that representations learned from images scraped from the nigh unlimited source\nof search engines and photo-sharing websites improve object detection performance.\n3 OUTLIER EXPOSURE\nWe consider the task of deciding whether or not a sample is from a learned distribution called Din.\nSamples from Din are called \u201cin-distribution,\u201d and otherwise are said to be \u201cout-of-distribution\u201d\n(OOD) or samples from Dout. In real applications, it may be difficult to know the distribution\nof outliers one will encounter in advance. Thus, we consider the realistic setting where Dout is\nunknown. Given a parametri...",
      "url": "https://web.engr.oregonstate.edu/~tgd/publications/hendrycks-mazeika-dietterich-deep-anomaly-detection-with-outlier-exposure-arxiv2019.pdf"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[1803.09050] Learning to Reweight Examples for Robust Deep Learning\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1803.09050\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1803.09050**(cs)\n[Submitted on 24 Mar 2018 ([v1](https://arxiv.org/abs/1803.09050v1)), last revised 5 May 2019 (this version, v3)]\n# Title:Learning to Reweight Examples for Robust Deep Learning\nAuthors:[Mengye Ren](https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+M),[Wenyuan Zeng](https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+W),[Bin Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+B),[Raquel Urtasun](https://arxiv.org/search/cs?searchtype=author&amp;query=Urtasun,+R)\nView a PDF of the paper titled Learning to Reweight Examples for Robust Deep Learning, by Mengye Ren and 3 other authors\n[View PDF](https://arxiv.org/pdf/1803.09050)> > Abstract:\n> Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available. Comments:|13 pages; Published at ICML 2018; Code released at:[this https URL](https://github.com/uber-research/learning-to-reweight-examples)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1803.09050](https://arxiv.org/abs/1803.09050)[cs.LG]|\n|(or[arXiv:1803.09050v3](https://arxiv.org/abs/1803.09050v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1803.09050](https://doi.org/10.48550/arXiv.1803.09050)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Mengye Ren [[view email](https://arxiv.org/show-email/baf9aec0/1803.09050)]\n**[[v1]](https://arxiv.org/abs/1803.09050v1)**Sat, 24 Mar 2018 03:41:59 UTC (141 KB)\n**[[v2]](https://arxiv.org/abs/1803.09050v2)**Fri, 8 Jun 2018 15:29:31 UTC (2,579 KB)\n**[v3]**Sun, 5 May 2019 15:21:40 UTC (2,804 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning to Reweight Examples for Robust Deep Learning, by Mengye Ren and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1803.09050)\n* [TeX Source](https://arxiv.org/src/1803.09050)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1803.09050&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1803.09050&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-03](https://arxiv.org/list/cs.LG/2018-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/1803.09050?context=cs)\n[stat](https://arxiv.org/abs/1803.09050?context=stat)\n[stat.ML](https://arxiv.org/abs/1803.09050?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1803.09050)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1803.09050)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1803.09050)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1803.html#abs-1803-09050)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1803-09050)\n[Mengye Ren](<https://dblp.uni-trier.de/search/author?author=Mengye Ren>)\n[Wenyuan Zeng](<https://dblp.uni-trier.de/search/author?author=Wenyuan Zeng>)\n[Bin Yang](<https://dblp.uni-trier.de/search/author?author=Bin Yang>)\n[Raquel Urtasun](<https://dblp.uni-trier.de/search/author?author=Raquel Urtasun>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/1803.09050&amp;description=Learning to Reweight Examples for Robust Deep Learning>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/1803.09050&amp;title=Learning to Reweight Examples for Robust Deep Learning>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXiv...",
      "url": "https://arxiv.org/abs/1803.09050"
    },
    {
      "title": "How to Make Your Machine Learning Models Robust to Outliers - KDnuggets",
      "text": "**By [Alvira Swalin](https://www.linkedin.com/in/alvira-swalin), University of San Francisco**\n\n_\u201cSo unexpected was the hole that for several years computers analyzing ozone data had systematically thrown out the readings that should have pointed to its growth.\u201d\u200a\u2014\u200aNew Scientist 31st March 1988_\n\nAccording to Wikipedia, an\u00a0**outlier**\u00a0is an observation point that is distant from other observations. This definition is vague because it doesn\u2019t quantify the word \u201cdistant\u201d. In this blog, we\u2019ll try to understand the different interpretations of this \u201cdistant\u201d notion. We will also look into the outlier detection and treatment techniques while seeing their impact on different types of machine learning models.\n\nOutliers arise due to changes in system behavior, fraudulent behavior, human error, instrument error, or simply through natural deviations in populations. A sample may have been contaminated with elements from outside the population being examined.\n\nMany machine learning models, like linear & logistic regression, are easily impacted by the outliers in the training data. Models like AdaBoost increase the weights of misclassified points on every iteration and therefore might put high weights on these outliers as they tend to be often misclassified. This can become an issue if that outlier is an error of some type, or if we want our model to generalize well and not care for extreme values.\n\nTo overcome this issue, we can either change the model or metric, or we can make some changes in the data and use the same models. For the analysis, we will look into\u00a0[House Prices Kaggle Data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). All the codes for plots and implementation can be found on this\u00a0[Github Repository](https://github.com/aswalin/Outlier-Impact-Treatment).\n\n### What do we mean by outliers?\n\nExtreme values can be present in both dependent & independent variables, in the case of supervised learning methods.\n\nThese extreme values need not necessarily impact the model performance or accuracy, but when they do they are called\u00a0**\u201cInfluential\u201d**\u00a0points.\n\n**Extreme Values in Independent Variables**\n\nThese are called points of\u00a0**\u201chigh leverage**\u201d. With a single predictor, an extreme value is simply one that is particularly high or low. With multiple predictors, extreme values may be particularly high or low for one or more predictors\u00a0**_(univariate analysis\u200a\u2014\u200aanalysis of one variable at a time)_**\u00a0or may be \u201cunusual\u201d combinations of predictor values\u00a0**_(multivariate analysis)_**\n\nIn the following figure, all the points on the right-hand side of the orange line are leverage points.\n\n**Extreme Values in Target Variables**\n\nRegression\u200a\u2014\u200athese extreme values are termed as\u00a0**\u201coutliers\u201d**. They may or may not be influential points, which we will see later. In the following figure, all the points above the orange line can be classified as outliers.\n\nClassification: Here, we have two types of extreme values:\n\n**1\\. Outliers:**\u00a0For example, in an image classification problem in which we\u2019re trying to identify dogs/cats, one of the images in the training set has a gorilla (or any other category not part of the goal of the problem) by mistake. Here, the gorilla image is clearly noise. Detecting outliers here does not make sense because we already know which categories we want to focus on and which to discard\n\n**2\\. Novelties:**\u00a0Many times we\u2019re dealing with novelties, and the problem is often called\u00a0**supervised anomaly detection**. In this case, the goal is not to remove outliers or reduce their impact, but we are interested in detecting anomalies in new observations. Therefore we won\u2019t be discussing it in this post. It is especially used for fraud detection in credit-card transactions, fake calls, etc.\n\nAll the points we have discussed above, including influential points, will become very clear once we visualize the following figure.\n\n> **_Inference_**\n>\n> _\\- Points in Q1: Outliers_\n>\n> _\\- Points in Q3: Leverage Points_\n>\n> _\\- Points in Q2: Both outliers & leverage but non-influential points_\n>\n> _\\- Circled points: Example of Influential Points. There can be more but these are the prominent ones_\n\nOur major focus will be outliers (extreme values in\u00a0**target variable**\u00a0for further investigation and treatment). We\u2019ll see the impact of these extreme values on the model\u2019s performance.\n\n### Common Methods for Detecting Outliers\n\nWhen detecting outliers, we are either doing univariate analysis or multivariate analysis. When your linear model has a single predictor, then you can use univariate analysis. However, it can give misleading results if you use it for multiple predictors. One common way of performing outlier detection is\u00a0**to assume that the regular data come from a known distribution**\u00a0(e.g. data are Gaussian distributed). This assumption is discussed in the Z-Score method section below.\n\n**Box-Plot**\n\nThe quickest and easiest way to identify outliers is by visualizing them using plots. If your dataset is not huge (approx. up to 10k observations & 100 features), I would highly recommend you build scatter plots & box-plots of variables. If there aren\u2019t outliers, you\u2019ll definitely gain some other insights like correlations, variability, or external factors like the impact of world war/recession on economic factors. However, this method is not recommended for high dimensional data where the power of visualization fails.\n\nThe box plot uses inter-quartile range to detect outliers. Here, we first determine the quartiles\u00a0_Q_ 1 and\u00a0_Q_ 3.\n\nInterquartile range is given by, IQR = Q3\u200a\u2014\u200aQ1\n\nUpper limit = Q3+1.5\\*IQR\n\nLower limit = Q1\u20131.5\\*IQR\n\nAnything below the lower limit and above the upper limit is considered an outlier\n\n**Cook\u2019s Distance**\n\nThis is a multivariate approach for finding influential points. These points may or may not be outliers as explained above, but they have the power to influence the regression model. We will see their impact in the later part of the blog.\n\nThis method is used only for linear regression and therefore has a limited application.\u00a0[Cook\u2019s distance](https://en.wikipedia.org/wiki/Cook%27s_distance)\u00a0measures the effect of deleting a given observation. It\u2019s represents the sum of all the changes in the regression model when observation\u00a0**\u201ci\u201d**\u00a0is removed from it.\n\nHere, p is the number of predictors and s\u00b2 is the mean squared error of the regression model. There are different views regarding the cut-off values to use for spotting highly influential points. A rule of thumb is that D(i) > 4/n, can be good cut off for influential points.\n\nR has the\u00a0[car](http://cran.r-project.org/web/packages/car/index.html)\u00a0(Companion to Applied Regression) package where you can directly find outliers using Cook\u2019s distance. Implementation is provided in this\u00a0[R-Tutorial](https://www.statmethods.net/stats/rdiagnostics.html). Another similar approach is\u00a0**DFFITS**, which you can see details of\u00a0[here](https://newonlinecourses.science.psu.edu/stat501/node/340/).\n\n**Z-Score**\n\nThis method assumes that the variable has a Gaussian distribution. It represents the number of standard deviations an observation is away from the mean:\n\nHere, we normally define outliers as points whose modulus of z-score is greater than a threshold value. This threshold value is usually greater than 2 (3 is a common value).\n\nAll the above methods are good for initial analysis of data, but they don\u2019t have much value in multivariate settings or with high dimensional data. For such datasets, we have to use advanced methods like\u00a0**PCA, LOF (Local Outlier Factor) & HiCS: High Contrast Subspaces for Density-Based Outlier Ranking**.\n\nWe won\u2019t be discussing these methods in this blog, as they are beyond its scope. Our focus here is to see how various outlier treatment techniques affect the performance of models. You can read\u00a0[this blog](https://www.datasciencecentral.com/profiles/blogs/introduction-to-outlier-detection-methods)\u00a0for details on these methods.\n\n### Impac...",
      "url": "https://kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html"
    },
    {
      "title": "How to Make Your Machine Learning Models Robust to Outliers",
      "text": "_\u201cSo unexpected was the hole that for several years computers analyzing ozone data had systematically thrown out the readings that should have pointed to its growth.\u201d \u2014 New Scientist 31st March 1988_\n\nAccording to [Wikipedia](https://en.wikipedia.org/wiki/Outlier), an **outlier** is an observation point that is distant from other observations. This definition is vague because it doesn\u2019t quantify the word \u201cdistant\u201d. In this blog, we\u2019ll try to understand the different interpretations of this \u201cdistant\u201d notion. We will also look into the outlier detection and treatment techniques while seeing their impact on different types of machine learning models.\n\nOutliers arise due to changes in system behavior, fraudulent behavior, human error, instrument error, or simply through natural deviations in populations. A sample may have been contaminated with elements from outside the population being examined.\n\nMany machine learning models, like [linear & logistic regression](https://fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0), are easily impacted by the outliers in the training data. Models like [AdaBoost](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/) increase the weights of misclassified points on every iteration and therefore might put high weights on these outliers as they tend to be often misclassified. This can become an issue if that outlier is an error of some type, or if we want our model to generalize well and not care for extreme values.\n\nTo overcome this issue, we can either change the model or metric, or we can make some changes in the data and use the same models. For the analysis, we will look into [House Prices Kaggle Data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). All the codes for plots and implementation can be found on this [GitHub Repository](https://github.com/aswalin/Outlier-Impact-Treatment).\n\n## What do we mean by outliers?\n\nExtreme values can be present in both dependent & independent variables, in the case of supervised learning methods.\n\nThese extreme values need not necessarily impact the model performance or accuracy, but when they do they are called **\u201cInfluential\u201d** points.\n\n### Extreme Values in Independent Variables\n\nThese are called points of **\u201chigh leverage**\u201d. With a single predictor, an extreme value is simply one that is particularly high or low. With multiple predictors, extreme values may be particularly high or low for one or more predictors **_(univariate analysis \u2014 analysis of one variable at a time)_** or may be \u201cunusual\u201d combinations of predictor values **_(multivariate analysis)_**\n\nIn the following figure, all the points on the right-hand side of the orange line are leverage points.\n\n### Extreme Values in Target Variables\n\nRegression \u2014 these extreme values are termed as **\u201coutliers\u201d**. They may or may not be influential points, which we will see later. In the following figure, all the points above the orange line can be classified as outliers.\n\nClassification: Here, we have two types of extreme values:\n\n**1\\. Outliers:** For example, in an image classification problem in which we\u2019re trying to identify dogs/cats, one of the images in the training set has a gorilla (or any other category not part of the goal of the problem) by mistake. Here, the gorilla image is clearly noise. Detecting outliers here does not make sense because we already know which categories we want to focus on and which to discard\n\n**2\\. Novelties:** Many times we\u2019re dealing with novelties, and the problem is often called **supervised anomaly detection**. In this case, the goal is not to remove outliers or reduce their impact, but we are interested in detecting anomalies in new observations. Therefore we won\u2019t be discussing it in this post. It is especially used for fraud detection in credit-card transactions, fake calls, etc.\n\nAll the points we have discussed above, including influential points, will become very clear once we visualize the following figure.\n\nOur major focus will be outliers (extreme values in **target variable** for further investigation and treatment). We\u2019ll see the impact of these extreme values on the model\u2019s performance.\n\n## Common Methods for Detecting Outliers\n\nWhen detecting outliers, we are either doing univariate analysis or multivariate analysis. When your linear model has a single predictor, then you can use univariate analysis. However, it can give misleading results if you use it for multiple predictors. One common way of performing outlier detection is **to assume that the regular data come from a known distribution** (e.g. data are Gaussian distributed). This assumption is discussed in the Z-Score method section below.\n\n### Box-Plot\n\nThe quickest and easiest way to identify outliers is by [visualizing them](https://fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39) using plots. If your dataset is not huge (approx. up to 10k observations & 100 features), I would highly recommend you build scatter plots & box-plots of variables. If there aren\u2019t outliers, you\u2019ll definitely gain some other insights like correlations, variability, or external factors like the impact of world war/recession on economic factors. However, this method is not recommended for high dimensional data where the power of visualization fails.\n\nThe box plot uses inter-quartile range to detect outliers. Here, we first determine the quartiles _Q_ 1 and _Q_ 3.\n\nInterquartile range is given by, IQR = Q3 \u2014 Q1\n\nUpper limit = Q3+1.5\\*IQR\n\nLower limit = Q1\u20131.5\\*IQR\n\nAnything below the lower limit and above the upper limit is considered an outlier\n\n### Cook\u2019s Distance\n\nThis is a multivariate approach for finding influential points. These points may or may not be outliers as explained above, but they have the power to influence the regression model. We will see their impact in the later part of the blog.\n\nThis method is used only for linear regression and therefore has a limited application. [Cook\u2019s distance](https://en.wikipedia.org/wiki/Cook%27s_distance) measures the effect of deleting a given observation. It\u2019s represents the sum of all the changes in the regression model when observation **\u201ci\u201d** is removed from it.\n\nHere, p is the number of predictors and s\u00b2 is the mean squared error of the regression model. There are different views regarding the cut-off values to use for spotting highly influential points. A rule of thumb is that D(i) > 4/n, can be good cut off for influential points.\n\nR has the [car](http://cran.r-project.org/web/packages/car/index.html) (Companion to Applied Regression) package where you can directly find outliers using Cook\u2019s distance. Implementation is provided in this [R-Tutorial](https://www.statmethods.net/stats/rdiagnostics.html). Another similar approach is **DFFITS**, which you can see details of [here](https://newonlinecourses.science.psu.edu/stat501/node/340/).\n\n### Z-Score\n\nThis method assumes that the variable has a Gaussian distribution. It represents the number of standard deviations an observation is away from the mean:\n\nHere, we normally define outliers as points whose modulus of z-score is greater than a threshold value. This threshold value is usually greater than 2 (3 is a common value).\n\nAll the above methods are good for initial analysis of data, but they don\u2019t have much value in multivariate settings or with high dimensional data. For such datasets, we have to use advanced methods like **PCA, LOF (Local Outlier Factor) & HiCS: High Contrast Subspaces for Density-Based Outlier Ranking**.\n\nWe won\u2019t be discussing these methods in this blog, as they are beyond its scope. Our focus here is to see how various outlier treatment techniques affect the performance of models. You can read [this blog](https://www.datasciencecentral.com/profiles/blogs/introduction-to-outlier-detection-methods) for details on these methods.\n\n## Impact & Treatment of Outliers\n\nThe impact of outliers can be seen not...",
      "url": "https://fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers"
    }
  ]
}