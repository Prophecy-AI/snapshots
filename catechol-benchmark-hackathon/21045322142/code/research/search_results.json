{
  "query": "How can I reduce the intercept in the relationship between cross-validation and test performance when there is a large distribution shift in chemistry machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Reducing the systematic gap (the intercept) between cross\u2011validation (CV) estimates and the true test performance under a large distribution shift can be tackled by making the CV procedure itself aware of the shift.  A classic approach is **importance\u2011weighted cross\u2011validation (IW\u2011CV)**, which re\u2011weights each training example by the ratio\u202fp_test(x)/p_train(x) so that the CV risk remains unbiased even when the covariate distribution changes\u202f([jmlr](https://www.jmlr.org/papers/volume8/sugiyama07a/sugiyama07a.pdf)).  Building on this idea, robust covariate\u2011shift adjustment (RCSA) learns weights that also account for possible model misspecification, further shrinking the CV\u2011test performance gap\u202f([mlrpress](https://proceedings.mlr.press/v32/wen14.pdf)).  \n\nIn chemistry\u2011focused settings, researchers have found that **updating the calibration (or validation) set with data that are chemically closer to the intended test domain** restores model validity and lowers the intercept.  For example, swapping the original calibration set for more recent Tox21 or proprietary assay data dramatically reduced the error inflation caused by drift\u202f([nature](https://www.nature.com/articles/s41598-022-09309-3.pdf?error=cookies_not_supported&code=0810be7a-ee95-4146-a65b-0cfe63d7fc86)); the same strategy works with conformal prediction, where only the calibration pool is refreshed without retraining the underlying model\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/63e2b82b6d032916bb650914)).  \n\nFinally, **time\u2011aware or forward\u2011step CV** (k\u2011fold n\u2011step forward CV) mimics the chronological drift that occurs in drug\u2011discovery pipelines, providing a more realistic estimate of out\u2011of\u2011distribution performance and therefore a smaller intercept when the model is finally applied to future data\u202f([ncbi](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  Combining importance weighting, robust re\u2011weighting, and a forward\u2011split validation scheme\u2014while periodically refreshing the calibration set with the latest chemical data\u2014offers a practical recipe for aligning CV predictions with real\u2011world test performance despite large distribution shifts.",
      "url": ""
    },
    {
      "title": "Studying and mitigating the effects of data drifts on ML model performance at the example of chemical toxicity data",
      "text": "1\nVol.:(0123456789)\nScientifc Reports | (2022) 12:7244 | https://doi.org/10.1038/s41598-022-09309-3\nwww.nature.com/scientificreports\nStudying and mitigating the efects \nof data drifts on ML model \nperformance at the example \nof chemical toxicity data\nAndrea Morger1,8, MarinaGarcia de Lomana2,3,8, Ulf Norinder4,5,6, Fredrik Svensson7, \nJohannes Kirchmair3\n, Miriam Mathea2* & AndreaVolkamer1*\nMachine learning models are widely applied to predict molecular properties or the biological activity \nof small molecules on a specifc protein. Models can be integrated in a conformal prediction (CP) \nframework which adds a calibration step to estimate the confdence of the predictions. CP models \npresent the advantage of ensuring a predefned error rate under the assumption that test and \ncalibration set are exchangeable. In cases where the test data have drifted away from the descriptor \nspace of the training data, or where assay setups have changed, this assumption might not be fulflled \nand the models are not guaranteed to be valid. In this study, the performance of internally valid CP \nmodels when applied to either newer time-split data or to external data was evaluated. In detail, \ntemporal data drifts were analysed based on twelve datasets from the ChEMBL database. In addition, \ndiscrepancies between models trained on publicly-available data and applied to proprietary data \nfor the liver toxicity and MNT in vivo endpoints were investigated. In most cases, a drastic decrease \nin the validity of the models was observed when applied to the time-split or external (holdout) test \nsets. To overcome the decrease in model validity, a strategy for updating the calibration set with data \nmore similar to the holdout set was investigated. Updating the calibration set generally improved \nthe validity, restoring it completely to its expected value in many cases. The restored validity is the \nfrst requisite for applying the CP models with confdence. However, the increased validity comes \nat the cost of a decrease in model efciency, as more predictions are identifed as inconclusive. This \nstudy presents a strategy to recalibrate CP models to mitigate the efects of data drifts. Updating the \ncalibration sets without having to retrain the model has proven to be a useful approach to restore the \nvalidity of most models.\nMachine learning (ML) models are usually trained\u2014and evaluated\u2014on available historical data, and then used \nto make predictions on prospective data. Tis strategy is ofen applied in the context of toxicological data to \npredict potential toxic efects of novel compounds1\u20136\n. Internal cross-validation\u00a0(CV) is a common practice for \nassessing the performance of ML models. When applying the model to new data, it is advisable to observe the \napplicability domain\u00a0(AD) of an ML model7,8\n. Te AD determines the compound space and the response value \n(label) range in which the model makes reliable predictions9\n. Investigating classifcation models, Mathea et\u00a0al.8\ndistinguished AD methods that rely on novelty from those relying on confdence estimation. Novelty detection \nmethods focus on the ft of the query samples to the given descriptor space. Confdence estimation methods \ndetermine the reliability of the predictions by taking into account that samples may be well-embedded in the \ndescriptor space but be unusual in terms of their class membership.\nOPEN\n1\nIn Silico Toxicology and Structural Bioinformatics, Institute of Physiology, Charit\u00e9 Universit\u00e4tsmedizin Berlin, \nBerlin\u00a0 10117, Germany. 2\nBASF SE, 67056\u00a0 Ludwigshafen, Germany. 3Division of Pharmaceutical Chemistry, \nDepartment of Pharmaceutical Sciences, University of Vienna, Vienna\u00a0 1090, Austria. 4\nDepartment of \nPharmaceutical Biosciences, Uppsala University, Uppsala\u00a0751 24, Sweden. 5\nDept Computer and Systems Sciences, \nStockholm University, Kista\u00a0 164 07, Sweden. 6\nMTM Research Centre, School of Science and Technology, 701\n82\u00a0 \u00d6rebro, Sweden. 7\nAlzheimer\u2019s Research UK UCL Drug Discovery Institute, London\u00a0WC1E 6BT, UK. 8These \nauthors contributed equally: Andrea Morger and Marina Garcia de Lomana. *email: miriam.mathea@basf.com; \nandrea.volkamer@charite.de\n2\nVol:.(1234567890)\nScientifc Reports | (2022) 12:7244 | https://doi.org/10.1038/s41598-022-09309-3\nwww.nature.com/scientificreports/\nA popular method for confdence estimation is conformal prediction (CP)10,11. Te framework of an inductive \nconformal predictor uses three types of datasets: proper training, calibration, and test set. Te proper training \nset is used to train an underlying ML model. With this model, predictions are made for the calibration and test \nset. According to the rank that is obtained for the prediction outcome of the test compound as compared to the \ncalibration set, so-called p-values are calculated to give an estimate of the likelihood of a compound to belong to \na certain class. If a signifcance level, i.e. an expected error rate, is defned, the compounds are assigned labels for \nthose classes where the p-value is larger than the signifcance level. For binary classifcation, the possible predic\u0002tion sets are \u2018empty\u2019 ({\u2205}), \u2018single class\u2019 ({0}, {1}), and \u2018both\u2019 ({0,1}). Single class predictions indicate a confdent \nprediction for a certain class. Additionally, the CP framework recognises compounds for which it cannot make \na reliable prediction ({\u2205}) and compounds at the decision boundary, for which the predictions are reliable but \nindecisive ({0,1}). Provided that the calibration and test data are exchangeable, the framework of the conformal \npredictor is mathematically proven to yield valid predictions at a given signifcance level10,11.\nTe performance and AD of a model are determined by the quality and quantity of the data it has been \ntrained on. One prerequisite for building good models is the availability of large, well-distributed and consist\u0002ent datasets. To assemble large datasets, modellers ofen need to collect data from diferent sources, e.g. data \nwhich were produced in diferent assays or laboratories or over longer periods of time12\u201314. However, data from \ndiferent sources and data taken at diferent time points may have distinct property distributions, refecting, \nfor example, the evolution of research interests or changes in assay technologies and protocols15,16. Since the \npredictivity of ML models is constrained by their AD, data drifs pose a challenge to modelling tasks, including \ntoxicity or bioactivity prediction.\nWhen ML models are validated using CV, the data is usually randomly split into training and test data. Te \nresulting sets intrinsically stem from the same distribution and, typically, high model performance on the test \nset is observed. Nevertheless, it has been shown that model performance can be substantially lower for datasets \nobtained by time split or datasets from other sources5,17\u201319. Tis may be an indicator that the distribution of the \ndata has changed. Hence, it is essential to confrm that ML models can be applied to a specifc dataset and to \ndetermine the confdence in the predictions.\nTe data drifs, which challenge the underlying ML models, do also afect conformal predictors when the \ntrained and calibrated models are applied to a new dataset. In previous work17, a new strategy was introduced \nto mitigate the efects related to data drifs by exchanging the calibration set with data closer to the holdout \nset. Te study built on the Tox21 data challenge2\n, which was invented to support and compare ML models for \ntwelve toxicity endpoints and included three subsequently released datasets. We showed that internally valid CP \nmodels resulted in poor performance when predicting the holdout data. Te observed efects were associated \nto data drifs between datasets and could be mitigated by exchanging the calibration set with the intermediate \nset\u2014without the need to retrain the models.\nHere, we aim to expand and challenge our previous analysis on the recalibration strategy by a wider variety \nof datasets, be...",
      "url": "https://www.nature.com/articles/s41598-022-09309-3.pdf?error=cookies_not_supported&code=0810be7a-ee95-4146-a65b-0cfe63d7fc86"
    },
    {
      "title": "",
      "text": "Journal of Machine Learning Research 8 (2007) 985-1005 Submitted 6/06; Revised 8/06; Published 5/07\nCovariate Shift Adaptation by Importance Weighted Cross Validation\nMasashi Sugiyama SUGI@CS.TITECH.AC.JP\nDepartment of Computer Science\nTokyo Institute of Technology\n2-12-1, O-okayama, Meguro-ku, Tokyo, 152-8552, Japan\nMatthias Krauledat MATTHIAS.KRAULEDAT@FIRST.FHG.DE\nKlaus-Robert Muller \u00a8 KLAUS@FIRST.FHG.DE\nDepartment of Computer Science\nTechnical University Berlin\nFranklinstr. 28/29, 10587 Berlin, Germany\nEditor: Yoshua Bengio\nAbstract\nA common assumption in supervised learning is that the input points in the training set follow\nthe same probability distribution as the input points that will be given in the future test phase.\nHowever, this assumption is not satisfied, for example, when the outside of the training region is\nextrapolated. The situation where the training input points and test input points follow different\ndistributions while the conditional distribution of output values given input points is unchanged\nis called the covariate shift. Under the covariate shift, standard model selection techniques such\nas cross validation do not work as desired since its unbiasedness is no longer maintained. In this\npaper, we propose a new method called importance weighted cross validation (IWCV), for which\nwe prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one\nthat can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV\nexist for regression. The usefulness of our proposed method is illustrated by simulations, and\nfurthermore demonstrated in the brain-computer interface, where strong non-stationarity effects\ncan be seen between training and test sessions.\nKeywords: covariate shift, cross validation, importance sampling, extrapolation, brain-computer\ninterface\n1. Introduction\nThe goal of supervised learning is to infer an unknown input-output dependency from training\nsamples, by which output values for unseen test input points can be estimated. When developing\na method of supervised learning, it is commonly assumed that the input points in the training set\nand the input points used for testing follow the same probability distribution (e.g., Wahba, 1990;\nBishop, 1995; Vapnik, 1998; Duda et al., 2001; Hastie et al., 2001; Scholk \u00a8 opf and Smola, 2002).\nHowever, this common assumption is not fulfilled, for example, when we extrapolate outside of\nthe training region1 or when training input points are designed by an active learning (experimental\ndesign) algorithm. The situation where the training input points and test input points follow different\n1. The term \u2018extrapolation\u2019 could have been defined in a narrow sense as prediction in regions with no training samples.\nOn the other hand, the situation we are considering here is \u2018weak\u2019 extrapolation; prediction is carried out in the region\nwhere only a small number of training samples is available.\n\rc 2007 Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller \u00a8 .\nSUGIYAMA, KRAULEDAT AND MU\u00a8 LLER\nprobability distributions but the conditional distributions of output values given input points are\nunchanged is called the covariate shift (Shimodaira, 2000). For data from many applications such\nas off-policy reinforcement learning (Shelton, 2001), spam filtering (Bickel and Scheffer, 2007),\nbioinformatics (Baldi et al., 1998; Borgwardt et al., 2006) or brain-computer interfacing (Wolpaw\net al., 2002), the covariate shift phenomenon is conceivable. Sample selection bias (Heckman, 1979)\nin economics may also include a form of the covariate shift. Illustrative examples of covariate shift\nsituations are depicted in Figures 1 and 3.\nIn this paper, we develop a new learning method and prove that we can alleviate misestimation\ndue to covariate shift. From the beginning, we note that all the theoretical discussions will be made\nunder the assumption that the ratio of test and training input densities at training input points is\nknown; in experimental studies, the density ratio will be replaced by their empirical estimates and\nthe practical performance of our approach will be evaluated.\nModel selection is one of the key ingredients in machine learning. However, under the covariate\nshift, a standard model selection technique such as cross validation (CV) (Stone, 1974; Wahba,\n1990) does not work as desired; more specifically, the unbiasedness that guarantees the accuracy\nof CV does not hold under the covariate shift anymore. To cope with this problem, we propose a\nnovel variant of CV called importance weighted CV (IWCV). We prove that IWCV gives an almost\nunbiased estimate of the risk even under the covariate shift. Model selection under the covariate\nshift has been studied so far only by few researchers (e.g., Shimodaira, 2000; Sugiyama and Muller, \u00a8\n2005)\u2014existing methods have a number of limitations, for example, in the loss function, parameter\nlearning method, and model. In particular, the existing methods can not be applied to classification\nscenarios. On the other hand, the proposed IWCV overcomes these limitations: it allows for any\nloss function, parameter learning method, and model; even non-parametric learning methods can\nbe employed. To the best of our knowledge, the proposed IWCV is the first method that can be\nsuccessfully applied to model selection in covariate-shifted classification tasks. The usefulness of\nthe proposed method is demonstrated in the brain-computer interface applications, in which existing\nmethods for covariate shift compensation could not be employed.\n2. Problem Formulation\nIn this section, we formulate the supervised learning problem with the covariate shift, and review\nexisting learning methods.\n2.1 Supervised Learning under Covariate Shift\nLet us consider the supervised learning problem of estimating an unknown input-output depen\u0002dency from training samples. Let T = {(xi\n, yi)}\nn\ni=1\nbe the training samples, where xi \u2208 X \u2282 R\nd\nis\nan i.i.d. training input point following a probability distribution Ptrain(x) and yi \u2208 Y \u2282 R is a corre\u0002sponding training output value following a conditional probability distribution P(y|x). P(y|x) may\nbe regarded as the sum of true output f(x) and noise.\nLet `(x, y, yb): X \u00d7Y \u00d7Y \u2192 [0,\u221e) be the loss function, which measures the discrepancy between\nthe true output value y at an input point x and its estimate yb. Let us employ a parametric model\nfb(x;\u03b8) for estimating the output value y, where \u03b8 \u2208 \u0398 \u2282 R\nb\n. Note that the range of application\nof our proposed method given in Section 3 includes non-parametric methods, but we focus on a\nparametric setting for simplicity. A model fb(x;\u03b8) is said to be correctly specified if there exists a\nparameter \u03b8\n\u2217\nsuch that fb(x;\u03b8\n\u2217\n) = f(x); otherwise the model is said to be misspecified. In practice,\n986\nCOVARIATE SHIFT ADAPTATION BY IMPORTANCE WEIGHTED CROSS VALIDATION\nthe model used for learning would be misspecified to a greater or lesser extent. For this reason,\nwe do not assume that the model is correct in this paper. The goal of supervised learning is to\ndetermine the value of the parameter \u03b8 so that output values for unlearned test input points are\naccurately estimated.\nLet us consider a test sample, which is not given to the user in the training phase, but will be\ngiven in the test phase in the future. We denote the test sample by (t,u), where t \u2208 X is a test input\npoint and u \u2208 Y is a corresponding test output value. The test error expected over test samples is\nexpressed as\nEt,u\nh\n`(t,u, fb(t;b\u03b8))i, (1)\nwhere E denotes the expectation. Note that the learned parameter b\u03b8 generally depends on the train\u0002ing set T = {(xi\n, yi)}\nn\ni=1\n. In the following, we consider the expected test error over the training\nsamples, which is called the risk or the generalization error:\nR\n(n) \u2261 E{xi\n,yi}\nn\ni=1\n,t,u\nh\n`(t,u, fb(t;b\u03b8))i. (2)\nIn standard supervised learning theories, the testsample (t,u)is assumed to follow P(u|t)Ptrain(t),\nwhich is the same probability...",
      "url": "https://www.jmlr.org/papers/volume8/sugiyama07a/sugiyama07a.pdf"
    },
    {
      "title": "",
      "text": "Robust Learning under Uncertain Test Distributions:\nRelating Covariate Shift to Model Misspecification\nJunfeng Wen1JUNFENG.WEN@UALBERTA.CA\nChun-Nam Yu2 CHUN-NAM.YU@ALCATEL-LUCENT.COM\nRussell Greiner1 RGREINER@UALBERTA.CA\n1Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA\n2Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA\nAbstract\nMany learning situations involve learning the\nconditional distribution ppy|xq when the training\ninstances are drawn from the training distribu\u0002tion ptrpxq, even though it will later be used to\npredict for instances drawn from a different test\ndistribution ptepxq. Most current approaches fo\u0002cus on learning how to reweigh the training ex\u0002amples, to make them resemble the test distribu\u0002tion. However, reweighing does not always help,\nbecause (we show that) the test error also de\u0002pends on the correctness of the underlying model\nclass. This paper analyses this situation by view\u0002ing the problem of learning under changing dis\u0002tributions as a game between a learner and an ad\u0002versary. We characterize when such reweighing\nis needed, and also provide an algorithm, robust\ncovariate shift adjustment (RCSA), that provides\nrelevant weights. Our empirical studies, on UCI\ndatasets and a real-world cancer prognostic pre\u0002diction dataset, show that our analysis applies,\nand that our RCSA works effectively.\n1. Introduction\nTraditional machine learning often explicitly or implicitly\nassumes that the data used for training a model come from\nthe same distribution as that of the test data. However, this\nassumption is violated in many real-world applications. For\nexample, biostatisticians often try to collect a large and di\u0002verse training set, perhaps for building prognostic predic\u0002tors for patients with different diseases. When clinicians\ndeploy these predictors, they do not know whether the lo\u0002cal test patient population will be even close to that training\npopulation. Sometimes we can collect a small sample from\nthe target test population, but in most cases we have noth\u0002Proceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32. Copy\u0002right 2014 by the author(s).\ning more than weak prior knowledge about how the test\ndistribution may shift, such as anticipated changes in gen\u0002der ratio or age distribution. It is useful to build predictors\nthat are robust against such changes in test distributions.\nIn this work, we investigate the problem of distribu\u0002tion change under covariate shift assumption (Shimodaira,\n2000), in which both training and test distributions share\nthe same conditional distribution ppy|xq, while their\nmarginal distributions, ptrpxq and ptepxq, are different. To\ncorrect the shifted distribution, major efforts have been\ndedicated to importance reweighing (Quionero-Candela\net al., 2009; Sugiyama & Kawanabe, 2012). However,\nreweighing methods will not necessarily improve the per\u0002formance in test set, as prediction accuracy under covariate\nshift is also dependent on model misspecification (White,\n1981). Fig. 1 shows three examples of misspecified mod\u0002els, where we are considering the model class of straight\nlines of the form y\u201cax`b, for xP r\u00b41.5, 2.5s. In Fig. 1(a),\nno straight line is a good fit for the cubic curve across\nthe whole interval, but Model 2 fits the curve reasonably\nwell in the small interval r\u00b40.5, 0.5s. If training data is\nspread all over r\u00b41.5, 2.5s while test data concentrates on\nr\u00b40.5, 0.5s, improvement via reweighing could be signif\u0002icant. The situation in Fig. 1(b) is different: although the\ntrue model is a curve and not a straight line, the best linear\nfit is no more than \u000f away from the value of the true model.\nIn this case, no matter what test distributions we see in the\ninterval r\u00b41.5, 2.5s, the regression loss of the best linear\nmodel will never be more than \u000f from the Bayes optimal\nloss. In Fig. 1(c), the true model is a straight line except at\nx \u201c 0; perhaps this outlier is a cancer patient whose tumour\nspontaneously disappeared on its own. Unless the test dis\u0002tribution concentrates most of its mass at x \u201c 0, the straight\nline fit learned from the training data over the interval will\nstill be a very good predictor. Sometimes we can rule out\nthis type of covariate shift through prior knowledge. If such\noutliers are extremely rare during training time, we would\nnot expect the test population to have many such patients.\nReweighing will not help much in cases 1(b) and 1(c).\nRobust Learning under Uncertain Test Distributions\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n0\n2\n4\n6\n8\n10\n12\n14\n16\nInput\nOutput\nTrue model\nModel 1\nModel 2\n(a) Large misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\nInput\nOutput\n\u2191\n\u2193\n\u03b5\nTrue model\nBest linear fit\n(b) Small misspecification.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2 2.5\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nInput\nOutput\nTrue model\n(c) Single point misspecification.\nFigure 1. Three different scenarios of model misspecifications.\nIn this paper, we relate covariate shift to model misspecifi\u0002cation and investigate when reweighing can help a learner\ndeal with covariate shift. We introduce a game between a\nlearner and an adversary that performs robust learning. The\nlearner chooses a model \u03b8 from a set \u0398 to minimize the\nloss, while the adversary chooses a reweighing function \u03b1\nfrom a set A to create new test distributions to maximize the\nloss. There are two major contributions in this paper: First,\nwe provide an improved understanding of the relation be\u0002tween covariate shift and model misspecification through\nthis game analysis. If the learner can find a \u03b8 that min\u0002imizes the loss against any possible \u03b1 that the adversary\ncan play, then it is not necessary to perform reweighing\nagainst covariate shift scenarios represented by A. Sec\u0002ond, we provide a systematic method for checking a model\nclass \u0398 against different covariate shift scenarios, such as\nchanging gender ratio and age distributions in the prognos\u0002tic predictor example, to help user decide whether impor\u0002tance reweighing would be beneficial.\nFor practical use, our method can be used to decide if the\nmodel class is sufficient against shifts that are close to a test\nsample; or robust against a known range of potential shifts\nif test sample is unavailable. If the model class is insuffi\u0002cient, we can consider different ways to deal with covariate\nshifts, such as reweighing using unlabelled test samples, or\nexploring a different model class for the problem.\n2. Related Work\nOur work is inspired by Grunwald & Dawid \u00a8 (2004), who\ninterpret maximum entropy as a game between an adver\u0002sary and a learner on minimizing the worst case expected\nlog loss. Teo et al. (2008) and Globerson & Roweis (2006)\nalso consider an adversarial scenario under changing test\nset conditions, but they are concerned with corruption or\ndeletion of features rather than covariate shift.\nMany results on covariate shift correction involve density\nratio estimation. Shimodaira (2000) showed that, given co\u0002variate shift and model misspecification, reweighing each\ninstance with ptepxq{ptrpxq is asymptotically optimal for\nlog-likelihood estimation, where ptrpxq and ptepxq are as\u0002sumed to be known or estimated in advance. Sugiyama\n& Muller \u00a8 (2005) extended this work by proposing an\n(almost) unbiased estimator for L2 generalization error.\nThere are several works focusing on minimizing differ\u0002ent types of divergence between distributions in the liter\u0002ature (Kanamori et al., 2008; Sugiyama et al., 2008; Ya\u0002mada et al., 2011). Kernel mean matching (KMM) (Huang\net al., 2007) reweighs instances to match means in a\nRKHS (Scholkopf & Smola \u00a8 , 2002). Our work and some\nother approaches (Pan et al., 2009) adapt the idea of match\u0002ing means of the datasets to correct shifted distribution,\nbut we extend their approaches from a two-step optimiza\u0002tion to a game framework that jointly learns a model\nand weights with covariate shift correction. Some other\napproaches (Zadrozny, 2004; Bickel et al.,...",
      "url": "https://proceedings.mlr.press/v32/wen14.pdf"
    },
    {
      "title": "Characterizing Uncertainty in Machine Learning for Chemistry",
      "text": "Characterizing Uncertainty in Machine Learning for Chemistry | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Characterizing Uncertainty in Machine Learning for Chemistry\n08 February 2023, Version 1\nThis is not the most recent version. There is a[\nnewer version\n](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)of this content available\nWorking Paper\n## Authors\n* [Esther Heid](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Esther%20Heid)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0002-8404-6596),\n* [Charles J. McGill](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Charles%20J.%20McGill),\n* [Florence H. Vermeire](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Florence%20H.%20Vermeire),\n* [William H. Green](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=William%20H.%20Green)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nCharacterizing uncertainty in machine learning models has recently gained interest in the context of machine learning reliability, robustness, safety, and active learning. Here, we separate the total uncertainty into contributions from noise in the data (aleatoric) and shortcomings of the model (epistemic), further dividing epistemic uncertainty into model bias and variance contributions. We systematically address the influence of noise, model bias, and model variance in the context of chemical property predictions, where the diverse nature of target properties and the vast chemical chemical space give rise to many different distinct sources of prediction error. We demonstrate that different sources of error can each be significant in different contexts and must be individually addressed during model development. Through controlled experiments on datasets of molecular properties, we show important trends in model performance associated with the level of noise in the dataset, size of the dataset, model architecture, molecule representation, ensemble size, and dataset splitting. In particular, we show that 1) noise in the test set can limit a model's observed performance when the actual performance is much better, 2) using size-extensive model aggregation structures is crucial for extensive property prediction, 3) ensembling is a reliable tool for uncertainty quantification and improvement specifically for the contribution of model variance, and 4) evaluations of cross-validation models understate their performance. We develop general guidelines on how to improve an underperforming model when falling into different uncertainty contexts.\n## Keywords\n[Uncertainty](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Uncertainty)\n[Machine Learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Machine%20Learning)\n[Chemical property prediction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Chemical%20property%20prediction)\n## Supplementary weblinks\n**Title**\n**Description**\n**Actions**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/Weblink.b642c15.svg)\nCharacterizing Uncertainty in Machine Learning for Chemistry Scripts\n**Description**\nPython scripts to reproduce the results of the manuscript.\n**Actions**\n[**View**](https://github.com/cjmcgill/characterizing_uncertainty_scripts)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\n[May 17, 2023 Version\n3](https://chemrxiv.org/engage/chemrxiv/article-details/646380b8fb40f6b3eeaed1e9)\n[Feb 16, 2023 Version\n2](https://chemrxiv.org/engage/chemrxiv/article-details/63ecf74d1d2d1840638a8b75)\nFeb 08, 2023 Version 1\n## Metrics\n3,215\n1,200\n0\nViews\nDownloads\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\nThe content is available under[CC BY 4.0[opens in a new tab]](https://creativecommons.org/licenses/by/4.0/)\n## DOI\n[\n10.26434/chemrxiv-2023-00vcg\nD O I: 10.26434/chemrxiv-2023-00vcg [opens in a new tab]](https://doi.org/10.26434/chemrxiv-2023-00vcg)\n## Funding\n**Austrian Science Fund**\nJ-4415\n**Machine Learning for Pharmaceutical Discovery and Synthesis Consortium (MLPDS)**\n## Author\u2019s competing interest statement\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n## Ethics\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/63e2b82b6d032916bb650914"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Understanding and Mitigating Distribution Shifts \n For Machine Learning Force Fields",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2503.08674v2"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2505.01912] BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2505.01912\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2505.01912**(cs)\n[Submitted on 3 May 2025 ([v1](https://arxiv.org/abs/2505.01912v1)), last revised 19 Dec 2025 (this version, v2)]\n# Title:BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nAuthors:[Evan R. Antoniuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Antoniuk,+E+R),[Shehtab Zaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaman,+S),[Tal Ben-Nun](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Nun,+T),[Peggy Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[James Diffenderfer](https://arxiv.org/search/cs?searchtype=author&amp;query=Diffenderfer,+J),[Busra Sahin](https://arxiv.org/search/cs?searchtype=author&amp;query=Sahin,+B),[Obadiah Smolenski](https://arxiv.org/search/cs?searchtype=author&amp;query=Smolenski,+O),[Tim Hsu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsu,+T),[Anna M. Hiszpanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Hiszpanski,+A+M),[Kenneth Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+K),[Bhavya Kailkhura](https://arxiv.org/search/cs?searchtype=author&amp;query=Kailkhura,+B),[Brian Van Essen](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Essen,+B)\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n[View PDF](https://arxiv.org/pdf/2505.01912)[HTML (experimental)](https://arxiv.org/html/2505.01912v2)> > Abstract:\n> Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\\mathbf{BOOM}$, $\\mathbf{b}$enchmarks for $\\mathbf{o}$ut-$\\mathbf{o}$f-distribution $\\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at [> this https URL\n](https://github.com/FLASK-LLNL/BOOM)> Subjects:|Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2505.01912](https://arxiv.org/abs/2505.01912)[cs.LG]|\n|(or[arXiv:2505.01912v2](https://arxiv.org/abs/2505.01912v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2505.01912](https://doi.org/10.48550/arXiv.2505.01912)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Evan Antoniuk [[view email](https://arxiv.org/show-email/6abb5e36/2505.01912)]\n**[[v1]](https://arxiv.org/abs/2505.01912v1)**Sat, 3 May 2025 19:51:23 UTC (35,134 KB)\n**[v2]**Fri, 19 Dec 2025 23:00:10 UTC (16,070 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2505.01912)\n* [HTML (experimental)](https://arxiv.org/html/2505.01912v2)\n* [TeX Source](https://arxiv.org/src/2505.01912)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2505.01912&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2505.01912&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-05](https://arxiv.org/list/cs.LG/2025-05)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/2505.01912?context=cond-mat)\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2505.01912?context=cond-mat.mtrl-sci)\n[cs](https://arxiv.org/abs/2505.01912?context=cs)\n[cs.AI](https://arxiv.org/abs/2505.01912?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.01912)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.01912)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.01912)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2505.01912&amp;description=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2505.01912&amp;title=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*...",
      "url": "https://arxiv.org/abs/2505.01912"
    },
    {
      "title": "",
      "text": "Accuracy on the Line: On the Strong Correlation\nBetween Out-of-Distribution and In-Distribution Generalization\nJohn Miller 1 Rohan Taori 2 Aditi Raghunathan 2 Shiori Sagawa 2 Pang Wei Koh 2 Vaishaal Shankar 1\nPercy Liang 2 Yair Carmon 3 Ludwig Schmidt 4\nAbstract\nFor machine learning systems to be reliable, we\nmust understand their performance in unseen, out\u0002of-distribution environments. In this paper, we\nempirically show that out-of-distribution perfor\u0002mance is strongly correlated with in-distribution\nperformance for a wide range of models and distri\u0002bution shifts. Specifically, we demonstrate strong\ncorrelations between in-distribution and out-of\u0002distribution performance on variants of CIFAR\u000210 & ImageNet, a synthetic pose estimation task\nderived from YCB objects, FMoW-WILDS satel\u0002lite imagery classification, and wildlife classi\u0002fication in iWildCam-WILDS. The correlation\nholds across model architectures, hyperparame\u0002ters, training set size, and training duration, and\nis more precise than what is expected from exist\u0002ing domain adaptation theory. To complete the\npicture, we also investigate cases where the cor\u0002relation is weaker, for instance some synthetic\ndistribution shifts from CIFAR-10-C and the tis\u0002sue classification dataset Camelyon17-WILDS.\nFinally, we provide a candidate theory based on a\nGaussian data model that shows how changes in\nthe data covariance arising from distribution shift\ncan affect the observed correlations.\n1. Introduction\nMachine learning models often need to generalize from\ntraining data to new environments. A kitchen robot should\nwork reliably in different homes, autonomous vehicles\nshould drive reliably in different cities, and analysis software\nfor satellite imagery should still perform well next year. The\n1Department of Computer Science, UC Berkeley, CA, USA\n2Department of Computer Science, Stanford University, Stanford,\nCA, USA 3School of Computer Science, Tel Aviv University, Tel\nAviv, Israel 4Toyota Research Institute, Cambridge, MA, USA.\nCorrespondence to: John Miller <miller john@berkeley.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nstandard paradigm to measure generalization is to evaluate a\nmodel on a single test set drawn from the same distribution\nas the training set. But this paradigm provides only a narrow\nin-distribution performance guarantee: a small test error\ncertifies future performance on new samples from exactly\nthe same distribution as the training set. In many scenarios,\nit is hard or impossible to train a model on precisely the dis\u0002tribution it will be applied to. Hence a model will inevitably\nencounter out-of-distribution data on which its performance\ncould vary widely compared to in-distribution performance.\nUnderstanding the performance of models beyond the train\u0002ing distribution therefore raises the following fundamental\nquestion: how does out-of-distribution performance relate\nto in-distribution performance?\nClassical theory for generalization across different distri\u0002butions provides a partial answer (Mansour et al., 2009;\nBen-David et al., 2010). For a model f trained on a distribu\u0002tion D, known guarantees typically relate the in-distribution\ntest accuracy on D to the out-of-distribution test accuracy\non a new distribution D1 via inequalities of the form\n|accDpfq \u00b4 accD1pfq| \u010f dpD, D1q\nwhere d is a distance between the distributions D and D1\nsuch as the total variation distance. Qualitatively, these\nbounds suggest that out-of-distribution accuracy may vary\nwidely as a function of in-distribution accuracy unless the\ndistribution distance d is small and the accuracies are there\u0002fore close (see Figure 1 (top-left) for an illustration). More\nrecently, empirical studies have shown that in some set\u0002tings, models with similar in-distribution performance can\nindeed have different out-of-distribution performance (Mc\u0002Coy et al., 2019; Zhou et al., 2020; D\u2019Amour et al., 2020).\nIn contrast to the aforementioned results, recent dataset re\u0002constructions of the popular CIFAR-10, ImageNet, MNIST,\nand SQuAD benchmarks showed a much more regular pat\u0002tern (Recht et al., 2019; Miller et al., 2020; Yadav & Bottou,\n2019; Lu et al., 2020). The reconstructions closely followed\nthe original dataset creation processes to assemble new test\nsets, but small differences were still enough to cause substan\u0002tial changes in the resulting model accuracies. Nevertheless,\nthe new out-of-distribution accuracies are almost perfectly\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\n10 30 50 70 90\nIn-distribution accuracy\n10\n30\n50\n70\n90\nOut-of-distribution accuracy\nSketch of theoretical bounds\nPotential model\nOOD / ID discrepancy\nmeasure\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCIFAR-10.2 test accuracy\nCIFAR-10.2\n10 20 30 40 50 65\nIn-distribution test accuracy\n5\n10\n20\n30\n40\n50\nOOD test worst region accuracy\nfMoW-WILDS\n5 20 35 50 65 80\nImageNet accuracy\n5\n20\n35\n50\n65\n80\nImageNetV2 accuracy\nImageNetV2\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nFog accuracy\nCIFAR-10-C fog\n10 20 30 40 50 60 70\nID test macro F1\n5\n10\n20\n30\n40\n50\nOOD test macro F1\niWildCam-WILDS\n5 20 35 50 65 80\nYCB in-distribution accuracy\n5\n20\n35\n50\n65\n80\nYCB OOD accuracy\nYCB-Objects\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCINIC-10 test accuracy\nCINIC-10\ny = x\nLinear Fit\nNeural Network\nImageNet Pretrained Network\nRandom Features\nRandom Forest\nKNN\nSVM\nLinear Model\nAdaBoost\nFigure 1. Out-of-distribution accuracies vs. in-distribution accuracies for a wide range of models, datasets, and distribution shifts. Top left:\nA sketch of the current bounds from domain adaptation theory. These bounds depend on distributional distances between in-distribution\nand out-of-distribution data, and they are loose in that they limit the deviation away from the y = x diagonal but do not prescribe a\nspecific trend within these wide bounds (see Section 7). Remaining panels: In contrast, we show that for a wide range of models and\ndatasets, there is a precise linear trend between out-of-distribution accuracy and in-distribution accuracy. Unlike what we might expect\nfrom theory, the linear trend does not follow the y \u201c x diagonal. The different panels represent different pairs of in-distribution and\nout-of-distribution datasets. Within each panel, we plot the performances of many different models, with different model architectures\nand hyperparameters. These datasets capture a variety of distribution shifts from dataset reproduction (CIFAR-10.2, ImageNet-V2); a\nreal-world spatiotemporal distribution shift on satellite imagery (FMoW-WILDS); using a different benchmark test dataset (CINIC-10);\nsynthetic perturbations (CIFAR-10-C and YCB-Objects); and a real-world geographic shift in wildlife monitoring (iWildCam-WILDS).\nInterestingly, for iWildCam-WILDS, models pretrained on ImageNet follow a different linear trend than models trained from scratch\nin-distribution, and we plot a separate trend line for ImageNet pretrained models in the iWildCam-WILDS panel. We explore this\nphenomenon more in Section 5.\nlinearly correlated with the original in-distribution accura\u0002cies for a range of deep neural networks. Importantly, this\ncorrelation holds despite the substantial gap between in\u0002distribution and out-of-distribution accuracies (see Figure\n1 (top-middle) for an example). However, it is currently\nunclear how widely these linear trends apply since they\nhave been mainly observed for dataset reproductions and\ncommon variations of convolutional neural networks.\nIn this paper, we conduct a broad empirical investigation to\ncharacterize when precise linear trends such as in Figure 1\n(top-middle) may be expected, and when out-of-distribution\nperformance is less predictable as in Figure 1 (top-left).\nConcretely, we make the following contributions:\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribu...",
      "url": "https://proceedings.mlr.press/v139/miller21b/miller21b.pdf"
    }
  ]
}