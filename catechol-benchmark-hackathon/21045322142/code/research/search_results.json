{
  "query": "How can I improve out-of-distribution generalization when my model has a large gap between cross-validation and test performance due to distribution shift?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Out\u2011of\u2011distribution (OOD) generalization refers to a model\u2019s ability to maintain performance when the test data come from a different distribution than the data used for training and cross\u2011validation. A large gap between cross\u2011validation (in\u2011distribution) scores and test scores is a classic symptom of distribution shift, and recent work provides several complementary ways to narrow that gap.  \n\nFirst, evaluate and develop on benchmarks that explicitly capture real\u2011world shifts, such as the WILDS suite, which contains ten diverse datasets (e.g., hospital\u2011wise medical imaging, wildlife camera traps, satellite imagery) and shows that standard training leaves a substantial OOD drop even for strong baselines\u202f([mlr.press](https://proceedings.mlr.press/v139/koh21a)). Because OOD performance is strongly correlated with in\u2011distribution performance across many settings\u202f([mlr.press](https://proceedings.mlr.press/v139/miller21b)), improving the overall quality of the model (larger, well\u2011regularized architectures, longer training, careful hyper\u2011parameter tuning) can also raise the OOD ceiling.  \n\nSecond, adopt training objectives that explicitly reduce sensitivity to shift. **Risk Extrapolation (REx)** penalizes variance of risks across training domains, encouraging the model to learn invariant causal features and to be robust to both covariate and causal shifts\u202f([mlr.press](https://proceedings.mlr.press/v162/krueger21a/krueger21a.pdf)). **Selective augmentation (LISA)** builds invariant predictors by mixing samples that share either the same label across domains or the same domain across labels, a simple mixup\u2011based technique that consistently improves worst\u2011group error on sub\u2011population and domain\u2011shift benchmarks\u202f([arxiv.org/pdf/2206.01919.pdf](https://proceedings.mlr.press/v162/yao22b/yao22b.pdf)).  \n\nThird, apply **test\u2011time adaptation** methods that adjust the model on the incoming test batch without labels. Recent surveys describe techniques such as entropy minimization, batch\u2011norm statistics updating, and online self\u2011training that can recover performance after a shift\u202f([arxiv.org/abs/2303.15361](https://arxiv.org/abs/2303.15361)). Complement these with calibrated uncertainty estimates\u2014methods that evaluate predictive confidence under shift (e.g., temperature scaling, deep ensembles) help detect when adaptation is needed and avoid over\u2011confident errors\u202f([arxiv.org/abs/1906.02530](https://arxiv.org/abs/1906.02530)).  \n\nIn practice, a workflow that (1) validates on realistic OOD benchmarks, (2) trains with risk\u2011balancing or selective\u2011mixup objectives, and (3) fine\u2011tunes at test time while monitoring calibrated uncertainty can dramatically shrink the cross\u2011validation\u2011to\u2011test performance gap caused by distribution shift.",
      "url": ""
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2303.15361] A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.15361\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2303.15361**(cs)\n[Submitted on 27 Mar 2023 ([v1](https://arxiv.org/abs/2303.15361v1)), last revised 12 Dec 2024 (this version, v2)]\n# Title:A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts\nAuthors:[Jian Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J),[Ran He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+R),[Tieniu Tan](https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+T)\nView a PDF of the paper titled A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts, by Jian Liang and Ran He and Tieniu Tan\n[View PDF](https://arxiv.org/pdf/2303.15361)[HTML (experimental)](https://arxiv.org/html/2303.15361v2)> > Abstract:\n> Machine learning methods strive to acquire a robust model during the training process that can effectively generalize to test samples, even in the presence of distribution shifts. However, these methods often suffer from performance degradation due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm has highlighted the significant benefits of using unlabeled data to train self-adapted models prior to inference. In this survey, we categorize TTA into several distinct groups based on the form of test data, namely, test-time domain adaptation, test-time batch adaptation, and online test-time adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms and discuss various learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. For a comprehensive list of TTA methods, kindly refer to \\url{\n[> this https URL\n](https://github.com/tim-learn/awesome-test-time-adaptation)> }. Comments:|Discussions, comments, and questions are all welcomed in \\\\url{[this https URL](https://github.com/tim-learn/awesome-test-time-adaptation)}|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2303.15361](https://arxiv.org/abs/2303.15361)[cs.LG]|\n|(or[arXiv:2303.15361v2](https://arxiv.org/abs/2303.15361v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.15361](https://doi.org/10.48550/arXiv.2303.15361)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|International Journal of Computer Vision (2024)|\nRelated DOI:|[https://doi.org/10.1007/s11263-024-02181-w](https://doi.org/10.1007/s11263-024-02181-w)\nFocus to learn more\nDOI(s) linking to related resources\n|\n## Submission history\nFrom: Jian Liang [[view email](https://arxiv.org/show-email/a8a6c287/2303.15361)]\n**[[v1]](https://arxiv.org/abs/2303.15361v1)**Mon, 27 Mar 2023 16:32:21 UTC (1,021 KB)\n**[v2]**Thu, 12 Dec 2024 09:06:56 UTC (686 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts, by Jian Liang and Ran He and Tieniu Tan\n* [View PDF](https://arxiv.org/pdf/2303.15361)\n* [HTML (experimental)](https://arxiv.org/html/2303.15361v2)\n* [TeX Source](https://arxiv.org/src/2303.15361)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.15361&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.15361&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-03](https://arxiv.org/list/cs.LG/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.15361?context=cs)\n[cs.AI](https://arxiv.org/abs/2303.15361?context=cs.AI)\n[cs.CV](https://arxiv.org/abs/2303.15361?context=cs.CV)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.15361)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.15361)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.15361)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2303.15361&amp;description=A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2303.15361&amp;title=A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organiza...",
      "url": "https://arxiv.org/abs/2303.15361"
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[1906.02530] Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1906.02530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1906.02530**(stat)\n[Submitted on 6 Jun 2019 ([v1](https://arxiv.org/abs/1906.02530v1)), last revised 17 Dec 2019 (this version, v2)]\n# Title:Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\nAuthors:[Yaniv Ovadia](https://arxiv.org/search/stat?searchtype=author&amp;query=Ovadia,+Y),[Emily Fertig](https://arxiv.org/search/stat?searchtype=author&amp;query=Fertig,+E),[Jie Ren](https://arxiv.org/search/stat?searchtype=author&amp;query=Ren,+J),[Zachary Nado](https://arxiv.org/search/stat?searchtype=author&amp;query=Nado,+Z),[D Sculley](https://arxiv.org/search/stat?searchtype=author&amp;query=Sculley,+D),[Sebastian Nowozin](https://arxiv.org/search/stat?searchtype=author&amp;query=Nowozin,+S),[Joshua V. Dillon](https://arxiv.org/search/stat?searchtype=author&amp;query=Dillon,+J+V),[Balaji Lakshminarayanan](https://arxiv.org/search/stat?searchtype=author&amp;query=Lakshminarayanan,+B),[Jasper Snoek](https://arxiv.org/search/stat?searchtype=author&amp;query=Snoek,+J)\nView a PDF of the paper titled Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift, by Yaniv Ovadia and 8 other authors\n[View PDF](https://arxiv.org/pdf/1906.02530)> > Abstract:\n> Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model&#39;s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks. Comments:|Advances in Neural Information Processing Systems, 2019|\nSubjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:1906.02530](https://arxiv.org/abs/1906.02530)[stat.ML]|\n|(or[arXiv:1906.02530v2](https://arxiv.org/abs/1906.02530v2)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1906.02530](https://doi.org/10.48550/arXiv.1906.02530)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jasper Snoek [[view email](https://arxiv.org/show-email/5aa8e553/1906.02530)]\n**[[v1]](https://arxiv.org/abs/1906.02530v1)**Thu, 6 Jun 2019 11:42:53 UTC (6,961 KB)\n**[v2]**Tue, 17 Dec 2019 21:30:28 UTC (10,075 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift, by Yaniv Ovadia and 8 other authors\n* [View PDF](https://arxiv.org/pdf/1906.02530)\n* [TeX Source](https://arxiv.org/src/1906.02530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1906.02530&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1906.02530&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2019-06](https://arxiv.org/list/stat.ML/2019-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1906.02530?context=cs)\n[cs.LG](https://arxiv.org/abs/1906.02530?context=cs.LG)\n[stat](https://arxiv.org/abs/1906.02530?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1906.02530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1906.02530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1906.02530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/1906.02530&amp;description=Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/1906.02530&amp;title=Can You Trust Your Model&#39;s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://i...",
      "url": "https://arxiv.org/abs/1906.02530"
    },
    {
      "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
      "text": "WILDS: A Benchmark of in-the-Wild Distribution Shifts\n[![[International Conference on Machine Learning Logo]](https://proceedings.mlr.press/v139/assets/images/logo-pmlr.svg)](https://proceedings.mlr.press/)Proceedings of Machine Learning Research\n[[edit](https://github.com/mlresearch/v139/edit/gh-pages/_posts/2021-07-01-koh21a.md)]\n# WILDS: A Benchmark of in-the-Wild Distribution Shifts\nPang Wei Koh,Shiori Sagawa,Henrik Marklund,Sang Michael Xie,Marvin Zhang,Akshay Balsubramani,Weihua Hu,Michihiro Yasunaga,Richard Lanas Phillips,Irena Gao,Tony Lee,Etienne David,Ian Stavness,Wei Guo,Berton Earnshaw,Imran Haque,Sara M Beery,Jure Leskovec,Anshul Kundaje,Emma Pierson,Sergey Levine,Chelsea Finn,Percy Liang\n*Proceedings of the 38th International Conference on Machine Learning*,PMLR 139:5637-5664,2021.\n#### Abstract\nDistribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.\n#### Cite this Paper\nBibTeX\n`@InProceedings{pmlr-v139-koh21a,\ntitle = {WILDS: A Benchmark of in-the-Wild Distribution Shifts},\nauthor = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},\nbooktitle = {Proceedings of the 38th International Conference on Machine Learning},\npages = {5637--5664},\nyear = {2021},\neditor = {Meila, Marina and Zhang, Tong},\nvolume = {139},\nseries = {Proceedings of Machine Learning Research},\nmonth = {18--24 Jul},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v139/koh21a/koh21a.pdf},\nurl = {https://proceedings.mlr.press/v139/koh21a.html},\nabstract = {Distribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.}\n}`\nCopy to ClipboardDownload\nEndnote\n`%0 Conference Paper\n%T WILDS: A Benchmark of in-the-Wild Distribution Shifts\n%A Pang Wei Koh\n%A Shiori Sagawa\n%A Henrik Marklund\n%A Sang Michael Xie\n%A Marvin Zhang\n%A Akshay Balsubramani\n%A Weihua Hu\n%A Michihiro Yasunaga\n%A Richard Lanas Phillips\n%A Irena Gao\n%A Tony Lee\n%A Etienne David\n%A Ian Stavness\n%A Wei Guo\n%A Berton Earnshaw\n%A Imran Haque\n%A Sara M Beery\n%A Jure Leskovec\n%A Anshul Kundaje\n%A Emma Pierson\n%A Sergey Levine\n%A Chelsea Finn\n%A Percy Liang\n%B Proceedings of the 38th International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2021\n%E Marina Meila\n%E Tong Zhang\t%F pmlr-v139-koh21a\n%I PMLR\n%P 5637--5664\n%U https://proceedings.mlr.press/v139/koh21a.html\n%V 139\n%X Distribution shifts\u2014where the training distribution differs from the test distribution\u2014can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.`\nCopy to ClipboardDownload\nAPA\n`Koh, P.W., Sagawa, S., Marklund, H., Xie, S.M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R.L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B., Haque, I., Beery, S.M., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C. & Liang, P.. (2021). WILDS: A Benchmark of in-the-Wild Distribution Shifts.*Proceedings of the 38th International Conference on Machine Learning*, in*Proceedings of Machine Learning Research*139:5637-5664 Available from https://proceedings.mlr.press/v139/koh21a.html.`\nCopy to ClipboardDownload\n#### Related Material\n* [Download PDF](http://proceedings.mlr.press/v139/koh21a/koh21a.pdf)\n* [Supplementary PDF](http://proceedings.mlr.press/v139/koh21a/koh21a-supp.pdf)",
      "url": "https://proceedings.mlr.press/v139/koh21a"
    },
    {
      "title": "",
      "text": "Improving Out-of-Distribution Robustness via Selective Augmentation\nHuaxiu Yao * 1 Yu Wang * 2 Sai Li 3 Linjun Zhang 4 Weixin Liang 1\nJames Zou 1 Chelsea Finn 1\nAbstract\nMachine learning algorithms typically assume\nthat training and test examples are drawn from the\nsame distribution. However, distribution shift is a\ncommon problem in real-world applications and\ncan cause models to perform dramatically worse\nat test time. In this paper, we specifically consider\nthe problems of subpopulation shifts (e.g., imbal\u0002anced data) and domain shifts. While prior works\noften seek to explicitly regularize internal repre\u0002sentations or predictors of the model to be domain\ninvariant, we instead aim to learn invariant pre\u0002dictors without restricting the model\u2019s internal\nrepresentations or predictors. This leads to a sim\u0002ple mixup-based technique which learns invariant\npredictors via selective augmentation called LISA.\nLISA selectively interpolates samples either with\nthe same labels but different domains or with the\nsame domain but different labels. Empirically, we\nstudy the effectiveness of LISA on nine bench\u0002marks ranging from subpopulation shifts to do\u0002main shifts, and we find that LISA consistently\noutperforms other state-of-the-art methods and\nleads to more invariant predictors. We further an\u0002alyze a linear setting and theoretically show how\nLISA leads to a smaller worst-group error. Code\nis released in https://github.com/huaxiuyao/LISA\n1. Introduction\nTo deploy machine learning algorithms in real-world appli\u0002cations, we must pay attention to distribution shift, i.e. when\nthe test distribution is different from the training distribution,\nwhich substantially degrades model performance. In this\n*Equal contribution . This work was done when Yu Wang was\nmentored by Huaxiu Yao remotely. 1Stanford University, CA,\nUSA 2University of California San Diego, CA, USA 3Renmin\nUniversity of China, Beijing, China 4Rutgers University, NJ, USA.\nCorrespondence to: Huaxiu Yao <huaxiu@cs.stanford.edu>, Sai\nLi <saili@ruc.edu.cn>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy\u0002right 2022 by the author(s).\npaper, we refer this problem as out-of-distribution (OOD)\ngeneralization and specifically consider performance gaps\ncaused by two kinds of distribution shifts: subpopulation\nshifts and domain shifts. In subpopulation shifts, the test\ndomains (or subpopulations) are seen but underrepresented\nin the training data. When subpopulation shift occurs, mod\u0002els may perform poorly when they falsely rely on spurious\ncorrelations between the particular subpopulation and the\nlabel. For example, in health risk prediction, a machine\nlearning model trained on the entire population may asso\u0002ciate the labels with demographic features (e.g., gender and\nage), making the model fail on the test set when such an\nassociation does not hold in reality. In domain shifts, the\ntest data is from new domains, which requires the trained\nmodel to generalize well to test domains without seeing the\ndata from those domains at training time. In the health risk\nexample, we may want to train a model on patients from\na few sampled hospitals and then deploy the model to a\nbroader set of hospitals (Koh et al., 2021).\nTo improve model robustness under these two kinds of distri\u0002bution shifts, prior works have proposed various regularizers\nto learn representations or predictors that are invariant to\ndifferent domains while still containing sufficient informa\u0002tion to fulfill the task (Li et al., 2018; Sun & Saenko, 2016;\nArjovsky et al., 2019; Krueger et al., 2021; Rosenfeld et al.,\n2021). However, designing regularizers that are widely suit\u0002able to datasets from diverse domains is challenging, and\nunsuitable regularizers may adversely limit the model\u2019s ex\u0002pressive power or yield a difficult optimization problem,\nleading to inconsistent performance among various real\u0002world datasets. For example, on the WILDS datasets, invari\u0002ant risk minimization (IRM) (Arjovsky et al., 2019) with\nreweighting \u2013 a representative method for learning invariant\npredictor \u2013 outperforms empirical risk minimization (ERM)\non CivilComments, but fails to improve robustness on a\nvariety of other datasets like Camelyon17 and RxRx1 (Koh\net al., 2021).\nInstead of explicitly imposing regularization, we propose to\nlearn invariant predictors through data interpolation, leading\nto a simple algorithm called LISA (Learning Invariant Pre\u0002dictors with Selective Augmentation). Concretely, inspired\nby mixup (Zhang et al., 2018), LISA linearly interpolates\nthe features for a pair of samples and applies the same\nImproving Out-of-Distribution Robustness via Selective Augmentation\n = 0.0  = 0.25  = 0.5  = 0.75  = 1.0\n = 0.0  = 0.25  = 0.5  = 0.75  = 1.0\nDomain information is not the reason for the label change\ny = [1, 0] y = [0.25, 0.75] y = [0.5, 0.5] y = [0.75, 0.25] y = [0, 1]\nAll y = [0, 1] \n!: Green\n\n\": Red\n!: [1, 0] \": [0, 1]\n40% of train data\n10% of train data 40% of train data\n10% of train data\nOne-hot label\nDomain\n(b) Intra-label LISA: interpolates samples with the same label but different domains\n(c) Intra-domain LISA: interpolates samples with the same domain but different labels\n(a) Colored MNIST dataset\nFigure 1. Illustration of the variants of LISA (Intra-label LISA and Intra-domain LISA) on Colored MNIST dataset. \u03bb represents the\ninterpolation ratio, which is sampled from a Beta distribution. (a) Colored MNIST (CMNIST). We classify MNIST digits as two classes,\nand original digits (0,1,2,3,4) and (5,6,7,8,9) are labeled as class 0 and 1, respectively. Digit color is used as domain information, which is\nspuriously correlated with labels in training data; (b) Intra-label LISA (LISA-L) cancels out spurious correlation by interpolating samples\nwith the same label; (c) Intra-domain LISA (LISA-D) interpolates samples with the same domain but different labels to encourage the\nmodel to learn specific features within a domain.\ninterpolation strategy on the corresponding labels. Crit\u0002ically, the pairs are selectively chosen according to two\nselective augmentation strategies \u2013 intra-label LISA (LISA\u0002L) and intra-domain LISA (LISA-D), which are described\nbelow and illustrated on Colored MNIST dataset in Figure 1.\nIntra-label LISA (Figure 1(b)) interpolates samples with the\nsame label but from different domains, aiming to eliminate\ndomain-related spurious correlations. Intra-domain LISA\n(Figure 1(c)) interpolates samples with the same domain but\ndifferent labels, such that the model should learn to ignore\nthe domain information and generate different predicted\nvalues as the interpolation ratio changes. In this way, LISA\nencourages the model to learn domain-invariant predictors\nwithout any explicit constraints or regularizers.\nThe primary contributions of this paper are as follows:\n(1) We propose a simple yet widely-applicable method for\nlearning domain invariant predictors that is shown to be\nrobust to subpopulation shifts and domain shifts. (2) We\nconduct broad experiments to evaluate LISA on nine bench\u0002mark datasets from diverse domains. In these experiments,\nwe make the following key observations. First, we observe\nthat LISA consistently outperforms seven prior methods to\naddress subpopulation and domain shifts. Second, we find\nthat LISA produces predictors that are consistently more\ndomain invariant than prior approaches. Third, we identify\nthat the performance gains of LISA are from canceling out\ndomain-specific information or spurious correlations and\nlearning invariant predictors, rather than simply involving\nmore data via interpolation. Finally, when the degree of\ndistribution shift increases, LISA achieves more significant\nperformance gains. (3) We provide a theoretical analysis of\nthe phenomena distilled from the empirical studies, where\nwe provably demonstrate that LISA can mitigate spurious\ncorrelations and therefore lead to smaller worst-domain er\u0002ror compared with ERM and vanilla mixup...",
      "url": "https://proceedings.mlr.press/v162/yao22b/yao22b.pdf"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2403.01874] A Survey on Evaluation of Out-of-Distribution Generalization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2403.01874\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2403.01874**(cs)\n[Submitted on 4 Mar 2024]\n# Title:A Survey on Evaluation of Out-of-Distribution Generalization\nAuthors:[Han Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+H),[Jiashuo Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J),[Xingxuan Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Jiayun Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J),[Peng Cui](https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+P)\nView a PDF of the paper titled A Survey on Evaluation of Out-of-Distribution Generalization, by Han Yu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2403.01874)[HTML (experimental)](https://arxiv.org/html/2403.01874v1)> > Abstract:\n> Machine learning models, while progressively advanced, rely heavily on the IID assumption, which is often unfulfilled in practice due to inevitable distribution shifts. This renders them susceptible and untrustworthy for deployment in risk-sensitive applications. Such a significant problem has consequently spawned various branches of works dedicated to developing algorithms capable of Out-of-Distribution (OOD) generalization. Despite these efforts, much less attention has been paid to the evaluation of OOD generalization, which is also a complex and fundamental problem. Its goal is not only to assess whether a model&#39;s OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly. This entails characterizing the types of distribution shifts that a model can effectively address, and identifying the safe and risky input regions given a model. This paper serves as the first effort to conduct a comprehensive review of OOD evaluation. We categorize existing research into three paradigms: OOD performance testing, OOD performance prediction, and OOD intrinsic property characterization, according to the availability of test data. Additionally, we briefly discuss OOD evaluation in the context of pretrained models. In closing, we propose several promising directions for future research in OOD evaluation. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2403.01874](https://arxiv.org/abs/2403.01874)[cs.LG]|\n|(or[arXiv:2403.01874v1](https://arxiv.org/abs/2403.01874v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2403.01874](https://doi.org/10.48550/arXiv.2403.01874)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Han Yu [[view email](https://arxiv.org/show-email/6465b1ce/2403.01874)]\n**[v1]**Mon, 4 Mar 2024 09:30:35 UTC (289 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled A Survey on Evaluation of Out-of-Distribution Generalization, by Han Yu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2403.01874)\n* [HTML (experimental)](https://arxiv.org/html/2403.01874v1)\n* [TeX Source](https://arxiv.org/src/2403.01874)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2403.01874&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2403.01874&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-03](https://arxiv.org/list/cs.LG/2024-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2403.01874?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.01874)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.01874)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.01874)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2403.01874&amp;description=A Survey on Evaluation of Out-of-Distribution Generalization>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2403.01874&amp;title=A Survey on Evaluation of Out-of-Distribution Generalization>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.01874)|[Disable MathJax](javascript:setMath...",
      "url": "https://arxiv.org/abs/2403.01874"
    },
    {
      "title": "Generalizing to any diverse distribution: uniformity, gentle finetuning and rebalancing",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2410.05980"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2107.04649] Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2107.04649\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2107.04649**(cs)\n[Submitted on 9 Jul 2021 ([v1](https://arxiv.org/abs/2107.04649v1)), last revised 7 Oct 2021 (this version, v2)]\n# Title:Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\nAuthors:[John Miller](https://arxiv.org/search/cs?searchtype=author&amp;query=Miller,+J),[Rohan Taori](https://arxiv.org/search/cs?searchtype=author&amp;query=Taori,+R),[Aditi Raghunathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Raghunathan,+A),[Shiori Sagawa](https://arxiv.org/search/cs?searchtype=author&amp;query=Sagawa,+S),[Pang Wei Koh](https://arxiv.org/search/cs?searchtype=author&amp;query=Koh,+P+W),[Vaishaal Shankar](https://arxiv.org/search/cs?searchtype=author&amp;query=Shankar,+V),[Percy Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+P),[Yair Carmon](https://arxiv.org/search/cs?searchtype=author&amp;query=Carmon,+Y),[Ludwig Schmidt](https://arxiv.org/search/cs?searchtype=author&amp;query=Schmidt,+L)\nView a PDF of the paper titled Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization, by John Miller and 8 other authors\n[View PDF](https://arxiv.org/pdf/2107.04649)> > Abstract:\n> For machine learning systems to be reliable, we must understand their performance in unseen, out-of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of-distribution performance on variants of CIFAR-10 &amp; ImageNet, a synthetic pose estimation task derived from YCB objects, satellite imagery classification in FMoW-WILDS, and wildlife classification in iWildCam-WILDS. The strong correlations hold across model architectures, hyperparameters, training set size, and training duration, and are more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2107.04649](https://arxiv.org/abs/2107.04649)[cs.LG]|\n|(or[arXiv:2107.04649v2](https://arxiv.org/abs/2107.04649v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2107.04649](https://doi.org/10.48550/arXiv.2107.04649)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: John Miller [[view email](https://arxiv.org/show-email/8b446f5b/2107.04649)]\n**[[v1]](https://arxiv.org/abs/2107.04649v1)**Fri, 9 Jul 2021 19:48:23 UTC (46,790 KB)\n**[v2]**Thu, 7 Oct 2021 23:59:19 UTC (46,791 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization, by John Miller and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2107.04649)\n* [TeX Source](https://arxiv.org/src/2107.04649)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2107.04649&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2107.04649&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-07](https://arxiv.org/list/cs.LG/2021-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2107.04649?context=cs)\n[stat](https://arxiv.org/abs/2107.04649?context=stat)\n[stat.ML](https://arxiv.org/abs/2107.04649?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.04649)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.04649)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.04649)\n### [1 blog link](https://arxiv.org/tb/2107.04649)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-04649)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-04649)\n[John Miller](<https://dblp.uni-trier.de/search/author?author=John Miller>)\n[Rohan Taori](<https://dblp.uni-trier.de/search/author?author=Rohan Taori>)\n[Aditi Raghunathan](<https://dblp.uni-trier.de/search/author?author=Aditi Raghunathan>)\n[Pang Wei Koh](<https://dblp.uni-trier.de/search/author?author=Pang Wei Koh>)\n[Vaishaal Shankar](<https://dblp.uni-trier.de/search/author?author=Vaishaal Shankar>)\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2107.04649&amp;description=Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2107.04649&amp;title=Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/...",
      "url": "https://arxiv.org/abs/2107.04649"
    },
    {
      "title": "",
      "text": "Accuracy on the Line: On the Strong Correlation\nBetween Out-of-Distribution and In-Distribution Generalization\nJohn Miller 1 Rohan Taori 2 Aditi Raghunathan 2 Shiori Sagawa 2 Pang Wei Koh 2 Vaishaal Shankar 1\nPercy Liang 2 Yair Carmon 3 Ludwig Schmidt 4\nAbstract\nFor machine learning systems to be reliable, we\nmust understand their performance in unseen, out\u0002of-distribution environments. In this paper, we\nempirically show that out-of-distribution perfor\u0002mance is strongly correlated with in-distribution\nperformance for a wide range of models and distri\u0002bution shifts. Specifically, we demonstrate strong\ncorrelations between in-distribution and out-of\u0002distribution performance on variants of CIFAR\u000210 & ImageNet, a synthetic pose estimation task\nderived from YCB objects, FMoW-WILDS satel\u0002lite imagery classification, and wildlife classi\u0002fication in iWildCam-WILDS. The correlation\nholds across model architectures, hyperparame\u0002ters, training set size, and training duration, and\nis more precise than what is expected from exist\u0002ing domain adaptation theory. To complete the\npicture, we also investigate cases where the cor\u0002relation is weaker, for instance some synthetic\ndistribution shifts from CIFAR-10-C and the tis\u0002sue classification dataset Camelyon17-WILDS.\nFinally, we provide a candidate theory based on a\nGaussian data model that shows how changes in\nthe data covariance arising from distribution shift\ncan affect the observed correlations.\n1. Introduction\nMachine learning models often need to generalize from\ntraining data to new environments. A kitchen robot should\nwork reliably in different homes, autonomous vehicles\nshould drive reliably in different cities, and analysis software\nfor satellite imagery should still perform well next year. The\n1Department of Computer Science, UC Berkeley, CA, USA\n2Department of Computer Science, Stanford University, Stanford,\nCA, USA 3School of Computer Science, Tel Aviv University, Tel\nAviv, Israel 4Toyota Research Institute, Cambridge, MA, USA.\nCorrespondence to: John Miller <miller john@berkeley.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nstandard paradigm to measure generalization is to evaluate a\nmodel on a single test set drawn from the same distribution\nas the training set. But this paradigm provides only a narrow\nin-distribution performance guarantee: a small test error\ncertifies future performance on new samples from exactly\nthe same distribution as the training set. In many scenarios,\nit is hard or impossible to train a model on precisely the dis\u0002tribution it will be applied to. Hence a model will inevitably\nencounter out-of-distribution data on which its performance\ncould vary widely compared to in-distribution performance.\nUnderstanding the performance of models beyond the train\u0002ing distribution therefore raises the following fundamental\nquestion: how does out-of-distribution performance relate\nto in-distribution performance?\nClassical theory for generalization across different distri\u0002butions provides a partial answer (Mansour et al., 2009;\nBen-David et al., 2010). For a model f trained on a distribu\u0002tion D, known guarantees typically relate the in-distribution\ntest accuracy on D to the out-of-distribution test accuracy\non a new distribution D1 via inequalities of the form\n|accDpfq \u00b4 accD1pfq| \u010f dpD, D1q\nwhere d is a distance between the distributions D and D1\nsuch as the total variation distance. Qualitatively, these\nbounds suggest that out-of-distribution accuracy may vary\nwidely as a function of in-distribution accuracy unless the\ndistribution distance d is small and the accuracies are there\u0002fore close (see Figure 1 (top-left) for an illustration). More\nrecently, empirical studies have shown that in some set\u0002tings, models with similar in-distribution performance can\nindeed have different out-of-distribution performance (Mc\u0002Coy et al., 2019; Zhou et al., 2020; D\u2019Amour et al., 2020).\nIn contrast to the aforementioned results, recent dataset re\u0002constructions of the popular CIFAR-10, ImageNet, MNIST,\nand SQuAD benchmarks showed a much more regular pat\u0002tern (Recht et al., 2019; Miller et al., 2020; Yadav & Bottou,\n2019; Lu et al., 2020). The reconstructions closely followed\nthe original dataset creation processes to assemble new test\nsets, but small differences were still enough to cause substan\u0002tial changes in the resulting model accuracies. Nevertheless,\nthe new out-of-distribution accuracies are almost perfectly\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\n10 30 50 70 90\nIn-distribution accuracy\n10\n30\n50\n70\n90\nOut-of-distribution accuracy\nSketch of theoretical bounds\nPotential model\nOOD / ID discrepancy\nmeasure\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCIFAR-10.2 test accuracy\nCIFAR-10.2\n10 20 30 40 50 65\nIn-distribution test accuracy\n5\n10\n20\n30\n40\n50\nOOD test worst region accuracy\nfMoW-WILDS\n5 20 35 50 65 80\nImageNet accuracy\n5\n20\n35\n50\n65\n80\nImageNetV2 accuracy\nImageNetV2\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nFog accuracy\nCIFAR-10-C fog\n10 20 30 40 50 60 70\nID test macro F1\n5\n10\n20\n30\n40\n50\nOOD test macro F1\niWildCam-WILDS\n5 20 35 50 65 80\nYCB in-distribution accuracy\n5\n20\n35\n50\n65\n80\nYCB OOD accuracy\nYCB-Objects\n10 30 50 70 90\nCIFAR-10 test accuracy\n10\n30\n50\n70\n90\nCINIC-10 test accuracy\nCINIC-10\ny = x\nLinear Fit\nNeural Network\nImageNet Pretrained Network\nRandom Features\nRandom Forest\nKNN\nSVM\nLinear Model\nAdaBoost\nFigure 1. Out-of-distribution accuracies vs. in-distribution accuracies for a wide range of models, datasets, and distribution shifts. Top left:\nA sketch of the current bounds from domain adaptation theory. These bounds depend on distributional distances between in-distribution\nand out-of-distribution data, and they are loose in that they limit the deviation away from the y = x diagonal but do not prescribe a\nspecific trend within these wide bounds (see Section 7). Remaining panels: In contrast, we show that for a wide range of models and\ndatasets, there is a precise linear trend between out-of-distribution accuracy and in-distribution accuracy. Unlike what we might expect\nfrom theory, the linear trend does not follow the y \u201c x diagonal. The different panels represent different pairs of in-distribution and\nout-of-distribution datasets. Within each panel, we plot the performances of many different models, with different model architectures\nand hyperparameters. These datasets capture a variety of distribution shifts from dataset reproduction (CIFAR-10.2, ImageNet-V2); a\nreal-world spatiotemporal distribution shift on satellite imagery (FMoW-WILDS); using a different benchmark test dataset (CINIC-10);\nsynthetic perturbations (CIFAR-10-C and YCB-Objects); and a real-world geographic shift in wildlife monitoring (iWildCam-WILDS).\nInterestingly, for iWildCam-WILDS, models pretrained on ImageNet follow a different linear trend than models trained from scratch\nin-distribution, and we plot a separate trend line for ImageNet pretrained models in the iWildCam-WILDS panel. We explore this\nphenomenon more in Section 5.\nlinearly correlated with the original in-distribution accura\u0002cies for a range of deep neural networks. Importantly, this\ncorrelation holds despite the substantial gap between in\u0002distribution and out-of-distribution accuracies (see Figure\n1 (top-middle) for an example). However, it is currently\nunclear how widely these linear trends apply since they\nhave been mainly observed for dataset reproductions and\ncommon variations of convolutional neural networks.\nIn this paper, we conduct a broad empirical investigation to\ncharacterize when precise linear trends such as in Figure 1\n(top-middle) may be expected, and when out-of-distribution\nperformance is less predictable as in Figure 1 (top-left).\nConcretely, we make the following contributions:\nAccuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribu...",
      "url": "https://proceedings.mlr.press/v139/miller21b/miller21b.pdf"
    }
  ]
}