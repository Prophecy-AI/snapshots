{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04e011c",
   "metadata": {},
   "source": [
    "# Experiment 053: Mixall-Style 4-Model Ensemble with Full Features\n",
    "\n",
    "**Hypothesis**: The mixall kernel uses a 4-model ensemble (MLP + XGBoost + RF + LightGBM) that may have different CV-LB characteristics than our GP + MLP + LGBM ensemble.\n",
    "\n",
    "**Key differences from exp_049b (which used Spange only):**\n",
    "1. Use FULL features: Spange + DRFP + Arrhenius (145 features)\n",
    "2. Replace GP with XGBoost and RandomForest\n",
    "3. This is a fundamentally different ensemble composition\n",
    "\n",
    "**Why this should work:**\n",
    "- XGBoost and RandomForest have different inductive biases than GP\n",
    "- The ensemble diversity may improve generalization to unseen solvents\n",
    "- This may change the CV-LB relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/code/experiments/049_manual_ood_handling')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "from utils_local import load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_single_raw, Y_single = load_data(\"single_solvent\")\n",
    "X_full_raw, Y_full = load_data(\"full\")\n",
    "\n",
    "print(f\"Single solvent: {X_single_raw.shape}, Mixtures: {X_full_raw.shape}\")\n",
    "\n",
    "# Load features\n",
    "spange = load_features(\"spange_descriptors\")\n",
    "drfp = load_features(\"drfps_catechol\")\n",
    "print(f\"Spange: {spange.shape}, DRFP: {drfp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51aab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets with FULL features (Spange + DRFP)\n",
    "def prepare_single_solvent_dataset(X_raw, spange, drfp):\n",
    "    \"\"\"Prepare single solvent dataset with all features\"\"\"\n",
    "    solvent_name = X_raw['SOLVENT NAME'].values\n",
    "    spange_features = spange.loc[solvent_name].values\n",
    "    drfp_features = drfp.loc[solvent_name].values\n",
    "    time = X_raw['Residence Time'].values\n",
    "    temp = X_raw['Temperature'].values\n",
    "    \n",
    "    spange_cols = spange.columns.tolist()\n",
    "    drfp_cols = [f'DRFP_{i}' for i in range(drfp.shape[1])]\n",
    "    \n",
    "    df = pd.DataFrame(spange_features, columns=spange_cols)\n",
    "    df_drfp = pd.DataFrame(drfp_features, columns=drfp_cols)\n",
    "    df = pd.concat([df, df_drfp], axis=1)\n",
    "    df['TEMPERATURE'] = temp\n",
    "    df['TIME'] = time\n",
    "    df['SOLVENT NAME'] = solvent_name\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_mixture_dataset(X_raw, spange, drfp):\n",
    "    \"\"\"Prepare mixture dataset with all features\"\"\"\n",
    "    solvent_a = X_raw['SOLVENT A NAME'].values\n",
    "    solvent_b = X_raw['SOLVENT B NAME'].values\n",
    "    solvent_b_pct = X_raw['SolventB%'].values / 100.0\n",
    "    \n",
    "    spange_a = spange.loc[solvent_a].values\n",
    "    spange_b = spange.loc[solvent_b].values\n",
    "    spange_mix = (1 - solvent_b_pct[:, None]) * spange_a + solvent_b_pct[:, None] * spange_b\n",
    "    \n",
    "    drfp_a = drfp.loc[solvent_a].values\n",
    "    drfp_b = drfp.loc[solvent_b].values\n",
    "    drfp_mix = (1 - solvent_b_pct[:, None]) * drfp_a + solvent_b_pct[:, None] * drfp_b\n",
    "    \n",
    "    solvent_name = [f\"{a}.{b}\" for a, b in zip(solvent_a, solvent_b)]\n",
    "    time = X_raw['Residence Time'].values\n",
    "    temp = X_raw['Temperature'].values\n",
    "    \n",
    "    spange_cols = spange.columns.tolist()\n",
    "    drfp_cols = [f'DRFP_{i}' for i in range(drfp.shape[1])]\n",
    "    \n",
    "    df = pd.DataFrame(spange_mix, columns=spange_cols)\n",
    "    df_drfp = pd.DataFrame(drfp_mix, columns=drfp_cols)\n",
    "    df = pd.concat([df, df_drfp], axis=1)\n",
    "    df['TEMPERATURE'] = temp\n",
    "    df['TIME'] = time\n",
    "    df['SOLVENT NAME'] = solvent_name\n",
    "    df['SOLVENT A NAME'] = solvent_a\n",
    "    df['SOLVENT B NAME'] = solvent_b\n",
    "    df['SolventB%'] = X_raw['SolventB%'].values\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_single = prepare_single_solvent_dataset(X_single_raw, spange, drfp)\n",
    "X_mix = prepare_mixture_dataset(X_full_raw, spange, drfp)\n",
    "\n",
    "print(f\"Single solvent dataset: {X_single.shape}\")\n",
    "print(f\"Mixture dataset: {X_mix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with Arrhenius kinetics\n",
    "def get_spange_features(X_data):\n",
    "    spange_cols = ['dielectric constant', 'ET(30)', 'alpha', 'beta', 'pi*', \n",
    "                   'SA', 'SB', 'SP', 'SdP', 'N', 'n', 'f(n)', 'delta']\n",
    "    return X_data[spange_cols].values\n",
    "\n",
    "def get_arrhenius_features(X_data):\n",
    "    T = X_data['TEMPERATURE'].values\n",
    "    t = X_data['TIME'].values\n",
    "    T_kelvin = T + 273.15\n",
    "    inv_T = 1.0 / T_kelvin\n",
    "    ln_t = np.log(t + 1e-6)\n",
    "    interaction = inv_T * ln_t\n",
    "    return np.column_stack([inv_T, ln_t, interaction, T, t])\n",
    "\n",
    "def prepare_features(X_data, drfp_mask=None, include_drfp=True):\n",
    "    \"\"\"Prepare all features (Spange + DRFP + Arrhenius)\"\"\"\n",
    "    spange = get_spange_features(X_data)\n",
    "    arrhenius = get_arrhenius_features(X_data)\n",
    "    \n",
    "    if include_drfp:\n",
    "        drfp_cols = [col for col in X_data.columns if col.startswith('DRFP_')]\n",
    "        drfp_data = X_data[drfp_cols].values\n",
    "        if drfp_mask is not None:\n",
    "            drfp_data = drfp_data[:, drfp_mask]\n",
    "        features = np.hstack([spange, drfp_data, arrhenius])\n",
    "    else:\n",
    "        features = np.hstack([spange, arrhenius])\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_mlp(X_train, Y_train, input_dim, epochs=200, lr=5e-4, weight_decay=1e-4, hidden_dims=[128, 64]):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MLPModel(input_dim, hidden_dims=hidden_dims).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20)\n",
    "    criterion = nn.HuberLoss()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    Y_tensor = torch.FloatTensor(Y_train).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_tensor)\n",
    "        loss = criterion(pred, Y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"MLP model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0be298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixall-Style 4-Model Ensemble with FULL features\n",
    "class MixallFullFeaturesModel:\n",
    "    \"\"\"\n",
    "    4-model ensemble: MLP + XGBoost + RandomForest + LightGBM\n",
    "    Uses FULL features: Spange + DRFP + Arrhenius\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_weight=0.3, xgb_weight=0.25, rf_weight=0.25, lgbm_weight=0.2):\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.xgb_weight = xgb_weight\n",
    "        self.rf_weight = rf_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp_models = []\n",
    "        self.xgb_models = []\n",
    "        self.rf_models = []\n",
    "        self.lgbm_models = []\n",
    "        \n",
    "        self.drfp_mask = None\n",
    "        self.input_dim = None\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Train all 4 model types\"\"\"\n",
    "        # Get DRFP mask\n",
    "        drfp_cols = [col for col in X_train.columns if col.startswith('DRFP_')]\n",
    "        drfp_data = X_train[drfp_cols].values\n",
    "        self.drfp_mask = drfp_data.var(axis=0) > 0\n",
    "        \n",
    "        # Prepare features\n",
    "        X_features = prepare_features(X_train, self.drfp_mask, include_drfp=True)\n",
    "        self.input_dim = X_features.shape[1]\n",
    "        X_scaled = self.scaler.fit_transform(X_features)\n",
    "        \n",
    "        Y_values = Y_train.values\n",
    "        \n",
    "        # Train MLP (3 models for bagging)\n",
    "        for _ in range(3):\n",
    "            mlp = train_mlp(X_scaled, Y_values, self.input_dim, epochs=200, hidden_dims=[128, 64])\n",
    "            self.mlp_models.append(mlp)\n",
    "        \n",
    "        # Train XGBoost (per-target)\n",
    "        xgb_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 6,\n",
    "            'n_estimators': 200,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        for i in range(3):\n",
    "            model = xgb.XGBRegressor(**xgb_params)\n",
    "            model.fit(X_scaled, Y_values[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "        \n",
    "        # Train RandomForest (per-target)\n",
    "        rf_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        for i in range(3):\n",
    "            model = RandomForestRegressor(**rf_params)\n",
    "            model.fit(X_scaled, Y_values[:, i])\n",
    "            self.rf_models.append(model)\n",
    "        \n",
    "        # Train LightGBM (per-target)\n",
    "        lgbm_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 31,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'verbose': -1,\n",
    "            'n_estimators': 500\n",
    "        }\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(**lgbm_params)\n",
    "            model.fit(X_scaled, Y_values[:, i])\n",
    "            self.lgbm_models.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Predict using 4-model ensemble\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Prepare features\n",
    "        X_features = prepare_features(X_test, self.drfp_mask, include_drfp=True)\n",
    "        X_scaled = self.scaler.transform(X_features)\n",
    "        \n",
    "        # MLP predictions\n",
    "        mlp_preds = []\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        for mlp in self.mlp_models:\n",
    "            mlp.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = mlp(X_tensor).cpu().numpy()\n",
    "            mlp_preds.append(pred)\n",
    "        mlp_pred = np.mean(mlp_preds, axis=0)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_pred = np.zeros((len(X_test), 3))\n",
    "        for i, model in enumerate(self.xgb_models):\n",
    "            xgb_pred[:, i] = model.predict(X_scaled)\n",
    "        xgb_pred = np.clip(xgb_pred, 0, 1)\n",
    "        \n",
    "        # RandomForest predictions\n",
    "        rf_pred = np.zeros((len(X_test), 3))\n",
    "        for i, model in enumerate(self.rf_models):\n",
    "            rf_pred[:, i] = model.predict(X_scaled)\n",
    "        rf_pred = np.clip(rf_pred, 0, 1)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_pred = np.zeros((len(X_test), 3))\n",
    "        for i, model in enumerate(self.lgbm_models):\n",
    "            lgbm_pred[:, i] = model.predict(X_scaled)\n",
    "        lgbm_pred = np.clip(lgbm_pred, 0, 1)\n",
    "        \n",
    "        # Ensemble\n",
    "        final_pred = (self.mlp_weight * mlp_pred + \n",
    "                      self.xgb_weight * xgb_pred + \n",
    "                      self.rf_weight * rf_pred + \n",
    "                      self.lgbm_weight * lgbm_pred)\n",
    "        \n",
    "        return np.clip(final_pred, 0, 1)\n",
    "\n",
    "print(\"MixallFullFeaturesModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ddd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvents\n",
    "print(\"Running Single Solvent CV with Mixall-Style 4-Model Ensemble...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits = list(generate_leave_one_out_splits(X_single, Y_single))\n",
    "print(f\"Number of folds: {len(splits)}\")\n",
    "\n",
    "solvent_errors = {}\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
    "    X_train = X_single.iloc[train_idx]\n",
    "    Y_train = Y_single.iloc[train_idx]\n",
    "    X_test = X_single.iloc[test_idx]\n",
    "    Y_test = Y_single.iloc[test_idx]\n",
    "    \n",
    "    test_solvent = X_test['SOLVENT NAME'].iloc[0]\n",
    "    \n",
    "    # Train model\n",
    "    model = MixallFullFeaturesModel()\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((preds - Y_test.values) ** 2)\n",
    "    solvent_errors[test_solvent] = mse\n",
    "    \n",
    "    all_preds.append(preds)\n",
    "    all_true.append(Y_test.values)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_solvent:45s} MSE = {mse:.6f}\")\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true = np.vstack(all_true)\n",
    "single_mse = np.mean((all_preds - all_true) ** 2)\n",
    "single_std = np.std([solvent_errors[s] for s in solvent_errors])\n",
    "\n",
    "print(f\"\\nMixall-Style Single Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dac55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for mixtures\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running Mixture CV with Mixall-Style 4-Model Ensemble...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mix_splits = list(generate_leave_one_ramp_out_splits(X_mix, Y_full))\n",
    "print(f\"Number of folds: {len(mix_splits)}\")\n",
    "\n",
    "mix_errors = {}\n",
    "mix_preds = []\n",
    "mix_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(mix_splits):\n",
    "    X_train = X_mix.iloc[train_idx]\n",
    "    Y_train = Y_full.iloc[train_idx]\n",
    "    X_test = X_mix.iloc[test_idx]\n",
    "    Y_test = Y_full.iloc[test_idx]\n",
    "    \n",
    "    test_mixture = X_test['SOLVENT NAME'].iloc[0]\n",
    "    \n",
    "    # Train model\n",
    "    model = MixallFullFeaturesModel()\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((preds - Y_test.values) ** 2)\n",
    "    mix_errors[test_mixture] = mse\n",
    "    \n",
    "    mix_preds.append(preds)\n",
    "    mix_true.append(Y_test.values)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_mixture:55s} MSE = {mse:.6f}\")\n",
    "\n",
    "mix_preds = np.vstack(mix_preds)\n",
    "mix_true = np.vstack(mix_true)\n",
    "mix_mse = np.mean((mix_preds - mix_true) ** 2)\n",
    "mix_std = np.std([mix_errors[s] for s in mix_errors])\n",
    "\n",
    "print(f\"\\nMixall-Style Mixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Mixall-Style 4-Model Ensemble Overall Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_single = len(all_true)\n",
    "n_mix = len(mix_true)\n",
    "n_total = n_single + n_mix\n",
    "\n",
    "overall_mse = (n_single * single_mse + n_mix * mix_mse) / n_total\n",
    "\n",
    "print(f\"\\nSingle Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f} (n={n_single})\")\n",
    "print(f\"Mixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f} (n={n_mix})\")\n",
    "print(f\"Overall CV MSE: {overall_mse:.6f}\")\n",
    "\n",
    "print(f\"\\nBaseline (exp_030): CV = 0.008298\")\n",
    "print(f\"exp_049b (Mixall with Spange only): CV = 0.014196\")\n",
    "print(f\"Improvement vs baseline: {(0.008298 - overall_mse) / 0.008298 * 100:.1f}%\")\n",
    "print(f\"Improvement vs exp_049b: {(0.014196 - overall_mse) / 0.014196 * 100:.1f}%\")\n",
    "\n",
    "if overall_mse < 0.008298:\n",
    "    print(\"\\n✓ BETTER than baseline!\")\n",
    "else:\n",
    "    print(\"\\n✗ WORSE than baseline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 053 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nMixall-Style 4-Model Ensemble (MLP + XGBoost + RF + LightGBM):\")\n",
    "print(f\"  Features: Spange + DRFP + Arrhenius (full features)\")\n",
    "print(f\"  Single Solvent CV: {single_mse:.6f}\")\n",
    "print(f\"  Mixture CV: {mix_mse:.6f}\")\n",
    "print(f\"  Overall CV: {overall_mse:.6f}\")\n",
    "print(f\"  vs Baseline (exp_030): {(overall_mse - 0.008298) / 0.008298 * 100:+.1f}%\")\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"1. 4-model ensemble: MLP + XGBoost + RF + LightGBM\")\n",
    "print(\"2. Uses FULL features (Spange + DRFP + Arrhenius)\")\n",
    "print(\"3. Different from exp_049b which used Spange only\")\n",
    "\n",
    "if overall_mse < 0.008298:\n",
    "    print(\"\\nCONCLUSION: Mixall-style ensemble with full features IMPROVES CV!\")\n",
    "    print(\"This is a fundamentally different approach that may change the CV-LB relationship.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: Mixall-style ensemble with full features does NOT improve CV.\")\n",
    "    print(\"The 4-model ensemble doesn't help with the current features.\")\n",
    "\n",
    "print(f\"\\nRemaining submissions: 5\")\n",
    "print(f\"Best model: exp_030 (GP 0.15 + MLP 0.55 + LGBM 0.3) with CV 0.008298, LB 0.0877\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
