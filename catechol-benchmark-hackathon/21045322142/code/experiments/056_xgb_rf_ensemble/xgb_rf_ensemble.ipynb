{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "802bad10",
   "metadata": {},
   "source": [
    "# Experiment 056: XGBoost + RandomForest Ensemble (Like 'mixall' Kernel)\n",
    "\n",
    "**Hypothesis**: The 'mixall' kernel uses MLP + XGBoost + RandomForest + LightGBM ensemble. We haven't tried XGBoost or RandomForest in our ensemble. These may provide different inductive biases that could change the CV-LB relationship.\n",
    "\n",
    "**Key changes from baseline (exp_030):**\n",
    "1. Add XGBoost and RandomForest to the ensemble\n",
    "2. Use 4-model ensemble: MLP + XGB + RF + LGBM\n",
    "3. Keep full features (Spange + DRFP + Arrhenius)\n",
    "\n",
    "**Why this might change CV-LB relationship**: Different ensemble members have different inductive biases. XGBoost and RandomForest may generalize differently to unseen solvents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0958ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "full_data = pd.read_csv('/home/data/catechol_full_data_yields.csv')\n",
    "single_data = pd.read_csv('/home/data/catechol_single_solvent_yields.csv')\n",
    "spange = pd.read_csv('/home/data/spange_descriptors_lookup.csv')\n",
    "drfp = pd.read_csv('/home/data/drfps_catechol_lookup.csv')\n",
    "smiles = pd.read_csv('/home/data/smiles_lookup.csv')\n",
    "\n",
    "print(f\"Full data: {full_data.shape}\")\n",
    "print(f\"Single solvent data: {single_data.shape}\")\n",
    "print(f\"Spange descriptors: {spange.shape}\")\n",
    "print(f\"DRFP features: {drfp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features - same as exp_030\n",
    "def prepare_features_single(data, spange, drfp):\n",
    "    \"\"\"Prepare features for single solvent dataset\"\"\"\n",
    "    # Rename column for consistency\n",
    "    data = data.copy()\n",
    "    data['Solvent'] = data['SOLVENT NAME']\n",
    "    \n",
    "    # Merge Spange descriptors\n",
    "    spange_renamed = spange.rename(columns={'SOLVENT NAME': 'Solvent'})\n",
    "    spange_cols = [c for c in spange_renamed.columns if c != 'Solvent']\n",
    "    data_merged = data.merge(spange_renamed, on='Solvent', how='left')\n",
    "    \n",
    "    # Merge DRFP features\n",
    "    drfp_renamed = drfp.rename(columns={'SOLVENT NAME': 'Solvent'})\n",
    "    drfp_cols = [c for c in drfp_renamed.columns if c != 'Solvent']\n",
    "    data_merged = data_merged.merge(drfp_renamed, on='Solvent', how='left')\n",
    "    \n",
    "    # Add Arrhenius features\n",
    "    data_merged['inv_temp'] = 1.0 / (data_merged['Temperature'] + 273.15)\n",
    "    data_merged['log_time'] = np.log1p(data_merged['Residence Time'])\n",
    "    \n",
    "    # Feature columns\n",
    "    feature_cols = spange_cols + drfp_cols + ['inv_temp', 'log_time']\n",
    "    \n",
    "    X = data_merged[feature_cols].values\n",
    "    Y = data_merged[['SM', 'Product 2', 'Product 3']].values\n",
    "    \n",
    "    return X, Y, feature_cols, data_merged\n",
    "\n",
    "# Prepare single solvent data\n",
    "X_single, Y_single, feature_cols, single_merged = prepare_features_single(single_data, spange, drfp)\n",
    "print(f\"Single solvent features: {X_single.shape}\\\")\\nprint(f\\\"Single solvent targets: {Y_single.shape}\\\")\\nprint(f\\\"Number of features: {len(feature_cols)}\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare mixture data\n",
    "def prepare_features_mix(data, spange, drfp):\n",
    "    \"\"\"Prepare features for mixture dataset\"\"\"\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Create solvent identifier for mixtures\n",
    "    data['Solvent'] = data['SOLVENT A NAME'] + '.' + data['SOLVENT B NAME']\n",
    "    \n",
    "    # Rename columns in lookup tables\n",
    "    spange_renamed = spange.rename(columns={'SOLVENT NAME': 'Solvent'})\n",
    "    drfp_renamed = drfp.rename(columns={'SOLVENT NAME': 'Solvent'})\n",
    "    \n",
    "    spange_cols = [c for c in spange_renamed.columns if c != 'Solvent']\n",
    "    drfp_cols = [c for c in drfp_renamed.columns if c != 'Solvent']\n",
    "    \n",
    "    # For mixtures, we need to average the features of both solvents\n",
    "    # First, get features for solvent A\n",
    "    spange_a = spange_renamed.copy()\n",
    "    spange_a.columns = ['SOLVENT A NAME'] + [f'{c}_A' for c in spange_cols]\n",
    "    drfp_a = drfp_renamed.copy()\n",
    "    drfp_a.columns = ['SOLVENT A NAME'] + [f'{c}_A' for c in drfp_cols]\n",
    "    \n",
    "    # Get features for solvent B\n",
    "    spange_b = spange_renamed.copy()\n",
    "    spange_b.columns = ['SOLVENT B NAME'] + [f'{c}_B' for c in spange_cols]\n",
    "    drfp_b = drfp_renamed.copy()\n",
    "    drfp_b.columns = ['SOLVENT B NAME'] + [f'{c}_B' for c in drfp_cols]\n",
    "    \n",
    "    # Merge\n",
    "    data_merged = data.merge(spange_a, on='SOLVENT A NAME', how='left')\n",
    "    data_merged = data_merged.merge(spange_b, on='SOLVENT B NAME', how='left')\n",
    "    data_merged = data_merged.merge(drfp_a, on='SOLVENT A NAME', how='left')\n",
    "    data_merged = data_merged.merge(drfp_b, on='SOLVENT B NAME', how='left')\n",
    "    \n",
    "    # Average features (weighted by ratio if available)\n",
    "    ratio_b = data_merged['SolventB%'].values / 100.0\n",
    "    ratio_a = 1.0 - ratio_b\n",
    "    \n",
    "    feature_list = []\n",
    "    for col in spange_cols:\n",
    "        avg = ratio_a * data_merged[f'{col}_A'].values + ratio_b * data_merged[f'{col}_B'].values\n",
    "        data_merged[col] = avg\n",
    "        feature_list.append(col)\n",
    "    \n",
    "    for col in drfp_cols:\n",
    "        avg = ratio_a * data_merged[f'{col}_A'].values + ratio_b * data_merged[f'{col}_B'].values\n",
    "        data_merged[col] = avg\n",
    "        feature_list.append(col)\n",
    "    \n",
    "    # Add Arrhenius features\n",
    "    data_merged['inv_temp'] = 1.0 / (data_merged['Temperature'] + 273.15)\n",
    "    data_merged['log_time'] = np.log1p(data_merged['Residence Time'])\n",
    "    feature_list.extend(['inv_temp', 'log_time'])\n",
    "    \n",
    "    X = data_merged[feature_list].values\n",
    "    Y = data_merged[['SM', 'Product 2', 'Product 3']].values\n",
    "    \n",
    "    return X, Y, feature_list, data_merged\n",
    "\n",
    "# Filter to mixtures only (where SolventB% > 0)\n",
    "full_data_mix = full_data[full_data['SolventB%'] > 0].copy()\n",
    "X_mix, Y_full, _, mix_merged = prepare_features_mix(full_data_mix, spange, drfp)\n",
    "print(f\"Mixture features: {X_mix.shape}\")\n",
    "print(f\"Mixture targets: {Y_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model (same as exp_030)\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32], output_dim=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())  # Yields are 0-1\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_mlp(X_train, Y_train, input_dim, epochs=200, lr=0.001, batch_size=32):\n",
    "    \"\"\"Train MLP model\"\"\"\n",
    "    model = MLPModel(input_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train)\n",
    "    Y_tensor = torch.FloatTensor(Y_train)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch training\n",
    "        indices = torch.randperm(len(X_tensor))\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch_idx = indices[i:i+batch_size]\n",
    "            X_batch = X_tensor[batch_idx]\n",
    "            Y_batch = Y_tensor[batch_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"MLP model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-Model Ensemble: MLP + XGBoost + RandomForest + LightGBM\n",
    "class FourModelEnsemble:\n",
    "    def __init__(self, input_dim, weights=[0.30, 0.25, 0.20, 0.25]):\n",
    "        \"\"\"\n",
    "        4-model ensemble like 'mixall' kernel\n",
    "        weights: [MLP, XGB, RF, LGBM]\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.weights = weights\n",
    "        self.mlp = None\n",
    "        self.xgb_models = []  # One per target\n",
    "        self.rf_models = []   # One per target\n",
    "        self.lgbm_models = [] # One per target\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Train MLP\n",
    "        self.mlp = train_mlp(X_scaled, Y_train, self.input_dim, epochs=200)\n",
    "        \n",
    "        # Train XGBoost (one per target)\n",
    "        self.xgb_models = []\n",
    "        for i in range(Y_train.shape[1]):\n",
    "            xgb = XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            xgb.fit(X_scaled, Y_train[:, i])\n",
    "            self.xgb_models.append(xgb)\n",
    "        \n",
    "        # Train RandomForest (one per target)\n",
    "        self.rf_models = []\n",
    "        for i in range(Y_train.shape[1]):\n",
    "            rf = RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf.fit(X_scaled, Y_train[:, i])\n",
    "            self.rf_models.append(rf)\n",
    "        \n",
    "        # Train LightGBM (one per target)\n",
    "        self.lgbm_models = []\n",
    "        for i in range(Y_train.shape[1]):\n",
    "            lgbm = LGBMRegressor(\n",
    "                n_estimators=500,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.03,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "            lgbm.fit(X_scaled, Y_train[:, i])\n",
    "            self.lgbm_models.append(lgbm)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # MLP predictions\n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            mlp_pred = self.mlp(torch.FloatTensor(X_scaled)).numpy()\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_pred = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # RandomForest predictions\n",
    "        rf_pred = np.column_stack([m.predict(X_scaled) for m in self.rf_models])\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_pred = np.column_stack([m.predict(X_scaled) for m in self.lgbm_models])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        pred = (self.weights[0] * mlp_pred + \n",
    "                self.weights[1] * xgb_pred + \n",
    "                self.weights[2] * rf_pred + \n",
    "                self.weights[3] * lgbm_pred)\n",
    "        \n",
    "        return np.clip(pred, 0, 1)\n",
    "\n",
    "print(\"FourModelEnsemble defined\")\n",
    "print(f\"Ensemble weights: MLP={0.30}, XGB={0.25}, RF={0.20}, LGBM={0.25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV functions from competition template\n",
    "def generate_leave_one_solvent_out_splits(X, Y, data):\n",
    "    \"\"\"Leave-one-solvent-out CV for single solvents\"\"\"\n",
    "    solvents = data['Solvent'].unique()\n",
    "    for solvent in solvents:\n",
    "        test_mask = data['Solvent'] == solvent\n",
    "        train_mask = ~test_mask\n",
    "        train_idx = np.where(train_mask)[0]\n",
    "        test_idx = np.where(test_mask)[0]\n",
    "        yield train_idx, test_idx\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Leave-one-ramp-out CV for mixtures\"\"\"\n",
    "    # Each unique solvent combination is a ramp\n",
    "    ramps = mix_merged['Solvent'].unique()\n",
    "    for ramp in ramps:\n",
    "        test_mask = mix_merged['Solvent'] == ramp\n",
    "        train_mask = ~test_mask\n",
    "        train_idx = np.where(train_mask)[0]\n",
    "        test_idx = np.where(test_mask)[0]\n",
    "        yield train_idx, test_idx\n",
    "\n",
    "print(\"CV functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893570c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvents\n",
    "print(\"=\"*60)\n",
    "print(\"Running Single Solvent CV with 4-Model Ensemble...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "single_splits = list(generate_leave_one_solvent_out_splits(X_single, Y_single, single_merged))\n",
    "print(f\"Number of folds: {len(single_splits)}\")\n",
    "\n",
    "single_errors = {}\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(single_splits):\n",
    "    X_train = X_single[train_idx]\n",
    "    Y_train = Y_single[train_idx]\n",
    "    X_test = X_single[test_idx]\n",
    "    Y_test = Y_single[test_idx]\n",
    "    \n",
    "    test_solvent = single_merged.iloc[test_idx]['Solvent'].iloc[0]\n",
    "    \n",
    "    # Train ensemble\n",
    "    model = FourModelEnsemble(input_dim=len(feature_cols))\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((pred - Y_test) ** 2)\n",
    "    single_errors[test_solvent] = mse\n",
    "    \n",
    "    all_preds.append(pred)\n",
    "    all_true.append(Y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_solvent:50s} MSE = {mse:.6f}\")\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true = np.vstack(all_true)\n",
    "single_mse = np.mean((all_preds - all_true) ** 2)\n",
    "single_std = np.std(list(single_errors.values()))\n",
    "\n",
    "print(f\"\\n4-Model Ensemble Single Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for mixtures\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running Mixture CV with 4-Model Ensemble...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mix_splits = list(generate_leave_one_ramp_out_splits(X_mix, Y_full))\n",
    "print(f\"Number of folds: {len(mix_splits)}\")\n",
    "\n",
    "mix_errors = {}\n",
    "mix_preds = []\n",
    "mix_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(mix_splits):\n",
    "    X_train = X_mix[train_idx]\n",
    "    Y_train = Y_full[train_idx]\n",
    "    X_test = X_mix[test_idx]\n",
    "    Y_test = Y_full[test_idx]\n",
    "    \n",
    "    test_mixture = mix_merged.iloc[test_idx]['Solvent'].iloc[0]\n",
    "    \n",
    "    # Train ensemble\n",
    "    model = FourModelEnsemble(input_dim=len(feature_cols))\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((pred - Y_test) ** 2)\n",
    "    mix_errors[test_mixture] = mse\n",
    "    \n",
    "    mix_preds.append(pred)\n",
    "    mix_true.append(Y_test)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_mixture:50s} MSE = {mse:.6f}\")\n",
    "\n",
    "mix_preds = np.vstack(mix_preds)\n",
    "mix_true = np.vstack(mix_true)\n",
    "mix_mse = np.mean((mix_preds - mix_true) ** 2)\n",
    "mix_std = np.std(list(mix_errors.values()))\n",
    "\n",
    "print(f\"\\n4-Model Ensemble Mixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4-Model Ensemble Overall Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_single = len(all_true)\n",
    "n_mix = len(mix_true)\n",
    "n_total = n_single + n_mix\n",
    "\n",
    "overall_mse = (n_single * single_mse + n_mix * mix_mse) / n_total\n",
    "\n",
    "print(f\"\\nSingle Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f} (n={n_single})\")\n",
    "print(f\"Mixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f} (n={n_mix})\")\n",
    "print(f\"Overall CV MSE: {overall_mse:.6f}\")\n",
    "\n",
    "print(f\"\\nBaseline (exp_030): CV = 0.008298\")\n",
    "print(f\"Improvement vs baseline: {(overall_mse - 0.008298) / 0.008298 * 100:+.1f}%\")\n",
    "\n",
    "if overall_mse < 0.008298:\n",
    "    print(\"\\n✓ BETTER than baseline!\")\n",
    "else:\n",
    "    print(\"\\n✗ WORSE than baseline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 056 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n4-Model Ensemble (MLP + XGB + RF + LGBM):\")\n",
    "print(f\"  Features: Spange + DRFP + Arrhenius ({len(feature_cols)} features)\")\n",
    "print(f\"  Weights: MLP=0.30, XGB=0.25, RF=0.20, LGBM=0.25\")\n",
    "print(f\"\\n  Single Solvent CV: {single_mse:.6f}\")\n",
    "print(f\"  Mixture CV: {mix_mse:.6f}\")\n",
    "print(f\"  Overall CV: {overall_mse:.6f}\")\n",
    "print(f\"  vs Baseline (exp_030): {(overall_mse - 0.008298) / 0.008298 * 100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"1. XGBoost and RandomForest add different inductive biases\")\n",
    "print(f\"2. 4-model ensemble may have different CV-LB relationship\")\n",
    "print(f\"3. This approach is similar to the 'mixall' kernel\")\n",
    "\n",
    "print(f\"\\nRemaining submissions: 5\")\n",
    "print(f\"Best model: exp_030 (GP 0.15 + MLP 0.55 + LGBM 0.3) with CV 0.008298, LB 0.0877\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
