{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc86b67c",
   "metadata": {},
   "source": [
    "# Experiment 047: Diverse Ensemble Model\n",
    "\n",
    "**Inspiration:** The 'mixall' kernel achieves good CV/LB with an ensemble of MLP + XGBoost + RandomForest + LightGBM.\n",
    "\n",
    "**Hypothesis:** Our current ensemble (GP + MLP + LGBM) may be too homogeneous. A more diverse ensemble could capture different patterns and potentially change the CV-LB relationship.\n",
    "\n",
    "**Implementation:**\n",
    "1. MLP: [128, 64, 32] with BatchNorm, ReLU, Dropout(0.1)\n",
    "2. XGBoost: n_estimators=300, max_depth=6\n",
    "3. RandomForest: n_estimators=300, max_depth=15\n",
    "4. LightGBM: n_estimators=300, num_leaves=31\n",
    "5. Weighted ensemble: [0.4, 0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[[\"SM\", \"Product 2\", \"Product 3\"]]\n",
    "    return X, Y\n",
    "\n",
    "# Load feature lookup tables\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "SPANGE_COLS = [c for c in spange_df.columns if c != 'solvent smiles']\n",
    "\n",
    "print(f'Spange: {len(SPANGE_COLS)} features')\n",
    "\n",
    "# Load data\n",
    "X_single, Y_single = load_data('single_solvent')\n",
    "X_full, Y_full = load_data('full')\n",
    "\n",
    "print(f'Single solvent: {len(X_single)} samples')\n",
    "print(f'Full data: {len(X_full)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction - simpler features (Spange + kinetics only, no DRFP)\n",
    "def get_features_simple(X, data_type='single'):\n",
    "    \"\"\"Extract simpler features: Spange descriptors + kinetics only.\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in X.iterrows():\n",
    "        time_m = row['Residence Time']\n",
    "        temp_c = row['Temperature']\n",
    "        temp_k = temp_c + 273.15\n",
    "        \n",
    "        kinetics = np.array([\n",
    "            time_m, temp_c, 1.0 / temp_k,\n",
    "            np.log(time_m + 1), time_m / temp_k\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if data_type == 'single':\n",
    "            solvent = row['SOLVENT NAME']\n",
    "            spange = spange_df.loc[solvent, SPANGE_COLS].values.astype(np.float32) if solvent in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "        else:\n",
    "            solvent_a = row['SOLVENT A NAME']\n",
    "            solvent_b = row['SOLVENT B NAME']\n",
    "            pct_b = row['SolventB%'] / 100.0\n",
    "            pct_a = 1 - pct_b\n",
    "            \n",
    "            sp_a = spange_df.loc[solvent_a, SPANGE_COLS].values.astype(np.float32) if solvent_a in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            sp_b = spange_df.loc[solvent_b, SPANGE_COLS].values.astype(np.float32) if solvent_b in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            spange = pct_a * sp_a + pct_b * sp_b\n",
    "        \n",
    "        features = np.concatenate([kinetics, spange])\n",
    "        features_list.append(features)\n",
    "    \n",
    "    return np.array(features_list, dtype=np.float32)\n",
    "\n",
    "print('Feature extraction defined')\n",
    "print(f'Feature dimension: 5 (kinetics) + {len(SPANGE_COLS)} (Spange) = {5 + len(SPANGE_COLS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced MLP Model\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('EnhancedMLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diverse Ensemble Model (MLP + XGBoost + RandomForest + LightGBM)\n",
    "class DiverseEnsembleModel:\n",
    "    \"\"\"Diverse ensemble inspired by 'mixall' kernel.\n",
    "    \n",
    "    Uses MLP + XGBoost + RandomForest + LightGBM with weighted averaging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', weights=[0.4, 0.2, 0.2, 0.2]):\n",
    "        self.data_type = data\n",
    "        self.weights = weights  # MLP, XGB, RF, LGBM\n",
    "        \n",
    "        self.scaler = None\n",
    "        self.mlp_models = []\n",
    "        self.xgb_model = None\n",
    "        self.rf_model = None\n",
    "        self.lgbm_model = None\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs=200):\n",
    "        X_feat = get_features_simple(X_train, self.data_type)\n",
    "        y_np = y_train.values.astype(np.float32)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # Train MLP ensemble (3 models)\n",
    "        self.mlp_models = []\n",
    "        for _ in range(3):\n",
    "            model = EnhancedMLP(X_scaled.shape[1], hidden_dims=[128, 64, 32], dropout=0.1).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            y_tensor = torch.tensor(y_np).to(device)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                for X_batch, y_batch in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(X_batch)\n",
    "                    loss = nn.MSELoss()(pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            model.eval()\n",
    "            self.mlp_models.append(model)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        self.xgb_model = MultiOutputRegressor(\n",
    "            xgb.XGBRegressor(\n",
    "                n_estimators=300, learning_rate=0.05, max_depth=6,\n",
    "                subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0\n",
    "            )\n",
    "        )\n",
    "        self.xgb_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # Train RandomForest\n",
    "        self.rf_model = MultiOutputRegressor(\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=300, max_depth=15, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        )\n",
    "        self.rf_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm_model = MultiOutputRegressor(\n",
    "            lgb.LGBMRegressor(\n",
    "                n_estimators=300, learning_rate=0.05, num_leaves=31,\n",
    "                random_state=42, verbose=-1\n",
    "            )\n",
    "        )\n",
    "        self.lgbm_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = get_features_simple(X_test, self.data_type)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # MLP predictions (average of ensemble)\n",
    "        mlp_preds = []\n",
    "        for model in self.mlp_models:\n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(X_tensor).cpu().numpy()\n",
    "            mlp_preds.append(pred)\n",
    "        mlp_preds = np.mean(mlp_preds, axis=0)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_preds = self.xgb_model.predict(X_scaled)\n",
    "        \n",
    "        # RandomForest predictions\n",
    "        rf_preds = self.rf_model.predict(X_scaled)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_preds = self.lgbm_model.predict(X_scaled)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_preds = (self.weights[0] * mlp_preds + \n",
    "                          self.weights[1] * xgb_preds + \n",
    "                          self.weights[2] * rf_preds + \n",
    "                          self.weights[3] * lgbm_preds)\n",
    "        \n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(ensemble_preds, dtype=torch.float32)\n",
    "\n",
    "print('DiverseEnsembleModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f042680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test diverse ensemble on single solvent data\n",
    "print(\"Testing diverse ensemble on single solvent data...\")\n",
    "print()\n",
    "\n",
    "all_solvents = sorted(X_single[\"SOLVENT NAME\"].unique())\n",
    "fold_mses = []\n",
    "\n",
    "for test_solvent in all_solvents:\n",
    "    mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "    \n",
    "    model = DiverseEnsembleModel(data='single', weights=[0.4, 0.2, 0.2, 0.2])\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    \n",
    "    actuals = Y_single[~mask].values\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    fold_mses.append(mse)\n",
    "    print(f\"{test_solvent}: MSE = {mse:.6f}\")\n",
    "\n",
    "mean_mse = np.mean(fold_mses)\n",
    "std_mse = np.std(fold_mses)\n",
    "print(f\"\\n=== Diverse Ensemble CV Results ===\")\n",
    "print(f\"Mean MSE: {mean_mse:.6f} +/- {std_mse:.6f}\")\n",
    "print(f\"Baseline (exp_030): CV = 0.008298\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
