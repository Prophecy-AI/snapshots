{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f8b49e",
   "metadata": {},
   "source": [
    "# Experiment 049: Manual OOD Solvent Handling\n",
    "\n",
    "**Hypothesis**: Manually identify high-error solvents and use simpler features (Spange only) for them.\n",
    "\n",
    "**Based on evaluator's suggestion**: The cosine similarity approach in exp_048 failed because all solvents had similarity >0.99. Instead, we manually identify the high-error solvents from exp_048 analysis:\n",
    "- HFIP: 0.038 (4.3x mean error)\n",
    "- Water.Ethanol: 0.028 (3.2x mean error)\n",
    "- Acetonitrile.Acetic Acid: 0.022 (2.5x mean error)\n",
    "- TFE: 0.015 (1.7x mean error)\n",
    "\n",
    "**Implementation**:\n",
    "- Model A: Full features (Spange + DRFP) with GP + MLP + LGBM ensemble for normal solvents\n",
    "- Model B: Simple features (Spange only) with MLP + LGBM for high-error solvents\n",
    "- Hard-coded list of high-error solvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03725b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/code')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "from utils import load_data, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X, Y = load_data()\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "print(f\"Unique solvents: {X['SOLVENT NAME'].nunique()}\")\n",
    "print(f\"Target columns: {Y.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define high-error solvents based on exp_048 analysis\n",
    "# These are the solvents that consistently have high errors across experiments\n",
    "HIGH_ERROR_SOLVENTS = [\n",
    "    '1,1,1,3,3,3-Hexafluoropropan-2-ol',  # HFIP - 0.038 (4.3x mean error)\n",
    "    '2,2,2-Trifluoroethanol',  # TFE - 0.015 (1.7x mean error)\n",
    "    'Cyclohexane',  # Biggest outlier by distance from cluster center (4.63)\n",
    "]\n",
    "\n",
    "# Also check for mixture solvents that might be problematic\n",
    "HIGH_ERROR_MIXTURES = [\n",
    "    'Acetonitrile.Acetic Acid',  # 0.022 (2.5x mean error)\n",
    "    'Water.Ethanol',  # 0.028 (3.2x mean error)\n",
    "    'Water.2,2,2-Trifluoroethanol',  # Contains TFE\n",
    "]\n",
    "\n",
    "print(f\"High-error single solvents: {HIGH_ERROR_SOLVENTS}\")\n",
    "print(f\"High-error mixtures: {HIGH_ERROR_MIXTURES}\")\n",
    "\n",
    "# Check which solvents are in the data\n",
    "all_solvents = X['SOLVENT NAME'].unique()\n",
    "print(f\"\\nSolvents in data that match high-error list:\")\n",
    "for s in HIGH_ERROR_SOLVENTS + HIGH_ERROR_MIXTURES:\n",
    "    if s in all_solvents:\n",
    "        count = (X['SOLVENT NAME'] == s).sum()\n",
    "        print(f\"  {s}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a58f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction functions\n",
    "def get_spange_features(X_data):\n",
    "    \"\"\"Extract Spange descriptors (13 features)\"\"\"\n",
    "    spange_cols = ['SP', 'SdP', 'SA', 'SB', 'Dipolarity', 'Polarizability', \n",
    "                   'Acidity', 'Basicity', 'Viscosity', 'Surface Tension', \n",
    "                   'Refractive Index', 'Density', 'Molar Volume']\n",
    "    return X_data[spange_cols].values\n",
    "\n",
    "def get_drfp_features(X_data):\n",
    "    \"\"\"Extract DRFP features (high-variance only)\"\"\"\n",
    "    drfp_cols = [col for col in X_data.columns if col.startswith('DRFP_')]\n",
    "    drfp_data = X_data[drfp_cols].values\n",
    "    # Filter by variance > 0\n",
    "    var_mask = drfp_data.var(axis=0) > 0\n",
    "    return drfp_data[:, var_mask], var_mask\n",
    "\n",
    "def get_arrhenius_features(X_data):\n",
    "    \"\"\"Extract Arrhenius kinetics features\"\"\"\n",
    "    T = X_data['TEMPERATURE'].values\n",
    "    t = X_data['TIME'].values\n",
    "    T_kelvin = T + 273.15\n",
    "    inv_T = 1.0 / T_kelvin\n",
    "    ln_t = np.log(t + 1e-6)\n",
    "    interaction = inv_T * ln_t\n",
    "    return np.column_stack([inv_T, ln_t, interaction, T, t])\n",
    "\n",
    "def prepare_features(X_data, drfp_mask=None, include_drfp=True):\n",
    "    \"\"\"Prepare all features\"\"\"\n",
    "    spange = get_spange_features(X_data)\n",
    "    arrhenius = get_arrhenius_features(X_data)\n",
    "    \n",
    "    if include_drfp:\n",
    "        drfp_cols = [col for col in X_data.columns if col.startswith('DRFP_')]\n",
    "        drfp_data = X_data[drfp_cols].values\n",
    "        if drfp_mask is not None:\n",
    "            drfp_data = drfp_data[:, drfp_mask]\n",
    "        features = np.hstack([spange, drfp_data, arrhenius])\n",
    "    else:\n",
    "        features = np.hstack([spange, arrhenius])\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))  # 3 outputs\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_mlp(X_train, Y_train, input_dim, epochs=200, lr=5e-4, weight_decay=1e-4, hidden_dims=[128, 64]):\n",
    "    \"\"\"Train MLP model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MLPModel(input_dim, hidden_dims=hidden_dims).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=20)\n",
    "    criterion = nn.HuberLoss()\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    Y_tensor = torch.FloatTensor(Y_train).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_tensor)\n",
    "        loss = criterion(pred, Y_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"MLP model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d384cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual OOD Handling Model\n",
    "class ManualOODModel:\n",
    "    \"\"\"\n",
    "    Model that uses different feature sets for high-error solvents.\n",
    "    - Full features (Spange + DRFP) for normal solvents\n",
    "    - Simple features (Spange only) for high-error solvents\n",
    "    \"\"\"\n",
    "    def __init__(self, high_error_solvents, gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3):\n",
    "        self.high_error_solvents = high_error_solvents\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        \n",
    "        # Full feature models\n",
    "        self.scaler_full = StandardScaler()\n",
    "        self.gp_models_full = []\n",
    "        self.mlp_models_full = []\n",
    "        self.lgbm_models_full = []\n",
    "        \n",
    "        # Simple feature models (for high-error solvents)\n",
    "        self.scaler_simple = StandardScaler()\n",
    "        self.mlp_models_simple = []\n",
    "        self.lgbm_models_simple = []\n",
    "        \n",
    "        self.drfp_mask = None\n",
    "        self.input_dim_full = None\n",
    "        self.input_dim_simple = None\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"Train both full and simple feature models\"\"\"\n",
    "        # Get DRFP mask from training data\n",
    "        drfp_cols = [col for col in X_train.columns if col.startswith('DRFP_')]\n",
    "        drfp_data = X_train[drfp_cols].values\n",
    "        self.drfp_mask = drfp_data.var(axis=0) > 0\n",
    "        \n",
    "        # Prepare full features\n",
    "        X_full = prepare_features(X_train, self.drfp_mask, include_drfp=True)\n",
    "        self.input_dim_full = X_full.shape[1]\n",
    "        X_full_scaled = self.scaler_full.fit_transform(X_full)\n",
    "        \n",
    "        # Prepare simple features\n",
    "        X_simple = prepare_features(X_train, None, include_drfp=False)\n",
    "        self.input_dim_simple = X_simple.shape[1]\n",
    "        X_simple_scaled = self.scaler_simple.fit_transform(X_simple)\n",
    "        \n",
    "        Y_values = Y_train.values\n",
    "        \n",
    "        # Train full feature models\n",
    "        # GP (only on subset for speed)\n",
    "        n_gp = min(200, len(X_full_scaled))\n",
    "        idx_gp = np.random.choice(len(X_full_scaled), n_gp, replace=False)\n",
    "        for i in range(3):\n",
    "            kernel = Matern(nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=2)\n",
    "            gp.fit(X_full_scaled[idx_gp], Y_values[idx_gp, i])\n",
    "            self.gp_models_full.append(gp)\n",
    "        \n",
    "        # MLP (3 models for bagging)\n",
    "        for _ in range(3):\n",
    "            mlp = train_mlp(X_full_scaled, Y_values, self.input_dim_full, epochs=200, hidden_dims=[128, 64])\n",
    "            self.mlp_models_full.append(mlp)\n",
    "        \n",
    "        # LightGBM\n",
    "        lgbm_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 31,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(**lgbm_params, n_estimators=500)\n",
    "            model.fit(X_full_scaled, Y_values[:, i])\n",
    "            self.lgbm_models_full.append(model)\n",
    "        \n",
    "        # Train simple feature models\n",
    "        # MLP (3 models for bagging)\n",
    "        for _ in range(3):\n",
    "            mlp = train_mlp(X_simple_scaled, Y_values, self.input_dim_simple, epochs=200, hidden_dims=[64, 32])\n",
    "            self.mlp_models_simple.append(mlp)\n",
    "        \n",
    "        # LightGBM\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(**lgbm_params, n_estimators=500)\n",
    "            model.fit(X_simple_scaled, Y_values[:, i])\n",
    "            self.lgbm_models_simple.append(model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Predict using appropriate model based on solvent\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Prepare features\n",
    "        X_full = prepare_features(X_test, self.drfp_mask, include_drfp=True)\n",
    "        X_full_scaled = self.scaler_full.transform(X_full)\n",
    "        \n",
    "        X_simple = prepare_features(X_test, None, include_drfp=False)\n",
    "        X_simple_scaled = self.scaler_simple.transform(X_simple)\n",
    "        \n",
    "        # Get predictions from full feature models\n",
    "        # GP predictions\n",
    "        gp_preds = np.zeros((len(X_test), 3))\n",
    "        for i, gp in enumerate(self.gp_models_full):\n",
    "            gp_preds[:, i] = gp.predict(X_full_scaled)\n",
    "        gp_preds = np.clip(gp_preds, 0, 1)\n",
    "        \n",
    "        # MLP predictions\n",
    "        mlp_preds_full = []\n",
    "        X_tensor = torch.FloatTensor(X_full_scaled).to(device)\n",
    "        for mlp in self.mlp_models_full:\n",
    "            mlp.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = mlp(X_tensor).cpu().numpy()\n",
    "            mlp_preds_full.append(pred)\n",
    "        mlp_pred_full = np.mean(mlp_preds_full, axis=0)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_preds_full = np.zeros((len(X_test), 3))\n",
    "        for i, model in enumerate(self.lgbm_models_full):\n",
    "            lgbm_preds_full[:, i] = model.predict(X_full_scaled)\n",
    "        lgbm_preds_full = np.clip(lgbm_preds_full, 0, 1)\n",
    "        \n",
    "        # Ensemble full feature predictions\n",
    "        pred_full = self.gp_weight * gp_preds + self.mlp_weight * mlp_pred_full + self.lgbm_weight * lgbm_preds_full\n",
    "        \n",
    "        # Get predictions from simple feature models\n",
    "        # MLP predictions\n",
    "        mlp_preds_simple = []\n",
    "        X_tensor_simple = torch.FloatTensor(X_simple_scaled).to(device)\n",
    "        for mlp in self.mlp_models_simple:\n",
    "            mlp.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = mlp(X_tensor_simple).cpu().numpy()\n",
    "            mlp_preds_simple.append(pred)\n",
    "        mlp_pred_simple = np.mean(mlp_preds_simple, axis=0)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_preds_simple = np.zeros((len(X_test), 3))\n",
    "        for i, model in enumerate(self.lgbm_models_simple):\n",
    "            lgbm_preds_simple[:, i] = model.predict(X_simple_scaled)\n",
    "        lgbm_preds_simple = np.clip(lgbm_preds_simple, 0, 1)\n",
    "        \n",
    "        # Ensemble simple feature predictions (no GP, just MLP + LGBM)\n",
    "        pred_simple = 0.6 * mlp_pred_simple + 0.4 * lgbm_preds_simple\n",
    "        \n",
    "        # Select predictions based on solvent\n",
    "        final_preds = np.zeros((len(X_test), 3))\n",
    "        solvents = X_test['SOLVENT NAME'].values\n",
    "        \n",
    "        for idx in range(len(X_test)):\n",
    "            solvent = solvents[idx]\n",
    "            if solvent in self.high_error_solvents:\n",
    "                final_preds[idx] = pred_simple[idx]\n",
    "            else:\n",
    "                final_preds[idx] = pred_full[idx]\n",
    "        \n",
    "        return np.clip(final_preds, 0, 1)\n",
    "\n",
    "print(\"ManualOODModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for single solvents\n",
    "print(\"Running Single Solvent CV (Leave-One-Out, 24 folds)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to single solvents only\n",
    "single_mask = ~X['SOLVENT NAME'].str.contains('\\\\.')\n",
    "X_single = X[single_mask].reset_index(drop=True)\n",
    "Y_single = Y[single_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Single solvent samples: {len(X_single)}\")\n",
    "print(f\"Unique single solvents: {X_single['SOLVENT NAME'].nunique()}\")\n",
    "\n",
    "# Define high-error solvents for single solvents\n",
    "high_error_single = [\n",
    "    '1,1,1,3,3,3-Hexafluoropropan-2-ol',  # HFIP\n",
    "    '2,2,2-Trifluoroethanol',  # TFE\n",
    "    'Cyclohexane',\n",
    "]\n",
    "\n",
    "# Generate splits\n",
    "splits = generate_leave_one_out_splits(X_single, Y_single)\n",
    "print(f\"Number of folds: {len(splits)}\")\n",
    "\n",
    "# Track per-solvent errors\n",
    "solvent_errors = {}\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
    "    X_train = X_single.iloc[train_idx]\n",
    "    Y_train = Y_single.iloc[train_idx]\n",
    "    X_test = X_single.iloc[test_idx]\n",
    "    Y_test = Y_single.iloc[test_idx]\n",
    "    \n",
    "    test_solvent = X_test['SOLVENT NAME'].iloc[0]\n",
    "    \n",
    "    # Train model\n",
    "    model = ManualOODModel(high_error_single)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((preds - Y_test.values) ** 2)\n",
    "    solvent_errors[test_solvent] = mse\n",
    "    \n",
    "    all_preds.append(preds)\n",
    "    all_true.append(Y_test.values)\n",
    "    \n",
    "    is_high_error = test_solvent in high_error_single\n",
    "    marker = \" [HIGH-ERROR]\" if is_high_error else \"\"\n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_solvent:40s} MSE = {mse:.6f}{marker}\")\n",
    "\n",
    "# Calculate overall MSE\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_true = np.vstack(all_true)\n",
    "single_mse = np.mean((all_preds - all_true) ** 2)\n",
    "single_std = np.std([solvent_errors[s] for s in solvent_errors])\n",
    "\n",
    "print(f\"\\nSingle Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b29479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-solvent errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Per-Solvent Error Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by error\n",
    "sorted_errors = sorted(solvent_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 highest error solvents:\")\n",
    "for i, (solvent, mse) in enumerate(sorted_errors[:10]):\n",
    "    is_high_error = solvent in high_error_single\n",
    "    marker = \" [TARGETED]\" if is_high_error else \"\"\n",
    "    print(f\"  {i+1:2d}. {solvent:40s}: {mse:.6f}{marker}\")\n",
    "\n",
    "print(\"\\nTop 10 lowest error solvents:\")\n",
    "for i, (solvent, mse) in enumerate(sorted_errors[-10:]):\n",
    "    print(f\"  {i+1:2d}. {solvent:40s}: {mse:.6f}\")\n",
    "\n",
    "# Compare high-error solvents to baseline\n",
    "print(\"\\nHigh-error solvent comparison:\")\n",
    "print(f\"{'Solvent':<45} {'This Exp':>12} {'Baseline':>12} {'Change':>12}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "# Baseline errors from exp_030 (approximate from previous experiments)\n",
    "baseline_errors = {\n",
    "    '1,1,1,3,3,3-Hexafluoropropan-2-ol': 0.096,  # HFIP\n",
    "    '2,2,2-Trifluoroethanol': 0.042,  # TFE\n",
    "    'Cyclohexane': 0.198,\n",
    "}\n",
    "\n",
    "for solvent in high_error_single:\n",
    "    if solvent in solvent_errors:\n",
    "        this_exp = solvent_errors[solvent]\n",
    "        baseline = baseline_errors.get(solvent, 'N/A')\n",
    "        if isinstance(baseline, float):\n",
    "            change = (this_exp - baseline) / baseline * 100\n",
    "            print(f\"{solvent:<45} {this_exp:>12.6f} {baseline:>12.6f} {change:>11.1f}%\")\n",
    "        else:\n",
    "            print(f\"{solvent:<45} {this_exp:>12.6f} {baseline:>12}\")\n",
    "\n",
    "print(f\"\\nMean error for high-error solvents: {np.mean([solvent_errors.get(s, 0) for s in high_error_single if s in solvent_errors]):.6f}\")\n",
    "print(f\"Mean error for other solvents: {np.mean([v for k, v in solvent_errors.items() if k not in high_error_single]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV for mixtures\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running Mixture CV (Leave-One-Ramp-Out, 13 folds)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to mixtures only\n",
    "mix_mask = X['SOLVENT NAME'].str.contains('\\\\.')\n",
    "X_mix = X[mix_mask].reset_index(drop=True)\n",
    "Y_mix = Y[mix_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Mixture samples: {len(X_mix)}\")\n",
    "print(f\"Unique mixtures: {X_mix['SOLVENT NAME'].nunique()}\")\n",
    "\n",
    "# Define high-error solvents for mixtures\n",
    "high_error_mix = [\n",
    "    'Acetonitrile.Acetic Acid',\n",
    "    'Water.Ethanol',\n",
    "    'Water.2,2,2-Trifluoroethanol',\n",
    "]\n",
    "\n",
    "# Generate splits\n",
    "mix_splits = generate_leave_one_ramp_out_splits(X_mix, Y_mix)\n",
    "print(f\"Number of folds: {len(mix_splits)}\")\n",
    "\n",
    "# Track per-mixture errors\n",
    "mix_errors = {}\n",
    "mix_preds = []\n",
    "mix_true = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(mix_splits):\n",
    "    X_train = X_mix.iloc[train_idx]\n",
    "    Y_train = Y_mix.iloc[train_idx]\n",
    "    X_test = X_mix.iloc[test_idx]\n",
    "    Y_test = Y_mix.iloc[test_idx]\n",
    "    \n",
    "    test_mixture = X_test['SOLVENT NAME'].iloc[0]\n",
    "    \n",
    "    # Train model\n",
    "    model = ManualOODModel(high_error_mix)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = np.mean((preds - Y_test.values) ** 2)\n",
    "    mix_errors[test_mixture] = mse\n",
    "    \n",
    "    mix_preds.append(preds)\n",
    "    mix_true.append(Y_test.values)\n",
    "    \n",
    "    is_high_error = test_mixture in high_error_mix\n",
    "    marker = \" [HIGH-ERROR]\" if is_high_error else \"\"\n",
    "    print(f\"Fold {fold_idx+1:2d}: {test_mixture:40s} MSE = {mse:.6f}{marker}\")\n",
    "\n",
    "# Calculate overall MSE\n",
    "mix_preds = np.vstack(mix_preds)\n",
    "mix_true = np.vstack(mix_true)\n",
    "mix_mse = np.mean((mix_preds - mix_true) ** 2)\n",
    "mix_std = np.std([mix_errors[s] for s in mix_errors])\n",
    "\n",
    "print(f\"\\nMixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfadbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall CV score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Weighted average (same as competition)\n",
    "n_single = len(all_true)\n",
    "n_mix = len(mix_true)\n",
    "n_total = n_single + n_mix\n",
    "\n",
    "overall_mse = (n_single * single_mse + n_mix * mix_mse) / n_total\n",
    "\n",
    "print(f\"\\nSingle Solvent CV MSE: {single_mse:.6f} +/- {single_std:.6f} (n={n_single})\")\n",
    "print(f\"Mixture CV MSE: {mix_mse:.6f} +/- {mix_std:.6f} (n={n_mix})\")\n",
    "print(f\"Overall CV MSE: {overall_mse:.6f}\")\n",
    "\n",
    "print(f\"\\nBaseline (exp_030): CV = 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - overall_mse) / 0.008298 * 100:.1f}%\")\n",
    "\n",
    "# Check if this is better than baseline\n",
    "if overall_mse < 0.008298:\n",
    "    print(\"\\n✓ BETTER than baseline! Consider submitting.\")\n",
    "else:\n",
    "    print(\"\\n✗ WORSE than baseline. Need to adjust approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary of Manual OOD Handling Experiment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nManual OOD Handling CV MSE: {overall_mse:.6f}\")\n",
    "print(f\"Baseline (exp_030): CV = 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - overall_mse) / 0.008298 * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nPer-solvent comparison for high-error solvents:\")\n",
    "for solvent in high_error_single:\n",
    "    if solvent in solvent_errors:\n",
    "        this_exp = solvent_errors[solvent]\n",
    "        baseline = baseline_errors.get(solvent, 'N/A')\n",
    "        if isinstance(baseline, float):\n",
    "            change = (this_exp - baseline) / baseline * 100\n",
    "            print(f\"  {solvent}: {this_exp:.6f} (was {baseline:.6f}, {change:+.1f}%)\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Manual OOD handling targets specific high-error solvents\")\n",
    "print(\"2. Uses simpler features (Spange only) for these solvents\")\n",
    "print(\"3. Uses full features (Spange + DRFP) for other solvents\")\n",
    "\n",
    "if overall_mse < 0.008298:\n",
    "    print(\"\\nCONCLUSION: Manual OOD handling IMPROVES overall CV.\")\n",
    "    print(\"Consider submitting to test if this changes the CV-LB relationship.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: Manual OOD handling does NOT improve overall CV.\")\n",
    "    print(\"The improvement on high-error solvents doesn't compensate for other solvents.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Try mixall-style ensemble (MLP + XGBoost + RF + LightGBM)\")\n",
    "    print(\"2. Try ensemble disagreement for OOD detection\")\n",
    "    print(\"3. Consider different high-error solvent list\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
