{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facc8918",
   "metadata": {},
   "source": [
    "# Loop 48 Analysis: Critical Strategic Assessment\n",
    "\n",
    "## Key Discovery: The mixall Kernel Overwrites Validation Functions!\n",
    "\n",
    "The evaluator correctly identified that the 'mixall' kernel uses **GroupKFold (5 splits)** instead of **Leave-One-Out (24 folds)**. This is done by OVERWRITING the utility functions:\n",
    "\n",
    "```python\n",
    "# From mixall kernel:\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    groups = X[\"SOLVENT NAME\"]\n",
    "    n_splits = min(5, len(groups.unique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Critical Question: Is This Allowed?\n",
    "\n",
    "The competition rules state:\n",
    "> \"the submission must have the same last three cells as in the notebook template, with the only allowed change being the line where the model is defined\"\n",
    "\n",
    "BUT the mixall kernel overwrites the utility functions BEFORE the last three cells. This is a gray area - they're not changing the last three cells, but they're changing the behavior of functions called by those cells.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "If the LB evaluation uses the ORIGINAL `generate_leave_one_out_splits` (24 folds), then:\n",
    "- Our CV (24 folds) matches LB evaluation\n",
    "- The mixall kernel's CV (5 folds) does NOT match LB evaluation\n",
    "- Their good LB score comes from their MODEL, not their CV scheme\n",
    "\n",
    "If the LB evaluation uses GroupKFold (5 folds), then:\n",
    "- Our CV (24 folds) does NOT match LB evaluation\n",
    "- The mixall kernel's CV (5 folds) matches LB evaluation\n",
    "- We should switch to GroupKFold for better CV-LB correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission history:')\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'CV-LB Relationship: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'Intercept = {intercept:.4f}')\n",
    "print(f'Target = 0.0347')\n",
    "print()\n",
    "print(f'CRITICAL: Intercept ({intercept:.4f}) > Target (0.0347)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13409f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would it take to reach the target?\n",
    "target = 0.0347\n",
    "best_lb = 0.0877\n",
    "best_cv = 0.0083\n",
    "\n",
    "print('=== PATH TO TARGET ===')\n",
    "print()\n",
    "print(f'Current best LB: {best_lb:.4f}')\n",
    "print(f'Target: {target:.4f}')\n",
    "print(f'Gap: {(best_lb - target) / target * 100:.1f}%')\n",
    "print()\n",
    "print('Option 1: Improve CV (current approach)')\n",
    "required_cv = (target - intercept) / slope\n",
    "print(f'  Required CV: {required_cv:.6f}')\n",
    "if required_cv < 0:\n",
    "    print('  IMPOSSIBLE: Required CV is negative!')\n",
    "print()\n",
    "print('Option 2: Change the CV-LB relationship')\n",
    "print('  Need to reduce the intercept from 0.0525 to < 0.0347')\n",
    "print('  OR change the slope to make CV improvements more impactful')\n",
    "print()\n",
    "print('Option 3: Find a fundamentally different approach')\n",
    "print('  The GNN benchmark achieved MSE 0.0039')\n",
    "print('  This is 22x better than our best LB')\n",
    "print('  There IS a path to much better performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341923a",
   "metadata": {},
   "source": [
    "## Key Insight: The Validation Scheme Hypothesis\n",
    "\n",
    "The evaluator has been recommending testing GroupKFold for several loops. Let me analyze why this could help:\n",
    "\n",
    "### Leave-One-Out (24 folds) vs GroupKFold (5 folds)\n",
    "\n",
    "**Leave-One-Out (24 folds):**\n",
    "- Each fold holds out ONE solvent (all samples for that solvent)\n",
    "- 23 solvents in training, 1 in test\n",
    "- Very pessimistic: model must predict completely unseen solvent\n",
    "- High variance: some solvents are easy, some are hard\n",
    "\n",
    "**GroupKFold (5 folds):**\n",
    "- Each fold holds out ~5 solvents (all samples for those solvents)\n",
    "- ~19 solvents in training, ~5 in test\n",
    "- Less pessimistic: model sees more diversity in test set\n",
    "- Lower variance: averaging over multiple solvents per fold\n",
    "\n",
    "### Why GroupKFold Might Help\n",
    "\n",
    "1. **Better CV-LB correlation**: If LB uses a similar scheme, our CV would better predict LB\n",
    "2. **Different model selection**: Models that perform best under GroupKFold may be different\n",
    "3. **More stable CV**: Averaging over 5 solvents per fold reduces variance\n",
    "\n",
    "### BUT: The Competition Rules\n",
    "\n",
    "The rules say we can only change the model definition line. We CANNOT overwrite the utility functions like mixall does. So even if GroupKFold is better, we can't use it in our submission.\n",
    "\n",
    "**HOWEVER:** We CAN use GroupKFold for LOCAL model selection, then submit the best model using the original Leave-One-Out scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a00742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the experiment history\n",
    "print('=== EXPERIMENT HISTORY ANALYSIS ===')\n",
    "print()\n",
    "print('Last 17 experiments (exp_030 to exp_047) have ALL been worse than exp_030:')\n",
    "print()\n",
    "\n",
    "experiments = [\n",
    "    ('exp_030', 0.008298, 'GP + MLP + LGBM ensemble (BEST)'),\n",
    "    ('exp_031', 0.008504, 'Higher GP weight'),\n",
    "    ('exp_032', 0.039713, 'Pure GP'),\n",
    "    ('exp_033', 0.010286, 'Ridge regression'),\n",
    "    ('exp_034', 0.010286, 'Kernel Ridge'),\n",
    "    ('exp_035', 0.009800, 'Lower GP weight'),\n",
    "    ('exp_036', 0.008680, 'No GP'),\n",
    "    ('exp_037', 0.026541, 'Similarity weighting'),\n",
    "    ('exp_038', 0.009948, 'Minimal features'),\n",
    "    ('exp_039', 0.080438, 'Learned embeddings'),\n",
    "    ('exp_040', 0.068767, 'GNN (AttentiveFP)'),\n",
    "    ('exp_041', 0.010288, 'ChemBERTa'),\n",
    "    ('exp_042', 0.010008, 'Calibration'),\n",
    "    ('exp_043', 0.008700, 'Non-linear mixture'),\n",
    "    ('exp_044', 0.008700, 'Hybrid model'),\n",
    "    ('exp_045', 0.008840, 'Mean reversion'),\n",
    "    ('exp_046', 0.008600, 'Adaptive weighting'),\n",
    "    ('exp_047', 0.009393, 'Diverse ensemble'),\n",
    "]\n",
    "\n",
    "for exp, cv, desc in experiments:\n",
    "    diff = (cv - 0.008298) / 0.008298 * 100\n",
    "    status = '✓ BEST' if exp == 'exp_030' else f'{diff:+.1f}%'\n",
    "    print(f'{exp}: CV={cv:.6f} ({status}) - {desc}')\n",
    "\n",
    "print()\n",
    "print('CONCLUSION: We are in a LOCAL OPTIMUM.')\n",
    "print('Incremental changes are not working.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches haven't been tried?\n",
    "print('=== UNEXPLORED APPROACHES ===')\n",
    "print()\n",
    "print('1. PROPER GNN IMPLEMENTATION')\n",
    "print('   - exp_040 was a quick test on single fold')\n",
    "print('   - Need proper hyperparameter tuning')\n",
    "print('   - Need full CV evaluation')\n",
    "print('   - The GNN benchmark achieved MSE 0.0039 - 22x better!')\n",
    "print()\n",
    "print('2. TRANSFER LEARNING / PRE-TRAINING')\n",
    "print('   - Pre-train on mixture data, fine-tune on single solvents')\n",
    "print('   - Use auxiliary tasks to improve representations')\n",
    "print('   - Competition rules allow different hyperparameters for different tasks')\n",
    "print()\n",
    "print('3. SOLVENT CLUSTERING + SPECIALIZED MODELS')\n",
    "print('   - Cluster solvents by Spange descriptors')\n",
    "print('   - Train specialized models for each cluster')\n",
    "print('   - For outliers (Cyclohexane, HFIP), use nearest neighbor approach')\n",
    "print()\n",
    "print('4. ENSEMBLE OF DIVERSE MODELS WITH DIFFERENT FEATURES')\n",
    "print('   - exp_047 used simpler features (18 vs 2066)')\n",
    "print('   - It reduced error on Cyclohexane from 0.198 to 0.014!')\n",
    "print('   - But overall CV was worse')\n",
    "print('   - What if we ensemble models with different feature sets?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597561a6",
   "metadata": {},
   "source": [
    "## Strategic Recommendation\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "The CV-LB relationship has an intercept (0.0525) that is HIGHER than the target (0.0347). This means:\n",
    "- Even with CV = 0, we'd get LB = 0.0525\n",
    "- The target is mathematically UNREACHABLE by improving CV alone\n",
    "- We need to CHANGE the relationship, not just improve CV\n",
    "\n",
    "### What Could Change the Relationship?\n",
    "\n",
    "1. **Different model architecture** - GNN could have a different CV-LB relationship\n",
    "2. **Different features** - Simpler features might generalize better to LB\n",
    "3. **Ensemble of diverse models** - Combining models with different strengths\n",
    "\n",
    "### The Diverse Ensemble Insight\n",
    "\n",
    "Experiment 047 showed that:\n",
    "- Simpler features (18 vs 2066) reduced Cyclohexane error from 0.198 to 0.014\n",
    "- But overall CV was 13.2% worse\n",
    "- This suggests DRFP features help most solvents but hurt outliers\n",
    "\n",
    "**What if we ensemble:**\n",
    "1. Model A: Full features (Spange + DRFP) - good for most solvents\n",
    "2. Model B: Simple features (Spange only) - good for outliers\n",
    "3. Weighted by solvent similarity to training set?\n",
    "\n",
    "### Recommended Next Experiment\n",
    "\n",
    "**Hybrid Feature Ensemble:**\n",
    "- Train two models: one with full features, one with simple features\n",
    "- For each test solvent, compute similarity to training solvents\n",
    "- If similar (in-distribution): use full-feature model\n",
    "- If dissimilar (out-of-distribution): use simple-feature model\n",
    "- This could reduce error on outliers without hurting in-distribution solvents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key findings\n",
    "print('=== SUMMARY OF KEY FINDINGS ===')\n",
    "print()\n",
    "print('1. CV-LB RELATIONSHIP:')\n",
    "print('   - LB = 4.31 * CV + 0.0525 (R² = 0.95)')\n",
    "print('   - Intercept (0.0525) > Target (0.0347)')\n",
    "print('   - Current approach CANNOT reach target')\n",
    "print()\n",
    "print('2. BEST MODEL:')\n",
    "print('   - exp_030: GP 0.15 + MLP 0.55 + LGBM 0.3')\n",
    "print('   - CV = 0.008298, LB = 0.0877')\n",
    "print('   - 17 subsequent experiments all worse')\n",
    "print()\n",
    "print('3. KEY INSIGHTS:')\n",
    "print('   - Simpler features reduce error on outliers (Cyclohexane: 0.198 -> 0.014)')\n",
    "print('   - But simpler features hurt overall CV')\n",
    "print('   - Need to combine strengths of both approaches')\n",
    "print()\n",
    "print('4. REMAINING SUBMISSIONS: 5')\n",
    "print('   - Need to be strategic')\n",
    "print('   - Should test fundamentally different approaches')\n",
    "print()\n",
    "print('5. RECOMMENDED APPROACH:')\n",
    "print('   - Hybrid Feature Ensemble: full features for in-distribution, simple for OOD')\n",
    "print('   - OR: Proper GNN implementation (benchmark achieved 0.0039)')\n",
    "print('   - OR: Solvent clustering + specialized models')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
