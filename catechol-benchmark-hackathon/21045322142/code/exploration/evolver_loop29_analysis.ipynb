{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce133d7",
   "metadata": {},
   "source": [
    "# Loop 29 Analysis: Critical Assessment After 28 Experiments\n",
    "\n",
    "**Current State:**\n",
    "- Best CV: 0.008465 (exp_026)\n",
    "- Best LB: 0.0887 (exp_026)\n",
    "- Target: 0.01727\n",
    "- CV-LB ratio: ~10.5x\n",
    "- Linear fit: LB = 4.22*CV + 0.0533 (R²=0.96)\n",
    "- Submissions remaining: 3\n",
    "\n",
    "**Latest Experiment (exp_028):**\n",
    "- Four-model ensemble (MLP+LGBM+XGB+CatBoost)\n",
    "- CV 0.008674 (2.47% WORSE than exp_026)\n",
    "- Adding more tree models did NOT help\n",
    "\n",
    "**Critical Question:**\n",
    "What approaches remain unexplored that could break the CV-LB pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6318a105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:45.978905Z",
     "iopub.status.busy": "2026-01-14T08:59:45.978369Z",
     "iopub.status.idle": "2026-01-14T08:59:47.146086Z",
     "shell.execute_reply": "2026-01-14T08:59:47.145487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All submissions:\n",
      "     id       cv      lb\n",
      "exp_000 0.011081 0.09816\n",
      "exp_001 0.012297 0.10649\n",
      "exp_003 0.010501 0.09719\n",
      "exp_005 0.010430 0.09691\n",
      "exp_006 0.009749 0.09457\n",
      "exp_007 0.009262 0.09316\n",
      "exp_009 0.009192 0.09364\n",
      "exp_012 0.009004 0.09134\n",
      "exp_024 0.008689 0.08929\n",
      "exp_026 0.008465 0.08875\n",
      "\n",
      "Linear fit: LB = 4.2168 * CV + 0.05334\n",
      "R² = 0.9618\n",
      "\n",
      "Target: 0.01727\n",
      "Best LB: 0.08875\n",
      "Gap to target: 5.14x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# All 10 submissions with CV and LB scores\n",
    "submissions = [\n",
    "    {'id': 'exp_000', 'cv': 0.011081, 'lb': 0.09816},\n",
    "    {'id': 'exp_001', 'cv': 0.012297, 'lb': 0.10649},\n",
    "    {'id': 'exp_003', 'cv': 0.010501, 'lb': 0.09719},\n",
    "    {'id': 'exp_005', 'cv': 0.01043, 'lb': 0.09691},\n",
    "    {'id': 'exp_006', 'cv': 0.009749, 'lb': 0.09457},\n",
    "    {'id': 'exp_007', 'cv': 0.009262, 'lb': 0.09316},\n",
    "    {'id': 'exp_009', 'cv': 0.009192, 'lb': 0.09364},\n",
    "    {'id': 'exp_012', 'cv': 0.009004, 'lb': 0.09134},\n",
    "    {'id': 'exp_024', 'cv': 0.008689, 'lb': 0.08929},\n",
    "    {'id': 'exp_026', 'cv': 0.008465, 'lb': 0.08875},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('All submissions:')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Linear fit\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "print(f'\\nLinear fit: LB = {slope:.4f} * CV + {intercept:.5f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nTarget: 0.01727')\n",
    "print(f'Best LB: {df[\"lb\"].min():.5f}')\n",
    "print(f'Gap to target: {df[\"lb\"].min() / 0.01727:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598974c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.148290Z",
     "iopub.status.busy": "2026-01-14T08:59:47.147989Z",
     "iopub.status.idle": "2026-01-14T08:59:47.152387Z",
     "shell.execute_reply": "2026-01-14T08:59:47.151893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target Analysis ===\n",
      "Target LB: 0.01727\n",
      "Required CV (from linear fit): -0.008555\n",
      "This is NEGATIVE - impossible with current approach!\n",
      "\n",
      "The intercept (0.05334) is 3.09x higher than target.\n",
      "Even with CV=0, predicted LB would be 0.05334\n"
     ]
    }
   ],
   "source": [
    "# What CV would we need to reach the target?\n",
    "target = 0.01727\n",
    "required_cv = (target - intercept) / slope\n",
    "print(f'=== Target Analysis ===')\n",
    "print(f'Target LB: {target}')\n",
    "print(f'Required CV (from linear fit): {required_cv:.6f}')\n",
    "print(f'This is NEGATIVE - impossible with current approach!')\n",
    "print(f'\\nThe intercept ({intercept:.5f}) is {intercept/target:.2f}x higher than target.')\n",
    "print(f'Even with CV=0, predicted LB would be {intercept:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d507f921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.154089Z",
     "iopub.status.busy": "2026-01-14T08:59:47.153906Z",
     "iopub.status.idle": "2026-01-14T08:59:47.160811Z",
     "shell.execute_reply": "2026-01-14T08:59:47.160294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiments Tried (28 total) ===\n",
      "exp_000: MLP [128,128,64], Spange only, HuberLoss, 3 models\n",
      "exp_001: LightGBM, Spange only\n",
      "exp_002: DRFP with PCA (100 components) - FAILED\n",
      "exp_003: MLP [256,128,64], Spange+DRFP, HuberLoss, 5 models\n",
      "exp_004: Deep Residual MLP - FAILED\n",
      "exp_005: MLP [256,128,64], Spange+DRFP, 15 models\n",
      "exp_006: MLP [64,32], Spange+DRFP, 5 models\n",
      "exp_007: MLP [32,16], Spange+DRFP, 5 models\n",
      "exp_008: MLP [16], single layer\n",
      "exp_009: Ridge Regression\n",
      "exp_010: MLP [16], single layer\n",
      "exp_011: Diverse ensemble\n",
      "exp_012: MLP [32,16] + LightGBM ensemble\n",
      "exp_013: Compliant ensemble\n",
      "exp_014: Ensemble weights tuning\n",
      "exp_015: Three-model ensemble\n",
      "exp_016: Final summary\n",
      "exp_017: Attention model\n",
      "exp_018: Fragprints features\n",
      "exp_019: ACS PCA features\n",
      "exp_023: ACS PCA compliant\n",
      "exp_024: ACS PCA fixed\n",
      "exp_025: Per-target models - FAILED\n",
      "exp_026: Weighted loss [1,1,2] - BEST\n",
      "exp_027: Simple features (23) - FAILED\n",
      "exp_028: Four-model ensemble (MLP+LGBM+XGB+CatBoost) - FAILED\n"
     ]
    }
   ],
   "source": [
    "# What experiments have been tried?\n",
    "experiments_tried = [\n",
    "    ('exp_000', 'MLP [128,128,64], Spange only, HuberLoss, 3 models'),\n",
    "    ('exp_001', 'LightGBM, Spange only'),\n",
    "    ('exp_002', 'DRFP with PCA (100 components) - FAILED'),\n",
    "    ('exp_003', 'MLP [256,128,64], Spange+DRFP, HuberLoss, 5 models'),\n",
    "    ('exp_004', 'Deep Residual MLP - FAILED'),\n",
    "    ('exp_005', 'MLP [256,128,64], Spange+DRFP, 15 models'),\n",
    "    ('exp_006', 'MLP [64,32], Spange+DRFP, 5 models'),\n",
    "    ('exp_007', 'MLP [32,16], Spange+DRFP, 5 models'),\n",
    "    ('exp_008', 'MLP [16], single layer'),\n",
    "    ('exp_009', 'Ridge Regression'),\n",
    "    ('exp_010', 'MLP [16], single layer'),\n",
    "    ('exp_011', 'Diverse ensemble'),\n",
    "    ('exp_012', 'MLP [32,16] + LightGBM ensemble'),\n",
    "    ('exp_013', 'Compliant ensemble'),\n",
    "    ('exp_014', 'Ensemble weights tuning'),\n",
    "    ('exp_015', 'Three-model ensemble'),\n",
    "    ('exp_016', 'Final summary'),\n",
    "    ('exp_017', 'Attention model'),\n",
    "    ('exp_018', 'Fragprints features'),\n",
    "    ('exp_019', 'ACS PCA features'),\n",
    "    ('exp_023', 'ACS PCA compliant'),\n",
    "    ('exp_024', 'ACS PCA fixed'),\n",
    "    ('exp_025', 'Per-target models - FAILED'),\n",
    "    ('exp_026', 'Weighted loss [1,1,2] - BEST'),\n",
    "    ('exp_027', 'Simple features (23) - FAILED'),\n",
    "    ('exp_028', 'Four-model ensemble (MLP+LGBM+XGB+CatBoost) - FAILED'),\n",
    "]\n",
    "\n",
    "print('=== Experiments Tried (28 total) ===')\n",
    "for exp_id, desc in experiments_tried:\n",
    "    print(f'{exp_id}: {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70eb126f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.162517Z",
     "iopub.status.busy": "2026-01-14T08:59:47.162339Z",
     "iopub.status.idle": "2026-01-14T08:59:47.167848Z",
     "shell.execute_reply": "2026-01-14T08:59:47.167354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNEXPLORED Approaches ===\n",
      "\n",
      "Gaussian Process Regression:\n",
      "  Rationale: Competition description mentions GP for imputation\n",
      "\n",
      "Physics constraint (SM+P2+P3≈1):\n",
      "  Rationale: Mass balance constraint for regularization\n",
      "\n",
      "Higher SM weights [1,1,3] or [1,1,4]:\n",
      "  Rationale: SM is still the bottleneck\n",
      "\n",
      "Stacking meta-learner:\n",
      "  Rationale: Train a meta-model on base predictions\n",
      "\n",
      "Learned loss weights (homoscedastic):\n",
      "  Rationale: Kendall et al. uncertainty weighting\n",
      "\n",
      "Post-processing normalization:\n",
      "  Rationale: Normalize predictions to sum to 1\n",
      "\n",
      "Domain adaptation:\n",
      "  Rationale: Handle distribution shift explicitly\n",
      "\n",
      "Adversarial validation:\n",
      "  Rationale: Identify features causing distribution shift\n"
     ]
    }
   ],
   "source": [
    "# What approaches have NOT been tried?\n",
    "print('=== UNEXPLORED Approaches ===')\n",
    "unexplored = [\n",
    "    ('Gaussian Process Regression', 'Competition description mentions GP for imputation'),\n",
    "    ('Physics constraint (SM+P2+P3≈1)', 'Mass balance constraint for regularization'),\n",
    "    ('Higher SM weights [1,1,3] or [1,1,4]', 'SM is still the bottleneck'),\n",
    "    ('Stacking meta-learner', 'Train a meta-model on base predictions'),\n",
    "    ('Learned loss weights (homoscedastic)', 'Kendall et al. uncertainty weighting'),\n",
    "    ('Post-processing normalization', 'Normalize predictions to sum to 1'),\n",
    "    ('Domain adaptation', 'Handle distribution shift explicitly'),\n",
    "    ('Adversarial validation', 'Identify features causing distribution shift'),\n",
    "]\n",
    "\n",
    "for approach, rationale in unexplored:\n",
    "    print(f'\\n{approach}:')\n",
    "    print(f'  Rationale: {rationale}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d029e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.169492Z",
     "iopub.status.busy": "2026-01-14T08:59:47.169317Z",
     "iopub.status.idle": "2026-01-14T08:59:47.174504Z",
     "shell.execute_reply": "2026-01-14T08:59:47.173926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Key Insight from Kaggle Kernels ===\n",
      "\n",
      "\"mixall\" kernel uses GroupKFold(n_splits=5) instead of Leave-One-Out!\n",
      "This is a DIFFERENT CV scheme than what we use.\n",
      "\n",
      "Our CV scheme:\n",
      "- Single solvents: Leave-one-solvent-out (24 folds)\n",
      "- Mixtures: Leave-one-ramp-out (13 folds)\n",
      "- Total: 37 folds\n",
      "\n",
      "Possible LB CV scheme:\n",
      "- GroupKFold (5 folds)\n",
      "- Different random seed\n",
      "- Different data ordering\n",
      "\n",
      "This could explain the CV-LB gap!\n"
     ]
    }
   ],
   "source": [
    "# Key insight from \"mixall\" kernel\n",
    "print('=== Key Insight from Kaggle Kernels ===')\n",
    "print('\\n\"mixall\" kernel uses GroupKFold(n_splits=5) instead of Leave-One-Out!')\n",
    "print('This is a DIFFERENT CV scheme than what we use.')\n",
    "print('\\nOur CV scheme:')\n",
    "print('- Single solvents: Leave-one-solvent-out (24 folds)')\n",
    "print('- Mixtures: Leave-one-ramp-out (13 folds)')\n",
    "print('- Total: 37 folds')\n",
    "print('\\nPossible LB CV scheme:')\n",
    "print('- GroupKFold (5 folds)')\n",
    "print('- Different random seed')\n",
    "print('- Different data ordering')\n",
    "print('\\nThis could explain the CV-LB gap!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22e0ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.176124Z",
     "iopub.status.busy": "2026-01-14T08:59:47.175946Z",
     "iopub.status.idle": "2026-01-14T08:59:47.180796Z",
     "shell.execute_reply": "2026-01-14T08:59:47.180260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post-Processing Normalization ===\n",
      "\n",
      "From \"mr0106/catechol\" kernel:\n",
      "```python\n",
      "# Post-processing: Chemical constraints (Clip and Normalize)\n",
      "# Ensure outputs are between 0 and 1\n",
      "preds = np.clip(preds, 0, 1)\n",
      "\n",
      "# Normalize rows so the sum of products equals 1 (or 100%)\n",
      "row_sums = preds.sum(axis=1)[:, np.newaxis]\n",
      "row_sums[row_sums == 0] = 1 # Avoid division by zero\n",
      "preds = preds / row_sums\n",
      "```\n",
      "\n",
      "This enforces the physical constraint that SM + P2 + P3 = 1!\n",
      "We have NOT tried this post-processing step.\n"
     ]
    }
   ],
   "source": [
    "# Another key insight: Post-processing normalization\n",
    "print('=== Post-Processing Normalization ===')\n",
    "print('\\nFrom \"mr0106/catechol\" kernel:')\n",
    "print('```python')\n",
    "print('# Post-processing: Chemical constraints (Clip and Normalize)')\n",
    "print('# Ensure outputs are between 0 and 1')\n",
    "print('preds = np.clip(preds, 0, 1)')\n",
    "print('')\n",
    "print('# Normalize rows so the sum of products equals 1 (or 100%)')\n",
    "print('row_sums = preds.sum(axis=1)[:, np.newaxis]')\n",
    "print('row_sums[row_sums == 0] = 1 # Avoid division by zero')\n",
    "print('preds = preds / row_sums')\n",
    "print('```')\n",
    "print('\\nThis enforces the physical constraint that SM + P2 + P3 = 1!')\n",
    "print('We have NOT tried this post-processing step.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5d63d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.182420Z",
     "iopub.status.busy": "2026-01-14T08:59:47.182234Z",
     "iopub.status.idle": "2026-01-14T08:59:47.187487Z",
     "shell.execute_reply": "2026-01-14T08:59:47.186950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRIORITY RANKING ===\n",
      "\n",
      "1. HIGHEST PRIORITY: Post-Processing Normalization\n",
      "   - Enforce SM + P2 + P3 = 1 constraint\n",
      "   - Simple to implement, no retraining needed\n",
      "   - Used by other competitors\n",
      "   - Physical constraint for regularization\n",
      "\n",
      "2. HIGH PRIORITY: Higher SM Weights [1,1,3]\n",
      "   - SM is still the hardest target\n",
      "   - Weighted loss [1,1,2] improved all targets by 2.58%\n",
      "   - More aggressive weighting may help further\n",
      "\n",
      "3. MEDIUM PRIORITY: Gaussian Process Regression\n",
      "   - Competition description mentions GP\n",
      "   - Different model type with uncertainty quantification\n",
      "   - May have different generalization properties\n",
      "\n",
      "4. LOWER PRIORITY: Stacking Meta-Learner\n",
      "   - Train a simple model on base predictions\n",
      "   - Can learn optimal combination weights\n",
      "   - May improve generalization\n"
     ]
    }
   ],
   "source": [
    "# Priority ranking for next experiments\n",
    "print('=== PRIORITY RANKING ===')\n",
    "print('\\n1. HIGHEST PRIORITY: Post-Processing Normalization')\n",
    "print('   - Enforce SM + P2 + P3 = 1 constraint')\n",
    "print('   - Simple to implement, no retraining needed')\n",
    "print('   - Used by other competitors')\n",
    "print('   - Physical constraint for regularization')\n",
    "\n",
    "print('\\n2. HIGH PRIORITY: Higher SM Weights [1,1,3]')\n",
    "print('   - SM is still the hardest target')\n",
    "print('   - Weighted loss [1,1,2] improved all targets by 2.58%')\n",
    "print('   - More aggressive weighting may help further')\n",
    "\n",
    "print('\\n3. MEDIUM PRIORITY: Gaussian Process Regression')\n",
    "print('   - Competition description mentions GP')\n",
    "print('   - Different model type with uncertainty quantification')\n",
    "print('   - May have different generalization properties')\n",
    "\n",
    "print('\\n4. LOWER PRIORITY: Stacking Meta-Learner')\n",
    "print('   - Train a simple model on base predictions')\n",
    "print('   - Can learn optimal combination weights')\n",
    "print('   - May improve generalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b855c185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T08:59:47.189098Z",
     "iopub.status.busy": "2026-01-14T08:59:47.188911Z",
     "iopub.status.idle": "2026-01-14T08:59:47.194003Z",
     "shell.execute_reply": "2026-01-14T08:59:47.193387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RECOMMENDATION ===\n",
      "\n",
      "Given:\n",
      "- 3 submissions remaining\n",
      "- CV-LB gap is ~10x (structural, not model-specific)\n",
      "- Target 0.01727 is 5x better than best LB 0.0887\n",
      "- exp_028 (four-model ensemble) was WORSE than exp_026\n",
      "\n",
      "Strategy:\n",
      "1. Try post-processing normalization (SM+P2+P3=1)\n",
      "2. Try higher SM weights [1,1,3]\n",
      "3. Combine both: weighted loss + normalization\n",
      "\n",
      "Key insight:\n",
      "The CV-LB gap is the fundamental problem.\n",
      "We need approaches that GENERALIZE better, not just improve CV.\n",
      "Post-processing normalization is a physics-based constraint that may help.\n",
      "\n",
      "DO NOT SUBMIT exp_028 - it is worse than exp_026.\n"
     ]
    }
   ],
   "source": [
    "# Final recommendation\n",
    "print('=== FINAL RECOMMENDATION ===')\n",
    "print('\\nGiven:')\n",
    "print('- 3 submissions remaining')\n",
    "print('- CV-LB gap is ~10x (structural, not model-specific)')\n",
    "print('- Target 0.01727 is 5x better than best LB 0.0887')\n",
    "print('- exp_028 (four-model ensemble) was WORSE than exp_026')\n",
    "\n",
    "print('\\nStrategy:')\n",
    "print('1. Try post-processing normalization (SM+P2+P3=1)')\n",
    "print('2. Try higher SM weights [1,1,3]')\n",
    "print('3. Combine both: weighted loss + normalization')\n",
    "\n",
    "print('\\nKey insight:')\n",
    "print('The CV-LB gap is the fundamental problem.')\n",
    "print('We need approaches that GENERALIZE better, not just improve CV.')\n",
    "print('Post-processing normalization is a physics-based constraint that may help.')\n",
    "print('\\nDO NOT SUBMIT exp_028 - it is worse than exp_026.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
