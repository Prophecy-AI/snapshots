{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc4faed",
   "metadata": {},
   "source": [
    "# Loop 51 Strategic Analysis\n",
    "\n",
    "## Key Questions:\n",
    "1. Why have 20 consecutive experiments failed to beat exp_030?\n",
    "2. What fundamentally different approaches remain untried?\n",
    "3. How can we change the CV-LB relationship?\n",
    "4. What is the path to the target (0.0347)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6eaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission history analysis\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.011081, 'lb': 0.09816},\n",
    "    {'exp': 'exp_001', 'cv': 0.012297, 'lb': 0.10649},\n",
    "    {'exp': 'exp_003', 'cv': 0.010501, 'lb': 0.09719},\n",
    "    {'exp': 'exp_005', 'cv': 0.01043, 'lb': 0.09691},\n",
    "    {'exp': 'exp_006', 'cv': 0.009749, 'lb': 0.09457},\n",
    "    {'exp': 'exp_007', 'cv': 0.009262, 'lb': 0.09316},\n",
    "    {'exp': 'exp_009', 'cv': 0.009192, 'lb': 0.09364},\n",
    "    {'exp': 'exp_012', 'cv': 0.009004, 'lb': 0.09134},\n",
    "    {'exp': 'exp_024', 'cv': 0.008689, 'lb': 0.08929},\n",
    "    {'exp': 'exp_026', 'cv': 0.008465, 'lb': 0.08875},\n",
    "    {'exp': 'exp_030', 'cv': 0.008298, 'lb': 0.08772},\n",
    "    {'exp': 'exp_035', 'cv': 0.009825, 'lb': 0.09696},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('=== Submission History ===')\n",
    "print(df)\n",
    "\n",
    "# Linear regression on CV-LB relationship\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'\\n=== CV-LB Relationship ===')\n",
    "print(f'LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nTarget LB: 0.0347')\n",
    "print(f'Required CV to hit target: {(0.0347 - intercept) / slope:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals (how much better/worse than predicted)\n",
    "df['predicted_lb'] = slope * df['cv'] + intercept\n",
    "df['residual'] = df['lb'] - df['predicted_lb']\n",
    "df['residual_pct'] = df['residual'] / df['predicted_lb'] * 100\n",
    "\n",
    "print('=== Residuals (LB - Predicted LB) ===')\n",
    "print('Negative = better than predicted, Positive = worse than predicted')\n",
    "print(df[['exp', 'cv', 'lb', 'predicted_lb', 'residual', 'residual_pct']].to_string())\n",
    "\n",
    "print(f'\\nBest residual: {df.loc[df[\"residual\"].idxmin(), \"exp\"]} with {df[\"residual\"].min():.6f}')\n",
    "print(f'Worst residual: {df.loc[df[\"residual\"].idxmax(), \"exp\"]} with {df[\"residual\"].max():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55588a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: CV vs LB with regression line\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(df['cv'], df['lb'], c='blue', s=100, label='Submissions')\n",
    "ax1.plot([0, 0.015], [intercept, slope*0.015 + intercept], 'r--', label=f'LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "ax1.axhline(y=0.0347, color='green', linestyle=':', label='Target LB (0.0347)')\n",
    "ax1.set_xlabel('CV Score')\n",
    "ax1.set_ylabel('LB Score')\n",
    "ax1.set_title('CV vs LB Relationship')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if r < 0 else 'red' for r in df['residual']]\n",
    "ax2.barh(df['exp'], df['residual'], color=colors)\n",
    "ax2.axvline(x=0, color='black', linestyle='-')\n",
    "ax2.set_xlabel('Residual (LB - Predicted LB)')\n",
    "ax2.set_title('Residuals: Green = Better than predicted')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_analysis.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('\\nSaved to /home/code/exploration/cv_lb_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the gap to target\n",
    "print('=== Gap Analysis ===')\n",
    "print(f'Best LB achieved: {df[\"lb\"].min():.4f} (exp_030)')\n",
    "print(f'Target LB: 0.0347')\n",
    "print(f'Gap: {df[\"lb\"].min() - 0.0347:.4f} ({(df[\"lb\"].min() - 0.0347) / 0.0347 * 100:.1f}% above target)')\n",
    "\n",
    "print(f'\\n=== What would it take? ===')\n",
    "print(f'Current CV-LB relationship: LB = {slope:.4f} * CV + {intercept:.4f}')\n",
    "print(f'Intercept ({intercept:.4f}) > Target ({0.0347}) by {intercept - 0.0347:.4f}')\n",
    "print(f'\\nThis means: Even with CV = 0, predicted LB = {intercept:.4f} > 0.0347')\n",
    "print(f'\\nTo reach target, we need to CHANGE the relationship, not just improve CV!')\n",
    "\n",
    "# What if we could reduce the intercept?\n",
    "print(f'\\n=== Hypothetical: What if intercept was lower? ===')\n",
    "for new_intercept in [0.03, 0.02, 0.01, 0.0]:\n",
    "    required_cv = (0.0347 - new_intercept) / slope\n",
    "    print(f'  Intercept = {new_intercept:.2f}: Required CV = {required_cv:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches have been tried?\n",
    "print('=== Approaches Tried (50 experiments) ===')\n",
    "approaches = {\n",
    "    'MLP architectures': ['exp_000', 'exp_004', 'exp_005', 'exp_006', 'exp_007', 'exp_008', 'exp_009', 'exp_010'],\n",
    "    'LightGBM': ['exp_001'],\n",
    "    'Feature engineering': ['exp_002', 'exp_003', 'exp_018', 'exp_019', 'exp_023', 'exp_024', 'exp_027', 'exp_038'],\n",
    "    'Ensembles': ['exp_011', 'exp_012', 'exp_013', 'exp_014', 'exp_015', 'exp_028', 'exp_030', 'exp_047'],\n",
    "    'GP-based': ['exp_030', 'exp_031', 'exp_032', 'exp_033', 'exp_034', 'exp_035', 'exp_036'],\n",
    "    'GNN/Transformers': ['exp_040', 'exp_041'],\n",
    "    'Domain adaptation': ['exp_049', 'exp_050'],\n",
    "    'Calibration/Post-processing': ['exp_042', 'exp_045', 'exp_046'],\n",
    "}\n",
    "\n",
    "for approach, exps in approaches.items():\n",
    "    print(f'\\n{approach}: {len(exps)} experiments')\n",
    "    print(f'  {exps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What HASN'T been tried?\n",
    "print('=== Approaches NOT Tried or Insufficiently Explored ===')\n",
    "\n",
    "untried = [\n",
    "    ('Proper GNN with full CV', 'exp_040 was only 50 epochs on one fold'),\n",
    "    ('Transfer learning', 'Pre-train on related chemistry datasets'),\n",
    "    ('Meta-learning', 'Learn to learn from few examples per solvent'),\n",
    "    ('Adversarial training', 'Train to be robust to solvent perturbations'),\n",
    "    ('Conformal prediction', 'Calibrated uncertainty for OOD detection'),\n",
    "    ('Kernel methods with custom kernels', 'Chemical similarity kernels'),\n",
    "    ('Bayesian optimization of model selection', 'Not just hyperparameters'),\n",
    "]\n",
    "\n",
    "for approach, note in untried:\n",
    "    print(f'\\n• {approach}')\n",
    "    print(f'  Note: {note}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5add72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The mixall kernel uses GroupKFold(5) instead of Leave-One-Out(24)\n",
    "print('=== CRITICAL INSIGHT: Mixall Kernel Validation Scheme ===')\n",
    "print()\n",
    "print('The mixall kernel OVERWRITES the utility functions:')\n",
    "print('  - generate_leave_one_out_splits: GroupKFold(5) instead of Leave-One-Out(24)')\n",
    "print('  - generate_leave_one_ramp_out_splits: GroupKFold(5) instead of Leave-One-Ramp-Out(13)')\n",
    "print()\n",
    "print('This means:')\n",
    "print('  1. Their local CV is NOT comparable to ours')\n",
    "print('  2. They train on ~80% of solvents per fold, we train on ~96%')\n",
    "print('  3. Their CV is HARDER (more solvents held out), so lower CV is expected')\n",
    "print('  4. BUT the LB evaluation uses the OFFICIAL scheme')\n",
    "print()\n",
    "print('Implication:')\n",
    "print('  - The mixall kernel\\'s success on LB is due to its MODEL, not its validation')\n",
    "print('  - We should focus on what makes their model good, not their CV scheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What makes the mixall model good?\n",
    "print('=== Mixall Model Analysis ===')\n",
    "print()\n",
    "print('Model components:')\n",
    "print('  1. Spange features only (no DRFP)')\n",
    "print('  2. MLP + XGBoost + RF + LightGBM ensemble')\n",
    "print('  3. Optuna hyperparameter tuning per fold')\n",
    "print('  4. Weighted ensemble (learned weights)')\n",
    "print()\n",
    "print('Key differences from our best (exp_030):')\n",
    "print('  - exp_030: GP + MLP + LGBM with Spange + DRFP')\n",
    "print('  - mixall: MLP + XGB + RF + LGBM with Spange only')\n",
    "print()\n",
    "print('Hypothesis: The mixall model may generalize better because:')\n",
    "print('  1. No DRFP (simpler features)')\n",
    "print('  2. More diverse ensemble (4 models vs 3)')\n",
    "print('  3. Optuna tuning per fold (adaptive)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77794e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final strategic recommendations\n",
    "print('=== STRATEGIC RECOMMENDATIONS ===')\n",
    "print()\n",
    "print('PRIORITY 1: Try mixall-style ensemble WITHOUT DRFP')\n",
    "print('  - MLP + XGBoost + RF + LightGBM')\n",
    "print('  - Spange features only')\n",
    "print('  - This is fundamentally different from our current approach')\n",
    "print()\n",
    "print('PRIORITY 2: Proper GNN implementation')\n",
    "print('  - The benchmark achieved MSE 0.0039 with GNN')\n",
    "print('  - Our attempt (exp_040) was too quick (50 epochs, one fold)')\n",
    "print('  - Need full CV, more epochs, proper tuning')\n",
    "print()\n",
    "print('PRIORITY 3: Ensemble models with different CV-LB characteristics')\n",
    "print('  - exp_000 has the best residual (-0.002213)')\n",
    "print('  - Combine models that beat their predicted LB')\n",
    "print('  - Weight by residual, not CV')\n",
    "print()\n",
    "print('PRIORITY 4: Adversarial validation')\n",
    "print('  - Identify features that distinguish train from test')\n",
    "print('  - Remove or downweight these features')\n",
    "print('  - This could change the CV-LB relationship')\n",
    "print()\n",
    "print('DO NOT:')\n",
    "print('  - Continue refining GP + MLP + LGBM (20 failures)')\n",
    "print('  - Try more domain adaptation (LISA, REx failed)')\n",
    "print('  - Assume CV improvement will translate to LB improvement')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
