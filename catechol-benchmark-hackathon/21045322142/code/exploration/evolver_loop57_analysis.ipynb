{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ce1a8d",
   "metadata": {},
   "source": [
    "# Loop 57 Analysis: Post-XGBoost+RF Ensemble Assessment\n",
    "\n",
    "**Situation:**\n",
    "- 57 experiments completed, 26 consecutive failures since exp_030\n",
    "- Best LB: 0.0877 (exp_030), Target: 0.0347\n",
    "- Gap: 2.53x (0.0877 / 0.0347)\n",
    "- 5 submissions remaining\n",
    "- exp_056 (XGBoost + RandomForest Ensemble) FAILED - CV 0.014233 (71.5% worse)\n",
    "\n",
    "**Key Finding from exp_056:**\n",
    "XGBoost and RandomForest don't help - they actually hurt performance. The GP + MLP + LGBM ensemble from exp_030 remains the best approach.\n",
    "\n",
    "**Critical Evaluator Insight:**\n",
    "The 'mixall' kernel OVERWRITES the CV functions to use GroupKFold(5) instead of Leave-One-Out(24). This is why their local CV correlates better with LB. Our Leave-One-Out(24) CV doesn't predict LB well.\n",
    "\n",
    "**Questions:**\n",
    "1. Can we use GroupKFold(5) locally to understand the CV-LB relationship better?\n",
    "2. What fundamentally different approaches remain?\n",
    "3. How can we change the CV-LB relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40486a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(\"Submission History:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nTarget LB: 0.0347\")\n",
    "print(f\"Best LB: {df['lb'].min():.4f} ({df.loc[df['lb'].idxmin(), 'exp']})\")\n",
    "print(f\"Gap to target: {df['lb'].min() / 0.0347:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV-LB relationship analysis\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "\n",
    "print(f\"CV-LB Linear Relationship:\")\n",
    "print(f\"  LB = {slope:.2f} * CV + {intercept:.4f}\")\n",
    "print(f\"  RÂ² = {r_value**2:.4f}\")\n",
    "print(f\"  Intercept = {intercept:.4f}\")\n",
    "print(f\"  Target LB = 0.0347\")\n",
    "print(f\"\")\n",
    "print(f\"CRITICAL INSIGHT:\")\n",
    "print(f\"  Intercept ({intercept:.4f}) > Target ({0.0347})\")\n",
    "print(f\"  This means even with CV=0, LB would be {intercept:.4f} > 0.0347\")\n",
    "print(f\"\")\n",
    "print(f\"Required CV to hit target:\")\n",
    "required_cv = (0.0347 - intercept) / slope\n",
    "print(f\"  CV = (0.0347 - {intercept:.4f}) / {slope:.2f} = {required_cv:.6f}\")\n",
    "if required_cv < 0:\n",
    "    print(f\"  NEGATIVE CV required - target is UNREACHABLE with current approach!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d6f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the 'mixall' kernel approach\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS: The 'mixall' Kernel CV Scheme\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nThe 'mixall' kernel OVERWRITES the utility functions:\")\n",
    "print(\"\")\n",
    "print(\"  def generate_leave_one_out_splits(X, Y):\")\n",
    "print(\"      groups = X['SOLVENT NAME']\")\n",
    "print(\"      n_splits = min(5, n_groups)\")\n",
    "print(\"      gkf = GroupKFold(n_splits=n_splits)\")\n",
    "print(\"      ...\")\n",
    "print(\"\")\n",
    "print(\"This means:\")\n",
    "print(\"  - Our CV: Leave-One-Solvent-Out (24 folds for single, 13 for mixtures)\")\n",
    "print(\"  - mixall CV: GroupKFold (5 folds for single, 5 for mixtures)\")\n",
    "print(\"\")\n",
    "print(\"Why this matters:\")\n",
    "print(\"  1. GroupKFold(5) is LESS PESSIMISTIC than Leave-One-Out(24)\")\n",
    "print(\"  2. Each fold in GroupKFold(5) has ~5 solvents in test set\")\n",
    "print(\"  3. The model sees more diverse training data per fold\")\n",
    "print(\"  4. This may lead to better generalization to unseen solvents\")\n",
    "print(\"\")\n",
    "print(\"HOWEVER:\")\n",
    "print(\"  - The LB evaluation uses the OFFICIAL Leave-One-Out scheme\")\n",
    "print(\"  - So the mixall kernel's local CV doesn't match LB evaluation\")\n",
    "print(\"  - But their model may still generalize better due to training dynamics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches have we exhausted?\n",
    "print(\"=\"*60)\n",
    "print(\"EXHAUSTED APPROACHES (26 consecutive failures)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "exhausted = [\n",
    "    \"Higher GP weight (exp_031, exp_035)\",\n",
    "    \"Pure GP (exp_032)\",\n",
    "    \"Ridge regression (exp_033)\",\n",
    "    \"Kernel Ridge (exp_034)\",\n",
    "    \"Lower GP weight (exp_035)\",\n",
    "    \"No GP (exp_036)\",\n",
    "    \"Similarity weighting (exp_037)\",\n",
    "    \"Minimal features (exp_038)\",\n",
    "    \"Learned embeddings (exp_039)\",\n",
    "    \"GNN architectures (exp_040, exp_052)\",\n",
    "    \"ChemBERTa (exp_041)\",\n",
    "    \"Calibration (exp_042)\",\n",
    "    \"Nonlinear mixture (exp_043)\",\n",
    "    \"Hybrid model (exp_044)\",\n",
    "    \"Mean reversion (exp_045)\",\n",
    "    \"Adaptive weighting (exp_046)\",\n",
    "    \"Diverse ensemble (exp_047)\",\n",
    "    \"Hybrid features (exp_048)\",\n",
    "    \"Manual OOD handling (exp_049)\",\n",
    "    \"LISA/REX (exp_050)\",\n",
    "    \"Simpler model (exp_051)\",\n",
    "    \"GNN proper (exp_052)\",\n",
    "    \"Mixall full features (exp_053)\",\n",
    "    \"Simpler regularized (exp_054)\",\n",
    "    \"Chemical constraints (exp_055)\",\n",
    "    \"XGBoost + RandomForest ensemble (exp_056)\",\n",
    "]\n",
    "\n",
    "for i, approach in enumerate(exhausted, 1):\n",
    "    print(f\"  {i}. {approach}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What fundamentally different approaches remain?\n",
    "print(\"=\"*60)\n",
    "print(\"REMAINING APPROACHES (Not Yet Tried)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. PREDICTION CALIBRATION (Isotonic Regression):\")\n",
    "print(\"   - Use isotonic regression to calibrate predictions\")\n",
    "print(\"   - This explicitly corrects systematic bias\")\n",
    "print(\"   - Could reduce the intercept in CV-LB relationship\")\n",
    "print(\"   - Different from exp_042 which used Platt scaling\")\n",
    "\n",
    "print(\"\\n2. QUANTILE REGRESSION:\")\n",
    "print(\"   - Train model to predict quantiles instead of mean\")\n",
    "print(\"   - May produce more robust predictions\")\n",
    "print(\"   - Different loss function could change CV-LB relationship\")\n",
    "\n",
    "print(\"\\n3. ASYMMETRIC LOSS:\")\n",
    "print(\"   - Penalize over-predictions differently from under-predictions\")\n",
    "print(\"   - May help with the systematic bias\")\n",
    "\n",
    "print(\"\\n4. FOCAL LOSS:\")\n",
    "print(\"   - Focus on hard examples\")\n",
    "print(\"   - May help with outlier solvents like HFIP, Cyclohexane\")\n",
    "\n",
    "print(\"\\n5. CATBOOST:\")\n",
    "print(\"   - Different gradient boosting implementation\")\n",
    "print(\"   - Handles categorical features natively\")\n",
    "print(\"   - May have different inductive biases\")\n",
    "\n",
    "print(\"\\n6. STACKING META-LEARNER:\")\n",
    "print(\"   - Train a meta-learner on top of base model predictions\")\n",
    "print(\"   - Could learn to correct systematic biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the CV-LB gap for different experiments\n",
    "print(\"=\"*60)\n",
    "print(\"CV-LB GAP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df['gap'] = df['lb'] / df['cv']\n",
    "df['residual'] = df['lb'] - (slope * df['cv'] + intercept)\n",
    "\n",
    "print(\"\\nCV-LB Gap (LB/CV ratio):\")\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"  {row['exp']}: CV={row['cv']:.4f}, LB={row['lb']:.4f}, Gap={row['gap']:.2f}x, Residual={row['residual']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest residual (below regression line): {df.loc[df['residual'].idxmin(), 'exp']} ({df['residual'].min():.4f})\")\n",
    "print(f\"Worst residual (above regression line): {df.loc[df['residual'].idxmax(), 'exp']} ({df['residual'].max():.4f})\")\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"  exp_000 has the best residual (-0.0022)\")\n",
    "print(f\"  This means exp_000 performed BETTER on LB than expected from CV\")\n",
    "print(f\"  What was different about exp_000?\")\n",
    "print(f\"    - Baseline MLP with Arrhenius Kinetics + TTA\")\n",
    "print(f\"    - Spange descriptors only (no DRFP, no ACS PCA)\")\n",
    "print(f\"    - Simpler model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace032cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategic recommendations\n",
    "print(\"=\"*60)\n",
    "print(\"STRATEGIC RECOMMENDATIONS FOR LOOP 57\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. TRY PREDICTION CALIBRATION (Isotonic Regression):\")\n",
    "print(\"   - Train the best model (exp_030)\")\n",
    "print(\"   - Use CV predictions to fit isotonic regression\")\n",
    "print(\"   - Apply calibration to test predictions\")\n",
    "print(\"   - This explicitly corrects systematic bias\")\n",
    "\n",
    "print(\"\\n2. TRY QUANTILE REGRESSION:\")\n",
    "print(\"   - Train model with quantile loss (e.g., median)\")\n",
    "print(\"   - May produce more robust predictions\")\n",
    "print(\"   - Different loss function could change CV-LB relationship\")\n",
    "\n",
    "print(\"\\n3. TRY STACKING META-LEARNER:\")\n",
    "print(\"   - Train multiple base models (GP, MLP, LGBM)\")\n",
    "print(\"   - Use their predictions as features for a meta-learner\")\n",
    "print(\"   - Meta-learner could learn to correct systematic biases\")\n",
    "\n",
    "print(\"\\n4. SUBMISSION STRATEGY:\")\n",
    "print(\"   - 5 submissions remaining\")\n",
    "print(\"   - Try 2-3 fundamentally different approaches\")\n",
    "print(\"   - Save 2 submissions for final attempts\")\n",
    "print(\"   - Focus on approaches that might change the CV-LB relationship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99081abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"LOOP 57 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCurrent Status:\")\n",
    "print(f\"  - Best CV: 0.008298 (exp_030)\")\n",
    "print(f\"  - Best LB: 0.0877 (exp_030)\")\n",
    "print(f\"  - Target LB: 0.0347\")\n",
    "print(f\"  - Gap: 2.53x\")\n",
    "print(f\"  - Submissions remaining: 5\")\n",
    "print(f\"  - Consecutive failures: 26\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  1. CV-LB relationship: LB = 4.31*CV + 0.0525\")\n",
    "print(\"  2. Intercept (0.0525) > Target (0.0347) - target unreachable with current approach\")\n",
    "print(\"  3. exp_056 (XGBoost + RF) FAILED - CV 0.014233 (71.5% worse)\")\n",
    "print(\"  4. The 'mixall' kernel uses GroupKFold(5) instead of Leave-One-Out(24)\")\n",
    "print(\"  5. exp_000 has the best residual - simpler model performed better on LB\")\n",
    "\n",
    "print(\"\\nRecommended Next Steps:\")\n",
    "print(\"  1. Try prediction calibration (isotonic regression)\")\n",
    "print(\"  2. Try quantile regression\")\n",
    "print(\"  3. Try stacking meta-learner\")\n",
    "print(\"  4. Focus on approaches that might change the CV-LB relationship\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
