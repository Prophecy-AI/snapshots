{
  "query": "How can I reduce the gap between local cross-validation and leaderboard score when predicting for unseen chemical compounds?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent studies of ADMET and bioactivity modeling, the most effective way to narrow the discrepancy between your internal cross\u2011validation metrics and the leaderboard score for truly unseen compounds is to make the validation protocol and the model itself more representative of the out\u2011of\u2011distribution (OOD) challenge you will face on the competition leaderboard\u202f([Step\u2011forward CV study](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006))\u202f([Data\u2011bias paper](https://www.nature.com/articles/s42256-025-01124-5)).  \n\n**Practical steps**\n\n1. **Adopt a forward\u2011time or scaffold\u2011aware split instead of random k\u2011fold**  \n   - Partition the data so that each fold contains compounds that are *later* in time or belong to *different scaffolds* than those in the training folds. This mimics the OOD nature of the leaderboard and has been shown to give a more realistic estimate of performance\u202f([Step\u2011forward CV study](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  \n\n2. **Combine the forward split with statistical hypothesis testing**  \n   - After each fold, run a paired\u2011t or Wilcoxon test between the validation scores of competing models. Keeping only models whose improvement is statistically significant reduces over\u2011optimistic CV scores\u202f([NIH ADMET benchmarking](https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724)).  \n\n3. **Mitigate data bias before training**  \n   - Re\u2011weight or down\u2011sample over\u2011represented chemotypes, and apply feature\u2011level debiasing (e.g., removing highly correlated descriptors). Studies show that correcting bias markedly improves generalization to new compounds\u202f([Nature Machine Intelligence, 2025](https://www.nature.com/articles/s42256-025-01124-5)).  \n\n4. **Use uncertainty\u2011aware models such as Chemprop with calibration**  \n   - Train a D\u2011MPNN via Chemprop, enable its built\u2011in uncertainty quantification (Monte\u2011Carlo dropout or ensemble mode), and calibrate the predicted uncertainties on a held\u2011out set. Well\u2011calibrated uncertainties help you identify and down\u2011weight predictions on compounds that lie far from the training distribution\u202f([Chemprop paper](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf)).  \n\n5. **Ensemble multiple CV\u2011trained models**  \n   - Build an ensemble of the top\u2011k models from different forward folds (or different random seeds) and average their predictions. Large\u2011scale evaluations confirm that k\u2011fold ensembles reduce variance and improve leaderboard scores\u202f([Cross\u2011validation ensembles study](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9)).  \n\n6. **Leverage transfer learning or pre\u2011training on large public datasets**  \n   - Pre\u2011train the D\u2011MPNN on a broad chemical property dataset (e.g., MoleculeNet) before fine\u2011tuning on your competition data. Transfer learning has been shown to boost out\u2011of\u2011distribution performance when the target set is small\u202f([Chemprop extensions](https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf)).  \n\n7. **Validate the final leaderboard submission with a \u201cpseudo\u2011leaderboard\u201d**  \n   - Reserve a small, truly unseen subset of your data (e.g., a later\u2011year scaffold split) that you never touch during model selection. Use its score as a proxy for the competition leaderboard; iterate only if the gap remains large. This extra hold\u2011out mimics the competition\u2019s hidden test set and prevents over\u2011fitting to the CV folds\u202f([Step\u2011forward CV recommendation](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).  \n\nFollowing these seven actions\u2014realistic forward splits, bias correction, uncertainty\u2011aware modeling, ensembling, transfer learning, and a final pseudo\u2011leaderboard check\u2014will align your local CV estimates with the true leaderboard performance for unseen chemical compounds.",
      "url": ""
    },
    {
      "title": "Benchmarking ML in ADMET predictions: the practical impact ... - NIH",
      "text": "Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1186/s13321-025-01041-0)\n* [](pdf/13321_2025_Article_1041.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Journal of Cheminformatics logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jcheminfo.png)\nJ Cheminform\n. 2025 Jul 21;17:108. doi:[10.1186/s13321-025-01041-0](https://doi.org/10.1186/s13321-025-01041-0)\n# Benchmarking ML in ADMET predictions: the practical impact of feature representations in ligand-based models\n[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n### Gintautas Kamuntavi\u010dius\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Gintautas Kamuntavi\u010dius](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kamuntavi\u010dius G\"[Author]>)\n1,\u2709,#,[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n### Tanya Paquet\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Tanya Paquet](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Paquet T\"[Author]>)\n1,#,[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n### Orestis Bastas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Orestis Bastas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bastas O\"[Author]>)\n1,#,[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n### Dainius \u0160alkauskas\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Dainius \u0160alkauskas](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u0160alkauskas D\"[Author]>)\n1,[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n### Alvaro Prat\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Alvaro Prat](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Prat A\"[Author]>)\n1,[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n### Hisham Abdel Aty\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Hisham Abdel Aty](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Aty HA\"[Author]>)\n1,[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n### Aurimas Pabrinkis\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Aurimas Pabrinkis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Pabrinkis A\"[Author]>)\n1,[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n### Povilas Norvai\u0161as\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Povilas Norvai\u0161as](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Norvai\u0161as P\"[Author]>)\n1,[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n### Roy Tal\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\nFind articles by[Roy Tal](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tal R\"[Author]>)\n1\n* Author information\n* Article notes\n* Copyright and License information\n1AI Chemistry, Ro5, 2801 Gateway Drive, 75063 Irving, TX USA\n\u2709Corresponding author.\n#\nContributed equally.\nReceived 2025 Jan 27; Accepted 2025 Jun 1; Collection date 2025.\n\u00a9The Author(s) 2025\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC12281724\u00a0\u00a0PMID:[40691635](https://pubmed.ncbi.nlm.nih.gov/40691635/)\n## Abstract\nThis study, focusing on predicting Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) properties, addresses the key challenges of ML models trained using ligand-based representations. We propose a structured approach to data feature selection, taking a step beyond the conventional practice of combining different representations without systematic reasoning. Additionally, we enhance model evaluation methods by integrating cross-validation with statistical hypothesis testing, adding a layer of reliability to the model assessments. Our final evaluations include a practical scenario, where models trained on one source of data are evaluated on a different one. This approach aims to bolster the reliability of ADMET predictions, providing more dependable and informative model evaluations.\n**Scientific contribution**\nThis study provided a structured approach to feature selection. We improve model evaluation by combining cross-validation with statistical hypothesis testing, making results more reliable. The methodology used in our study can be generalized beyond feature selection, boosting the confidence in selected models which is crucial in a noisy domain such as the ADMET prediction tasks. Additionally, we assess how well models trained on one dataset perform on another, offering practical insights for using external data in drug discovery.\n## Introduction\nThe Absorption, Distribution, Metabolism, Excretion, and Toxicology (ADMET) of compounds are commonly estimated throughout drug discovery projects, as the feasibility of a compound to become a viable drug highly depends on it. Through the years, a lot of work has gone into building and evaluating machine learning (ML) systems designed to predict molecular properties that are associated with ADMET. Public curated datasets and benchmarks for ADMET associated properties are becoming increasingly available to the community, creating the opportunity for more widespread exploration of ML algorithms and techniques in this space. The Therapeutics Data Commons (TDC) ADMET leaderboard showcases this [[1](#CR1)], highlighting a wide variety of models, features, and processing methods investigated...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12281724"
    },
    {
      "title": "Chemprop: Machine Learning Package for Chemical Property ...",
      "text": "Chemprop: Machine Learning Package for Chemical Property Prediction\nEsther Heid,1, 2 Kevin P. Greenman,1 Yunsie Chung,1 Shih-Cheng Li,1, 3 David E. Graff,4, 1 Florence H.\nVermeire,1, 5 Haoyang Wu,1 William H. Green,1and Charles J. McGill1, 6, a)\n1)Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139,\nUnited States\n2)Institute of Materials Chemistry, TU Wien, 1060 Vienna, Austria\n3)Department of Chemical Engineering, National Taiwan University, Taipei 10617,\nTaiwan\n4)Department of Chemistry and Chemical Biology, Harvard University, Cambridge, Massachusetts 02138,\nUnited States\n5)Department of Chemical Engineering, KU Leuven, Celestijnenlaan 200F, B-3001 Leuven,\nBelgium\n6)Department of Chemical and Life Science Engineering, Virginia Commonwealth University, Richmond,\nVirginia 23284, United States\nDeep learning has become a powerful and frequently employed tool for the prediction of molecular properties,\nthus creating a need for open-source and versatile software solutions that can be operated by non-experts.\nAmong current approaches, directed message-passing neural networks (D-MPNNs) have proven to perform\nwell on a variety of property prediction tasks. The software package Chemprop implements the D-MPNN\narchitecture, and offers simple, easy, and fast access to machine-learned molecular properties. Compared to its\ninitial version, we present a multitude of new Chemprop functionalities such as the support of multi-molecule\nproperties, reactions, atom/bond-level properties, and spectra. Further, we incorporate various uncertainty\nquantification and calibration methods along with related metrics, as well as pretraining and transfer learning\nworkflows, improved hyperparameter optimization, and other customization options concerning loss functions\nor atom/bond features. We benchmark D-MPNN models trained using Chemprop with the new reaction,\natom-level and spectra functionality on a variety of property prediction datasets, including MoleculeNet and\nSAMPL, and observe state-of-the-art performance on the prediction of water-octanol partition coefficients,\nreaction barrier heights, atomic partial charges, and absorption spectra. Chemprop enables out-of-the-box\ntraining of D-MPNN models for a variety of problem settings in a fast, user-friendly, and open-source software.\nI. INTRODUCTION\nMachine learning in general, and especially deep learn\u0002ing, has become a powerful tool in various fields of\nchemistry. Applications range from the prediction of\nphysico-chemical1\u20139 and pharmacological10 properties of\nmolecules to the design of molecules or materials with\ncertain properties,11\u201313 the exploration of chemical syn\u0002thesis pathways,14\u201327 or the prediction of properties im\u0002portant for chemical analysis like IR,28 UV/VIS29 or\nmass spectra.30\u201333\nMany combinations of molecular representation and\nmodel architecture have been developed to extract fea\u0002tures from molecules and predict molecular properties.\nMolecules can be represented as graphs, strings, pre\u0002computed feature vectors, or sets of atomic coordi\u0002nates and processed using graph-convolutional neural\nnetworks, transformers, or feed-forward neural networks\nto train predictive models. While early works focused\non hand-made features or simple fingerprinting methods\nlike circular fingerprints combined with kernel regres\u0002sion methods or neural networks,34 the current state\u0002of-the-art has shifted to end-to-end trainable models\nwhich directly learn to extract their own features.35\nHere, the model complexity can be nearly endless based\na)Electronic mail: mcgillc2@vcu.edu\non the mechanisms of information exchange between\nparts of the molecule. For example, graph convolu\u0002tional neural networks (GCNNs) extract local informa\u0002tion from the molecular graph for single or small groups\nof atoms, and use that information to update the imme\u0002diate neighborhood.1\u20133,36 Graph attention transformers\nallow for a less local information exchange via attention\nlayers, which learn to accumulate the features of atoms\nboth close and far away in the graph.37,38 Another impor\u0002tant line of research comprises the prediction of proper\u0002ties dependent on the three-dimensional conformation of\na molecule, such as the prediction of properties obtained\nfrom quantum mechanics.39\u201342 Finally, transformer mod\u0002els from natural language processing can be trained on\nstring representations such as SMILES or SELFIES, also\nleading to promising results.43\u201346\nIn general, larger models with more parameters suf\u0002fer from less restrictions in what and how they can learn\nfrom data, but require larger datasets and pre-training.47\nSmaller models can be very efficient if the right fea\u0002tures are known and used,47 but usually suffer from lim\u0002itations to their performance or generalization caused\nby a suboptimal representation or model architecture.\nFor example, models focusing only on the local struc\u0002ture of an atom and its immediate neighborhood usu\u0002ally fail to learn long-range interactions in a detailed\nfashion.48 In this work, we focus on GCNNs, which pre\u0002dict many chemical properties accurately49 at a moderate\nmodel size. They offer robust performance if the three\u00021\nhttps://doi.org/10.26434/chemrxiv-2023-3zcfl ORCID: https://orcid.org/0000-0002-8404-6596 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\ndimensional conformation of a molecule is not known or\nnot relevant for a prediction task. They perform best for\nmedium- to large-sized datasets (thousands to hundred\nthousands of data points). Many flavors of GCNNs have\nbeen published, among them our own approach called\nChemprop,36 a directed-message passing algorithm de\u0002rived from the seminal work of Gilmer et al.1\nAn early version of Chemprop was published in Ref. 36.\nSince then, the code has substantially evolved, and\nnow includes a vast collection of new features ranging\nfrom support of inputs beyond single molecules/targets\nand scalar molecular properties, to uncertainty estima\u0002tion, customized atom and bond features, and trans\u0002fer learning, among others. For example, Chemprop\nis now able to predict properties for systems contain\u0002ing multiple molecules, such as solute/solvent combina\u0002tions, or reactions with and without solvent. It can\ntrain on molecular targets, spectra, or atom/bond-level\ntargets, and output the latent representation for anal\u0002ysis of the learned feature embedding. Available uncer\u0002tainty metrics include popular approaches such as ensem\u0002bling, mean-variance estimation and evidential learning.\nChemprop is thus a general and versatile deep learning\ntoolbox, and enjoys a wide user base. Several studies\nhave been published based on Chemprop models rang\u0002ing from topics such as drug discovery,10,50,51 to spectro\u0002scopic properties,28,29 physico-chemical properties,4,52\u201358\nand reaction properties,59,60 with many of them making\nuse of the new features described in this manuscript, and\nachieving state-of-the-art performances in their respec\u0002tive tasks.\nIn this manuscript, we describe and benchmark the\nnew Chemprop features, as well as give usage examples\nand general advice on using Chemprop. The remainder\nof the article is structured as follows: The methods sec\u0002tion summarizes the architecture of Chemprop, as well as\nthe model details and parameters used in this study. We\nfurthermore describe the data acquisition, preprocessing\nand splitting of all datasets used in this study. We then\ndiscuss a selection of Chemprop features, with a focus on\nfeatures introduced after the initial release of Chemprop.\nIn the results section, we benchmark Chemprop on a\nlarge variety of datasets showcasing its performance for\nboth simple and advanced prediction tasks. We then\nconclude the article, and provide details on the data and\nsoftware, which we open-sourced including all scripts to\nallow for full reproducibility.\nII. METHODS\nIn the following, we describe the general architecture\nof Chemprop and a selection of the hyperparameters. We\nthen describe the hyperparamete...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/64bbe0a6b053dad33ab29040/original/chemprop-machine-learning-package-for-chemical-property-prediction.pdf"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Resolving data bias improves generalization in binding affinity prediction",
      "text": "Resolving data bias improves generalization in binding affinity prediction | Nature Machine Intelligence\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Machine Intelligence](https://media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-3a46d3127519f23b44cac085d4e82c58.svg)](https://www.nature.com/natmachintell)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42256-025-01124-5?error=cookies_not_supported&code=bb5c8458-66a5-4609-9a01-1e7c692a70bb)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42256)\n* [RSS feed](https://www.nature.com/natmachintell.rss)\nResolving data bias improves generalization in binding affinity prediction\n[Download PDF](https://www.nature.com/articles/s42256-025-01124-5.pdf)\n[Download PDF](https://www.nature.com/articles/s42256-025-01124-5.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:21 October 2025# Resolving data bias improves generalization in binding affinity prediction\n* [David Graber](#auth-David-Graber-Aff1-Aff2-Aff3)[ORCID:orcid.org/0009-0000-3814-506X](https://orcid.org/0009-0000-3814-506X)[1](#Aff1),[2](#Aff2),[3](#Aff3),\n* [Peter Stockinger](#auth-Peter-Stockinger-Aff2-Aff4)[ORCID:orcid.org/0000-0002-3494-7302](https://orcid.org/0000-0002-3494-7302)[2](#Aff2),[4](#Aff4),\n* [Fabian Meyer](#auth-Fabian-Meyer-Aff2)[2](#Aff2),\n* [Siddhartha Mishra](#auth-Siddhartha-Mishra-Aff1)[ORCID:orcid.org/0000-0002-2665-5385](https://orcid.org/0000-0002-2665-5385)[1](#Aff1)[na1](#na1),\n* [Claus Horn](#auth-Claus-Horn-Aff5)[ORCID:orcid.org/0000-0003-1557-7913](https://orcid.org/0000-0003-1557-7913)[5](#Aff5)[na1](#na1)&amp;\n* \u2026* [Rebecca Buller](#auth-Rebecca-Buller-Aff2-Aff4)[ORCID:orcid.org/0000-0002-5997-1616](https://orcid.org/0000-0002-5997-1616)[2](#Aff2),[4](#Aff4)[na1](#na1)Show authors\n[*Nature Machine Intelligence*](https://www.nature.com/natmachintell)**volume7**,pages1713\u20131725 (2025)[Cite this article](#citeas)\n* 22kAccesses\n* 1Citations\n* 30Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42256-025-01124-5/metrics)\n### Subjects\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n* [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n* [Machine learning](https://www.nature.com/subjects/machine-learning)\n* [Scientific data](https://www.nature.com/subjects/scientific-data)\nA[preprint version](https://doi.org/10.1101/2024.12.09.627482)of the article is available at bioRxiv.\n## Abstract\nThe field of computational drug design requires accurate scoring functions to predict binding affinities for protein\u2013ligand interactions. However, train\u2013test data leakage between the PDBbind database and the Comparative Assessment of Scoring Function benchmark datasets has severely inflated the performance metrics of currently available deep-learning-based binding affinity prediction models, leading to overestimation of their generalization capabilities. Here we address this issue by proposing PDBbind CleanSplit, a training dataset curated by a new structure-based filtering algorithm that eliminates train\u2013test data leakage as well as redundancies within the training set. Retraining current top-performing models on CleanSplit caused their benchmark performance to drop substantially, indicating that the performance of existing models is largely driven by data leakage. By contrast, our graph neural network model maintains high benchmark performance when trained on CleanSplit. Leveraging a sparse graph modelling of protein\u2013ligand interactions and transfer learning from language models, our model is able to generalize to strictly independent test datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-023-37572-z/MediaObjects/41467_2023_37572_Fig1_HTML.png)\n### [Improving the generalizability of protein-ligand binding predictions with AI-Bind](https://www.nature.com/articles/s41467-023-37572-z?fromPaywallRec=false)\nArticleOpen access08 April 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-72784-3/MediaObjects/41598_2024_72784_Fig1_HTML.png)\n### [Ensembling methods for protein-ligand binding affinity prediction](https://www.nature.com/articles/s41598-024-72784-3?fromPaywallRec=false)\nArticleOpen access18 October 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-023-02872-y/MediaObjects/41597_2023_2872_Fig1_HTML.png)\n### [PLAS-20k: Extended Dataset of Protein-Ligand Affinities from MD Simulations for Machine Learning Applications](https://www.nature.com/articles/s41597-023-02872-y?fromPaywallRec=false)\nArticleOpen access09 February 2024\n## Main\nStructure-based drug design (SBDD) aims to design small-molecule drugs that bind with high affinity to specific protein targets. In recent years, deep neural networks have begun to revolutionize the field, offering new possibilities for computational drug design. These include new protein folding models such as RoseTTAFold All-Atom[1](https://www.nature.com/articles/s42256-025-01124-5#ref-CR1), AlphaFold3[2](https://www.nature.com/articles/s42256-025-01124-5#ref-CR2)and Boltz-1[3](https://www.nature.com/articles/s42256-025-01124-5#ref-CR3), which can also consider small-molecule ligands to predict potential binding conformations. Furthermore, generative artificial intelligence can design entirely new protein\u2013ligand interactions. For example, RFdiffusion[4](https://www.nature.com/articles/s42256-025-01124-5#ref-CR4)can construct proteins around small molecules starting from random clouds of amino acids whereas the denoising diffusion model DiffSBDD[5](https://www.nature.com/articles/s42256-025-01124-5#ref-CR5)generates new ligands tailored to fit specific protein pockets. Although these methods excel at generating diverse collections of protein\u2013ligand interactions, these interactions are not necessarily characterized by drug-like affinity. Therefore, using these models for development of small-molecule drugs requires scoring functions that can accurately predict the absolute binding affinities for protein\u2013ligand poses and identify high-affinity complexes. Classical scoring functions, such as force-field-based, empirical and knowledge-based methods implemented in docking tools such as AutoDock Vina[6](https://www.nature.com/articles/s42256-025-01124-5#ref-CR6)and GOLD[7](https://www.nature.com/articles/s42256-025-01124-5#ref-CR7)are computationally intensive and show limited accuracy in binding affinity prediction[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42256-025-01124-5#ref-CR11). Despite notable advancements in deep-learning-based scoring functions, including the design of many convolutional[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](#ref-CR16),[17](#ref-CR17),[18](#ref-CR18),[19](#ref-CR19),[20](#ref-CR20),[21](#ref-CR21),[22](#ref-CR22),[23](https://www.nature.com/articles/s42256-025-01124-5#ref-CR23)and graph neural networks[24](#ref-CR24),[25](#ref-CR25),[26](#ref-CR26),[27](#ref-CR27),[28](#ref-CR28),[29](#ref-CR29),[30](#ref-CR30),[31](#ref-CR31),[32](https://www.nature.com/articles/s42256-025-01124-5#ref-CR32), accurately predicting binding affinities for pro...",
      "url": "https://www.nature.com/articles/s42256-025-01124-5"
    },
    {
      "title": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation",
      "text": "Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00709-9?)\n# Large-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 April 2023\n* Volume\u00a015, article\u00a0number49, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nLarge-scale evaluation of k-fold cross-validation ensembles for uncertainty estimation\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00709-9.pdf)\n* [Thomas-Martin Dutschmann](#auth-Thomas_Martin-Dutschmann-Aff1)[1](#Aff1),\n* [Lennart Kinzel](#auth-Lennart-Kinzel-Aff1)[1](#Aff1),\n* [Antonius ter Laak](#auth-Antonius-Laak-Aff2)[2](#Aff2)&amp;\n* \u2026* [Knut Baumann](#auth-Knut-Baumann-Aff1)[1](#Aff1)Show authors\n* 7359Accesses\n* 50Citations\n* 3Altmetric\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9/metrics)\n## Abstract\nIt is insightful to report an estimator that describes how certain a model is in a prediction, additionally to the prediction alone. For regression tasks, most approaches implement a variation of the ensemble method, apart from few exceptions. Instead of a single estimator, a group of estimators yields several predictions for an input. The uncertainty can then be quantified by measuring the disagreement between the predictions, for example by the standard deviation. In theory, ensembles should not only provide uncertainties, they also boost the predictive performance by reducing errors arising from variance. Despite the development of novel methods, they are still considered the \u201cgolden-standard\u201d to quantify the uncertainty of regression models. Subsampling-based methods to obtain ensembles can be applied to all models, regardless whether they are related to deep learning or traditional machine learning. However, little attention has been given to the question whether the ensemble method is applicable to virtually all scenarios occurring in the field of cheminformatics. In a widespread and diversified attempt, ensembles are evaluated for 32 datasets of different sizes and modeling difficulty, ranging from physicochemical properties to biological activities. For increasing ensemble sizes with up to 200 members, the predictive performance as well as the applicability as uncertainty estimator are shown for all combinations of five modeling techniques and four molecular featurizations. Useful recommendations were derived for practitioners regarding the success and minimum size of ensembles, depending on whether predictive performance or uncertainty quantification is of more importance for the task at hand.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-45630-5?as&#x3D;webp)\n### [Ensemble Models](https://link.springer.com/10.1007/978-3-031-45630-5_12?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-48317-7?as&#x3D;webp)\n### [Ensemble Methods for Time Series Forecasting](https://link.springer.com/10.1007/978-3-319-48317-7_13?fromPaywallRec=false)\nChapter\u00a9 2017\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-12240-8?as&#x3D;webp)\n### [Ensemble Models Using Symbolic Regression and Genetic Programming for Uncertainty Estimation in ESG and Alternative Investments](https://link.springer.com/10.1007/978-3-031-12240-8_5?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://jcheminf.biomedcentral.com/subjects/applied-probability)\n* [Applied Statistics](https://jcheminf.biomedcentral.com/subjects/applied-statistics)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://jcheminf.biomedcentral.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Statistical Theory and Methods](https://jcheminf.biomedcentral.com/subjects/statistical-theory-and-methods)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nMachine learning (ML) for drug design purposes holds a long tradition\u00a0[[1](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR1)], but has recently started to gain further attention due to the success of deep learning (DL)\u00a0[[2](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR2)]. Yet, the prediction of chemical properties and activities is only one step in a long and resource-intense process of drug design, discovery, and development. When developing ML models, predictions alone are not sufficient and require further analysis\u00a0[[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR3)]. During model construction and testing, errors made by the model can easily be evaluated since the true target values are known. The error distribution allows the estimation of the quality of the model, but cannot be applied when predicting values for new compounds with unknown target values. In this case, it is good practice to provide an estimate of the uncertainty associated with the prediction. Measures quantifying the predictive uncertainty can be used to set a threshold which defines the model\u2019s applicability domain. The latter is defined as follows: \u201cThe applicability domain of a (Q)SAR model is the response and chemical structure space in which the model makes predictions with a given reliability.\u201d\u00a0[[4](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR4)]. The reliability of a model can either be addressed by quantifying its confidence, or, conversely, its uncertainty. Recent studies make use of the term uncertainty quantification (UQ)\u00a0[[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR5)]. Uncertainty can be of aleatoric nature, relating to the random process that generates the target values, or epistemic nature, implying model-related uncertainty\u00a0[[6](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR6)]. Usually, these two types cannot be fully distinguished\u00a0[[7](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00709-9#ref-CR7)].\nClassification algorithms often provide built-in mechanisms or augmentations to measure their uncertainty\u00a0[[8](https://jcheminf.biomedcentral.com/...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00709-9"
    },
    {
      "title": "[PDF] BENCHMARKING BAND GAP PREDICTION FOR ...",
      "text": "Accepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nBENCHMARKING BAND GAP PREDICTION FOR\nSEMICONDUCTOR MATERIALS USING\nMULTIMODAL AND MULTI-FIDELITY DATA\nHaolin Wang1,2, Xianyuan Liu1,2, Anna Jungbluth3, Alex Ramadan4, Robert Oliver5,\nHaiping Lu1,2\n1 Centre for Machine Intelligence, University of Sheffield\n2 School of Computer Science, University of Sheffield\n3 Climate Office, European Space Agency\n4 School of Mathematical and Physical Sciences, University of Sheffield\n5 School of Chemical, Materials and Biological Engineering, University of Sheffield\n{haolin.wang, h.lu}@shef.ac.uk\nABSTRACT\nThe band gap is critical for understanding the electronic properties of materi\u0002als in semiconductor applications. While density functional theory is commonly\nused to estimate band gaps, it often underestimates values and remains compu\u0002tationally expensive, limiting its practical usefulness. Machine learning (ML)\nhas become a promising alternative for accurate and efficient band gap predic\u0002tions. However, existing datasets are limited in data modality, fidelity and sam\u0002ple size, and performance evaluation studies often lack direct comparisons be\u0002tween traditional and advanced ML models. Therefore, a more comprehensive\nevaluation is needed to make progress towards real-world impacts. In this pa\u0002per, we developed a benchmarking framework for ML-based band gap prediction\nto address this gap. We compiled a new multimodal, multi-fidelity dataset from\nthe Materials Project and BandgapDatabase1, consisting of 60,218 low-fidelity\ncomputational band gaps and 1,183 high-fidelity experimental band gaps across\n10 material categories. We evaluated seven ML models, from traditional meth\u0002ods to graph neural networks, assessing their ability to learn from atomic prop\u0002erties and structural information. To promote real-world applicability, we em\u0002ployed three metrics: mean absolute error, mean relative absolute error, and co\u0002efficient of determination R2\n. Moreover, we introduced a leave-one-material\u0002out evaluation strategy to better reflect real-world scenarios where new mate\u0002rials have little to no prior training data. Our findings offer valuable insights\ninto model selection and evaluation for band gap prediction across material cate\u0002gories, providing guidance for real-world applications in materials discovery and\nsemiconductor design. The data and code used in this work are available at:\nhttps://github.com/Shef-AIRE/bandgap-benchmark.\n1 INTRODUCTION\nThe band gap, defined as the energy difference between the valence and conduction bands, is a\nfundamental property of periodic solids and plays a critical role in determining their electrical con\u0002ductivity. This property is widely utilized in semiconductor applications (Yoder, 1996), including\nlight-emitting diodes (LEDs) (Lisensky et al., 1992), transistors (Ueno et al., 2004), and photovoltaic\ndevices (Goetzberger & Hebling, 2000). However, accurately determining the band gap of a mate\u0002rial remains a significant challenge. Theoretical methods, such as density functional theory (DFT),\nare commonly used but often underestimate band gaps due to limitations in exchange-correlation\nfunctionals. More advanced methods, such as the G0W0 approximation and hybrid functionals\n(Heyd et al., 2003), provide improved accuracy but are computationally intensive and require metic\u0002ulous parameter tuning. Recently, machine learning (ML) has emerged as a promising alternative\n1\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\nML Pipeline\nCGCNN\nLEFTNet-Z\nLEFTNet-Prop\nNeural Networks\nCartNet\nTraditional ML Methods\nLinear Regression\nRandom Forest Regression\nSupport Vector Regression\nLow Fidelity (DFT) High Fidelity (Expt.)\nDataset\nMulti-Fidelity\nMultimodality Atomic Properties Crystal Structure\nSplitting K-fold Cross-Validation Leave-One-Material-Out\nEvaluation MAE MRAE\nFigure 1: Flowchart of our proposed benchmark. The benchmark categorizes data based on fi\u0002delity and modality, incorporating low-fidelity (DFT) and high-fidelity (experimental) band gaps,\nalong with multimodal features. Beyond traditional K-fold cross-validation, a leave-one-material\u0002out strategy is introduced to better reflect real-world scenarios. The machine learning (ML) pipeline\nstudies both traditional ML methods and more recent neural networks. For evaluation, mean relative\nabsolute error (MRAE) is introduced to enhance applicability, alongside mean absolute error (MAE)\nand the coefficient of determination R2.\nfor predicting band gaps. Unlike conventional theoretical methods, ML methods can capture com\u0002plex structure-property relationships from large datasets, enabling accurate and efficient predictions\nwithout expensive calculations.\nMachine learning predicts band gaps primarily using two complementary types of information:\natomic properties and crystal structure. These modalities represent the material from different per\u0002spectives, forming a multimodal data representation (Liu et al., 2025). Atomic properties capture\nintrinsic characteristics of individual atoms that influence electronic behavior and have been widely\nused in band gap modeling (Talapatra et al., 2023). For instance, Sabagh Moeini et al. (2024) used\neight atomic features to train linear models and identified the standard deviation of valence electrons\nas a key predictor for band gaps in perovskites.\nAdvanced graph representation learning models (Schutt et al., 2017; Choudhary & DeCost, 2021) \u00a8\nextract crystal structure information via graph neural networks (GNNs) and capture atomic inter\u0002actions by analyzing distance and orientation. These models better utilize the underlying physics\nof crystal structures via the three-dimensional arrangement of atoms, making them an intuitive and\nsuitable approach for accurate property prediction. Additionally, structural information comple\u0002ments atomic properties by providing a global context that connects the local information carried by\nindividual atoms. The Crystal Graph Convolutional Neural Network (CGCNN) (Xie & Grossman,\n2018) is one of the most widely used models for structure-based materials property prediction. It in\u0002corporates nine atomic properties along with interatomic distance information. Subsequent models,\nsuch as CartNet (Sole et al., 2025), extend this idea by explicitly encoding the full 3D structure of \u00b4\nmaterials. Another approach, LEFTNet (Du et al., 2023), further improves predictive performance\nby capturing higher-order geometric features, including bond angles and local orientations. How\u0002ever, none of these methods have been evaluated against traditional machine learning models within\na unified benchmark.\nSeveral benchmark studies have compared machine learning models for predicting various material\nproperties. For example, MatBench (Dunn et al., 2020) provides a leaderboard for structure-based\nproperty predictions in inorganic materials, covering 13 supervised learning tasks (including band\ngap prediction) and incorporating both DFT and experimental data. Similar ML benchmarking\nefforts for band gap prediction include MatDeepLearn (Fung et al., 2021), Varivoda et al. (2023),\nand the JARVIS-Leaderboard (Choudhary et al., 2024). These benchmarks primarily rely on band\n2\nAccepted at the ICLR 2025 Workshop on AI for Accelerated Materials Design\ngap databases, the Materials Project (Jain et al., 2013) and QMOF (Rosen et al., 2021; 2022), that\nprovide DFT-calculated band gaps only. On the other hand, experimental datasets, such as the\none from Zhuo et al. (2018), contain only compositional information, making them unsuitable for\nstructure-based approaches.\nMasood et al. (2023) introduced a multi-fidelity open-access dataset that includes 3D structures,\ncomputational band gaps, and experimental band gaps, offering a more suitable resource for\nstructure-based band gap prediction. However, the evaluation dataset is relatively small, containing\nonly 30 materials, and lacks representation of key material categories suc...",
      "url": "https://openreview.net/pdf?id=u8FripvaG5"
    },
    {
      "title": "What Is Cross-Validation in Machine Learning? - Coursera",
      "text": "# What Is Cross-Validation in Machine Learning?\n\nWritten by Coursera Staff \u2022 Updated on May 5, 2025\n\nExplore the process of cross-validation in machine learning while discovering the different types of cross-validation methods and the best practices for implementation.\n\nCross-validation is a predictive assessment technique used in machine learning to estimate the capabilities of a [machine learning model](https://www.coursera.org/articles/machine-learning-models). If you work in machine learning, you can use cross-validation as a statistical model to compare and select machine learning models for a specific application. The technique can help you address problems such as overfitting a model, which can create issues like suboptimal performance in real-world scenarios. Cross-validation works to avoid this issue, and others such as underfitting, by [evaluating the model\u2019s performance](https://www.coursera.org/articles/model-evaluation) across multiple validation datasets during training. It divides or partitions the data set into subgroups for training and testing to accomplish this.\n\nThe goal of cross-validation is to provide a more accurate estimate of your model\u2019s ability to perform on new or unseen data by providing you with an unbiased estimate of the generalization error (a measure of how well an algorithm predicts future observations for previously unseen data). By providing you with the generalization error, cross-validation yields information about the model\u2019s generalization capabilities and is commonly used to tune or estimate the hyperparameters of a model. This helps you find the ideal configuration for the best generalization performance.\n\nRead further to learn more about cross-validation in machine learning while discovering the different types of cross-validation methods and the best ways for you to implement them.\n\n## What is cross-validation in machine learning?\n\nCross-validation determines the accuracy of your machine learning model by partitioning the data into two different groups, called a training set and a testing set. The data is then randomly separated into a certain number of groups or subsets called folds. Each fold contains about the same amount of data. The number of folds used depends on factors like the size, data type, and model. For example, if you separate your data into 10 subsets or folds, you would use nine as the training group and only one as the testing group. Repeat the process as many times as you have folds\u2014in this case, you would perform the training/testing a total of 10 times. After repeating the process (referred to as iteration) 10 times, you aggregate the results to create a single model estimation. The model estimation provides an estimate of your model\u2019s performance on new and unseen data.\n\nThese are the steps you would take to implement cross-validation, using the k-fold cross-validation form across 10 k-folds as an example:\n\n**1\\. Partition the data**\n\nDivide the data set into 10 subsets, also referred to as folds. Each fold contains an approximately equal proportion of the data. Common choices for _k_ include five or 10, but you can adjust the value based on the size and overall requirements of the data set.\n\n**2\\. Train and test model**\n\nTrain and test the model 10 times, using a different fold as the test set in each iteration. In the first iteration, the first fold is the test set, and you train the model on the remaining k-1 folds. In the second iteration, the second fold is the test set, and the process continues in this way until you reach 10 times.\n\n**3\\. Calculate performance metrics**\n\nAfter each iteration, calculate your model's performance metrics based on the model\u2019s predictions on the test set. These metrics assess or estimate how well your model generalizes to new, unseen data.\n\n**4\\. Aggregate results**\n\nThe performance metrics gathered in each iteration are usually aggregated to generate an overall assessment of the model's performance and create an evaluation model.\n\n## What is cross-validation used for in machine learning?\n\nUsing cross-validation, practitioners can build more reliable models that generalize well to new, unseen data, strengthening the algorithms' reliability, performance, and capabilities. The versatility of cross-validation allows you to choose from various methods based on the specific characteristics of the data set and the problem at hand. Common [uses for machine learning](https://www.coursera.org/articles/machine-learning-examples) include:\n\n### Mitigating overfitting\n\nTo identify if your model is overfitting, cross-validation works to help recognize patterns not tied to the specific partitioning of data. For example, if your model performs poorly on different test data sets but well on the training data during testing, this indicates overfitting. With cross-validation, you can break the training data into subsets and make further adjustments to your algorithm before applying it once again to the test data.\n\n### Ensuring generalization\n\nCross-validation assesses how well your model generalizes to new, unseen data. Cross-validation in this application aims to evaluate your model\u2019s performance across multiple subsets of data to ensure a comprehensive understanding of the model\u2019s ability to generalize. Ensuring generalization is critical for real-world applications where the model encounters diverse data inputs.\n\n### Performance evaluation\n\nPerformance evaluation in cross-validation refers to assessing a model\u2019s predictive performance on different subsets of training data. To run a performance evaluation, you can train and test the model on multiple subsets of training data over and over, ensuring the evaluation isn\u2019t too dependent on one specific data split. You can then compute the performance metrics for each fold and aggregate the results to provide an overall assessment of the model\u2019s performance. This can help you estimate how well the model generalizes to new, unseen data.\n\n### Hyperparameter tuning\n\nHyperparameter tuning in cross-validation determines the optimal configuration of your model's hyperparameters by assessing the model's performance with different hyperparameter values across multiple cross-validation folds. Hyperparameters are external configuration variables used to manage your model\u2019s training. They can help you figure out how to train and configure the model. Hyperparameter tuning is the process of choosing a set of hyperparameters and running them through your model multiple times.\n\nThrough multiple rounds of testing, the tuning process aids you in selecting hyperparameters that lead to the best generalization performance, which enhances your model's overall effectiveness.\n\n### Comparing models\n\nUsing cross-validation, practitioners can compare the performance of different models to check for efficiency and performance. Cross-validation provides unbiased, fair comparisons between your models by evaluating each model under the same conditions across multiple data subsets. The process ensures a reliable basis for selecting a suitable model for a specific task.\n\n### Solution for testing limited data\n\nCross-validation is an excellent tool if you have only a small data set with which to work because it allows you to still train and evaluate the model on different splits of that same data set to assess the fit and utility of the model on unseen data. Because cross-validation splits the data set into test and training sets, practitioners can train and evaluate models on different portions of the data, even when limited.\n\n### Identifying data variability\n\nBy testing the model on different subsets, cross-validation helps you identify how robust the model is to variations in input patterns. Cross-validation ensures a model's reliability in real-world data variability scenarios. Through cross-validation, it\u2019s easier to understand how well a model operates and copes with variability in the data.\n\n### What is SVC in cross-validation (machine learning...",
      "url": "https://www.coursera.org/articles/what-is-cross-validation-in-machine-learning"
    },
    {
      "title": "Gap leaderboard score and model scoring on a Competition",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Gap leaderboard score and model scoring on a Competition](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 4 months ago\n\nModified [6 years, 4 months ago](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?lastactivity)\n\nViewed\n694 times\n\n3\n\n$\\\\begingroup$\n\nI'm working on a Veolia challenge on Ens Data Challenge [ens-data](https://challengedata.ens.fr/en/home) (equivalent to Kaggle) the goal is to classify very rare binary events (the failure of a pipeline ) for 2014 and 2015 (y={2014,2015}). In input we have 5 features 3 categorical features (which I turned into dummy variable) and two continuous. The score is average AUC, $0.6\\*AUC\\_1 + 0.4\\*AUC\\_2$.\n\nMy problem is the following, when I compute each AUC (for 2014 and 2015) with a stratified kfold cross validation and I compute the average AUC I get roughly 0.88 and when I submit on the website I end up with 0.67, I guess there is a problem in my code.\n\nHere is my code for choosing the best model for 2014:\n\nRk: to predict on the test set (2014,2015 unknown), I first predict with all 5 features, 2014. Then I add the prediction of 2014 to my feature to predict 2015\n\n```\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LG', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\n\n# stratifiedkfold is defined by default when there is an integer\nscoring = 'roc_auc'\nnum_folds = 10\n\nfor name, model in models:\n    cv_results = cross_validation.cross_val_score(model, X, Y, cv=num_folds, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [python](https://datascience.stackexchange.com/questions/tagged/python)\n\n[Share](https://datascience.stackexchange.com/q/16789)\n\n[Improve this question](https://datascience.stackexchange.com/posts/16789/edit)\n\nFollow\n\n[edited Feb 7, 2017 at 9:17](https://datascience.stackexchange.com/posts/16789/revisions)\n\nbouritosse\n\nasked Feb 6, 2017 at 20:09\n\n[![bouritosse's user avatar](https://www.gravatar.com/avatar/d05cc42fc90ecb44bf614b1f69a90fcc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/19065/bouritosse)\n\n[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse) bouritosse\n\n9366 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- $\\\\begingroup$You mean you have a model scoring 0.88 while the leaderboard(the score on the website) shows 0.67?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 2:27\n\n- $\\\\begingroup$yes that is correct$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 9:15\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Your test data(the 0.88 one) is different from the test data on the website(the 0.67 one). There must be some difference between their scores. Are you concerning about over-fitting?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 10:13\n\n- $\\\\begingroup$kfold cross validation is not supposed to downsize the overfitting ?$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 12:21\n\n- $\\\\begingroup$Can you add your score on training set to your question, that's crucial.$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 12:49\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nAs you have commented, you are concerning about over-fitting.\n\nIn fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find:\n\n1. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) (the famous ResNet paper). Checkout figure.1\n2. This [kernel](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006) of [Two Sigma Financial Modeling Challenge](https://www.kaggle.com/c/two-sigma-financial-modeling) on [Kaggle](https://www.kaggle.com/) says:\n\n> we are getting a public score of 0.0169 which is slightly better than the previous one.\n> Submitting this model to the LB gave me a score of 0.006\n\nIn my practice, I am more willing to concern the scoring gap between TRAIN, VALIDATION(DEVELOP) and TEST dataset. Personally speaking, TRAIN>VALIDATION=TEST is better than TRAIN=VALIDATION>TEST.\n\nEdit: For class imbalance problem, there are some resources:\n\n1. [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This blog shows a common workflow dealing with imbalanced class issue.\n\n2. [Class Imbalance Problem in Data Mining: Review](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This paper compares several algorithms created for solving the class imbalance problem.\n\n\n[Share](https://datascience.stackexchange.com/a/16808)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/16808/edit)\n\nFollow\n\n[edited Feb 8, 2017 at 1:58](https://datascience.stackexchange.com/posts/16808/revisions)\n\nanswered Feb 7, 2017 at 12:48\n\n[![Icyblade's user avatar](https://www.gravatar.com/avatar/0696854959b681c4cefaf494015e8bf4?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/28628/icyblade)\n\n[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade) Icyblade\n\n4,33611 gold badge2424 silver badges3434 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$On the training I get 0.847700964724 AUC and on the validation set LG: 0.909566 (0.032773). With LogisticRegression(class\\_weight='balanced')$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 13:09\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I think the problem is that my class are really imbalance I have 0.19% of failure in class 2014$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 20:06\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f16789%2fgap-leaderboard-score-and-model-scoring-on-a-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [pri...",
      "url": "https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition"
    }
  ]
}