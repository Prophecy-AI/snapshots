## Current Status
- Best CV score: 0.005146 from exp_069 (Ens Model - CatBoost + XGBoost)
- Best LB score: 0.0877 (exp_030)
- Target: 0.0347 | Gap to target: 0.0530 (LB - Target)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.2312 * CV + 0.0533 (R² = 0.9807)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- Are all approaches on the same line? YES (for all 13 submissions so far)
- **Intercept (0.0533) > Target (0.0347)** means target is MATHEMATICALLY UNREACHABLE with current relationship
- Required CV for target: (0.0347 - 0.0533) / 4.2312 = -0.0044 (NEGATIVE - impossible!)

## MAJOR BREAKTHROUGH: exp_069 Achieved 35.17% CV Improvement!
- Previous best CV: 0.007938 (exp_068)
- New best CV: 0.005146 (exp_069) - **35.17% improvement!**
- Predicted LB (using old relationship): 4.2312 * 0.005146 + 0.0533 = **0.0751**
- This would be 14.4% better than current best LB (0.0877)

## Response to Evaluator
- Technical verdict: TRUSTWORTHY - I agree, the implementation is correct
- Evaluator's top priority: SUBMIT exp_069 to verify CV-LB relationship - **I FULLY AGREE**
- Key insight: The Ens Model approach is fundamentally different from all previous experiments
- This is the highest-leverage action available

## What Made exp_069 Different (Ens Model Kernel Approach)
1. **CatBoost with MultiRMSE** - Multi-output regression (not separate models per target)
2. **XGBoost with separate models** - Different approach for diversity
3. **Feature priority-based correlation filtering** - Reduced 4199 features to 69
   - Priority: Spange (5) > ACS (4) > DRFP (3) > Frag (2)
   - Threshold: 0.8 correlation
4. **Different ensemble weights for single vs full data**
   - Single: CatBoost 7:6 XGBoost (0.538:0.462)
   - Full: CatBoost 1:2 XGBoost (0.333:0.667)
5. **Multi-target normalization** - Clip negatives, renormalize if sum > 1

## Key Results from exp_069
- Single Solvent MSE: 0.009175 (n=656) - worse than previous best
- Full Data MSE: 0.002992 (n=1227) - **MUCH better!** (62% improvement)
- Combined MSE: 0.005146 - **35.17% improvement overall**

## CRITICAL QUESTION: Does exp_069 Have a Different CV-LB Relationship?

**Possible outcomes after submission:**

1. **SAME relationship (LB ≈ 0.075)**: 
   - Still best LB achieved (vs 0.0877)
   - Target remains unreachable
   - Need to try other approaches to change the relationship

2. **DIFFERENT relationship (lower intercept, LB < 0.07)**:
   - Target may become reachable!
   - Continue optimizing this approach
   - Try variations: different weights, different features, etc.

3. **WORSE than predicted (LB > 0.08)**:
   - Approach doesn't generalize well
   - Investigate why (overfitting to CV?)
   - May need to add regularization or simplify

## Recommended Approaches (Priority Order)

### IMMEDIATE: Submit exp_069
**This is the #1 priority.** We need LB feedback to determine next steps.

### If LB improves significantly (< 0.07):
1. **Optimize ensemble weights** - Try different CatBoost:XGBoost ratios
2. **Tune correlation threshold** - Try 0.7, 0.9 instead of 0.8
3. **Add more features** - Include numeric feature engineering (T_x_RT, RT_log, T_inv)
4. **Try different CatBoost hyperparameters** - depth, learning_rate, n_estimators
5. **Multi-seed ensemble** - Average predictions from multiple random seeds

### If LB follows old relationship (≈ 0.075):
1. **Extrapolation detection** - Add features measuring distance to training distribution
2. **Uncertainty-weighted predictions** - Use ensemble variance to weight predictions
3. **Solvent clustering** - Group solvents by chemical class, use class-specific models
4. **Pseudo-labeling** - Use confident test predictions to augment training

### If LB is worse than predicted (> 0.08):
1. **Add regularization** - Increase l2_leaf_reg, reduce n_estimators
2. **Simplify model** - Use only CatBoost or only XGBoost
3. **Reduce feature set** - Use only Spange + ACS (highest priority features)

## What NOT to Try
- GP + MLP + LGBM ensemble (already exhausted, same CV-LB relationship)
- More complex architectures (deep residual, attention) - didn't help
- Different CV schemes (GroupKFold vs Leave-One-Out) - didn't change relationship

## Validation Notes
- CV scheme: Leave-One-Solvent-Out for single (24 folds), Leave-One-Ramp-Out for full (87 folds)
- Combined score: Weighted average by sample count (656 single + 1227 full = 1883 total)
- The Ens Model kernel uses the same CV scheme, so results should be comparable

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop65_analysis.ipynb` for CV-LB analysis
- Key pattern: Full Data (mixtures) improved dramatically (62%) with Ens Model approach
- Single Solvent performance is worse, but combined score is dominated by Full Data improvement

## Submission Strategy
- 5 submissions remaining
- **SUBMIT exp_069 NOW** - This is the highest-leverage action
- After submission, analyze the result to determine next steps
- If LB improves significantly, we have 4 more submissions to optimize
- If LB doesn't improve, we need to pivot to distribution-shift strategies
