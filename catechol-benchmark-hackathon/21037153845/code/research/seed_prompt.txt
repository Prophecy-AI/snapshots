## Current Status
- Best CV score: 0.005146 from exp_069 (CatBoost + XGBoost Ensemble)
- Best LB score: 0.0877 (exp_030, MLP+GP)
- Target: 0.0347 | Gap to target: 0.0530 (from best LB)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.231 * CV + 0.0533 (R² = 0.9807)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- Are all approaches on the same line? YES (R² = 0.98)
- **CRITICAL**: Intercept (0.0533) > Target (0.0347) means target is MATHEMATICALLY UNREACHABLE by improving CV alone
- Required CV for target: (0.0347 - 0.0533) / 4.231 = NEGATIVE (impossible!)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Implementation is correct and verified.
- Evaluator's top priority: SUBMIT exp_069 to verify CV-LB relationship. **AGREED 100%**
- Key concerns raised:
  1. Verify numeric feature engineering - ADDRESSED (T_x_RT, RT_log, T_inv, RT_scaled included)
  2. Single solvent performance degradation - NOTED (0.009175 vs 0.008216), but full data improvement dominates
  3. Submission strategy - FOLLOWING (submitting to verify if CV-LB relationship changes)

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop65_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. exp_069 achieved 35% CV improvement (0.007938 → 0.005146)
  2. Full Data MSE improved by 62% (0.007789 → 0.002992)
  3. CatBoost + XGBoost with feature priority filtering is dramatically better for mixture data
  4. The approach is fundamentally different from all previous submissions

## MAJOR BREAKTHROUGH: exp_069 Results
- **Single Solvent MSE**: 0.009175 (n=656) - slightly worse than best
- **Full Data MSE**: 0.002992 (n=1227) - MUCH better (62% improvement!)
- **Combined MSE**: 0.005146 (NEW BEST CV!)

Key differences from previous approaches:
1. CatBoost with MultiRMSE (multi-output regression)
2. XGBoost with separate models per target
3. Different ensemble weights: Single (7:6), Full (1:2)
4. Feature priority-based correlation filtering (4199 → 69 features)
5. Multi-target normalization (clip + renormalize if sum > 1)

## Recommended Approaches (Priority Order)

### IMMEDIATE: SUBMIT exp_069
This is the highest-leverage action. Reasons:
1. 35% CV improvement - largest in 69 experiments
2. Fundamentally different approach (gradient boosting vs neural networks)
3. May have a DIFFERENT CV-LB relationship
4. Even if relationship holds, predicted LB (0.0751) would be best

### After Submission Feedback:

**If LB improves significantly (< 0.075):**
1. The CV-LB relationship HAS changed - this approach generalizes better
2. Continue optimizing CatBoost + XGBoost:
   - Tune hyperparameters (depth, learning_rate, n_estimators)
   - Try different ensemble weights
   - Add more feature engineering (polynomial features, interactions)
   - Try different correlation thresholds

**If LB ≈ 0.075 (follows old relationship):**
1. The intercept problem persists
2. Need to try approaches that CHANGE the relationship:
   - Extrapolation detection features (distance to training distribution)
   - Uncertainty-weighted predictions (blend toward mean when uncertain)
   - Domain constraints (Arrhenius kinetics, mass balance)
   - Pseudo-labeling with confident test predictions

**If LB doesn't improve much (> 0.08):**
1. Something is wrong with the implementation
2. Verify against the original Ens Model kernel
3. Check for differences in feature sources or preprocessing

## What NOT to Try
- More MLP/GP variations - all fall on the same CV-LB line
- Simple hyperparameter tuning - won't change the intercept
- More ensemble members of the same type - diminishing returns

## Validation Notes
- CV scheme: Leave-One-Solvent-Out (single), Leave-One-Ramp-Out (full)
- CV-LB gap: ~4.2x multiplier + 0.053 intercept
- The intercept represents EXTRAPOLATION ERROR to unseen solvents
- CatBoost + XGBoost may have lower extrapolation error due to:
  - Feature priority filtering (keeps most informative features)
  - Gradient boosting's ability to handle tabular data
  - Different regularization approach

## Critical Path to Target
1. **SUBMIT exp_069** - verify if CV-LB relationship changes
2. If relationship changes, optimize this approach
3. If relationship doesn't change, need fundamentally different strategies:
   - Extrapolation detection
   - Uncertainty quantification
   - Domain-specific constraints
   - Study what top competitors do differently
