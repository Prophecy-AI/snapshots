## Current Status
- Best CV score: 0.008194 from exp_032 (GP 0.15 + MLP 0.55 + LGBM 0.30)
- Best LB score: 0.0877 from exp_030
- Target: 0.0347 | Gap to target: 0.0530 (152.7% above target)

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit: LB = 4.23 * CV + 0.0533 (R² = 0.98)
- Intercept interpretation: Even at CV=0, expected LB is 0.0533
- **CRITICAL: Intercept (0.0533) > Target (0.0347)**
- Are all approaches on the same line? YES - ALL 13 submissions follow this line
- Required CV for target: (0.0347 - 0.0533) / 4.23 = -0.0044 (NEGATIVE = IMPOSSIBLE)

**CONCLUSION: The target is UNREACHABLE with our current approach. We MUST change the CV-LB relationship, not just improve CV.**

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The implementation is correct.
- Evaluator's top priority: Implement data-type-specific models (Single vs Full). AGREE - this is key.
- Key concerns raised:
  1. Full Data MSE is 2.4x higher than Single Solvent MSE with Fragprints
  2. Fragprints help single solvent but HURT mixture predictions
  3. The CV-LB intercept problem remains unsolved
- How we're addressing: Implementing the "Ens Model" kernel approach which uses different weights for single vs full data

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop62_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. Single solvent data: 656 samples, 24 solvents (Leave-One-Solvent-Out CV)
  2. Full/mixture data: 1227 samples, 87 ramps (Leave-One-Ramp-Out CV)
  3. Full data dominates combined score (65% of samples)
  4. Fragprints help single solvent but hurt mixture predictions

## Recommended Approaches

### PRIORITY 1: Implement "Ens Model" Kernel Approach (PROVEN TO WORK)
The "Ens Model" kernel (8 votes) has several techniques we haven't tried:

1. **CatBoost + XGBoost ensemble** (instead of GP + MLP + LGBM)
   - CatBoost handles categorical features well
   - XGBoost is robust and fast

2. **Different weights for single vs full data**:
   - Single: CatBoost 7, XGBoost 6 (normalized to 0.538, 0.462)
   - Full: CatBoost 1, XGBoost 2 (normalized to 0.333, 0.667)
   - This addresses the evaluator's concern about data-type-specific models

3. **Feature priority-based correlation filtering**:
   ```python
   def feature_priority(name):
       if name.startswith("spange_"): return 5  # Highest
       if name.startswith("acs_"): return 4
       if name.startswith("drfps_"): return 3
       if name.startswith("frag_"): return 2
       if name.startswith("smiles_"): return 1
       return 0
   ```
   - When two features are correlated (>0.8), keep the higher-priority one
   - This is more sophisticated than variance-based filtering

4. **Multi-target normalization**:
   ```python
   out = np.clip(out, a_min=0.0, a_max=None)
   totals = out.sum(axis=1, keepdims=True)
   divisor = np.maximum(totals, 1.0)
   out = out / divisor
   ```
   - Ensures predictions are non-negative
   - Ensures predictions sum to ≤ 1 (physically meaningful for yields)

5. **Combine ALL feature sources**:
   - spange_descriptors (13 features)
   - acs_pca_descriptors (5 features)
   - drfps_catechol (filtered)
   - fragprints (filtered)
   - smiles (optional)

**Implementation steps:**
1. Load all feature sources and apply feature priority filtering
2. Create CatBoost and XGBoost models for each target
3. Use different ensemble weights for single vs full data
4. Apply multi-target normalization to predictions
5. Run Leave-One-Solvent-Out CV for single solvent
6. Run Leave-One-Ramp-Out CV for full data

### PRIORITY 2: Data-Type-Specific Feature Sets
Based on exp_066 findings:
- **For Single Solvent**: Use Fragprints (they help: 5.7% improvement)
- **For Full/Mixture Data**: DO NOT use Fragprints (they hurt: 2.4x worse)

### PRIORITY 3: Investigate Why Our CV-LB Gap is So Large
- Our local CV uses Leave-One-Solvent-Out and Leave-One-Ramp-Out
- The server may use a different CV scheme
- The "Ens Model" kernel may have better alignment with server CV

## What NOT to Try
- **More GP/MLP/LGBM variations**: All fall on the same CV-LB line
- **Fragprints for mixture data**: Proven to hurt performance
- **Bias correction / uncertainty weighting**: Already tried, didn't help
- **Pure GP or pure LGBM**: Already tried, worse than ensemble

## Validation Notes
- CV scheme: Leave-One-Solvent-Out for single, Leave-One-Ramp-Out for full
- The server uses its own CV evaluation - our local CV may not match
- Focus on approaches that change the CV-LB relationship, not just improve CV

## CRITICAL REMINDER
- **DO NOT submit models with CV > 0.010** (predicted LB > 0.095)
- **The target IS reachable** - the "Ens Model" kernel proves it
- **We need to change the relationship**, not just improve CV
- **5 submissions remaining** - be strategic!

## Competition-Specific Requirements
- Must follow the template structure (last 3 cells unchanged)
- Only the model definition line can be changed
- Pre-training on mixture data to predict full solvent is NOT allowed
- Same hyperparameters must be used across all folds (unless explainable rationale)
