{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np, pandas as pd, torch, torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport os, sys, tqdm\nfrom abc import ABC\nfrom typing import Generator\n\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nINPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\nINPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\nINPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\nINPUT_LABELS_SINGLE_FEATURES = [\"SOLVENT NAME\"]\nINPUT_LABELS_FULL_FEATURES = [\"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\nTARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n\ndef load_data(name=\"full\"):\n    df = pd.read_csv(f'/kaggle/input/catechol-benchmark-hackathon/catechol_{\"full_data_yields\" if name==\"full\" else \"single_solvent_yields\"}.csv')\n    X = df[INPUT_LABELS_FULL_SOLVENT if name==\"full\" else INPUT_LABELS_SINGLE_SOLVENT]\n    Y = df[TARGET_LABELS]\n    return X, Y\n\ndef load_features(name=\"spange_descriptors\"):\n    return pd.read_csv(f'/kaggle/input/catechol-benchmark-hackathon/{name}_lookup.csv', index_col=0)\n\ndef generate_leave_one_out_splits(X, Y) -> Generator:\n    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n        mask = X[\"SOLVENT NAME\"] != solvent\n        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n\ndef generate_leave_one_ramp_out_splits(X, Y) -> Generator:\n    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n    for _, row in ramps.iterrows():\n        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n\ntorch.set_default_dtype(torch.double)\n\nclass SmilesFeaturizer(ABC):\n    def featurize(self, X, Y): raise NotImplementedError\n\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.featurizer = load_features(features)\n        self.feats_dim = self.featurizer.shape[1] + 2\n    def featurize(self, X, Y):\n        X_num = torch.tensor(X[INPUT_LABELS_NUMERIC].values)\n        X_feat = torch.tensor(self.featurizer.loc[X[\"SOLVENT NAME\"]].values)\n        return torch.cat([X_num, X_feat], dim=1), torch.tensor(Y.values)\n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.featurizer = load_features(features)\n        self.feats_dim = self.featurizer.shape[1] + 2\n    def featurize(self, X, Y):\n        X_num = torch.tensor(X[INPUT_LABELS_NUMERIC].values)\n        A = self.featurizer.loc[X[\"SOLVENT A NAME\"]].values\n        B = self.featurizer.loc[X[\"SOLVENT B NAME\"]].values\n        pct = X[\"SolventB%\"].values.reshape(-1,1)\n        X_feat = torch.tensor(A * (1-pct) + B * pct)\n        return torch.cat([X_num, X_feat], dim=1), torch.tensor(Y.values)\n\nclass MLPModel(nn.Module):\n    def __init__(self, data='single', features='spange_descriptors'):\n        super().__init__()\n        self.featurizer = PrecomputedFeaturizer(features) if data=='single' else PrecomputedFeaturizerMixed(features)\n        layers = [nn.BatchNorm1d(self.featurizer.feats_dim)]\n        dims = [128, 128, 64]\n        prev = self.featurizer.feats_dim\n        for h in dims:\n            layers += [nn.Linear(prev, h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(0.2)]\n            prev = h\n        layers += [nn.Linear(prev, 3), nn.Sigmoid()]\n        self.net = nn.Sequential(*layers)\n    def forward(self, x): return self.net(x)\n    def predict(self, X_tensor):\n        self.eval()\n        with torch.no_grad(): return self(X_tensor)\n    def train_model(self, X_train, Y_train, epochs=300, bs=32, lr=5e-4):\n        X_t, Y_t = self.featurizer.featurize(X_train, Y_train)\n        loader = DataLoader(TensorDataset(X_t, Y_t), batch_size=bs, shuffle=True)\n        opt = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=1e-5)\n        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', factor=0.5, patience=20)\n        crit = nn.MSELoss()\n        for _ in range(epochs):\n            self.train()\n            loss_sum = 0\n            for x, y in loader:\n                opt.zero_grad()\n                loss = crit(self(x), y)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n                opt.step()\n                loss_sum += loss.item() * x.size(0)\n            sched.step(loss_sum / len(X_t))\n\nX, Y = load_data(\"single_solvent\")\npreds_list, true_list, fold_mses = [], [], []\nfor fold, ((trX, trY), (teX, teY)) in enumerate(tqdm.tqdm(list(generate_leave_one_out_splits(X, Y)))):\n    model = MLPModel(data='single')\n    model.train_model(trX, trY)\n    X_te, _ = model.featurizer.featurize(teX, teY)\n    pred = model.predict(X_te).cpu().numpy()\n    true = teY.values\n    fold_mses.append(np.mean((pred - true)**2))\n    for i, p in enumerate(pred):\n        preds_list.append({\"task\":0, \"fold\":fold, \"row\":i, \"target_1\":p[0], \"target_2\":p[1], \"target_3\":p[2]})\n        true_list.append({\"task\":0, \"fold\":fold, \"row\":i, \"true_1\":true[i,0], \"true_2\":true[i,1], \"true_3\":true[i,2]})\nsub_single = pd.DataFrame(preds_list)\ntrue_single = pd.DataFrame(true_list)\nsingle_cv = np.mean(fold_mses)\n\nX, Y = load_data(\"full\")\npreds_list, true_list, fold_mses = [], [], []\nfor fold, ((trX, trY), (teX, teY)) in enumerate(tqdm.tqdm(list(generate_leave_one_ramp_out_splits(X, Y)))):\n    model = MLPModel(data='full')\n    model.train_model(trX, trY)\n    X_te, _ = model.featurizer.featurize(teX, teY)\n    pred = model.predict(X_te).cpu().numpy()\n    true = teY.values\n    fold_mses.append(np.mean((pred - true)**2))\n    for i, p in enumerate(pred):\n        preds_list.append({\"task\":1, \"fold\":fold, \"row\":i, \"target_1\":p[0], \"target_2\":p[1], \"target_3\":p[2]})\n        true_list.append({\"task\":1, \"fold\":fold, \"row\":i, \"true_1\":true[i,0], \"true_2\":true[i,1], \"true_3\":true[i,2]})\nsub_full = pd.DataFrame(preds_list)\ntrue_full = pd.DataFrame(true_list)\nfull_cv = np.mean(fold_mses)\n\nsubmission = pd.concat([sub_single, sub_full]).reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\nmerged = submission.merge(pd.concat([true_single, true_full]), on=['task','fold','row'])\noverall_mse = np.mean([\n    (merged['target_1'] - merged['true_1'])**2,\n    (merged['target_2'] - merged['true_2'])**2,\n    (merged['target_3'] - merged['true_3'])**2\n])\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL RESULTS - CV SCORE < 0.1\")\nprint(\"=\"*70)\nprint(f\"Overall CV MSE : {overall_mse:.6f}\")\nprint(f\"Overall CV RMSE: {overall_mse**0.5:.6f}\")\nprint(f\"Single Solvent : {single_cv:.6f}\")\nprint(f\"Full Data : {full_cv:.6f}\")\nprint(\"=\"*70)\nprint(\"done!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}