{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811414e2",
   "metadata": {},
   "source": [
    "# Loop 63 Analysis: CV-LB Relationship and Strategy Assessment\n",
    "\n",
    "## Key Question: Can we reach the target (0.0347) with current approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history with CV and LB scores\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_041', 'cv': 0.0090, 'lb': 0.0932},\n",
    "    {'exp': 'exp_042', 'cv': 0.0145, 'lb': 0.1147},\n",
    "]\n",
    "\n",
    "cv_scores = np.array([s['cv'] for s in submissions])\n",
    "lb_scores = np.array([s['lb'] for s in submissions])\n",
    "\n",
    "# Linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "\n",
    "print('CV-LB Relationship Analysis')\n",
    "print('='*60)\n",
    "print(f'Linear fit: LB = {slope:.3f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'Intercept = {intercept:.4f}')\n",
    "print(f'Target LB = 0.0347')\n",
    "print()\n",
    "print('CRITICAL ANALYSIS:')\n",
    "print(f'  - Intercept ({intercept:.4f}) > Target ({0.0347:.4f}): {intercept > 0.0347}')\n",
    "if intercept > 0.0347:\n",
    "    print(f'  - Even with CV=0, predicted LB would be {intercept:.4f}')\n",
    "    print(f'  - This means target is UNREACHABLE with current CV-LB relationship!')\n",
    "else:\n",
    "    required_cv = (0.0347 - intercept) / slope\n",
    "    print(f'  - Required CV to hit target: {required_cv:.6f}')\n",
    "\n",
    "# New best CV\n",
    "new_cv = 0.007944\n",
    "predicted_lb = slope * new_cv + intercept\n",
    "print(f'\\nNew Best CV: {new_cv:.6f}')\n",
    "print(f'Predicted LB: {predicted_lb:.4f}')\n",
    "print(f'Best LB so far: 0.0877')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV vs LB\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cv_scores, lb_scores, s=100, alpha=0.7, label='Submissions')\n",
    "\n",
    "# Fit line\n",
    "cv_range = np.linspace(0, 0.016, 100)\n",
    "lb_fit = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_fit, 'r--', label=f'Fit: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.0347, color='g', linestyle=':', linewidth=2, label='Target (0.0347)')\n",
    "\n",
    "# New best CV\n",
    "plt.scatter([new_cv], [predicted_lb], s=200, c='orange', marker='*', label=f'New Best CV ({new_cv:.4f}) -> Pred LB {predicted_lb:.4f}')\n",
    "\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship - All Submissions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/cv_lb_analysis_loop63.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey Insight: All submissions fall on the same line (R² = 0.98)')\n",
    "print('This means the CV-LB relationship is STRUCTURAL, not random noise.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would it take to reach the target?\n",
    "print('WHAT WOULD IT TAKE TO REACH TARGET 0.0347?')\n",
    "print('='*60)\n",
    "\n",
    "# Option 1: Improve CV while keeping same relationship\n",
    "if slope > 0:\n",
    "    required_cv = (0.0347 - intercept) / slope\n",
    "    print(f'\\nOption 1: Improve CV (same relationship)')\n",
    "    print(f'  Required CV: {required_cv:.6f}')\n",
    "    if required_cv < 0:\n",
    "        print(f'  IMPOSSIBLE: Would need negative CV!')\n",
    "    else:\n",
    "        print(f'  Current best CV: {new_cv:.6f}')\n",
    "        print(f'  Improvement needed: {(new_cv - required_cv) / new_cv * 100:.1f}%')\n",
    "\n",
    "# Option 2: Change the intercept\n",
    "print(f'\\nOption 2: Change the CV-LB relationship')\n",
    "print(f'  Current intercept: {intercept:.4f}')\n",
    "print(f'  Target: 0.0347')\n",
    "print(f'  Gap: {intercept - 0.0347:.4f}')\n",
    "print(f'  Need to reduce intercept by: {(intercept - 0.0347) / intercept * 100:.1f}%')\n",
    "\n",
    "# Option 3: What intercept would we need with current best CV?\n",
    "required_intercept = 0.0347 - slope * new_cv\n",
    "print(f'\\nOption 3: With current best CV ({new_cv:.6f}), what intercept is needed?')\n",
    "print(f'  Required intercept: {required_intercept:.4f}')\n",
    "print(f'  Current intercept: {intercept:.4f}')\n",
    "print(f'  Reduction needed: {intercept - required_intercept:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89902095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what techniques might change the relationship\n",
    "print('TECHNIQUES THAT MIGHT CHANGE THE CV-LB RELATIONSHIP')\n",
    "print('='*60)\n",
    "\n",
    "print('''\n",
    "1. MULTI-TARGET NORMALIZATION (from Ens Model kernel)\n",
    "   - Clip predictions to [0, ∞)\n",
    "   - Renormalize so sum ≤ 1\n",
    "   - This ensures physical constraints are met\n",
    "   - May reduce extrapolation errors for unseen solvents\n",
    "\n",
    "2. DIFFERENT MODEL TYPES (CatBoost + XGBoost)\n",
    "   - Current: GP + MLP + LGBM\n",
    "   - Top kernels use: CatBoost + XGBoost\n",
    "   - Different model types may have different extrapolation behavior\n",
    "\n",
    "3. FEATURE PRIORITY-BASED CORRELATION FILTERING\n",
    "   - Prioritize: Spange > ACS > DRFP > Fragprints\n",
    "   - Remove correlated features (threshold 0.8)\n",
    "   - May reduce overfitting to training distribution\n",
    "\n",
    "4. DIFFERENT ENSEMBLE WEIGHTS FOR SINGLE VS FULL DATA\n",
    "   - Single: CatBoost 7, XGBoost 6 (normalized)\n",
    "   - Full: CatBoost 1, XGBoost 2 (normalized)\n",
    "   - Data-type-specific weighting\n",
    "\n",
    "5. UNCERTAINTY-WEIGHTED PREDICTIONS\n",
    "   - Use GP uncertainty to weight predictions\n",
    "   - High uncertainty → conservative prediction (closer to mean)\n",
    "   - May reduce extrapolation errors\n",
    "''')\n",
    "\n",
    "print('\\nCRITICAL: We have tried 67 experiments and all fall on the same line.')\n",
    "print('We MUST try something that fundamentally changes the relationship.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the top kernels actually do\n",
    "print('TOP KERNEL ANALYSIS')\n",
    "print('='*60)\n",
    "\n",
    "print('''\n",
    "From \"Ens Model\" kernel (matthewmaree):\n",
    "\n",
    "1. Uses CatBoost + XGBoost ensemble (NOT GP + MLP + LGBM)\n",
    "2. Feature priority-based correlation filtering:\n",
    "   - spange_ = 5 (highest priority)\n",
    "   - acs_ = 4\n",
    "   - drfps_ = 3\n",
    "   - frag_ = 2 (lowest priority)\n",
    "3. Different weights for single vs full:\n",
    "   - Single: CatBoost 7, XGBoost 6\n",
    "   - Full: CatBoost 1, XGBoost 2\n",
    "4. Multi-target normalization:\n",
    "   - out = np.clip(out, a_min=0.0, a_max=None)\n",
    "   - totals = out.sum(axis=1, keepdims=True)\n",
    "   - divisor = np.maximum(totals, 1.0)\n",
    "   - out = out / divisor\n",
    "\n",
    "We have NOT tried:\n",
    "- CatBoost + XGBoost ensemble (we use GP + MLP + LGBM)\n",
    "- Feature priority-based correlation filtering\n",
    "- Multi-target normalization\n",
    "- Different weights for single vs full\n",
    "\n",
    "These techniques may have a DIFFERENT CV-LB relationship!\n",
    "''')\n",
    "\n",
    "print('\\nRECOMMENDATION: Implement the Ens Model kernel approach exactly.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
