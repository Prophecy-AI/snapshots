{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cef6a6",
   "metadata": {},
   "source": [
    "# Experiment 064: Ens Model Kernel Approach\n",
    "\n",
    "Implementing the \"Ens Model\" kernel approach with:\n",
    "1. Feature priority-based correlation filtering\n",
    "2. Combine ALL feature sources (Spange + ACS PCA + DRFP + Fragprints)\n",
    "3. Different ensemble weights for single vs full data (CatBoost 7:6 for single, 1:2 for full)\n",
    "4. Multi-target normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b338122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b33448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all feature sources\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFPS_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "FRAGPRINTS_DF = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "SMILES_DF = pd.read_csv(f'{DATA_PATH}/smiles_lookup.csv', index_col=0)\n",
    "\n",
    "# Load yield data\n",
    "SINGLE_SOLVENT_DF = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "FULL_DATA_DF = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'DRFPS: {DRFPS_DF.shape}')\n",
    "print(f'ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Fragprints: {FRAGPRINTS_DF.shape}')\n",
    "print(f'Single solvent: {SINGLE_SOLVENT_DF.shape}')\n",
    "print(f'Full data: {FULL_DATA_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature priority function (from Ens Model kernel)\n",
    "def feature_priority(name):\n",
    "    \"\"\"Higher priority features are kept when correlated\"\"\"\n",
    "    if name.startswith('spange_'):\n",
    "        return 5  # Highest priority - physical descriptors\n",
    "    if name.startswith('acs_'):\n",
    "        return 4  # ACS PCA descriptors\n",
    "    if name.startswith('drfps_'):\n",
    "        return 3  # DRFP fingerprints\n",
    "    if name.startswith('frag_'):\n",
    "        return 2  # Fragprints\n",
    "    if name.startswith('smiles_'):\n",
    "        return 1  # SMILES-based\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.90):\n",
    "    \"\"\"Remove correlated features, keeping higher priority ones\"\"\"\n",
    "    # Get correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "    \n",
    "    # Get upper triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = set()\n",
    "    for col in upper.columns:\n",
    "        for idx in upper.index:\n",
    "            if upper.loc[idx, col] > threshold:\n",
    "                # Compare priorities\n",
    "                if feature_priority(col) >= feature_priority(idx):\n",
    "                    to_drop.add(idx)\n",
    "                else:\n",
    "                    to_drop.add(col)\n",
    "    \n",
    "    print(f'Dropping {len(to_drop)} correlated features')\n",
    "    return df.drop(columns=list(to_drop))\n",
    "\n",
    "print('Feature filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7739bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined feature table\n",
    "def build_feature_table(solvents, threshold=0.90):\n",
    "    \"\"\"Build combined feature table with priority-based filtering\"\"\"\n",
    "    \n",
    "    # Spange features (highest priority)\n",
    "    spange_cols = [f'spange_{c}' for c in SPANGE_DF.columns]\n",
    "    spange_features = SPANGE_DF.loc[solvents].copy()\n",
    "    spange_features.columns = spange_cols\n",
    "    \n",
    "    # ACS PCA features\n",
    "    acs_cols = [f'acs_{c}' for c in ACS_PCA_DF.columns]\n",
    "    acs_features = ACS_PCA_DF.loc[solvents].copy()\n",
    "    acs_features.columns = acs_cols\n",
    "    \n",
    "    # DRFP features - filter zero variance\n",
    "    drfp_variance = DRFPS_DF.var()\n",
    "    drfp_nonzero = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "    drfp_features = DRFPS_DF.loc[solvents, drfp_nonzero].copy()\n",
    "    drfp_features.columns = [f'drfps_{c}' for c in drfp_features.columns]\n",
    "    \n",
    "    # Fragprints - filter zero variance\n",
    "    frag_variance = FRAGPRINTS_DF.var()\n",
    "    frag_nonzero = frag_variance[frag_variance > 0].index.tolist()\n",
    "    frag_features = FRAGPRINTS_DF.loc[solvents, frag_nonzero].copy()\n",
    "    frag_features.columns = [f'frag_{c}' for c in frag_features.columns]\n",
    "    \n",
    "    print(f'Spange: {spange_features.shape[1]} features')\n",
    "    print(f'ACS PCA: {acs_features.shape[1]} features')\n",
    "    print(f'DRFP (non-zero var): {drfp_features.shape[1]} features')\n",
    "    print(f'Fragprints (non-zero var): {frag_features.shape[1]} features')\n",
    "    \n",
    "    # Combine all features\n",
    "    combined = pd.concat([spange_features, acs_features, drfp_features, frag_features], axis=1)\n",
    "    print(f'Combined: {combined.shape[1]} features')\n",
    "    \n",
    "    # Filter correlated features\n",
    "    filtered = filter_correlated_features(combined, threshold=threshold)\n",
    "    print(f'After filtering: {filtered.shape[1]} features')\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Test with single solvent data\n",
    "solvents = SINGLE_SOLVENT_DF['Solvent Name'].unique()\n",
    "print(f'\\nBuilding feature table for {len(solvents)} solvents...')\n",
    "feature_table = build_feature_table(solvents, threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ce14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ens Model class\n",
    "class EnsModel:\n",
    "    def __init__(self, data_type='single', cat_weight=None, xgb_weight=None):\n",
    "        self.data_type = data_type\n",
    "        self.targets = ['Product 2', 'Product 3', 'SM']\n",
    "        \n",
    "        # Different weights for single vs full (from Ens Model kernel)\n",
    "        if cat_weight is None or xgb_weight is None:\n",
    "            if data_type == 'single':\n",
    "                self.cat_weight = 7.0 / 13.0  # 0.538\n",
    "                self.xgb_weight = 6.0 / 13.0  # 0.462\n",
    "            else:\n",
    "                self.cat_weight = 1.0 / 3.0  # 0.333\n",
    "                self.xgb_weight = 2.0 / 3.0  # 0.667\n",
    "        else:\n",
    "            self.cat_weight = cat_weight\n",
    "            self.xgb_weight = xgb_weight\n",
    "        \n",
    "        self.cat_models = {}\n",
    "        self.xgb_models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Train CatBoost and XGBoost for each target\"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        for target in self.targets:\n",
    "            y = Y[target].values\n",
    "            \n",
    "            # CatBoost\n",
    "            self.cat_models[target] = CatBoostRegressor(\n",
    "                iterations=500,\n",
    "                learning_rate=0.05,\n",
    "                depth=6,\n",
    "                loss_function='MAE',\n",
    "                verbose=False,\n",
    "                random_seed=42\n",
    "            )\n",
    "            self.cat_models[target].fit(X_scaled, y)\n",
    "            \n",
    "            # XGBoost\n",
    "            self.xgb_models[target] = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                objective='reg:absoluteerror',\n",
    "                verbosity=0,\n",
    "                random_state=42\n",
    "            )\n",
    "            self.xgb_models[target].fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Weighted ensemble with multi-target normalization\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in self.targets:\n",
    "            cat_pred = self.cat_models[target].predict(X_scaled)\n",
    "            xgb_pred = self.xgb_models[target].predict(X_scaled)\n",
    "            preds[target] = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        # Stack predictions\n",
    "        pred_array = np.column_stack([preds[t] for t in self.targets])\n",
    "        \n",
    "        # Multi-target normalization: clip to [0, 1] and renormalize to sum to 1\n",
    "        pred_array = np.clip(pred_array, 0, 1)\n",
    "        totals = pred_array.sum(axis=1, keepdims=True)\n",
    "        pred_array = pred_array / np.maximum(totals, 1e-8)\n",
    "        \n",
    "        # Convert back to dict\n",
    "        return {t: pred_array[:, i] for i, t in enumerate(self.targets)}\n",
    "\n",
    "print('EnsModel class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Solvent-Out CV for single solvent data\n",
    "def run_loso_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    \"\"\"Leave-One-Solvent-Out cross-validation\"\"\"\n",
    "    solvents = data_df['Solvent Name'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_solvent in solvents:\n",
    "        # Split data\n",
    "        train_mask = data_df['Solvent Name'] != test_solvent\n",
    "        test_mask = data_df['Solvent Name'] == test_solvent\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        # Get features\n",
    "        train_solvents = train_df['Solvent Name'].values\n",
    "        test_solvents = test_df['Solvent Name'].values\n",
    "        \n",
    "        X_train = feature_table.loc[train_solvents].values\n",
    "        X_test = feature_table.loc[test_solvents].values\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate errors\n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    overall_mae = np.mean(all_errors)\n",
    "    fold_std = np.std(fold_errors)\n",
    "    \n",
    "    return overall_mae, fold_std, fold_errors\n",
    "\n",
    "print('CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed40b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on single solvent data\n",
    "print('Running Leave-One-Solvent-Out CV on single solvent data...')\n",
    "print('='*60)\n",
    "\n",
    "# Build feature table\n",
    "solvents = SINGLE_SOLVENT_DF['Solvent Name'].unique()\n",
    "feature_table = build_feature_table(solvents, threshold=0.90)\n",
    "\n",
    "# Run CV\n",
    "cv_mae, cv_std, fold_errors = run_loso_cv(\n",
    "    feature_table, \n",
    "    SINGLE_SOLVENT_DF, \n",
    "    EnsModel,\n",
    "    data_type='single'\n",
    ")\n",
    "\n",
    "print(f'\\nSingle Solvent CV MAE: {cv_mae:.6f} ± {cv_std:.6f}')\n",
    "print(f'Fold errors: {[f\"{e:.6f}\" for e in fold_errors]}')\n",
    "print(f'Min fold: {min(fold_errors):.6f}, Max fold: {max(fold_errors):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Ramp-Out CV for full data\n",
    "def run_loro_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    \"\"\"Leave-One-Ramp-Out cross-validation for full data\"\"\"\n",
    "    ramps = data_df['Ramp'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_ramp in ramps:\n",
    "        # Split data\n",
    "        train_mask = data_df['Ramp'] != test_ramp\n",
    "        test_mask = data_df['Ramp'] == test_ramp\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        # Get features - need to handle mixture solvents\n",
    "        def get_mixture_features(df, feature_table):\n",
    "            \"\"\"Get features for mixture solvents by averaging\"\"\"\n",
    "            features = []\n",
    "            for _, row in df.iterrows():\n",
    "                solvent = row['Solvent Name']\n",
    "                if solvent in feature_table.index:\n",
    "                    features.append(feature_table.loc[solvent].values)\n",
    "                else:\n",
    "                    # Handle mixture - average the components\n",
    "                    # For now, use mean of all solvents as fallback\n",
    "                    features.append(feature_table.mean().values)\n",
    "            return np.array(features)\n",
    "        \n",
    "        X_train = get_mixture_features(train_df, feature_table)\n",
    "        X_test = get_mixture_features(test_df, feature_table)\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate errors\n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    overall_mae = np.mean(all_errors)\n",
    "    fold_std = np.std(fold_errors)\n",
    "    \n",
    "    return overall_mae, fold_std, fold_errors\n",
    "\n",
    "print('LORO CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what solvents are in full data\n",
    "full_solvents = FULL_DATA_DF['Solvent Name'].unique()\n",
    "print(f'Full data solvents: {len(full_solvents)}')\n",
    "print(full_solvents[:10])\n",
    "\n",
    "# Check which are in feature table\n",
    "missing = [s for s in full_solvents if s not in feature_table.index]\n",
    "print(f'\\nMissing from feature table: {len(missing)}')\n",
    "if missing:\n",
    "    print(missing[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354eef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature table for full data (including mixtures)\n",
    "def build_full_feature_table(data_df, threshold=0.90):\n",
    "    \"\"\"Build feature table that handles mixture solvents\"\"\"\n",
    "    \n",
    "    # Get all unique solvents\n",
    "    all_solvents = data_df['Solvent Name'].unique()\n",
    "    \n",
    "    # Separate single solvents and mixtures\n",
    "    single_solvents = [s for s in all_solvents if s in SPANGE_DF.index]\n",
    "    mixture_solvents = [s for s in all_solvents if s not in SPANGE_DF.index]\n",
    "    \n",
    "    print(f'Single solvents: {len(single_solvents)}')\n",
    "    print(f'Mixture solvents: {len(mixture_solvents)}')\n",
    "    \n",
    "    # Build feature table for single solvents first\n",
    "    base_table = build_feature_table(single_solvents, threshold=threshold)\n",
    "    \n",
    "    # For mixtures, we need to parse and average\n",
    "    mixture_features = []\n",
    "    for mixture in mixture_solvents:\n",
    "        # Parse mixture (e.g., \"Solvent1:Solvent2\" or \"Solvent1/Solvent2\")\n",
    "        parts = mixture.replace('/', ':').split(':')\n",
    "        parts = [p.strip() for p in parts]\n",
    "        \n",
    "        # Get features for each component\n",
    "        component_features = []\n",
    "        for part in parts:\n",
    "            if part in base_table.index:\n",
    "                component_features.append(base_table.loc[part].values)\n",
    "        \n",
    "        if component_features:\n",
    "            # Average the components\n",
    "            avg_features = np.mean(component_features, axis=0)\n",
    "        else:\n",
    "            # Fallback to mean of all solvents\n",
    "            avg_features = base_table.mean().values\n",
    "        \n",
    "        mixture_features.append(avg_features)\n",
    "    \n",
    "    # Create DataFrame for mixtures\n",
    "    if mixture_features:\n",
    "        mixture_df = pd.DataFrame(\n",
    "            mixture_features,\n",
    "            index=mixture_solvents,\n",
    "            columns=base_table.columns\n",
    "        )\n",
    "        # Combine\n",
    "        full_table = pd.concat([base_table, mixture_df])\n",
    "    else:\n",
    "        full_table = base_table\n",
    "    \n",
    "    return full_table\n",
    "\n",
    "print('Building full feature table...')\n",
    "full_feature_table = build_full_feature_table(FULL_DATA_DF, threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on full data\n",
    "print('\\nRunning Leave-One-Ramp-Out CV on full data...')\n",
    "print('='*60)\n",
    "\n",
    "full_cv_mae, full_cv_std, full_fold_errors = run_loro_cv(\n",
    "    full_feature_table, \n",
    "    FULL_DATA_DF, \n",
    "    EnsModel,\n",
    "    data_type='full'\n",
    ")\n",
    "\n",
    "print(f'\\nFull Data CV MAE: {full_cv_mae:.6f} ± {full_cv_std:.6f}')\n",
    "print(f'Fold errors: {[f\"{e:.6f}\" for e in full_fold_errors]}')\n",
    "print(f'Min fold: {min(full_fold_errors):.6f}, Max fold: {max(full_fold_errors):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined CV score (weighted average as per competition)\n",
    "# Single solvent: 24 solvents, Full data: 13 ramps\n",
    "# Weight by number of samples\n",
    "n_single = len(SINGLE_SOLVENT_DF)\n",
    "n_full = len(FULL_DATA_DF)\n",
    "\n",
    "weighted_cv = (cv_mae * n_single + full_cv_mae * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f'='*60)\n",
    "print(f'Single Solvent CV MAE: {cv_mae:.6f} ± {cv_std:.6f}')\n",
    "print(f'Full Data CV MAE: {full_cv_mae:.6f} ± {full_cv_std:.6f}')\n",
    "print(f'Weighted Combined CV MAE: {weighted_cv:.6f}')\n",
    "print(f'\\nBest baseline CV: 0.008194')\n",
    "print(f'Improvement: {(0.008194 - weighted_cv) / 0.008194 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "\n",
    "# For submission, we need to follow the competition template\n",
    "# The submission is a notebook, not a CSV\n",
    "# But we can save our model predictions for reference\n",
    "\n",
    "print('Experiment complete!')\n",
    "print(f'\\nFinal CV: {weighted_cv:.6f}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
