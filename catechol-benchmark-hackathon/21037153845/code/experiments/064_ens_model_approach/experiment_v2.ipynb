{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926f6c74",
   "metadata": {},
   "source": [
    "# Experiment 064: Ens Model Kernel Approach\n",
    "\n",
    "Implementing the \"Ens Model\" kernel approach with:\n",
    "1. Feature priority-based correlation filtering\n",
    "2. Combine ALL feature sources (Spange + ACS PCA + DRFP + Fragprints)\n",
    "3. Different ensemble weights for single vs full data\n",
    "4. Multi-target normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea33ad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:08.540708Z",
     "iopub.status.busy": "2026-01-16T08:58:08.540142Z",
     "iopub.status.idle": "2026-01-16T08:58:10.202764Z",
     "shell.execute_reply": "2026-01-16T08:58:10.202282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891ea728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.204100Z",
     "iopub.status.busy": "2026-01-16T08:58:10.203922Z",
     "iopub.status.idle": "2026-01-16T08:58:10.264589Z",
     "shell.execute_reply": "2026-01-16T08:58:10.264156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13)\n",
      "DRFPS: (24, 2048)\n",
      "ACS PCA: (24, 5)\n",
      "Fragprints: (24, 2133)\n",
      "Single solvent: (656, 13)\n",
      "Full data: (1227, 19)\n",
      "\n",
      "Single solvent columns: ['EXP NUM', 'Residence Time', 'Temperature', 'SM', 'Product 2', 'Product 3', 'SM SMILES', 'Product 2 SMILES', 'Product 3 SMILES', 'SOLVENT NAME', 'SOLVENT SMILES', 'SOLVENT Ratio', 'Reaction SMILES']\n",
      "\n",
      "Full data columns: ['EXP NUM', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%', 'Residence Time', 'Temperature', 'SM', 'Product 2', 'Product 3', 'SM SMILES', 'Product 2 SMILES', 'Product 3 SMILES', 'SOLVENT A SMILES', 'SOLVENT B SMILES', 'SOLVENT A Ratio', 'SOLVENT B Ratio', 'Reaction SMILES A', 'Reaction SMILES B', 'RAMP NUM']\n"
     ]
    }
   ],
   "source": [
    "# Load all feature sources\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFPS_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "FRAGPRINTS_DF = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "\n",
    "# Load yield data\n",
    "SINGLE_SOLVENT_DF = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "FULL_DATA_DF = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'DRFPS: {DRFPS_DF.shape}')\n",
    "print(f'ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Fragprints: {FRAGPRINTS_DF.shape}')\n",
    "print(f'Single solvent: {SINGLE_SOLVENT_DF.shape}')\n",
    "print(f'Full data: {FULL_DATA_DF.shape}')\n",
    "print(f'\\nSingle solvent columns: {list(SINGLE_SOLVENT_DF.columns)}')\n",
    "print(f'\\nFull data columns: {list(FULL_DATA_DF.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5115e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.265804Z",
     "iopub.status.busy": "2026-01-16T08:58:10.265694Z",
     "iopub.status.idle": "2026-01-16T08:58:10.270367Z",
     "shell.execute_reply": "2026-01-16T08:58:10.269955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "# Feature priority function (from Ens Model kernel)\n",
    "def feature_priority(name):\n",
    "    if name.startswith('spange_'): return 5\n",
    "    if name.startswith('acs_'): return 4\n",
    "    if name.startswith('drfps_'): return 3\n",
    "    if name.startswith('frag_'): return 2\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.90):\n",
    "    \"\"\"Remove correlated features, keeping higher priority ones\"\"\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = set()\n",
    "    for col in upper.columns:\n",
    "        for idx in upper.index:\n",
    "            if pd.notna(upper.loc[idx, col]) and upper.loc[idx, col] > threshold:\n",
    "                if feature_priority(col) >= feature_priority(idx):\n",
    "                    to_drop.add(idx)\n",
    "                else:\n",
    "                    to_drop.add(col)\n",
    "    \n",
    "    print(f'Dropping {len(to_drop)} correlated features')\n",
    "    return df.drop(columns=list(to_drop))\n",
    "\n",
    "print('Feature filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2e85ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.271390Z",
     "iopub.status.busy": "2026-01-16T08:58:10.271270Z",
     "iopub.status.idle": "2026-01-16T08:58:10.276175Z",
     "shell.execute_reply": "2026-01-16T08:58:10.275788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build feature table function defined\n"
     ]
    }
   ],
   "source": [
    "# Build combined feature table\n",
    "def build_feature_table(solvents, threshold=0.90):\n",
    "    # Spange features\n",
    "    spange_cols = [f'spange_{c}' for c in SPANGE_DF.columns]\n",
    "    spange_features = SPANGE_DF.loc[solvents].copy()\n",
    "    spange_features.columns = spange_cols\n",
    "    \n",
    "    # ACS PCA features\n",
    "    acs_cols = [f'acs_{c}' for c in ACS_PCA_DF.columns]\n",
    "    acs_features = ACS_PCA_DF.loc[solvents].copy()\n",
    "    acs_features.columns = acs_cols\n",
    "    \n",
    "    # DRFP features - filter zero variance\n",
    "    drfp_variance = DRFPS_DF.var()\n",
    "    drfp_nonzero = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "    drfp_features = DRFPS_DF.loc[solvents, drfp_nonzero].copy()\n",
    "    drfp_features.columns = [f'drfps_{c}' for c in drfp_features.columns]\n",
    "    \n",
    "    # Fragprints - filter zero variance\n",
    "    frag_variance = FRAGPRINTS_DF.var()\n",
    "    frag_nonzero = frag_variance[frag_variance > 0].index.tolist()\n",
    "    frag_features = FRAGPRINTS_DF.loc[solvents, frag_nonzero].copy()\n",
    "    frag_features.columns = [f'frag_{c}' for c in frag_features.columns]\n",
    "    \n",
    "    print(f'Spange: {spange_features.shape[1]}, ACS: {acs_features.shape[1]}, DRFP: {drfp_features.shape[1]}, Frag: {frag_features.shape[1]}')\n",
    "    \n",
    "    # Combine all features\n",
    "    combined = pd.concat([spange_features, acs_features, drfp_features, frag_features], axis=1)\n",
    "    print(f'Combined: {combined.shape[1]} features')\n",
    "    \n",
    "    # Filter correlated features\n",
    "    filtered = filter_correlated_features(combined, threshold=threshold)\n",
    "    print(f'After filtering: {filtered.shape[1]} features')\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "print('Build feature table function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69d9043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.277299Z",
     "iopub.status.busy": "2026-01-16T08:58:10.277195Z",
     "iopub.status.idle": "2026-01-16T08:58:10.283185Z",
     "shell.execute_reply": "2026-01-16T08:58:10.282724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnsModel class defined\n"
     ]
    }
   ],
   "source": [
    "# Ens Model class\n",
    "class EnsModel:\n",
    "    def __init__(self, data_type='single'):\n",
    "        self.data_type = data_type\n",
    "        self.targets = ['Product 2', 'Product 3', 'SM']\n",
    "        \n",
    "        # Different weights for single vs full (from Ens Model kernel)\n",
    "        if data_type == 'single':\n",
    "            self.cat_weight = 7.0 / 13.0  # 0.538\n",
    "            self.xgb_weight = 6.0 / 13.0  # 0.462\n",
    "        else:\n",
    "            self.cat_weight = 1.0 / 3.0  # 0.333\n",
    "            self.xgb_weight = 2.0 / 3.0  # 0.667\n",
    "        \n",
    "        self.cat_models = {}\n",
    "        self.xgb_models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        for target in self.targets:\n",
    "            y = Y[target].values\n",
    "            \n",
    "            # CatBoost\n",
    "            self.cat_models[target] = CatBoostRegressor(\n",
    "                iterations=500, learning_rate=0.05, depth=6,\n",
    "                loss_function='MAE', verbose=False, random_seed=42\n",
    "            )\n",
    "            self.cat_models[target].fit(X_scaled, y)\n",
    "            \n",
    "            # XGBoost\n",
    "            self.xgb_models[target] = xgb.XGBRegressor(\n",
    "                n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "                objective='reg:absoluteerror', verbosity=0, random_state=42\n",
    "            )\n",
    "            self.xgb_models[target].fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in self.targets:\n",
    "            cat_pred = self.cat_models[target].predict(X_scaled)\n",
    "            xgb_pred = self.xgb_models[target].predict(X_scaled)\n",
    "            preds[target] = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        # Stack predictions\n",
    "        pred_array = np.column_stack([preds[t] for t in self.targets])\n",
    "        \n",
    "        # Multi-target normalization: clip to [0, 1] and renormalize to sum to 1\n",
    "        pred_array = np.clip(pred_array, 0, 1)\n",
    "        totals = pred_array.sum(axis=1, keepdims=True)\n",
    "        pred_array = pred_array / np.maximum(totals, 1e-8)\n",
    "        \n",
    "        return {t: pred_array[:, i] for i, t in enumerate(self.targets)}\n",
    "\n",
    "print('EnsModel class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e739fadc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.284320Z",
     "iopub.status.busy": "2026-01-16T08:58:10.284213Z",
     "iopub.status.idle": "2026-01-16T08:58:10.288868Z",
     "shell.execute_reply": "2026-01-16T08:58:10.288469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSO CV function defined\n"
     ]
    }
   ],
   "source": [
    "# Leave-One-Solvent-Out CV for single solvent data\n",
    "def run_loso_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    solvents = data_df['SOLVENT NAME'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_solvent in solvents:\n",
    "        train_mask = data_df['SOLVENT NAME'] != test_solvent\n",
    "        test_mask = data_df['SOLVENT NAME'] == test_solvent\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        train_solvents = train_df['SOLVENT NAME'].values\n",
    "        test_solvents = test_df['SOLVENT NAME'].values\n",
    "        \n",
    "        X_train = feature_table.loc[train_solvents].values\n",
    "        X_test = feature_table.loc[test_solvents].values\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    return np.mean(all_errors), np.std(fold_errors), fold_errors\n",
    "\n",
    "print('LOSO CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea579ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:10.289897Z",
     "iopub.status.busy": "2026-01-16T08:58:10.289794Z",
     "iopub.status.idle": "2026-01-16T08:58:45.775873Z",
     "shell.execute_reply": "2026-01-16T08:58:45.775426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Leave-One-Solvent-Out CV on single solvent data...\n",
      "============================================================\n",
      "Number of solvents: 24\n",
      "Spange: 13, ACS: 5, DRFP: 122, Frag: 144\n",
      "Combined: 284 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 203 correlated features\n",
      "After filtering: 81 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Solvent CV MAE: 0.182802 +/- 0.079702\n"
     ]
    }
   ],
   "source": [
    "# Run CV on single solvent data\n",
    "print('Running Leave-One-Solvent-Out CV on single solvent data...')\n",
    "print('='*60)\n",
    "\n",
    "solvents = SINGLE_SOLVENT_DF['SOLVENT NAME'].unique()\n",
    "print(f'Number of solvents: {len(solvents)}')\n",
    "\n",
    "feature_table = build_feature_table(solvents, threshold=0.90)\n",
    "\n",
    "cv_mae, cv_std, fold_errors = run_loso_cv(\n",
    "    feature_table, SINGLE_SOLVENT_DF, EnsModel, data_type='single'\n",
    ")\n",
    "\n",
    "print(f'\\nSingle Solvent CV MAE: {cv_mae:.6f} +/- {cv_std:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99964d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:59.059898Z",
     "iopub.status.busy": "2026-01-16T08:58:59.059444Z",
     "iopub.status.idle": "2026-01-16T08:58:59.063839Z",
     "shell.execute_reply": "2026-01-16T08:58:59.063387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data unique solvents:\n",
      "SOLVENT A: ['Methanol' '1,1,1,3,3,3-Hexafluoropropan-2-ol' 'Cyclohexane'\n",
      " 'Water.Acetonitrile' 'Acetonitrile' '2-Methyltetrahydrofuran [2-MeTHF]'\n",
      " '2,2,2-Trifluoroethanol' 'DMA [N,N-Dimethylacetamide]' 'Ethanol'\n",
      " 'Dihydrolevoglucosenone (Cyrene)' 'MTBE [tert-Butylmethylether]'\n",
      " 'tert-Butanol [2-Methylpropan-2-ol]' 'Methyl Propionate']\n",
      "SOLVENT B: ['Ethylene Glycol [1,2-Ethanediol]' '2-Methyltetrahydrofuran [2-MeTHF]'\n",
      " 'IPA [Propan-2-ol]' 'Acetonitrile' 'Acetonitrile.Acetic Acid'\n",
      " 'Diethyl Ether [Ether]' 'Water.2,2,2-Trifluoroethanol' 'Decanol'\n",
      " 'THF [Tetrahydrofuran]' 'Ethyl Acetate' 'Butanone [MEK]'\n",
      " 'Dimethyl Carbonate' 'Ethyl Lactate']\n",
      "\n",
      "Ramps: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86]\n"
     ]
    }
   ],
   "source": [
    "# For full data, we need to handle mixture solvents\n",
    "# Check the structure\n",
    "print('Full data unique solvents:')\n",
    "print(f\"SOLVENT A: {FULL_DATA_DF['SOLVENT A NAME'].unique()}\")\n",
    "print(f\"SOLVENT B: {FULL_DATA_DF['SOLVENT B NAME'].unique()}\")\n",
    "print(f\"\\nRamps: {FULL_DATA_DF['RAMP NUM'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084229b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:59.064909Z",
     "iopub.status.busy": "2026-01-16T08:58:59.064790Z",
     "iopub.status.idle": "2026-01-16T08:58:59.070482Z",
     "shell.execute_reply": "2026-01-16T08:58:59.070038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LORO CV function defined\n"
     ]
    }
   ],
   "source": [
    "# Build feature table for full data (mixture solvents)\n",
    "def get_mixture_features(row, feature_table):\n",
    "    \"\"\"Get features for a mixture by weighted average based on ratio\"\"\"\n",
    "    solvent_a = row['SOLVENT A NAME']\n",
    "    solvent_b = row['SOLVENT B NAME']\n",
    "    ratio_b = row['SolventB%'] / 100.0  # Convert percentage to fraction\n",
    "    ratio_a = 1.0 - ratio_b\n",
    "    \n",
    "    feat_a = feature_table.loc[solvent_a].values\n",
    "    feat_b = feature_table.loc[solvent_b].values\n",
    "    \n",
    "    return ratio_a * feat_a + ratio_b * feat_b\n",
    "\n",
    "def run_loro_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    \"\"\"Leave-One-Ramp-Out CV for full data\"\"\"\n",
    "    ramps = data_df['RAMP NUM'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_ramp in ramps:\n",
    "        train_mask = data_df['RAMP NUM'] != test_ramp\n",
    "        test_mask = data_df['RAMP NUM'] == test_ramp\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        # Get features for mixtures\n",
    "        X_train = np.array([get_mixture_features(row, feature_table) for _, row in train_df.iterrows()])\n",
    "        X_test = np.array([get_mixture_features(row, feature_table) for _, row in test_df.iterrows()])\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    return np.mean(all_errors), np.std(fold_errors), fold_errors\n",
    "\n",
    "print('LORO CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87afac07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:59.071653Z",
     "iopub.status.busy": "2026-01-16T08:58:59.071305Z",
     "iopub.status.idle": "2026-01-16T08:58:59.684477Z",
     "shell.execute_reply": "2026-01-16T08:58:59.684069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data unique solvents: 24\n",
      "Spange: 13, ACS: 5, DRFP: 122, Frag: 144\n",
      "Combined: 284 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 203 correlated features\n",
      "After filtering: 81 features\n"
     ]
    }
   ],
   "source": [
    "# Build feature table for full data solvents\n",
    "full_solvents_a = FULL_DATA_DF['SOLVENT A NAME'].unique()\n",
    "full_solvents_b = FULL_DATA_DF['SOLVENT B NAME'].unique()\n",
    "full_solvents = list(set(full_solvents_a) | set(full_solvents_b))\n",
    "print(f'Full data unique solvents: {len(full_solvents)}')\n",
    "\n",
    "# Build feature table for these solvents\n",
    "full_feature_table = build_feature_table(full_solvents, threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62015220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T08:58:59.685438Z",
     "iopub.status.busy": "2026-01-16T08:58:59.685322Z",
     "iopub.status.idle": "2026-01-16T09:03:04.625610Z",
     "shell.execute_reply": "2026-01-16T09:03:04.625133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Leave-One-Ramp-Out CV on full data...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Data CV MAE: 0.323242 +/- 0.132992\n"
     ]
    }
   ],
   "source": [
    "# Run CV on full data\n",
    "print('\\nRunning Leave-One-Ramp-Out CV on full data...')\n",
    "print('='*60)\n",
    "\n",
    "full_cv_mae, full_cv_std, full_fold_errors = run_loro_cv(\n",
    "    full_feature_table, FULL_DATA_DF, EnsModel, data_type='full'\n",
    ")\n",
    "\n",
    "print(f'\\nFull Data CV MAE: {full_cv_mae:.6f} +/- {full_cv_std:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92410e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T09:03:04.627348Z",
     "iopub.status.busy": "2026-01-16T09:03:04.626969Z",
     "iopub.status.idle": "2026-01-16T09:03:04.630859Z",
     "shell.execute_reply": "2026-01-16T09:03:04.630417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Single Solvent CV MAE: 0.182802 +/- 0.079702\n",
      "Full Data CV MAE: 0.323242 +/- 0.132992\n",
      "Weighted Combined CV MAE: 0.274316\n",
      "\n",
      "Best baseline CV: 0.008194\n",
      "Improvement: -3247.76%\n"
     ]
    }
   ],
   "source": [
    "# Combined CV score\n",
    "n_single = len(SINGLE_SOLVENT_DF)\n",
    "n_full = len(FULL_DATA_DF)\n",
    "\n",
    "weighted_cv = (cv_mae * n_single + full_cv_mae * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f'='*60)\n",
    "print(f'Single Solvent CV MAE: {cv_mae:.6f} +/- {cv_std:.6f}')\n",
    "print(f'Full Data CV MAE: {full_cv_mae:.6f} +/- {full_cv_std:.6f}')\n",
    "print(f'Weighted Combined CV MAE: {weighted_cv:.6f}')\n",
    "print(f'\\nBest baseline CV: 0.008194')\n",
    "print(f'Improvement: {(0.008194 - weighted_cv) / 0.008194 * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
