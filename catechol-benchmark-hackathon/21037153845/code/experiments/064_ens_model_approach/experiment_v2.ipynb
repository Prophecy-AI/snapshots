{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926f6c74",
   "metadata": {},
   "source": [
    "# Experiment 064: Ens Model Kernel Approach\n",
    "\n",
    "Implementing the \"Ens Model\" kernel approach with:\n",
    "1. Feature priority-based correlation filtering\n",
    "2. Combine ALL feature sources (Spange + ACS PCA + DRFP + Fragprints)\n",
    "3. Different ensemble weights for single vs full data\n",
    "4. Multi-target normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea33ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ea728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all feature sources\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFPS_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "FRAGPRINTS_DF = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "\n",
    "# Load yield data\n",
    "SINGLE_SOLVENT_DF = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "FULL_DATA_DF = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'DRFPS: {DRFPS_DF.shape}')\n",
    "print(f'ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Fragprints: {FRAGPRINTS_DF.shape}')\n",
    "print(f'Single solvent: {SINGLE_SOLVENT_DF.shape}')\n",
    "print(f'Full data: {FULL_DATA_DF.shape}')\n",
    "print(f'\\nSingle solvent columns: {list(SINGLE_SOLVENT_DF.columns)}')\n",
    "print(f'\\nFull data columns: {list(FULL_DATA_DF.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5115e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature priority function (from Ens Model kernel)\n",
    "def feature_priority(name):\n",
    "    if name.startswith('spange_'): return 5\n",
    "    if name.startswith('acs_'): return 4\n",
    "    if name.startswith('drfps_'): return 3\n",
    "    if name.startswith('frag_'): return 2\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.90):\n",
    "    \"\"\"Remove correlated features, keeping higher priority ones\"\"\"\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    to_drop = set()\n",
    "    for col in upper.columns:\n",
    "        for idx in upper.index:\n",
    "            if pd.notna(upper.loc[idx, col]) and upper.loc[idx, col] > threshold:\n",
    "                if feature_priority(col) >= feature_priority(idx):\n",
    "                    to_drop.add(idx)\n",
    "                else:\n",
    "                    to_drop.add(col)\n",
    "    \n",
    "    print(f'Dropping {len(to_drop)} correlated features')\n",
    "    return df.drop(columns=list(to_drop))\n",
    "\n",
    "print('Feature filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined feature table\n",
    "def build_feature_table(solvents, threshold=0.90):\n",
    "    # Spange features\n",
    "    spange_cols = [f'spange_{c}' for c in SPANGE_DF.columns]\n",
    "    spange_features = SPANGE_DF.loc[solvents].copy()\n",
    "    spange_features.columns = spange_cols\n",
    "    \n",
    "    # ACS PCA features\n",
    "    acs_cols = [f'acs_{c}' for c in ACS_PCA_DF.columns]\n",
    "    acs_features = ACS_PCA_DF.loc[solvents].copy()\n",
    "    acs_features.columns = acs_cols\n",
    "    \n",
    "    # DRFP features - filter zero variance\n",
    "    drfp_variance = DRFPS_DF.var()\n",
    "    drfp_nonzero = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "    drfp_features = DRFPS_DF.loc[solvents, drfp_nonzero].copy()\n",
    "    drfp_features.columns = [f'drfps_{c}' for c in drfp_features.columns]\n",
    "    \n",
    "    # Fragprints - filter zero variance\n",
    "    frag_variance = FRAGPRINTS_DF.var()\n",
    "    frag_nonzero = frag_variance[frag_variance > 0].index.tolist()\n",
    "    frag_features = FRAGPRINTS_DF.loc[solvents, frag_nonzero].copy()\n",
    "    frag_features.columns = [f'frag_{c}' for c in frag_features.columns]\n",
    "    \n",
    "    print(f'Spange: {spange_features.shape[1]}, ACS: {acs_features.shape[1]}, DRFP: {drfp_features.shape[1]}, Frag: {frag_features.shape[1]}')\n",
    "    \n",
    "    # Combine all features\n",
    "    combined = pd.concat([spange_features, acs_features, drfp_features, frag_features], axis=1)\n",
    "    print(f'Combined: {combined.shape[1]} features')\n",
    "    \n",
    "    # Filter correlated features\n",
    "    filtered = filter_correlated_features(combined, threshold=threshold)\n",
    "    print(f'After filtering: {filtered.shape[1]} features')\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "print('Build feature table function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ens Model class\n",
    "class EnsModel:\n",
    "    def __init__(self, data_type='single'):\n",
    "        self.data_type = data_type\n",
    "        self.targets = ['Product 2', 'Product 3', 'SM']\n",
    "        \n",
    "        # Different weights for single vs full (from Ens Model kernel)\n",
    "        if data_type == 'single':\n",
    "            self.cat_weight = 7.0 / 13.0  # 0.538\n",
    "            self.xgb_weight = 6.0 / 13.0  # 0.462\n",
    "        else:\n",
    "            self.cat_weight = 1.0 / 3.0  # 0.333\n",
    "            self.xgb_weight = 2.0 / 3.0  # 0.667\n",
    "        \n",
    "        self.cat_models = {}\n",
    "        self.xgb_models = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        for target in self.targets:\n",
    "            y = Y[target].values\n",
    "            \n",
    "            # CatBoost\n",
    "            self.cat_models[target] = CatBoostRegressor(\n",
    "                iterations=500, learning_rate=0.05, depth=6,\n",
    "                loss_function='MAE', verbose=False, random_seed=42\n",
    "            )\n",
    "            self.cat_models[target].fit(X_scaled, y)\n",
    "            \n",
    "            # XGBoost\n",
    "            self.xgb_models[target] = xgb.XGBRegressor(\n",
    "                n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "                objective='reg:absoluteerror', verbosity=0, random_state=42\n",
    "            )\n",
    "            self.xgb_models[target].fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in self.targets:\n",
    "            cat_pred = self.cat_models[target].predict(X_scaled)\n",
    "            xgb_pred = self.xgb_models[target].predict(X_scaled)\n",
    "            preds[target] = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        \n",
    "        # Stack predictions\n",
    "        pred_array = np.column_stack([preds[t] for t in self.targets])\n",
    "        \n",
    "        # Multi-target normalization: clip to [0, 1] and renormalize to sum to 1\n",
    "        pred_array = np.clip(pred_array, 0, 1)\n",
    "        totals = pred_array.sum(axis=1, keepdims=True)\n",
    "        pred_array = pred_array / np.maximum(totals, 1e-8)\n",
    "        \n",
    "        return {t: pred_array[:, i] for i, t in enumerate(self.targets)}\n",
    "\n",
    "print('EnsModel class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Solvent-Out CV for single solvent data\n",
    "def run_loso_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    solvents = data_df['SOLVENT NAME'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_solvent in solvents:\n",
    "        train_mask = data_df['SOLVENT NAME'] != test_solvent\n",
    "        test_mask = data_df['SOLVENT NAME'] == test_solvent\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        train_solvents = train_df['SOLVENT NAME'].values\n",
    "        test_solvents = test_df['SOLVENT NAME'].values\n",
    "        \n",
    "        X_train = feature_table.loc[train_solvents].values\n",
    "        X_test = feature_table.loc[test_solvents].values\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    return np.mean(all_errors), np.std(fold_errors), fold_errors\n",
    "\n",
    "print('LOSO CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on single solvent data\n",
    "print('Running Leave-One-Solvent-Out CV on single solvent data...')\n",
    "print('='*60)\n",
    "\n",
    "solvents = SINGLE_SOLVENT_DF['SOLVENT NAME'].unique()\n",
    "print(f'Number of solvents: {len(solvents)}')\n",
    "\n",
    "feature_table = build_feature_table(solvents, threshold=0.90)\n",
    "\n",
    "cv_mae, cv_std, fold_errors = run_loso_cv(\n",
    "    feature_table, SINGLE_SOLVENT_DF, EnsModel, data_type='single'\n",
    ")\n",
    "\n",
    "print(f'\\nSingle Solvent CV MAE: {cv_mae:.6f} +/- {cv_std:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full data, we need to handle mixture solvents\n",
    "# Check the structure\n",
    "print('Full data unique solvents:')\n",
    "print(f\"SOLVENT A: {FULL_DATA_DF['SOLVENT A NAME'].unique()}\")\n",
    "print(f\"SOLVENT B: {FULL_DATA_DF['SOLVENT B NAME'].unique()}\")\n",
    "print(f\"\\nRamps: {FULL_DATA_DF['RAMP NUM'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084229b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature table for full data (mixture solvents)\n",
    "def get_mixture_features(row, feature_table):\n",
    "    \"\"\"Get features for a mixture by weighted average based on ratio\"\"\"\n",
    "    solvent_a = row['SOLVENT A NAME']\n",
    "    solvent_b = row['SOLVENT B NAME']\n",
    "    ratio_b = row['SolventB%'] / 100.0  # Convert percentage to fraction\n",
    "    ratio_a = 1.0 - ratio_b\n",
    "    \n",
    "    feat_a = feature_table.loc[solvent_a].values\n",
    "    feat_b = feature_table.loc[solvent_b].values\n",
    "    \n",
    "    return ratio_a * feat_a + ratio_b * feat_b\n",
    "\n",
    "def run_loro_cv(feature_table, data_df, model_class, **model_kwargs):\n",
    "    \"\"\"Leave-One-Ramp-Out CV for full data\"\"\"\n",
    "    ramps = data_df['RAMP NUM'].unique()\n",
    "    targets = ['Product 2', 'Product 3', 'SM']\n",
    "    \n",
    "    all_errors = []\n",
    "    fold_errors = []\n",
    "    \n",
    "    for test_ramp in ramps:\n",
    "        train_mask = data_df['RAMP NUM'] != test_ramp\n",
    "        test_mask = data_df['RAMP NUM'] == test_ramp\n",
    "        \n",
    "        train_df = data_df[train_mask]\n",
    "        test_df = data_df[test_mask]\n",
    "        \n",
    "        # Get features for mixtures\n",
    "        X_train = np.array([get_mixture_features(row, feature_table) for _, row in train_df.iterrows()])\n",
    "        X_test = np.array([get_mixture_features(row, feature_table) for _, row in test_df.iterrows()])\n",
    "        \n",
    "        Y_train = train_df[targets]\n",
    "        Y_test = test_df[targets]\n",
    "        \n",
    "        model = model_class(**model_kwargs)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        fold_mae = []\n",
    "        for target in targets:\n",
    "            mae = mean_absolute_error(Y_test[target], preds[target])\n",
    "            fold_mae.append(mae)\n",
    "            all_errors.extend(np.abs(Y_test[target].values - preds[target]))\n",
    "        \n",
    "        fold_errors.append(np.mean(fold_mae))\n",
    "    \n",
    "    return np.mean(all_errors), np.std(fold_errors), fold_errors\n",
    "\n",
    "print('LORO CV function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature table for full data solvents\n",
    "full_solvents_a = FULL_DATA_DF['SOLVENT A NAME'].unique()\n",
    "full_solvents_b = FULL_DATA_DF['SOLVENT B NAME'].unique()\n",
    "full_solvents = list(set(full_solvents_a) | set(full_solvents_b))\n",
    "print(f'Full data unique solvents: {len(full_solvents)}')\n",
    "\n",
    "# Build feature table for these solvents\n",
    "full_feature_table = build_feature_table(full_solvents, threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62015220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CV on full data\n",
    "print('\\nRunning Leave-One-Ramp-Out CV on full data...')\n",
    "print('='*60)\n",
    "\n",
    "full_cv_mae, full_cv_std, full_fold_errors = run_loro_cv(\n",
    "    full_feature_table, FULL_DATA_DF, EnsModel, data_type='full'\n",
    ")\n",
    "\n",
    "print(f'\\nFull Data CV MAE: {full_cv_mae:.6f} +/- {full_cv_std:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92410e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined CV score\n",
    "n_single = len(SINGLE_SOLVENT_DF)\n",
    "n_full = len(FULL_DATA_DF)\n",
    "\n",
    "weighted_cv = (cv_mae * n_single + full_cv_mae * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f'='*60)\n",
    "print(f'Single Solvent CV MAE: {cv_mae:.6f} +/- {cv_std:.6f}')\n",
    "print(f'Full Data CV MAE: {full_cv_mae:.6f} +/- {full_cv_std:.6f}')\n",
    "print(f'Weighted Combined CV MAE: {weighted_cv:.6f}')\n",
    "print(f'\\nBest baseline CV: 0.008194')\n",
    "print(f'Improvement: {(0.008194 - weighted_cv) / 0.008194 * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
