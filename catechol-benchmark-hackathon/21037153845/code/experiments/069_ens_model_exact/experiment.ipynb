{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4c70df",
   "metadata": {},
   "source": [
    "# Experiment 069: Exact Ens Model Approach\n",
    "\n",
    "Implementing the exact \"Ens Model\" kernel approach:\n",
    "1. CatBoost with MultiRMSE (multi-output)\n",
    "2. XGBoost with separate models per target\n",
    "3. Different weights for single (7:6) vs full (1:2)\n",
    "4. Feature priority-based correlation filtering\n",
    "5. Multi-target normalization (clip + renormalize if sum > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all feature sources\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFPS_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "FRAGPRINTS_DF = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}')\n",
    "print(f'DRFPS: {DRFPS_DF.shape}')\n",
    "print(f'ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Fragprints: {FRAGPRINTS_DF.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature priority function (from Ens Model kernel)\n",
    "def feature_priority(name):\n",
    "    if name.startswith('spange_'): return 5\n",
    "    if name.startswith('acs_'): return 4\n",
    "    if name.startswith('drfps_'): return 3\n",
    "    if name.startswith('frag_'): return 2\n",
    "    return 0\n",
    "\n",
    "def filter_correlated_features(df, threshold=0.8):\n",
    "    \"\"\"Remove correlated features, keeping higher priority ones\"\"\"\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Drop constant columns\n",
    "    std = numeric_df.std(axis=0)\n",
    "    constant_cols = std[std == 0].index.tolist()\n",
    "    numeric_df = numeric_df.drop(columns=constant_cols, errors='ignore')\n",
    "    \n",
    "    if numeric_df.shape[1] == 0:\n",
    "        return df, []\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr = numeric_df.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool)).fillna(0.0)\n",
    "    \n",
    "    cols = upper.columns.tolist()\n",
    "    to_drop = set()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    for i, col_i in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            col_j = cols[j]\n",
    "            if upper.iloc[i, j] > threshold:\n",
    "                if col_i in to_drop or col_j in to_drop:\n",
    "                    continue\n",
    "                p_i = feature_priority(col_i)\n",
    "                p_j = feature_priority(col_j)\n",
    "                if p_i > p_j:\n",
    "                    to_drop.add(col_j)\n",
    "                elif p_j > p_i:\n",
    "                    to_drop.add(col_i)\n",
    "                else:\n",
    "                    to_drop.add(col_j)  # Drop later one\n",
    "    \n",
    "    all_to_drop = list(set(constant_cols).union(to_drop))\n",
    "    df_filtered = df.drop(columns=all_to_drop, errors='ignore')\n",
    "    \n",
    "    print(f'Dropped {len(all_to_drop)} features (threshold={threshold})')\n",
    "    return df_filtered, all_to_drop\n",
    "\n",
    "print('Feature filtering functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build combined feature table\n",
    "def build_solvent_table():\n",
    "    \"\"\"Build combined feature table with all feature sources\"\"\"\n",
    "    # Get common solvents\n",
    "    common_solvents = list(set(SPANGE_DF.index) & set(DRFPS_DF.index) & set(ACS_PCA_DF.index) & set(FRAGPRINTS_DF.index))\n",
    "    \n",
    "    # Spange features\n",
    "    spange = SPANGE_DF.loc[common_solvents].copy()\n",
    "    spange.columns = [f'spange_{c}' for c in spange.columns]\n",
    "    \n",
    "    # ACS PCA features\n",
    "    acs = ACS_PCA_DF.loc[common_solvents].copy()\n",
    "    acs.columns = [f'acs_{c}' for c in acs.columns]\n",
    "    \n",
    "    # DRFP features\n",
    "    drfp = DRFPS_DF.loc[common_solvents].copy()\n",
    "    drfp.columns = [f'drfps_{c}' for c in drfp.columns]\n",
    "    \n",
    "    # Fragprints features\n",
    "    frag = FRAGPRINTS_DF.loc[common_solvents].copy()\n",
    "    frag.columns = [f'frag_{c}' for c in frag.columns]\n",
    "    \n",
    "    # Combine\n",
    "    combined = pd.concat([spange, acs, drfp, frag], axis=1)\n",
    "    print(f'Combined features: {combined.shape}')\n",
    "    \n",
    "    # Filter correlated features\n",
    "    filtered, dropped = filter_correlated_features(combined, threshold=0.8)\n",
    "    print(f'After filtering: {filtered.shape}')\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "SOLVENT_TABLE = build_solvent_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\", \"RAMP NUM\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    for ramp in sorted(X[\"RAMP NUM\"].unique()):\n",
    "        mask = X[\"RAMP NUM\"] != ramp\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurizer for single solvent\n",
    "class PrecomputedFeaturizer:\n",
    "    def __init__(self, solvent_table):\n",
    "        self.solvent_table = solvent_table\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features\n",
    "        T = X[\"Temperature\"].values\n",
    "        rt = X[\"Residence Time\"].values\n",
    "        \n",
    "        # Arrhenius kinetics features\n",
    "        T_kelvin = T + 273.15\n",
    "        inv_T = 1000.0 / T_kelvin\n",
    "        log_rt = np.log(rt + 1e-6)\n",
    "        T_x_rt = T * rt\n",
    "        \n",
    "        numeric_features = np.column_stack([T, rt, inv_T, log_rt, T_x_rt])\n",
    "        \n",
    "        # Solvent features\n",
    "        solvent_features = self.solvent_table.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        # Combine\n",
    "        features = np.hstack([numeric_features, solvent_features])\n",
    "        return torch.tensor(features, dtype=torch.double)\n",
    "\n",
    "# Featurizer for mixed solvents\n",
    "class PrecomputedFeaturizerMixed:\n",
    "    def __init__(self, solvent_table):\n",
    "        self.solvent_table = solvent_table\n",
    "    \n",
    "    def featurize(self, X):\n",
    "        # Numeric features\n",
    "        T = X[\"Temperature\"].values\n",
    "        rt = X[\"Residence Time\"].values\n",
    "        pct = X[\"SolventB%\"].values / 100.0\n",
    "        \n",
    "        # Arrhenius kinetics features\n",
    "        T_kelvin = T + 273.15\n",
    "        inv_T = 1000.0 / T_kelvin\n",
    "        log_rt = np.log(rt + 1e-6)\n",
    "        T_x_rt = T * rt\n",
    "        \n",
    "        numeric_features = np.column_stack([T, rt, inv_T, log_rt, T_x_rt, pct])\n",
    "        \n",
    "        # Solvent features (weighted average)\n",
    "        A_features = self.solvent_table.loc[X[\"SOLVENT A NAME\"]].values\n",
    "        B_features = self.solvent_table.loc[X[\"SOLVENT B NAME\"]].values\n",
    "        solvent_features = A_features * (1 - pct.reshape(-1, 1)) + B_features * pct.reshape(-1, 1)\n",
    "        \n",
    "        # Combine\n",
    "        features = np.hstack([numeric_features, solvent_features])\n",
    "        return torch.tensor(features, dtype=torch.double)\n",
    "\n",
    "print('Featurizers defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-target normalization (from Ens Model kernel)\n",
    "def multi_target_normalize(out):\n",
    "    \"\"\"Clip negatives to 0, then if sum > 1, scale down\"\"\"\n",
    "    out = np.clip(out, a_min=0.0, a_max=None)\n",
    "    totals = out.sum(axis=1, keepdims=True)\n",
    "    divisor = np.maximum(totals, 1.0)\n",
    "    out = out / divisor\n",
    "    return out\n",
    "\n",
    "print('Multi-target normalization defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28170b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Model (from Ens Model kernel)\n",
    "class CatBoostModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.featurizer = PrecomputedFeaturizer(SOLVENT_TABLE)\n",
    "            self.cat_params = dict(\n",
    "                random_seed=42,\n",
    "                loss_function='MultiRMSE',\n",
    "                depth=3,\n",
    "                learning_rate=0.07,\n",
    "                n_estimators=1050,\n",
    "                l2_leaf_reg=3.5,\n",
    "                bootstrap_type='Bayesian',\n",
    "                bagging_temperature=0.225,\n",
    "                grow_policy='SymmetricTree',\n",
    "                rsm=0.75,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            self.featurizer = PrecomputedFeaturizerMixed(SOLVENT_TABLE)\n",
    "            self.cat_params = dict(\n",
    "                random_seed=42,\n",
    "                loss_function='MultiRMSE',\n",
    "                depth=3,\n",
    "                learning_rate=0.06,\n",
    "                n_estimators=1100,\n",
    "                l2_leaf_reg=2.5,\n",
    "                bootstrap_type='Bayesian',\n",
    "                bagging_temperature=0.25,\n",
    "                grow_policy='SymmetricTree',\n",
    "                rsm=0.75,\n",
    "                verbose=False,\n",
    "            )\n",
    "        \n",
    "        self.model = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_tensor = self.featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        self.model = CatBoostRegressor(**self.cat_params)\n",
    "        self.model.fit(X_np, Y_np)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.featurizer.featurize(X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        out = self.model.predict(X_np)\n",
    "        out = multi_target_normalize(out)\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "print('CatBoost model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf67048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model (from Ens Model kernel)\n",
    "class XGBModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        if data == 'single':\n",
    "            self.featurizer = PrecomputedFeaturizer(SOLVENT_TABLE)\n",
    "            self.xgb_params = dict(\n",
    "                random_state=42,\n",
    "                objective='reg:squarederror',\n",
    "                tree_method='hist',\n",
    "                subsample=0.5,\n",
    "                reg_lambda=0.6,\n",
    "                reg_alpha=0.0,\n",
    "                n_estimators=1000,\n",
    "                min_child_weight=1,\n",
    "                max_depth=4,\n",
    "                max_delta_step=1,\n",
    "                learning_rate=0.02,\n",
    "                grow_policy='depthwise',\n",
    "                gamma=0.0,\n",
    "                colsample_bytree=0.3,\n",
    "                colsample_bylevel=0.6,\n",
    "            )\n",
    "        else:\n",
    "            self.featurizer = PrecomputedFeaturizerMixed(SOLVENT_TABLE)\n",
    "            self.xgb_params = dict(\n",
    "                random_state=42,\n",
    "                objective='reg:squarederror',\n",
    "                tree_method='approx',\n",
    "                subsample=0.5,\n",
    "                reg_lambda=0.6,\n",
    "                reg_alpha=0.0,\n",
    "                n_estimators=1000,\n",
    "                min_child_weight=1,\n",
    "                max_depth=4,\n",
    "                max_delta_step=1,\n",
    "                learning_rate=0.02,\n",
    "                grow_policy='lossguide',\n",
    "                gamma=0.0,\n",
    "                colsample_bytree=0.3,\n",
    "                colsample_bylevel=0.6,\n",
    "            )\n",
    "        \n",
    "        self.models = None\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        X_tensor = self.featurizer.featurize(train_X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        Y_np = train_Y.values\n",
    "        \n",
    "        self.models = []\n",
    "        for t in range(Y_np.shape[1]):\n",
    "            model = xgb.XGBRegressor(**self.xgb_params)\n",
    "            model.fit(X_np, Y_np[:, t])\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_tensor = self.featurizer.featurize(X)\n",
    "        X_np = X_tensor.numpy()\n",
    "        preds = np.column_stack([m.predict(X_np) for m in self.models])\n",
    "        preds = multi_target_normalize(preds)\n",
    "        return torch.tensor(preds, dtype=torch.double)\n",
    "\n",
    "print('XGBoost model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Model (from Ens Model kernel)\n",
    "class EnsembleModel:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_mode = data\n",
    "        \n",
    "        # Different weights for single vs full (from Ens Model kernel)\n",
    "        if data == 'single':\n",
    "            cat_weight = 7.0\n",
    "            xgb_weight = 6.0\n",
    "        else:\n",
    "            cat_weight = 1.0\n",
    "            xgb_weight = 2.0\n",
    "        \n",
    "        w_sum = cat_weight + xgb_weight\n",
    "        self.cat_weight = cat_weight / w_sum\n",
    "        self.xgb_weight = xgb_weight / w_sum\n",
    "        \n",
    "        self.cat_model = CatBoostModel(data=data)\n",
    "        self.xgb_model = XGBModel(data=data)\n",
    "    \n",
    "    def train_model(self, train_X, train_Y):\n",
    "        self.cat_model.train_model(train_X, train_Y)\n",
    "        self.xgb_model.train_model(train_X, train_Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        cat_pred = self.cat_model.predict(X)\n",
    "        xgb_pred = self.xgb_model.predict(X)\n",
    "        out = self.cat_weight * cat_pred + self.xgb_weight * xgb_pred\n",
    "        return out\n",
    "\n",
    "print(f'Ensemble model defined:')\n",
    "print(f'  Single: CatBoost {7/13:.3f}, XGBoost {6/13:.3f}')\n",
    "print(f'  Full: CatBoost {1/3:.3f}, XGBoost {2/3:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Single Solvent CV\n",
    "print('Running Single Solvent CV...')\n",
    "print('='*60)\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "\n",
    "all_preds_single = []\n",
    "all_actuals_single = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    all_preds_single.append(predictions.numpy())\n",
    "    all_actuals_single.append(test_Y.values)\n",
    "\n",
    "all_preds_single = np.vstack(all_preds_single)\n",
    "all_actuals_single = np.vstack(all_actuals_single)\n",
    "mse_single = np.mean((all_preds_single - all_actuals_single) ** 2)\n",
    "print(f'\\nSingle Solvent MSE: {mse_single:.6f} (n={len(all_preds_single)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c502874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Full Data CV (87 folds)\n",
    "print('\\nRunning Full Data CV (87 ramps)...')\n",
    "print('='*60)\n",
    "\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "split_generator = generate_leave_one_ramp_out_splits(X_full, Y_full)\n",
    "\n",
    "all_preds_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=87):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    all_preds_full.append(predictions.numpy())\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "all_preds_full = np.vstack(all_preds_full)\n",
    "all_actuals_full = np.vstack(all_actuals_full)\n",
    "mse_full = np.mean((all_preds_full - all_actuals_full) ** 2)\n",
    "print(f'\\nFull Data MSE: {mse_full:.6f} (n={len(all_preds_full)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53425758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate combined CV score\n",
    "n_single = len(all_preds_single)\n",
    "n_full = len(all_preds_full)\n",
    "\n",
    "weighted_cv = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'FINAL RESULTS')\n",
    "print(f'='*60)\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Weighted Combined MSE: {weighted_cv:.6f}')\n",
    "print(f'\\nBest baseline CV (exp_068): 0.007938')\n",
    "print(f'Improvement: {(0.007938 - weighted_cv) / 0.007938 * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
