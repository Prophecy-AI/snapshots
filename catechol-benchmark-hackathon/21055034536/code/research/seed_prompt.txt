## Current Status
- Best CV score (historical): **0.008298** (exp_030). Best *submitted* baseline: **0.008303** (exp_068) → **LB 0.08774**.
- Latest experiment: **072A0_template_safe** (GP+MLP+LGBM ensemble, template-safe + flip bug fixed) → **CV 0.008621** (not submitted).
- Target: **0.034700** | Current best LB: **0.08774** | Gap to target: **0.05304**

## CV-LB Relationship Analysis (CRITICAL)
Using actual submission history (13 submissions with CV+LB present):
- Linear fit: **LB = 4.34 * CV + 0.05226** (R² = **0.957**)
- Intercept interpretation: Even with extremely strong CV, we still carry a large systematic error term; we must **change the mapping** (reduce intercept), not just improve CV.
- Are all approaches on the same line? **YES (so far)** → treat as **distribution shift / extrapolation** problem.

## Response to Evaluator (REQUIRED)
- Technical verdict was **CONCERNS**. I agree: template compliance + fold/index alignment were a real submission risk, and mixture flip/TTA was a real bug.
- Evaluator’s top priority: **port the A0 model into strict template scaffold and fix flip**. ✅ Done in **072A0_template_safe**:
  - Uses exact template split generators and writes to `/home/submission/submission.csv`.
  - Flip now correctly does **swap(A,B) and pct := 1-pct**, and TTA averages std+flip.
- Key concern remaining: now that submission mechanics are safe, we must spend remaining submissions only on **OOD/intercept reduction layers** (AD shrinkage, uncertainty-aware fallback), not more base-model tweaks.

## Data Understanding
- Reference CV↔LB diagnostics: `exploration/evolver_loop66_analysis.ipynb` … `loop72_analysis.ipynb`.
- Known mixture modeling signal: `exploration/evolver_loop69_analysis.ipynb` showed strong gains from **pairwise+interaction** mixture features vs linear blending (at least for Ridge baseline). This suggests mixture nonlinearity is real and may be part of OOD failure.

## Recommended Approaches (priority order)

### 1) Template-safe Applicability-Domain (AD) Shrinkage Layer (MINIMAL DELTA, INTERCEPT TARGET)
**Goal:** change OOD behavior: when a sample is far from training solvents/ramps, shrink predictions toward a conservative fallback.

Implement a new model class (keep last 3 template cells unchanged except model line):
- Base model: reuse **GP+MLP+LGBM** from `072A0_template_safe` (so we keep strong ID accuracy).
- Add an AD module in `predict()`:
  1. Build a **distance feature space** using solvent descriptors only (recommended):
     - For `data='single'`: use Spange+DRFP(nzvar)+ACS PCA for SOLVENT NAME.
     - For `data='full'`: use blended descriptors for A/B with pct (same blending as base featurizer) **but distance should ignore kinetics** to focus on solvent similarity; append pct.
  2. Fit `StandardScaler` on train-distance-features; fit `NearestNeighbors(k=10)`.
  3. Calibrate **alpha(distance)** *within each fold* (no leakage):
     - Inside `train_model`, split the fold’s training set into `train_inner` and `cal_inner` by **group**:
       - single: group = solvent name
       - full: group = (SOLVENT A NAME, SOLVENT B NAME)
     - Train base ensemble on `train_inner` only.
     - On `cal_inner`, compute distance-to-`train_inner` and base predictions.
     - Choose alpha by a simple grid search on cal_inner to minimize MSE of:
       - `pred_shrunk = (1-alpha)*pred + alpha*mu`, where `mu` is the per-target mean label on `train_inner`.
       - Do this **per distance bin** (e.g., 10 quantile bins), then fit a monotone `IsotonicRegression` mapping `distance -> alpha` (clip to [0,1]).
     - Refit base ensemble on **full fold training data** (train_inner + cal_inner) for final use.
     - Store: scaler, knn index, isotonic model, and `mu_train`.
  4. In `predict`, compute distance d; alpha = iso(d); output shrunk blend.
  5. Keep current clipping to [0,1]. Optionally add conservative quantile clip to [q01,q99] of train labels.

**Why this is priority:** This directly targets **intercept reduction** by being conservative on OOD ramps/solvents. It is a true “distribution-shift-aware” change, not CV optimization.

**Deliverable:** new experiment folder `experiments/073_AD_shrinkage_template_safe/` created by copying `072A0_template_safe` scaffold, changing only Cell 5 definitions, and updating the model line to `model = GPMLPLGBM_ADShrinkage(...)`.

**After local CV computed:** **SUBMIT** this model (uses 1 submission). Even if CV is slightly worse than 0.0083, it’s acceptable if LB improves (we need to break the line).

### 2) kNN Blending (alternative to mean shrinkage; more adaptive)
If mean-shrinkage doesn’t move LB, try a second variant (can reuse same distance features):
- For each query, compute kNN among training points in distance space and compute `y_knn = weighted_avg(y_train_neighbors)`.
- Blend: `pred = (1-w)*base_pred + w*y_knn` where `w` increases with distance (or decreases with similarity).
This is often stronger than shrinking to global mean because it uses “local analogs” instead of a constant.

### 3) Nonlinear Mixture Features (Redlich–Kister style) + AD module
Only after we have one AD submission result, consider adding a controlled nonlinear mixture featurization delta:
- Add cross/interactions like `pct*(1-pct)`, `(A_desc - B_desc)^2`, and low-rank bilinear interactions on Spange/ACS PCA.
- Keep it as a single isolated change so we can attribute any LB shift.

## What NOT to Try
- No more pure tabular/ensemble weight tweaks, extra seeds, or hyperparameter sweeps of the base GP/MLP/LGBM (forbidden by gap-to-target and it won’t change intercept).
- Do not reintroduce non-template split generators or write submission to wrong path.
- Do not claim “impossible” based on the linear fit; instead, use it as motivation to **change the relationship**.

## Validation Notes (MANDATORY)
- Use the template’s exact split generators (already in the scaffold).
- **Verify model class consistency**: the model used in CV template cells must match the submission cells exactly (same class name).
- For AD calibration: all calibration must be performed strictly within training folds (train_inner/cal_inner split by groups). Fix all random seeds.
- Log CV overall + single/full separately.

## Submission Plan
- Submit the first AD-shrinkage model after local CV is computed and sanity-checked.
- If LB improves vs 0.08774, iterate with kNN-blending variant (second submission).