## Current Status
- Best CV score: **0.00830** from **exp_030** (GP+MLP+LGBM ensemble)
- Best LB score: **0.0877** (exp_030 / exp_068)
- Target: **0.034700** | Gap to target: **0.0530**
- Submission hygiene: exp_069 shows **non-compliance / invalid LB (0.0000)** risk.

## CV-LB Relationship Analysis (CRITICAL)
- Linear fit from prior loops: **LB ≈ 4.31 × CV + 0.0525** (R² ≈ 0.95)
- Interpretation: there is a **structural extrapolation/intercept gap**; improving tabular CV alone won’t solve it.
- Goal of next experiments: **change the relationship** by improving OOD behavior (reduce effective intercept), even if CV stays similar.

## Response to Evaluator
- Technical verdict was **CONCERNS**. Agree: template compliance + experiment isolation is now the blocking issue.
- Evaluator’s top priority: **start from exact template (exp_053/exp_065) and add only a minimal, capped AD shrinkage layer**. Agree.
- Key concerns raised and how we address them:
  1) **Non-template split/output path** → Next notebook MUST be copied from `experiments/065_clean_exp030_template/clean_submit_exp030.ipynb` (or `experiments/053_exact_template/exact_template.ipynb`). Do **not** edit split generators or the last 3 cells besides the `model = ...` line. Ensure the final cell writes to `/home/submission/submission.csv` and there are **no cells after it**.
  2) **Bundled changes (pair features + AD + base drift)** → Next experiment is **AD-only** on the exact exp_030 feature pipeline (no pair-interaction expansion).
  3) **α(d) learned via proxy may over-shrink + is slow** → Replace with a **safe-by-construction, capped, thresholded α(d)** (no isotonic, no inner 5-fold training). This is faster and reduces miscalibration risk.

## Data Understanding (new evidence)
- From `exploration/evolver_loop70_analysis.ipynb`: nearest-neighbor solvent distance in (Spange+ACS) descriptor space is strongly predictive of leave-one-solvent-out error (corr ≈ **0.72**). **Cyrene** is the farthest and worst-error solvent. This supports an AD gate, but it must be **do-no-harm**: only activate for the highest-distance tail.

## Recommended Approaches (priority order)
### 1) A0 Control: exact exp_030 reproduction in a compliant notebook
- Copy `experiments/065_clean_exp030_template/clean_submit_exp030.ipynb` into a new experiment folder (e.g., `experiments/070A0_exp030_control/`).
- Run end-to-end CV (single + full) to confirm overall MSE matches ~0.0083.
- This is the reference baseline for all AD comparisons.

### 2) A1 Safe AD Shrinkage (AD-only, no new mixture features)
Implement a new model class (defined **before** the last 3 template cells) and then set `model = ...` to it.

**Design: Safe-by-construction AD wrapper around exp_030 ensemble**
- Base predictor: **exact exp_030 GP+MLP+LGBM ensemble** (no hyperparam drift).
- Baseline/fallback predictor: **cheap KNN regressor** trained inside each outer fold.
  - Features for KNN (low-dim, stable): kinetics + (Spange+ACS)
  - For mixtures: use **blend** descriptor only: `(1-pct)*A + pct*B` (no diff/prod).
  - KNN params: `n_neighbors=10` (or 7), `weights='distance'`.
- Distance-to-training: compute Euclidean distances in **scaled** solvent descriptor space.
  - Single: distance on solvent (Spange+ACS).
  - Mixture: distance on blend descriptor.
- α(d) gating (NO learning):
  - Compute d(x) = mean distance to k nearest training points (k=5) in the scaled descriptor space.
  - Compute thresholds from **training fold** distances: `q80 = quantile(d_train, 0.80)`, `q95 = quantile(d_train, 0.95)`.
  - Define capped ramp:
    - if d <= q80: α = 0
    - if q80 < d < q95: α = α_max * (d - q80)/(q95 - q80)
    - if d >= q95: α = α_max
  - Use small caps to avoid harming in-distribution accuracy:
    - Start with **α_max = 0.25** (shared) OR per-target `[0.20, 0.25, 0.30]`.
- Final prediction: `pred = (1-α)*base_pred + α*knn_pred`, clipped to [0,1].

**Success criteria (local CV):**
- Overall CV should stay near exp_030 (do-no-harm): regression > ~0.0005 is a red flag.
- If CV is similar but we believe OOD behavior is improved, we’ll submit to test whether LB intercept shifts.

### 3) If A1 is stable, consider a tiny ablation
- Compare α_max = 0.0 (baseline), 0.15, 0.25, 0.35 **without extra seeds**. Pick the best single setting globally (same across folds).
- This is not “optimization-heavy”; it’s a minimal safety sweep.

## What NOT to Try (this loop)
- No pair-interaction feature expansion yet (diff/prod/pct2 blocks) — too confounded.
- No inner 5-fold isotonic α(d) learning — too slow + high miscalibration risk.
- No multi-seed bagging / weight tuning.
- No changes to split generators or last 3 template cells beyond the model line.

## Validation Notes
- Use the exact template CV generators (`generate_leave_one_out_splits`, `generate_leave_one_ramp_out_splits`) from the copied notebook.
- Report **overall MSE over all held-out rows** (consistent with evaluator recomputation).
- Before logging, verify:
  1) The model class used in CV is the same class instantiated in BOTH template cells (single & full).
  2) Notebook ends with the 3 template cells and writes to `/home/submission/submission.csv`.

## Submission Plan (use submissions carefully)
- Submit only if A1 CV is within ~0.0003–0.0005 of exp_030 and runtime is stable.
- Goal of submission: check if the method **breaks the CV→LB line** (lower LB at similar CV).