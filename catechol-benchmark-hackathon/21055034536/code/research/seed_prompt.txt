## Current Status
- Best CV score: **0.00810** (exp_049/050/053 family; LB pending)
- Best verified LB score: **0.0877** (exp_030 / exp_068 clean template replication)
- Target: **0.0347** | Gap to target: **0.0530**
- Submissions remaining today: **4**

## CV-LB Relationship Analysis (CRITICAL)
- Updated fit using confirmed scored submissions (incl. exp_030/068): **LB ≈ 4.30 · CV + 0.0524** (R² ≈ **0.97**)  
  Source: `exploration/evolver_loop69_lb_feedback.ipynb`
- Intercept interpretation: even with perfect CV, expected LB is ~0.052 → systematic OOD/extrapolation error dominates.
- Are all approaches on the same line? **YES (so far)**.
- Required CV for target under current line: (0.0347 − 0.0524)/4.30 ≈ **−0.0041** → we must **change** the relationship (reduce intercept) via shift-aware prediction.

## Response to Evaluator
- Technical verdict was **TRUSTWORTHY**. We accept that the clean notebook is template-compliant and the model class matches in submission cells.
- Evaluator’s top priority was to submit the clean template to confirm Kaggle accepts it. **Done**: we now have a graded LB (0.0877), confirming the submission pipeline is fixed.
- Key concerns raised and how we address them:
  1) **Mixture representation too linear** → we will add explicit **(A,B,pct) interaction features** (concat/diff/prod) rather than only blended descriptors.
  2) **Distribution shift not integrated into submitted model** → we will implement **cross-fitted applicability-domain (AD) shrinkage** inside the model wrapper (train-fold only) to directly attack the intercept.

## Data Understanding
- We are predicting on **unseen solvents/ramps**; Group/leave-one-solvent style CV still underestimates how hard test solvents are.
- The stable empirical mapping LB ≈ 4.30*CV + 0.052 suggests a large systematic error component on OOD solvents.

## Recommended Approaches (priority order)

### 1) Add an Applicability-Domain (AD) correction layer ON TOP of the current GP+MLP+LGBM ensemble (must be template compliant)
Goal: reduce catastrophic errors on far-OOD points and **break the intercept**.

Implement a new model class (or modify existing) such as:
- `GPMLPLGBMEnsembleAD(data='single'|'full')`

**Core idea (per outer CV fold, using ONLY that fold’s training data):**
1. Train the base ensemble as in exp_030 (GP+MLP+LGBM).
2. Define an **AD distance** for any sample as mean kNN distance to training manifold in a *descriptor space*.
3. Compute a **baseline predictor** that is conservative under shift (kNN regression is ideal because it’s local and stable).
4. Learn a **distance→shrinkage α(d)** mapping **cross-fitted** within the training fold.
5. Final prediction:  
   `pred_final = (1-α(d))*pred_model + α(d)*pred_baseline`
6. Clip predictions to [0,1].

**Concrete, implementable recipe (low-risk, fast):**
- Descriptor space for distance:
  - Use **Spange descriptors** (low-dim) + optionally **ACS-PCA** (low-dim).
  - Avoid raw high-dim DRFP for distance unless you PCA it.
  - Fit `StandardScaler` **inside each outer fold** on training solvents/ramps only.
- Distance computation:
  - `NearestNeighbors(n_neighbors=5, metric='euclidean')` on scaled descriptor vectors.
  - `d(x) = mean(knn_distances)`.
- Baseline predictor:
  - `KNeighborsRegressor(n_neighbors=10, weights='distance')` on same descriptor space **plus (time,temp, pct)**, trained on the outer fold training rows.
  - For single-solvent, features can be `[time,temp] + solvent_desc`.
  - For full/mixture, features can be `[time,temp,pct] + pair_desc`.
- Learn α(d) using ONLY the outer fold’s training rows (cross-fitted):
  - Split the outer fold training rows into **K=5 inner folds** (random KFold, not group-based).
  - For each inner fold:
    - Fit base ensemble on inner-train.
    - Fit baseline KNN on inner-train.
    - Predict inner-val with both models.
    - Compute distances d for inner-val to inner-train.
  - After collecting all inner-val predictions, estimate α as a **monotonic function** of distance:
    - Bin distances into ~10 quantile bins.
    - For each bin, solve least-squares optimal shrinkage:
      `alpha_bin = clip( sum((y-m)*(b-m)) / sum((b-m)^2), 0, 1 )`  where m=model_pred, b=baseline_pred.
    - Enforce monotonicity with `IsotonicRegression(increasing=True)` mapping `bin_center -> alpha_bin`.
  - At inference, compute d for each sample vs outer-train, get α(d) from isotonic reg.

**Why this is the right next move:** it explicitly targets OOD behavior (the intercept) rather than chasing CV.

### 2) Upgrade mixture representation: explicit pair-interaction descriptors (A,B,pct)
Goal: fix structural limitation of linear blending.

For full-data featurization, compute and feed BOTH blended and interaction features:
- `blend = (1-pct)*A + pct*B`
- `diff = A - B`
- `prod = A * B` (elementwise)
- `pct2 = pct*(1-pct)`

Then build final pair descriptor vector like:
`pair_desc = concat([blend, diff, prod, pct2])`

Use this pair_desc:
- for MLP and LGBM feature input
- for distance / kNN baseline feature space

(Keep single-solvent pipeline unchanged except for AD correction.)

### 3) Only if AD correction fails to move LB: re-attempt representation change (GNN / ChemBERTa) with strict class-match + template compliance
Do NOT do this yet; AD correction is faster to iterate and more likely to change the intercept.
If we do pivot again:
- enforce: CV model class == submission cell model class (both single and full)
- keep last 3 cells EXACTLY template except the `model = ...` line.

## What NOT to Try
- More weight tuning, hyperparameter sweeps, or multi-seed bagging on tabular ensemble (optimization forbidden; will stay on same CV-LB line).
- Any notebook that adds cells after the template FINAL CELL.
- Any approach that changes the split generator (no GroupKFold-5 like some public kernels; must stick to template generators).

## Validation Notes
- Use the competition’s official split generators (already in template).
- Report CV overall + single/full components.
- Track whether AD correction changes the CV-LB relationship: we care about **same CV but lower LB**.
- Implementation hygiene:
  - Keep all unsupervised transforms (scalers/PCA) fitted **inside each fold** (train-only) to avoid any preprocessing concerns.
  - Ensure `model = GPMLPLGBMEnsembleAD(data='single')` and `model = GPMLPLGBMEnsembleAD(data='full')` appear in the last 3 template cells.

## Next Experiment Definition (do this now)
- Create new experiment notebook based on the clean exp_030 template submission notebook.
- Add the AD correction layer + mixture interaction features.
- Name the experiment clearly (e.g., `exp_069_ad_shrinkage_pair_features`).
- Do NOT submit until local CV run completes and the notebook ends exactly with the template final cell.
