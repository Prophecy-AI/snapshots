{
  "query": "How do Kaggle competition solutions handle distribution shift where test groups are unseen categories (e.g., leave-one-group-out), using applicability domain, nearest-neighbor blending, or uncertainty-aware prediction calibration?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**How Kaggle solutions typically cope with a leave\u2011one\u2011group\u2011out shift\u2014using an applicability\u2011domain check, nearest\u2011neighbor blending, and uncertainty\u2011aware calibration**\u202f([Deep Ensembles](https://arxiv.org/abs/1612.01474)\u202f|\u202f[Unified Uncertainty Calibration](https://arxiv.org/abs/2310.01202)\u202f|\u202f[Distance\u2011Awareness](https://arxiv.org/abs/2205.00403)\u202f|\u202f[Learn\u2011then\u2011Test](https://arxiv.org/abs/2110.01052)\u202f|\u202f[OOD\u2011Calibrated Inference](https://arxiv.org/abs/2201.13001)\u202f|\u202f[Dataset\u2011Shift Benchmark](https://proceedings.neurips.cc/paper/9547))  \n\n1. **Create a realistic validation split** \u2013 use a *leave\u2011one\u2011group\u2011out* (LOGO) split so each validation fold contains categories (or groups) that never appear in the training folds. This mimics the unseen\u2011group test set and lets you measure both accuracy and calibration on OOD data.  \n\n2. **Train a base predictor with an applicability\u2011domain signal** \u2013 augment the model with a distance\u2011aware head (e.g., a kernel\u2011density or Mahalanobis distance layer) that learns how far a new example lies from the training manifold\u202f([Distance\u2011Awareness](https://arxiv.org/abs/2205.00403)). The raw distance can later be used to flag out\u2011of\u2011distribution (OOD) points.  \n\n3. **Build a nearest\u2011neighbor (k\u2011NN) model on the learned embeddings** \u2013 after training, store the penultimate\u2011layer embeddings of all training samples. At inference, retrieve the *k* closest neighbors, compute similarity scores, and blend the base model\u2019s prediction with the k\u2011NN majority vote, weighting each component by the similarity (higher similarity\u202f\u2192\u202fmore weight). This \u201cnearest\u2011neighbor blending\u201d directly leverages the applicability domain to adapt predictions for unseen groups.  \n\n4. **Generate calibrated uncertainty estimates** \u2013  \n   * a. **Deep ensembles**: train several independently initialized copies of the base model and average their softmax outputs; the variance across ensemble members serves as a predictive uncertainty\u202f([Deep Ensembles](https://arxiv.org/abs/1612.01474)).  \n   * b. **Post\u2011hoc calibration**: apply temperature scaling or the *Unified Uncertainty Calibration* method to align predicted confidences with empirical accuracy, especially on the LOGO validation folds\u202f([Unified Uncertainty Calibration](https://arxiv.org/abs/2310.01202)).  \n\n5. **Apply uncertainty\u2011aware rejection or adjustment** \u2013 if the distance\u2011aware score or ensemble variance exceeds a pre\u2011defined threshold, either (i) abstain from predicting (reject) or (ii) shrink the predicted probability toward a uniform distribution. The *Learn\u2011then\u2011Test* framework provides a formal way to set such thresholds while controlling risk\u202f([Learn\u2011then\u2011Test](https://arxiv.org/abs/2110.01052)).  \n\n6. **Tune blending weights and rejection thresholds on LOGO folds** \u2013 use the validation results to optimize (a) the weight given to the k\u2011NN component versus the deep\u2011ensemble output, and (b) the uncertainty threshold that balances coverage and calibration. Monitoring metrics from the OOD\u2011shift benchmark (e.g., expected calibration error under shift) ensures the solution generalizes to truly unseen groups\u202f([Dataset\u2011Shift Benchmark](https://proceedings.neurips.cc/paper/9547)).  \n\n7. **Deploy the final pipeline** \u2013 at test time, for each sample:  \n   1. compute its embedding,  \n   2. obtain the distance\u2011aware applicability score,  \n   3. retrieve nearest neighbors and blend predictions,  \n   4. aggregate ensemble outputs,  \n   5. apply the calibrated uncertainty check and either output the blended prediction or reject/adjust it.  \n\nFollowing these steps lets Kaggle competitors robustly handle distribution shift where test groups are unseen categories, leveraging applicability\u2011domain filtering, similarity\u2011based blending, and rigorously calibrated uncertainty.",
      "url": ""
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[2310.01202] Unified Uncertainty Calibration\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2310.01202\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2310.01202**(stat)\n[Submitted on 2 Oct 2023 ([v1](https://arxiv.org/abs/2310.01202v1)), last revised 18 Jan 2024 (this version, v2)]\n# Title:Unified Uncertainty Calibration\nAuthors:[Kamalika Chaudhuri](https://arxiv.org/search/stat?searchtype=author&amp;query=Chaudhuri,+K),[David Lopez-Paz](https://arxiv.org/search/stat?searchtype=author&amp;query=Lopez-Paz,+D)\nView a PDF of the paper titled Unified Uncertainty Calibration, by Kamalika Chaudhuri and David Lopez-Paz\n[View PDF](https://arxiv.org/pdf/2310.01202)[HTML (experimental)](https://arxiv.org/html/2310.01202v2)> > Abstract:\n> To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don&#39;t know&#39;&#39; when facing test examples that are difficult or fall outside of the training [> this http URL\n](http://classes.The)> ubiquitous strategy to predict under uncertainty is the simplistic \\emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify [> this http URL\n](http://otherwise.Unfortunately)> , this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \\emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks. Our code is available at: [> this https URL\n](https://github.com/facebookresearch/UnifiedUncertaintyCalibration)> Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:2310.01202](https://arxiv.org/abs/2310.01202)[stat.ML]|\n|(or[arXiv:2310.01202v2](https://arxiv.org/abs/2310.01202v2)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2310.01202](https://doi.org/10.48550/arXiv.2310.01202)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Kamalika Chaudhuri [[view email](https://arxiv.org/show-email/297ad97f/2310.01202)]\n**[[v1]](https://arxiv.org/abs/2310.01202v1)**Mon, 2 Oct 2023 13:42:36 UTC (1,650 KB)\n**[v2]**Thu, 18 Jan 2024 22:48:17 UTC (1,653 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Unified Uncertainty Calibration, by Kamalika Chaudhuri and David Lopez-Paz\n* [View PDF](https://arxiv.org/pdf/2310.01202)\n* [HTML (experimental)](https://arxiv.org/html/2310.01202v2)\n* [TeX Source](https://arxiv.org/src/2310.01202)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2310.01202&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2310.01202&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2023-10](https://arxiv.org/list/stat.ML/2023-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2310.01202?context=cs)\n[cs.LG](https://arxiv.org/abs/2310.01202?context=cs.LG)\n[stat](https://arxiv.org/abs/2310.01202?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.01202)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.01202)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.01202)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2310.01202&amp;description=Unified Uncertainty Calibration>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2310.01202&amp;title=Unified Uncertainty Calibration>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2310.01202)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2310.01202"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2110.01052] Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2110.01052\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2110.01052**(cs)\n[Submitted on 3 Oct 2021 ([v1](https://arxiv.org/abs/2110.01052v1)), last revised 29 Sep 2022 (this version, v5)]\n# Title:Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control\nAuthors:[Anastasios N. Angelopoulos](https://arxiv.org/search/cs?searchtype=author&amp;query=Angelopoulos,+A+N),[Stephen Bates](https://arxiv.org/search/cs?searchtype=author&amp;query=Bates,+S),[Emmanuel J. Cand\u00e8s](https://arxiv.org/search/cs?searchtype=author&amp;query=Cand\u00e8s,+E+J),[Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&amp;query=Jordan,+M+I),[Lihua Lei](https://arxiv.org/search/cs?searchtype=author&amp;query=Lei,+L)\nView a PDF of the paper titled Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control, by Anastasios N. Angelopoulos and Stephen Bates and Emmanuel J. Cand\\\\`es and Michael I. Jordan and Lihua Lei\n[View PDF](https://arxiv.org/pdf/2110.01052)> > Abstract:\n> We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data. Comments:|Code available at[this https URL](https://github.com/aangelopoulos/ltt)|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Methodology (stat.ME); Machine Learning (stat.ML)|\nCite as:|[arXiv:2110.01052](https://arxiv.org/abs/2110.01052)[cs.LG]|\n|(or[arXiv:2110.01052v5](https://arxiv.org/abs/2110.01052v5)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2110.01052](https://doi.org/10.48550/arXiv.2110.01052)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Anastasios Angelopoulos [[view email](https://arxiv.org/show-email/71cc22cb/2110.01052)]\n**[[v1]](https://arxiv.org/abs/2110.01052v1)**Sun, 3 Oct 2021 17:42:03 UTC (23,600 KB)\n**[[v2]](https://arxiv.org/abs/2110.01052v2)**Thu, 14 Oct 2021 17:04:46 UTC (23,690 KB)\n**[[v3]](https://arxiv.org/abs/2110.01052v3)**Thu, 11 Nov 2021 16:20:26 UTC (24,114 KB)\n**[[v4]](https://arxiv.org/abs/2110.01052v4)**Wed, 27 Apr 2022 22:56:54 UTC (11,824 KB)\n**[v5]**Thu, 29 Sep 2022 23:51:57 UTC (3,868 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control, by Anastasios N. Angelopoulos and Stephen Bates and Emmanuel J. Cand\\\\`es and Michael I. Jordan and Lihua Lei\n* [View PDF](https://arxiv.org/pdf/2110.01052)\n* [TeX Source](https://arxiv.org/src/2110.01052)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2110.01052&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2110.01052&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-10](https://arxiv.org/list/cs.LG/2021-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2110.01052?context=cs)\n[cs.AI](https://arxiv.org/abs/2110.01052?context=cs.AI)\n[cs.CV](https://arxiv.org/abs/2110.01052?context=cs.CV)\n[stat](https://arxiv.org/abs/2110.01052?context=stat)\n[stat.ME](https://arxiv.org/abs/2110.01052?context=stat.ME)\n[stat.ML](https://arxiv.org/abs/2110.01052?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.01052)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.01052)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.01052)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-01052)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-01052)\n[Emmanuel J. Cand\u00e8s](<https://dblp.uni-trier.de/search/author?author=Emmanuel J. Cand\u00e8s>)\n[Michael I. Jordan](<https://dblp.uni-trier.de/search/author?author=Michael I. Jordan>)\n[Lihua Lei](<https://dblp.uni-trier.de/search/author?author=Lihua Lei>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2110.01052&amp;description=Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2110.01052&amp;title=Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/do...",
      "url": "https://arxiv.org/abs/2110.01052"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2205.00403] A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2205.00403\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2205.00403**(cs)\n[Submitted on 1 May 2022 ([v1](https://arxiv.org/abs/2205.00403v1)), last revised 30 Dec 2022 (this version, v2)]\n# Title:A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness\nAuthors:[Jeremiah Zhe Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J+Z),[Shreyas Padhy](https://arxiv.org/search/cs?searchtype=author&amp;query=Padhy,+S),[Jie Ren](https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+J),[Zi Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z),[Yeming Wen](https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+Y),[Ghassen Jerfel](https://arxiv.org/search/cs?searchtype=author&amp;query=Jerfel,+G),[Zack Nado](https://arxiv.org/search/cs?searchtype=author&amp;query=Nado,+Z),[Jasper Snoek](https://arxiv.org/search/cs?searchtype=author&amp;query=Snoek,+J),[Dustin Tran](https://arxiv.org/search/cs?searchtype=author&amp;query=Tran,+D),[Balaji Lakshminarayanan](https://arxiv.org/search/cs?searchtype=author&amp;query=Lakshminarayanan,+B)\nView a PDF of the paper titled A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness, by Jeremiah Zhe Liu and 9 other authors\n[View PDF](https://arxiv.org/pdf/2205.00403)> > Abstract:\n> Accurate uncertainty quantification is a major challenge in deep learning, as neural networks can make overconfident errors and assign high confidence predictions to out-of-distribution (OOD) inputs. The most popular approaches to estimate predictive uncertainty in deep learning are methods that combine predictions from multiple neural networks, such as Bayesian neural networks (BNNs) and deep ensembles. However their practicality in real-time, industrial-scale applications are limited due to the high memory and computational cost. Furthermore, ensembles and BNNs do not necessarily fix all the issues with the underlying member networks. In this work, we study principled approaches to improve uncertainty property of a single network, based on a single, deterministic representation. By formalizing the uncertainty quantification as a minimax learning problem, we first identify distance awareness, i.e., the model&#39;s ability to quantify the distance of a testing example from the training data, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs with two simple changes: (1) applying spectral normalization to hidden weights to enforce bi-Lipschitz smoothness in representations and (2) replacing the last output layer with a Gaussian process layer. On a suite of vision and language understanding benchmarks, SNGP outperforms other single-model approaches in prediction, calibration and out-of-domain detection. Furthermore, SNGP provides complementary benefits to popular techniques such as deep ensembles and data augmentation, making it a simple and scalable building block for probabilistic deep learning. Code is open-sourced at [> this https URL\n](https://github.com/google/uncertainty-baselines)> Comments:|arXiv admin note: text overlap with[arXiv:2006.10108](https://arxiv.org/abs/2006.10108)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2205.00403](https://arxiv.org/abs/2205.00403)[cs.LG]|\n|(or[arXiv:2205.00403v2](https://arxiv.org/abs/2205.00403v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2205.00403](https://doi.org/10.48550/arXiv.2205.00403)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jeremiah Zhe Liu [[view email](https://arxiv.org/show-email/f3944400/2205.00403)]\n**[[v1]](https://arxiv.org/abs/2205.00403v1)**Sun, 1 May 2022 05:46:13 UTC (2,007 KB)\n**[v2]**Fri, 30 Dec 2022 18:31:05 UTC (5,604 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness, by Jeremiah Zhe Liu and 9 other authors\n* [View PDF](https://arxiv.org/pdf/2205.00403)\n* [TeX Source](https://arxiv.org/src/2205.00403)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2205.00403&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2205.00403&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-05](https://arxiv.org/list/cs.LG/2022-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2205.00403?context=cs)\n[stat](https://arxiv.org/abs/2205.00403?context=stat)\n[stat.ML](https://arxiv.org/abs/2205.00403?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.00403)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.00403)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.00403)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2205.00403&amp;description=A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2205.00403&amp;title=A Simple Approach to Improve Single-Model Deep Uncertainty via Distance-Awareness>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with...",
      "url": "https://arxiv.org/abs/2205.00403"
    },
    {
      "title": "Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2201.13001** (cs)\n\n\\[Submitted on 31 Jan 2022 ( [v1](https://arxiv.org/abs/2201.13001v1)), last revised 7 Jun 2024 (this version, v8)\\]\n\n# Title:Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference\n\nAuthors: [Jayanta Dey](https://arxiv.org/search/cs?searchtype=author&query=Dey,+J), [Haoyin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Will LeVine](https://arxiv.org/search/cs?searchtype=author&query=LeVine,+W), [Ashwin De Silva](https://arxiv.org/search/cs?searchtype=author&query=De+Silva,+A), [Tyler M. Tomita](https://arxiv.org/search/cs?searchtype=author&query=Tomita,+T+M), [Ali Geisa](https://arxiv.org/search/cs?searchtype=author&query=Geisa,+A), [Tiffany Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu,+T), [Jacob Desman](https://arxiv.org/search/cs?searchtype=author&query=Desman,+J), [Joshua T. Vogelstein](https://arxiv.org/search/cs?searchtype=author&query=Vogelstein,+J+T)\n\nView a PDF of the paper titled Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference, by Jayanta Dey and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2201.13001) [HTML (experimental)](https://arxiv.org/html/2201.13001v8)\n\n> Abstract:Deep discriminative approaches like random forests and deep neural networks have recently found applications in many important real-world scenarios. However, deploying these learning algorithms in safety-critical applications raises concerns, particularly when it comes to ensuring confidence calibration for both in-distribution and out-of-distribution data points. Many popular methods for in-distribution (ID) calibration, such as isotonic and Platt's sigmoidal regression, exhibit excellent ID calibration performance. However, these methods are not calibrated for the entire feature space, leading to overconfidence in the case of out-of-distribution (OOD) samples. On the other end of the spectrum, existing out-of-distribution (OOD) calibration methods generally exhibit poor in-distribution (ID) calibration. In this paper, we address ID and OOD calibration problems jointly. We leveraged the fact that deep models, including both random forests and deep-nets, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as partitioning rules of the feature space. We replace the affine function in each polytope populated by the training data with a Gaussian kernel. Our experiments on both tabular and vision benchmarks show that the proposed approaches obtain well-calibrated posteriors while mostly preserving or improving the classification accuracy of the original algorithm for ID region, and extrapolate beyond the training data to handle OOD inputs appropriately.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2201.13001](https://arxiv.org/abs/2201.13001) \\[cs.LG\\] |\n|  | (or [arXiv:2201.13001v8](https://arxiv.org/abs/2201.13001v8) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2201.13001](https://doi.org/10.48550/arXiv.2201.13001)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanta Dey \\[ [view email](https://arxiv.org/show-email/28a14700/2201.13001)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2201.13001v1)**\nMon, 31 Jan 2022 05:07:16 UTC (16,278 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2201.13001v2)**\nSun, 6 Feb 2022 14:38:23 UTC (16,262 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2201.13001v3)**\nMon, 14 Feb 2022 14:14:47 UTC (16,263 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/2201.13001v4)**\nFri, 25 Mar 2022 03:41:23 UTC (16,263 KB)\n\n**[\\[v5\\]](https://arxiv.org/abs/2201.13001v5)**\nFri, 19 May 2023 21:15:28 UTC (18,650 KB)\n\n**[\\[v6\\]](https://arxiv.org/abs/2201.13001v6)**\nThu, 19 Oct 2023 02:27:07 UTC (33,996 KB)\n\n**[\\[v7\\]](https://arxiv.org/abs/2201.13001v7)**\nTue, 12 Mar 2024 12:57:20 UTC (33,151 KB)\n\n**\\[v8\\]**\nFri, 7 Jun 2024 17:10:00 UTC (32,931 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference, by Jayanta Dey and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2201.13001)\n- [HTML (experimental)](https://arxiv.org/html/2201.13001v8)\n- [TeX Source](https://arxiv.org/src/2201.13001)\n- [Other Formats](https://arxiv.org/format/2201.13001)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2201.13001&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2201.13001&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-01](https://arxiv.org/list/cs.LG/2022-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2201.13001?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2201.13001?context=cs.AI)\n\n[cs.DS](https://arxiv.org/abs/2201.13001?context=cs.DS)\n\n[q-bio](https://arxiv.org/abs/2201.13001?context=q-bio)\n\n[q-bio.NC](https://arxiv.org/abs/2201.13001?context=q-bio.NC)\n\n[stat](https://arxiv.org/abs/2201.13001?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2201.13001?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2201.13001)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2201.13001)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2201.13001)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2201.html#abs-2201-13001) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2201-13001)\n\n[Jayanta Dey](https://dblp.uni-trier.de/search/author?author=Jayanta%20Dey)\n\n[Leyla Isik](https://dblp.uni-trier.de/search/author?author=Leyla%20Isik)\n\n[Joshua T. Vogelstein](https://dblp.uni-trier.de/search/author?author=Joshua%20T.%20Vogelstein)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2201.13001&description=Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2201.13001&title=Deep Discriminative to Kernel Density Graph for In- and Out-of-distribution Calibrated Inference)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCa...",
      "url": "https://arxiv.org/abs/2201.13001"
    },
    {
      "title": "EXPLOR: Extrapolatory Pseudo-Label Matching for Out-of-distribution Uncertainty Based Rejection",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2406.01825"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2507.01831] Out-of-Distribution Detection Methods Answer the Wrong Questions\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2507.01831\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2507.01831**(cs)\n[Submitted on 2 Jul 2025]\n# Title:Out-of-Distribution Detection Methods Answer the Wrong Questions\nAuthors:[Yucen Lily Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y+L),[Daohan Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+D),[Polina Kirichenko](https://arxiv.org/search/cs?searchtype=author&amp;query=Kirichenko,+P),[Shikai Qiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+S),[Tim G. J. Rudner](https://arxiv.org/search/cs?searchtype=author&amp;query=Rudner,+T+G+J),[C. Bayan Bruss](https://arxiv.org/search/cs?searchtype=author&amp;query=Bruss,+C+B),[Andrew Gordon Wilson](https://arxiv.org/search/cs?searchtype=author&amp;query=Wilson,+A+G)\nView a PDF of the paper titled Out-of-Distribution Detection Methods Answer the Wrong Questions, by Yucen Lily Li and 6 other authors\n[View PDF](https://arxiv.org/pdf/2507.01831)[HTML (experimental)](https://arxiv.org/html/2507.01831v1)> > Abstract:\n> To detect distribution shifts and improve model safety, many out-of-distribution (OOD) detection methods rely on the predictive uncertainty or features of supervised models trained on in-distribution data. In this paper, we critically re-examine this popular family of OOD detection procedures, and we argue that these methods are fundamentally answering the wrong questions for OOD detection. There is no simple fix to this misalignment, since a classifier trained only on in-distribution classes cannot be expected to identify OOD points; for instance, a cat-dog classifier may confidently misclassify an airplane if it contains features that distinguish cats from dogs, despite generally appearing nothing alike. We find that uncertainty-based methods incorrectly conflate high uncertainty with being OOD, while feature-based methods incorrectly conflate far feature-space distance with being OOD. We show how these pathologies manifest as irreducible errors in OOD detection and identify common settings where these methods are ineffective. Additionally, interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, epistemic uncertainty representation, and outlier exposure also fail to address this fundamental misalignment in objectives. We additionally consider unsupervised density estimation and generative models for OOD detection, which we show have their own fundamental limitations. Comments:|Extended version of ICML 2025 paper|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2507.01831](https://arxiv.org/abs/2507.01831)[cs.LG]|\n|(or[arXiv:2507.01831v1](https://arxiv.org/abs/2507.01831v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2507.01831](https://doi.org/10.48550/arXiv.2507.01831)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yucen Li [[view email](https://arxiv.org/show-email/bed9a382/2507.01831)]\n**[v1]**Wed, 2 Jul 2025 15:45:17 UTC (12,222 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Out-of-Distribution Detection Methods Answer the Wrong Questions, by Yucen Lily Li and 6 other authors\n* [View PDF](https://arxiv.org/pdf/2507.01831)\n* [HTML (experimental)](https://arxiv.org/html/2507.01831v1)\n* [TeX Source](https://arxiv.org/src/2507.01831)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2507.01831&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2507.01831&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-07](https://arxiv.org/list/cs.LG/2025-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2507.01831?context=cs)\n[stat](https://arxiv.org/abs/2507.01831?context=stat)\n[stat.ML](https://arxiv.org/abs/2507.01831?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.01831)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.01831)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.01831)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2507.01831&amp;description=Out-of-Distribution Detection Methods Answer the Wrong Questions>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2507.01831&amp;title=Out-of-Distribution Detection Methods Answer the Wrong Questions>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website...",
      "url": "https://arxiv.org/abs/2507.01831"
    },
    {
      "title": "",
      "text": "Simple and Scalable Predictive Uncertainty\nEstimation using Deep Ensembles\nBalaji Lakshminarayanan Alexander Pritzel Charles Blundell\nDeepMind\n{balajiln,apritzel,cblundell}@google.com\nAbstract\nDeep neural networks (NNs) are powerful black box predictors that have recently\nachieved impressive performance on a wide spectrum of tasks. Quantifying pre\u0002dictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian\nNNs, which learn a distribution over weights, are currently the state-of-the-art\nfor estimating predictive uncertainty; however these require significant modifica\u0002tions to the training procedure and are computationally expensive compared to\nstandard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that\nis simple to implement, readily parallelizable, requires very little hyperparameter\ntuning, and yields high quality predictive uncertainty estimates. Through a series\nof experiments on classification and regression benchmarks, we demonstrate that\nour method produces well-calibrated uncertainty estimates which are as good or\nbetter than approximate Bayesian NNs. To assess robustness to dataset shift, we\nevaluate the predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty on\nout-of-distribution examples. We demonstrate the scalability of our method by\nevaluating predictive uncertainty estimates on ImageNet.\n1 Introduction\nDeep neural networks (NNs) have achieved state-of-the-art performance on a wide variety of machine\nlearning tasks [35] and are becoming increasingly popular in domains such as computer vision\n[32], speech recognition [25], natural language processing [42], and bioinformatics [2, 61]. Despite\nimpressive accuracies in supervised learning benchmarks, NNs are poor at quantifying predictive\nuncertainty, and tend to produce overconfident predictions. Overconfident incorrect predictions can be\nharmful or offensive [3], hence proper uncertainty quantification is crucial for practical applications.\nEvaluating the quality of predictive uncertainties is challenging as the \u2018ground truth\u2019 uncertainty\nestimates are usually not available. In this work, we shall focus upon two evaluation measures that\nare motivated by practical applications of NNs. Firstly, we shall examine calibration [12, 13], a\nfrequentist notion of uncertainty which measures the discrepancy between subjective forecasts and\n(empirical) long-run frequencies. The quality of calibration can be measured by proper scoring rules\n[17] such as log predictive probabilities and the Brier score [9]. Note that calibration is an orthogonal\nconcern to accuracy: a network\u2019s predictions may be accurate and yet miscalibrated, and vice versa.\nThe second notion of quality of predictive uncertainty we consider concerns generalization of the\npredictive uncertainty to domain shift (also referred to as out-of-distribution examples [23]), that is,\nmeasuring if the network knows what it knows. For example, if a network trained on one dataset is\nevaluated on a completely different dataset, then the network should output high predictive uncertainty\nas inputs from a different dataset would be far away from the training data. Well-calibrated predictions\nthat are robust to model misspecification and dataset shift have a number of important practical uses\n(e.g., weather forecasting, medical diagnosis).\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nThere has been a lot of recent interest in adapting NNs to encompass uncertainty and probabilistic\nmethods. The majority of this work revolves around a Bayesian formalism [4], whereby a prior\ndistribution is specified upon the parameters of a NN and then, given the training data, the posterior\ndistribution over the parameters is computed, which is used to quantify predictive uncertainty.\nSince exact Bayesian inference is computationally intractable for NNs, a variety of approximations\nhave been developed including Laplace approximation [40], Markov chain Monte Carlo (MCMC)\nmethods [46], as well as recent work on variational Bayesian methods [6, 19, 39], assumed density\nfiltering [24], expectation propagation [21, 38] and stochastic gradient MCMC variants such as\nLangevin diffusion methods [30, 59] and Hamiltonian methods [53]. The quality of predictive\nuncertainty obtained using Bayesian NNs crucially depends on (i) the degree of approximation due\nto computational constraints and (ii) if the prior distribution is \u2018correct\u2019, as priors of convenience\ncan lead to unreasonable predictive uncertainties [50]. In practice, Bayesian NNs are often harder\nto implement and computationally slower to train compared to non-Bayesian NNs, which raises\nthe need for a \u2018general purpose solution\u2019 that can deliver high-quality uncertainty estimates and yet\nrequires only minor modifications to the standard training pipeline.\nRecently, Gal and Ghahramani [15] proposed using Monte Carlo dropout (MC-dropout) to estimate\npredictive uncertainty by using Dropout [54] at test time. There has been work on approximate\nBayesian interpretation [15, 29, 41] of dropout. MC-dropout is relatively simple to implement\nleading to its popularity in practice. Interestingly, dropout may also be interpreted as ensemble model\ncombination [54] where the predictions are averaged over an ensemble of NNs (with parameter\nsharing). The ensemble interpretation seems more plausible particularly in the scenario where the\ndropout rates are not tuned based on the training data, since any sensible approximation to the true\nBayesian posterior distribution has to depend on the training data. This interpretation motivates the\ninvestigation of ensembles as an alternative solution for estimating predictive uncertainty.\nIt has long been observed that ensembles of models improve predictive performance (see [14] for a\nreview). However it is not obvious when and why an ensemble of NNs can be expected to produce\ngood uncertainty estimates. Bayesian model averaging (BMA) assumes that the true model lies within\nthe hypothesis class of the prior, and performs soft model selection to find the single best model within\nthe hypothesis class [43]. In contrast, ensembles perform model combination, i.e. they combine the\nmodels to obtain a more powerful model; ensembles can be expected to be better when the true model\ndoes not lie within the hypothesis class. We refer to [11, 43] and [34, \u00a72.5] for related discussions.\nIt is important to note that even exact BMA is not guaranteed be robust to mis-specification with\nrespect to domain shift.\nSummary of contributions: Our contribution in this paper is two fold. First, we describe a simple and\nscalable method for estimating predictive uncertainty estimates from NNs. We argue for training\nprobabilistic NNs (that model predictive distributions) using a proper scoring rule as the training\ncriteria. We additionally investigate the effect of two modifications to the training pipeline, namely\n(i) ensembles and (ii) adversarial training [18] and describe how they can produce smooth predictive\nestimates. Secondly, we propose a series of tasks for evaluating the quality of the predictive uncertainty\nestimates, in terms of calibration and generalization to unknown classes in supervised learning\nproblems. We show that our method significantly outperforms (or matches) MC-dropout. These tasks,\nalong with our simple yet strong baseline, serve as an useful benchmark for comparing predictive\nuncertainty estimates obtained using different Bayesian/non-Bayesian/hybrid methods.\nNovelty and Significance: Ensembles of NNs, or deep ensembles for short, have been successfully\nused to boost predictive performance (e.g. classification accuracy in ImageNet or Kaggle contests)\nand adversarial training has been used to improve robustness to adversarial examples. However, to\nthe best of our knowledge, ours is the first work to investigate their usefulness for predic...",
      "url": "https://papers.neurips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf"
    },
    {
      "title": "",
      "text": "Can You Trust Your Model\u2019s Uncertainty? Evaluating\nPredictive Uncertainty Under Dataset Shift\nYaniv Ovadia\u21e4\nGoogle Research\nyovadia@google.com\nEmily Fertig\u21e4\u2020\nGoogle Research\nemilyaf@google.com\nJie Ren\u2020\nGoogle Research\njjren@google.com\nZachary Nado\nGoogle Research\nznado@google.com\nD Sculley\nGoogle Research\ndsculley@google.com\nSebastian Nowozin\nGoogle Research\nnowozin@google.com\nJoshua V. Dillon\nGoogle Research\njvdillon@google.com\nBalaji Lakshminarayanan\u2021\nDeepMind\nbalajiln@google.com\nJasper Snoek\u2021\nGoogle Research\njsnoek@google.com\nAbstract\nModern machine learning methods including deep learning have achieved great\nsuccess in predictive accuracy for supervised learning tasks, but may still fall short\nin giving useful estimates of their predictive uncertainty. Quantifying uncertainty\nis especially critical in real-world settings, which often involve input distributions\nthat are shifted from the training distribution due to a variety of factors including\nsample bias and non-stationarity. In such settings, well calibrated uncertainty\nestimates convey information about when a model\u2019s output should (or should not)\nbe trusted. Many probabilistic deep learning methods, including Bayesian-and non\u0002Bayesian methods, have been proposed in the literature for quantifying predictive\nuncertainty, but to our knowledge there has not previously been a rigorous large\u0002scale empirical comparison of these methods under dataset shift. We present a large\u0002scale benchmark of existing state-of-the-art methods on classification problems\nand investigate the effect of dataset shift on accuracy and calibration. We find that\ntraditional post-hoc calibration does indeed fall short, as do several other previous\nmethods. However, some methods that marginalize over models give surprisingly\nstrong results across a broad spectrum of tasks.\n1 Introduction\nRecent successes across a variety of domains have led to the widespread deployment of deep\nneural networks (DNNs) in practice. Consequently, the predictive distributions of these models are\nincreasingly being used to make decisions in important applications ranging from machine-learning\naided medical diagnoses from imaging (Esteva et al., 2017) to self-driving cars (Bojarski et al., 2016).\nSuch high-stakes applications require not only point predictions but also accurate quantification\nof predictive uncertainty, i.e. meaningful confidence values in addition to class predictions. With\nsufficient independent labeled samples from a target data distribution, one can estimate how well\n\u21e4Equal contribution\n\u2020AI Resident\n\u2021Corresponding authors\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\na model\u2019s confidence aligns with its accuracy and adjust the predictions accordingly. However, in\npractice, once a model is deployed the distribution over observed data may shift and eventually be\nvery different from the original training data distribution. Consider, e.g., online services for which the\ndata distribution may change with the time of day, seasonality or popular trends. Indeed, robustness\nunder conditions of distributional shift and out-of-distribution (OOD) inputs is necessary for the\nsafe deployment of machine learning (Amodei et al., 2016). For such settings, calibrated predictive\nuncertainty is important because it enables accurate assessment of risk, allows practitioners to know\nhow accuracy may degrade, and allows a system to abstain from decisions due to low confidence.\nA variety of methods have been developed for quantifying predictive uncertainty in DNNs. Probabilis\u0002tic neural networks such as mixture density networks (MacKay & Gibbs, 1999) capture the inherent\nambiguity in outputs for a given input, also referred to as aleatoric uncertainty (Kendall & Gal, 2017).\nBayesian neural networks learn a posterior distribution over parameters that quantifies parameter\nuncertainty, a type of epistemic uncertainty that can be reduced through the collection of additional\ndata. Popular approximate Bayesian approaches include Laplace approximation (MacKay, 1992),\nvariational inference (Graves, 2011; Blundell et al., 2015), dropout-based variational inference (Gal\n& Ghahramani, 2016; Kingma et al., 2015), expectation propagation Hernandez-Lobato & Adams \u00b4\n(2015) and stochastic gradient MCMC (Welling & Teh, 2011). Non-Bayesian methods include\ntraining multiple probabilistic neural networks with bootstrap or ensembling (Osband et al., 2016;\nLakshminarayanan et al., 2017). Another popular non-Bayesian approach involves re-calibration of\nprobabilities on a held-out validation set through temperature scaling (Platt, 1999), which was shown\nby Guo et al. (2017) to lead to well-calibrated predictions on the i.i.d. test set.\nUsing Distributional Shift to Evaluate Predictive Uncertainty While previous work has evaluated\nthe quality of predictive uncertainty on OOD inputs (Lakshminarayanan et al., 2017), there has not\nto our knowledge been a comprehensive evaluation of uncertainty estimates from different methods\nunder dataset shift. Indeed, we suggest that effective evaluation of predictive uncertainty is most\nmeaningful under conditions of distributional shift. One reason for this is that post-hoc calibration\ngives good results in independent and identically distributed (i.i.d.) regimes, but can fail under even a\nmild shift in the input data. And in real world applications, as described above, distributional shift is\nwidely prevalent. Understanding questions of risk, uncertainty, and trust in a model\u2019s output becomes\nincreasingly critical as shift from the original training data grows larger.\nContributions In the spirit of calls for more rigorous understanding of existing methods (Lipton\n& Steinhardt, 2018; Sculley et al., 2018; Rahimi & Recht, 2017), this paper provides a benchmark\nfor evaluating uncertainty that focuses not only on the i.i.d. setting but also uncertainty under\ndistributional shift. We present a large-scale evaluation of popular approaches in probabilistic deep\nlearning, focusing on methods that operate well in large-scale settings, and evaluate them on a diverse\nrange of classification benchmarks across image, text, and categorical modalities. We use these\nexperiments to evaluate the following questions:\n\u2022 How trustworthy are the uncertainty estimates of different methods under dataset shift?\n\u2022 Does calibration in the i.i.d. setting translate to calibration under dataset shift?\n\u2022 How do uncertainty and accuracy of different methods co-vary under dataset shift? Are there\nmethods that consistently do well in this regime?\nIn addition to answering the questions above, our code is made available open-source along with our\nmodel predictions such that researchers can easily evaluate their approaches on these benchmarks 4.\n2 Background\nNotation and Problem Setup Let x 2 Rd represent a set of d-dimensional features and y 2\n{1,...,k} denote corresponding labels (targets) for k-class classification. We assume that a training\ndataset D consists of N i.i.d.samples D = {(xn, yn)}N\nn=1.\nLet p\u21e4(x, y) denote the true distribution (unknown, observed only through the samples D), also\nreferred to as the data generating process. We focus on classification problems, in which the true\ndistribution is assumed to be a discrete distribution over k classes, and the observed y 2 {1,...,k}\n4\nhttps://github.com/google-research/google-research/tree/master/uq benchmark 2019\n2\nis a sample from the conditional distribution p\u21e4(y|x). We use a neural network to model p\u2713(y|x) and\nestimate the parameters \u2713 using the training dataset. At test time, we evaluate the model predictions\nagainst a test set, sampled from the same distribution as the training dataset. However, here we also\nevaluate the model against OOD inputs sampled from q(x, y) 6= p\u21e4(x, y). In particular, we consider\ntwo kinds of shifts:\n\u2022 shifted versions of the test inputs where the ground truth label belongs to one of the k classes. We\nuse shifts such as corruptions and pert...",
      "url": "https://proceedings.neurips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf"
    }
  ]
}