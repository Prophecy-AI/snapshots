{
  "query": "In the Kaggle catechol-benchmark-hackathon competition, what are the notebook runtime limits and common causes of the error 'Evaluation metric raised an unexpected error'?",
  "results": [
    {
      "title": "ANSWER",
      "text": "720\u202fminutes (12\u202fhours) is the maximum notebook runtime for the catechol\u2011benchmark\u2011hackathon competition\u202fas of the latest Kaggle dataset description\u202f([arxiv.org](https://arxiv.org/html/2506.07619v1)). No recent sources detail the typical reasons for the \u201cEvaluation metric raised an unexpected error\u201d message.",
      "url": ""
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "[Product Launch] Introducing Community Benchmarks - Kaggle",
      "text": "[Product Launch] Introducing Community Benchmarks | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n[\n](https://www.kaggle.com/nicholaskanggoog)\n[Nicholas Kang](https://www.kaggle.com/nicholaskanggoog)\u00b7Posted16 hours agoin[Product Announcements](https://www.kaggle.com/discussions/product-announcements)\n\u00b7Kaggle Staff\narrow\\_drop\\_up40\nmore\\_vert\n# [Product Launch] Introducing Community Benchmarks\nHello Kagglers,\nToday, we\u2019re incredibly excited to launch[**Community Benchmarks**](https://www.kaggle.com/benchmarks?type=community), a new product that lets you \u2013the Kaggle community \u2013build, run, and share your own custom benchmarks for evaluating AI models at no cost.\n![Create a task](https://storage.googleapis.com/kaggle-media/cms/page-images/kaggle-benchmarks/task_creation(spedup).gif)\n# What you can do with Community Benchmarks\nPowered by the[kaggle-benchmarks SDK](https://github.com/Kaggle/kaggle-benchmarks/tree/ci), you can now create your own AI evaluations (\u201ctasks\u201d) and put them together into a collection (\u201cbenchmark\u201d).\n```\n`importkaggle\\_benchmarksaskbench@kbench.task(name=\"simple\\_riddle\")defsolve\\_riddle(llm, riddle:str, answer:str):\"\"\"Asks a riddle and checks for a keyword in the answer.\"\"\"response = llm.prompt(riddle)# Assert that the model's response contains the answer, ignoring case.kbench.assertions.assert\\_contains\\_regex(f\"(?i){answer}\", response, expectation=\"LLM should give the right answer.\")# Execute the tasksolve\\_riddle.run(\nllm=kbench.llm,# Uses the default LLMriddle=\"What gets wetter as it dries?\",\nanswer=\"Towel\",\n)`\ncontent\\_copy\n```\nThe SDK is designed for flexibility with many rich features:\n**Model interaction &amp; multi-turn conversations**\n* **Unified LLM interface**: Free access (within quota limits) to a wide range of frontier models (including Gemini, Claude, Qwen, and DeepSeek) using a consistent API.\n* **Multi-turn conversations**: The SDK handles conversation state, allowing you to append multiple messages (text or multimodal) before prompting the model.\n* **Multimodal capabilities**: Send images and other media to supported multimodal models to evaluate their vision and reasoning capabilities. We currently support text &amp; image inputs, and text &amp; Python objects as outputs.\n**Advanced evaluation framework**\n* **Robust Assertions**: Use a rich set of built-in assertions to validate model outputs or write custom Python-based assertions to enforce specific logic.\n* **LLM-as-a-Judge**: For subjective or complex tasks (like creative writing or code explanation), you can use a secondary \"Judge\" LLM to evaluate the candidate model's output against a set of criteria or a schema.\n* **Dataset evaluation**: Scale evaluations from single prompts to entire datasets. Use the`.evaluate()`method to run a task across a pandas DataFrame and aggregate performance metrics automatically.\n* **Structured Output Enforcement**: Define Pydantic-like schemas to force models to return data in specific JSON formats, ensuring reliability for downstream processing.\n**Agentic features &amp; tool use**\n* **Custom tool use**: Equip models with custom tools and functions, allowing you to interact with external APIs or perform specific actions.\n* **Built-in Python interpreter**: Enable models to execute code in a sandboxed environment.\n* **Interactive game loops**: Implement complex evaluation patterns like \"Game Loops\" where models compete against each other or an environment in real-time.\n**Kaggle integration &amp; sharing**\n* **Seamless dataset access**: Easily pull data from Kaggle Datasets using`kagglehub`for immediate benchmarking.\n* **Publishing benchmarks**: Add multiple tasks into a benchmark and publish it directly on Kaggle. You can also add a citation to formally credit and reference your benchmark in academic papers.\nHere\u2019s what others in the community have created already:\n* [Lemonasso benchmark: Evaluate LLMs on artistic drawing tasks](https://www.kaggle.com/benchmarks/anhoangvo/lemonasso)\n* [Medical &amp; cross-disciplinary benchmark: Evaluate capabilities and safety in a medical context](https://www.kaggle.com/benchmarks/juniam/mcait)\n* [Indonesian social intelligence benchmark: Test LLMs\u2019 cross-cultural competence](https://www.kaggle.com/benchmarks/hanifnoerrofiq/indonesian-social-intelligence)\n* [Cryptanalysis benchmark: Evaluate programmatic reasoning and decoding capabilities](https://www.kaggle.com/benchmarks/hanifnoerrofiq/cipher-frontlines-story-driven-cryptanalysis)\n* [Wastewater treatment plant engineering benchmark: Evaluates LLMs on real-world wastewater treatment plant engineering](https://www.kaggle.com/benchmarks/mehmetisik/wwtp-engineering-benchmark)# Why we launched Community Benchmarks\nAs many developers have discovered, evaluations are hard. As Andrej Karpathy (founding member of OpenAI &amp; ex-Director of AI at Tesla) famously noted:\n> > \u201cGood evals are very difficult to build - at Tesla I probably spent 1/3 of my time on data, 1/3 on evals, and 1/3 on everything else. They have to be comprehensive, representative, of high quality, and measure gradient signal\u201d\n> We launched[Kaggle Benchmarks](https://www.kaggle.com/blog/announcing-kaggle-benchmarks)last year with a goal to democratize access to the world's top research evaluations. By partnering with leading AI labs, we made it possible for anyone to reproduce evaluations like Meta\u2019s MultiLoKo and Google\u2019s FACTS.\nBut AI is evolving faster than ever. Models today don\u2019t just answer questions - they reason, create, collaborate and even surprise us. Measuring their intelligence requires more than a few research labs alone; it requires imagination, curiosity and the creativity of the global community.\nThat\u2019s why we\u2019re extending Kaggle Benchmarks to you \u2013our Kaggle community \u2013and naming it Community Benchmarks.\n# Feature roadmap\nThis launch is just the beginning. We have a rich feature roadmap planned out for Community Benchmarks, including support for more AI models (e.g., we don\u2019t currently support OpenAI), Task &amp; Benchmark versioning, multiple task runs (`pass@k`), and more.\nIf you have more feedback, we\u2019d love to hear it on our[Product Feedback forum](https://www.kaggle.com/discussions/product-feedback?sort=published).\n# Get started today\nReady to build? Try it out for yourself at kaggle.com/benchmarks. We\u2019ll select a few community benchmarks to feature on Kaggle and our social media every week!\nFor more helpful resources, see:\n* [Kaggle Benchmarks guide](https://www.kaggle.com/docs/benchmarks#intro)\n* [Getting started notebook](https://www.kaggle.com/code/nicholaskanggoog/kaggle-benchmarks-getting-started-notebook?scriptVersionId=290215074)\n* [YouTube tutorial for Community Benchmarks](https://www.youtube.com/watch?v=VBlyJJ7PTD8)\n* [Kaggle-benchmarks open source GitHub Repo](https://github.com/Kaggle/kaggle-benchmarks)\n* [Benchmarks cookbook: Guide to advanced features and use cases](https://github.com/Kaggle/kaggle-benchmarks/blob/ci/cookbook.md)\n* [Example tasks: Get inspired with a variety of pre-built tasks](https://github.com/Kaggle/kaggle-benchmarks/tree/ci/documentation/examples)\n* [Kaggle Community Benchmarks NotebookLM](https://notebooklm.google.com/notebook/56661d72-a74b-48cc-a2d0-08a6f7a595e8)\nNick, on behalf of the Kaggle team\n[Artificial Intelligence](https://www.kaggle.com/discussions?tags=12101-Artificial+Intelligence)\n![yay](https://www.kaggle.com/static/images/community/reactions/yay.svg)20\n![love](https://www.kaggle.com/static/images/community/reactions/love.svg)3\n![kaggle](https://www.kaggle.com/static/images/community/reactions/kaggle.svg)17\n![surprise](https://www.kaggle.com/static/images/community/reactions/surprise.svg)1\nPlease[sign in](https://www.kaggle.com/account/login)to reply to this topic.\ncomment\n## 4 Comments\nHotness\n[\n](https://www.kaggle.com/sanjeevthakur2024)\n[### Sanjeev Thakur\n](https://www.kaggl...",
      "url": "https://www.kaggle.com/product-announcements/667898"
    },
    {
      "title": "NeurIPS 2025 San Diego Datasets & Benchmarks",
      "text": "NeurIPS 2025 San Diego Datasets &amp; BenchmarksNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/events/datasets-benchmarks-2025)\n# San Diego Datasets &amp; Benchmarks\n**494 Events\n**\nPoster### [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://neurips.cc/virtual/2025/poster/121603)\nKuicai Dong &middot; CHANG YUJING &middot; Shijie Huang &middot; Yasheng Wang &middot; Ruiming Tang &middot; Yong Liu\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nDocument Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration. Key findings reveal that advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121603)\nPoster### [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://neurips.cc/virtual/2025/poster/121386)\nHyungyung Lee &middot; Geon Choi &middot; Jung-Oh Lee &middot; Hangyul Yoon &middot; Hyuk Hong &middot; Edward Choi\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nRecent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121386)\nPoster### [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://neurips.cc/virtual/2025/poster/121553)\nAtharva Gundawar &middot; Som Sagar &middot; Ransalu Senanayake\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nVision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that is largely unverified. To perform actions reliably, robots must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state like being closed). Despite their ubiquitous use in manipulation, we argue that off-the-shelf VLMs may lack this granular, physically-grounded understanding, as these specific prerequisites are often overlooked during training. Addressing this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of these core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with more than 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, 1\u20133 affordances defined per object class), 100 real-world humanoid view scenarios, and 120 unique simulated constraint scenarios across four tasks. Our evaluations reveal significant gaps in the ability of VLMs to grasp fundamental physical concepts, underscoring their current limitations for reliable robot manipulation and pointing to key areas that require targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating the physical reasoning capabilities of VLMs guiding the development of more robust and physically grounded models for robot manipulation.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121553)\nPoster### [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://neurips.cc/virtual/2025/poster/121706)\nXinran Wang &middot; Songyu Xu &middot; Shan Xiangxuan &middot; Yuxuan Zhang &middot; Muxi Diao &middot; Xueyan Duan &middot; Yanhua huang &middot; Kongming Liang &middot; Zhanyu Ma\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nCinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and repro...",
      "url": "https://neurips.cc/virtual/2025/loc/san-diego/events/datasets-benchmarks-2025"
    },
    {
      "title": "Benchmarks - Kaggle",
      "text": "Getting Started on Kaggle | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n# How to Use Kaggle\nkeyboard\\_arrow\\_right[Competitions](https://www.kaggle.com/docs/competitions)\nkeyboard\\_arrow\\_right[Datasets](https://www.kaggle.com/docs/datasets)\nkeyboard\\_arrow\\_right[Public API](https://www.kaggle.com/docs/api)\n[Efficient GPU Usage Tips](https://www.kaggle.com/docs/efficient-gpu-usage)\nkeyboard\\_arrow\\_right[Tensor Processing Units (TPUs)](https://www.kaggle.com/docs/tpu)\nkeyboard\\_arrow\\_right[Models](https://www.kaggle.com/docs/models)\nkeyboard\\_arrow\\_right[Competitions Setup](https://www.kaggle.com/docs/competitions-setup)\nkeyboard\\_arrow\\_right[Organizations](https://www.kaggle.com/docs/organizations)\nkeyboard\\_arrow\\_right[Groups](https://www.kaggle.com/docs/groups)\nkeyboard\\_arrow\\_right[Kaggle Packages](https://www.kaggle.com/docs/packages)\nkeyboard\\_arrow\\_right[Notebooks](https://www.kaggle.com/docs/notebooks)\nkeyboard\\_arrow\\_right[MCP Server](https://www.kaggle.com/docs/mcp)\nkeyboard\\_arrow\\_down\nBenchmarks\n[Overview](#benchmarks-overview)\n[Examples](#benchmark-examples)\n[Hosting a Benchmark on Kaggle](#benchmark-hosting)\nkeyboard\\_arrow\\_right\n[Downloading Benchmark Leaderboards](#downloading-leaderboards)\n[Models](#benchmark-models)\n## Benchmarks\nHow to use Kaggle Benchmarks as your home for trustworthy AI benchmarking\n### Overview\nKaggle seeks to be the home of a diverse ecosystem of high quality benchmarks assessing model capabilities on tasks of significant importance to the industry to help developers reliably understand and trust what works well on ML tasks. Building on Kaggle's decade-plus of experience as the home for hosting ML Competitions, which are a type of benchmark, for the industry and our partners, we will adhere to the following principles:\n* Kaggle believes in the importance of**robustness**: enduring, high-value benchmarks that truly help the industry measure progress in AI are ones that can\u2019t be easily hacked, saturated, or leaked\n* Kaggle believes in the importance of**reproducibility and transparency**for ensuring the industry can trust benchmarks and evaluations. We also take in extremely high regard the trust publishers place in us as a platform.\n* **Kaggle doesn\u2019t develop benchmarks**. Our role is to independently reproduce and publicly release results, provide a model-agnostic platform that streamlines evaluation of new models on new benchmarks over time, and drive community engagement and stress testing.\n### Examples\nBelow are some examples of benchmarks currently reproduced and hosted on Kaggle:\n* [SciCode](https://www.kaggle.com/benchmarks/kaggle/scicode)\nStay tuned for a landing page for discovering all Benchmarks hosted on Kaggle.\n### Hosting a Benchmark on Kaggle\nIf you're interested in hosting your benchmark on Kaggle, email kaggle-benchmarks@google.com to discuss.\n### Downloading Benchmark Leaderboards\nYou can download the benchmark leaderboard data for your own analysis. There are two ways to access the download options:\n* From the three-dot menu (\"\ufe19\") in the top right of the benchmark page.\n* Using the \"Download\" button located directly above the leaderboard table.\nBoth actions open a download popup that provides methods to retrieve the data.\n#### Download via API\nThe popup provides a cURL command to download the leaderboard data as a JSON object. If the Benchmark is not public, you will need to authenticate using your Kaggle credentials.\n```\n# Unauthenticated example\ncurl -L -o \\~/Downloads/open-benchmarks\\_scicode\\_leaderboard.json \\\\\nhttps://www.kaggle.com/api/v1/benchmarks/open-benchmarks/scicode/leaderboard\n# Authenticated example\n# Export your Kaggle username and API key\n# export KAGGLE\\_USERNAME=\n# export KAGGLE\\_KEY=\ncurl -L -u $KAGGLE\\_USERNAME:$KAGGLE\\_KEY \\\\\n-o \\~/Downloads/myusername\\_my-benchmark\\_leaderboard.json \\\\\nhttps://www.kaggle.com/api/v1/benchmarks/myusername/my-benchmark/leaderboard\n```\n#### Download as CSV\nAt the bottom of the download popup, you can click \"Download leaderboard as csv\" to directly download the data as a CSV file.\n### Models\nThe list below is Kaggle's \"Core Set\" of models available for benchmarking. This list will evolve over time as we add new models to our proxy and is not kept up-to-date in real time.\n* anthropic/claude-3-5-haiku-20241022[(Terms)](https://www.anthropic.com/legal/consumer-terms)\n* anthropic/claude-3-5-sonnet-20241022[(Terms)](https://www.anthropic.com/legal/consumer-terms)\n* anthropic/claude-3-7-sonnet-20250219[(Terms)](https://www.anthropic.com/legal/consumer-terms)\n* anthropic/claude-opus-4-20250514[(Terms)](https://www.anthropic.com/legal/consumer-terms)\n* anthropic/claude-sonnet-4-20250514[(Terms)](https://www.anthropic.com/legal/consumer-terms)\n* deepseek/deepseek-r1[(Deepseek License Agreement)](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL)\n* deepseek/deepseek-v3[(Deepseek License Agreement)](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL)\n* google/gemini-2.0-flash-001[(Terms)](https://ai.google.dev/gemini-api/terms)\n* google/gemini-2.0-flash-lite[(Terms)](https://ai.google.dev/gemini-api/terms)\n* google/gemini-2.5-flash[(Terms)](https://ai.google.dev/gemini-api/terms)\n* google/gemini-2.5-flash-lite[(Terms)](https://ai.google.dev/gemini-api/terms)\n* google/gemini-2.5-pro[(Terms)](https://ai.google.dev/gemini-api/terms)\n* google/gemma-3-12b-it[(Gemma Terms of Use)](https://ai.google.dev/gemma/terms)\n* google/gemma-3-27b-it[(Gemma Terms of Use)](https://ai.google.dev/gemma/terms)\n* google/gemma-3-4b-it[(Gemma Terms of Use)](https://ai.google.dev/gemma/terms)\n* mistral/codestral-2501[(The Mistral AI Non-Production License)](<The Mistral AI Non-Production License>)\n* mistral/ministral-3b-2410[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* mistral/ministral-8b-2410[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* mistral/mistral-large-2411[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* mistral/mistral-small-2503[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* mistral/open-mixtral-8x22b-2404 (Apache 2.0)\n* mistral/pixtral-12b-2409[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* mistral/pixtral-large-2411[(Mistral AI Research License)](https://mistral.ai/static/licenses/MRL-0.1.md)\n* openai/gpt-3.5-turbo-0125[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/gpt-4.1-2025-04-14[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/gpt-4o-2024-08-06[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/gpt-4o-mini-2024-07-18[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/o1-2024-12-17[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/o1-mini-2024-09-12[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/o3-2025-04-16[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/o3-mini-2025-01-31[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* openai/o4-mini-2025-04-16[(Terms of Use)](https://openai.com/policies/row-terms-of-use/)\n* xai/grok-2-1212[(Terms of Service)](https://x.ai/legal/terms-of-service)\n* xai/grok-2-vision-1212[(Terms of Service)](https://x.ai/legal/terms-of-service)",
      "url": "https://www.kaggle.com/docs/benchmarks"
    },
    {
      "title": "Notebook Threw Exception when submitting to Competition | Kaggle",
      "text": "<div><div><div><p>Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.</p></div><div><p></p><h6>Something went wrong and this page crashed!</h6><p>If the issue persists, it's likely a problem on our side.</p><div><pre><div><p>Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)</p></div>ChunkLoadError: Loading CSS chunk 4787 failed.\n(error: https://www.kaggle.com/static/assets/4787.f5129f48620a03126642.css)\nat b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=5453c5604bfaa4c2483b:1:11563)</pre></div></div></div></div>",
      "url": "https://www.kaggle.com/discussions/product-feedback/379362"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code-competition-debugging"
    },
    {
      "title": "How to Use Kaggle",
      "text": "Getting Started on Kaggle | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n# How to Use Kaggle\nkeyboard\\_arrow\\_right[Competitions](https://www.kaggle.com/docs/competitions)\nkeyboard\\_arrow\\_right[Datasets](https://www.kaggle.com/docs/datasets)\nkeyboard\\_arrow\\_right[Public API](https://www.kaggle.com/docs/api)\n[Efficient GPU Usage Tips](https://www.kaggle.com/docs/efficient-gpu-usage)\nkeyboard\\_arrow\\_right[Tensor Processing Units (TPUs)](https://www.kaggle.com/docs/tpu)\nkeyboard\\_arrow\\_right[Models](https://www.kaggle.com/docs/models)\nkeyboard\\_arrow\\_down\nCompetitions Setup\n[Overview](#overview)\nkeyboard\\_arrow\\_right\n[How Kaggle competitions work](#how-kaggle-competitions-work)\nkeyboard\\_arrow\\_right\n[Create your competition\ufe0f](#create-your-competition)\nkeyboard\\_arrow\\_right\n[Prepare the dataset](#prepare-the-dataset)\n[Set up scoring](#set-up-scoring)\n[Creating a New Metric](#creating-a-new-metric)\nkeyboard\\_arrow\\_right\n[Test your competition](#test-your-competition)\nkeyboard\\_arrow\\_right\n[Finalize your settings and descriptions](#finalize-your-settings-and-descriptions)\n[Launch and invite participants](#launch-and-invite-participants)\nkeyboard\\_arrow\\_right\n[FAQs](#faqs)\nkeyboard\\_arrow\\_right[Organizations](https://www.kaggle.com/docs/organizations)\nkeyboard\\_arrow\\_right[Groups](https://www.kaggle.com/docs/groups)\nkeyboard\\_arrow\\_right[Kaggle Packages](https://www.kaggle.com/docs/packages)\nkeyboard\\_arrow\\_right[Notebooks](https://www.kaggle.com/docs/notebooks)\nkeyboard\\_arrow\\_right[MCP Server](https://www.kaggle.com/docs/mcp)\nkeyboard\\_arrow\\_right[Benchmarks](https://www.kaggle.com/docs/benchmarks)\n## Competitions Setup\nCreate a new competition or competition metric\n### Overview\nAnybody can launch a machine learning competition using Kaggle's Community Competitions platform, including educators, researchers, companies, meetup groups, hackathon hosts, or inquisitive individuals! In this guide, you will learn how to set up your own competition, step-by-step.\nBefore diving in, it's helpful to understand how a Kaggle competition works.\n### How Kaggle competitions work\n#### Overview\nEvery competition has two things, a) a clearly defined problem that participants need to solve using a machine learning model and b) a dataset that\u2019s used both for training and evaluating the effectiveness of these models.\nFor example, in the[Store Sales \u2013Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)competition, participants must accurately predict how many of each grocery item will sell using a dataset of past product and sales information from a grocery retailer.\nOnce the competition starts participants can submit their predictions, Kaggle will score them for accuracy, and the team will be placed on a ranked leaderboard. The team at the top of the leaderboard at the deadline wins!\n#### Datasets, Submissions &amp; Leaderboards\nEvery competition\u2019s dataset is split into two smaller datasets.\nOne of these smaller datasets will be given to participants to train their models, typically named`train.csv`.\nThe other dataset will be mostly hidden from participants and used by Kaggle for testing and scoring, named`test.csv`and`solution.csv`(`test.csv`is the same as`solution.csv`except that`test.csv`contains the feature values and`solution.csv`contains the ground truth variable(s) \u2013participants will never, ever see`solution.csv`).\nWhen a participant feels ready to make a submission to the competition, they will use`test.csv`to generate a prediction and upload a CSV file. Kaggle will automatically score the submission for accuracy using the hidden`solution.csv`file.\nMost competitions have a maximum number of submissions that a participant can make each day and a final deadline at which point the leaderboard will be frozen.\nIt\u2019s conceivable that a participant could use the mechanics of a Kaggle competition to overfit a solution - which would be great for winning a competition, but not valuable for a real-world application.\nTo help prevent this, Kaggle has two leaderboards \u2013the public and private leaderboard. The competition host splits the`solution.csv`dataset into two parts, using one part for the public leaderboard and another part for the private leaderboard. Participants generally will now know which samples are public vs private. The private leaderboard is kept a secret until after the competition deadline and is used as the official leaderboard for determining the final ranking.\n### Create your competition\ufe0f\nTo create a new competition, click on the \u201cCreate new competition\u201d button at the top of the Kaggle Community landing page.\nThen, enter a descriptive title, subtitle and URL for your competition. Be as descriptive and to the point as possible. In our example above, the title \u201cStore Sales - Time Series Forecasting\u201d quickly outlines the type of data, the industry of the dataset, and the type of problem to be solved.\nIf you want to create a competition with more privacy, you can limit your competition's visibility and restrict who can join on this page.\nVisibility: Competitions with their visibility set to public are viewable on Kaggle and appear in Kaggle search results. Competitions with visibility set to private are hidden and only accessible via invitation URLs from the host.\nWho Can Join: Competitions access can be set to three levels: anyone, only people with a link and restricted email list. If you select anyone, all Kagglers can join your competition. Selecting only people with a link, will restrict access to those users you provide a special URL. Finally, restricted email list is the most private competition. Only Kagglers with accounts that match the emails or email domains you specify will be able to join. Note: if you select restricted email list, notebooks will be turned off. This provides a way to ensure that any private data that you have in a competition is not accidentally leaked through shared notebooks. You can choose to re-enable notebooks if you choose.\nReview and accept our terms of service, then click \u201cCreate Competition\u201d.\nYour competition listing is now in draft mode. You can take your time to prepare the details before making the competition public.\n#### Offering Prizes\nCommunity competition hosts have the option to offer prizes with a total value of up to $10,000 USD.\nTo set up prizes:\n* Enable Prize Awards: When creating a competition, select \"Competition will award prizes.\" Enter the total amount of prize money to be awarded.\n* Document Prize Rules: You'll need to specify the number of prizes and the amount for each prize on the Competition Rules and Overview pages. Clearly define the criteria for winning in this section. These sections must be completed to launch the competition.Adjusting prize amounts:\n* Prize amounts can be adjusted or turned off entirely only before launch. After you launch a competition, prize settings are locked. We advise you to double-check your prizes before scheduling, as you won't be able to change them after launch.\n* If you are offering a valuable prize that is not cash (eg. gift cards, or valuable objects), please list the monetary value of the prizes in US dollars. The value should not exceed the prize limit of $10,000 USD.Prize fulfillment:\n* Prizes for Community Competitions must be manually awarded and announced.\nLeaderboards for Community Competitions will not display an \"In the money\" designation for winning participants. We advise reaching out to winners directly on Kaggle and announcing winners using the Discussions feature.\n* When you enable prizes for a competition, you are solely responsible for providing and distributing all prizes, fulfilling all promises and commitments, and for complying with all applicable tax rules related to competition winners. Kaggle does not participate in prize distribution or rule enforcement for...",
      "url": "https://www.kaggle.com/docs/competitions-setup"
    },
    {
      "title": "What do you think are the limitations to Kaggle kernels? | Kaggle",
      "text": "<div><div><p>The main limitation of Kaggle is the 9 hour runtime limit on commit, and that Juypter notebooks don't provide access to all the tools of a localhost IDE (debugger, linter, profiler, git).</p>\n<p>The following line can be used to determine your current runtime environment</p>\n<pre><code>os<span>.environ</span><span>.get</span>(<span>'KAGGLE_KERNEL_RUN_TYPE'</span>, <span>'Localhost'</span>) <span>in</span> <span>[<span>'Batch'</span>, <span>'Interactive'</span>]</span>\n</code></pre>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/kaggle-environment-variables-os-environ\">https://www.kaggle.com/jamesmcguigan/kaggle-environment-variables-os-environ</a></li>\n</ul>\n<p>My original solution was <code>kaggle_compile.py</code>, which allowed me to concatenate my localhost IDE files into a single copy/paste bundle for upload </p>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/kaggle-compile-py-python-ide-to-kaggle-compiler\">https://www.kaggle.com/jamesmcguigan/kaggle-compile-py-python-ide-to-kaggle-compiler</a></li>\n</ul>\n<p>I since managed to figure out a better workflow for developing in a localhost IDE, version control in github, whilst retaining the ability to easily publish my research on Kaggle.</p>\n<p>Also as a workaround solution to the 9 hour runtme limit, it is possible to create a dataset reimport loop, by attaching your notebook as a dataset for itself. There may be a little bit of filepath/filesystem juggling involved, but this means you can commit for 9 hours, save to file, then look for an existing saved file in the input directory, merge/copy and then rerun for another 9 hours. </p>\n<p>It is also possible to setup a poor man's version of cluster compute to take advantage of the fact that Kaggle permits you 10 simultaneous commit sessions of 9 hours each. Host your code on github and then create 10 forks of your notebook, each importing the datasets from all the other forks. Use a modulo loop to subdivide your dataset <code>if id % 10 == N:</code> and then figure out a merge strategy to recombine multiple output files back into a single input file. For a submission.csv file, this can be done with <code>find | xargs cat | sort -nr | uniq | awk -F',' '!a[$1]++' | sort -n | sponge &gt; ./submission.csv</code></p>\n<p>I have a working example of this here:</p>\n<ul>\n<li><a href=\"https://www.kaggle.com/jamesmcguigan/game-of-life-z3-constraint-satisfaction\">https://www.kaggle.com/jamesmcguigan/game-of-life-z3-constraint-satisfaction</a></li>\n</ul></div></div>",
      "url": "https://www.kaggle.com/discussions/questions-and-answers/184059"
    }
  ]
}