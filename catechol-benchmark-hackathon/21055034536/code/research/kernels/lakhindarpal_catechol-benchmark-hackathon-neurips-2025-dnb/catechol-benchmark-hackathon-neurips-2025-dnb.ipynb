{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import (\n    INPUT_LABELS_NUMERIC,\n    INPUT_LABELS_SINGLE_FEATURES,\n    INPUT_LABELS_FULL_FEATURES,\n    load_data,\n    load_features,\n    generate_leave_one_out_splits,\n    generate_leave_one_ramp_out_splits,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        raise NotImplementedError\n\n    def featurize(self, X, flip=False):\n        raise NotImplementedError\n\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n\n    def train_model(self, X_train, y_train):\n        raise NotImplementedError\n\n    def predict(self, X):\n        raise NotImplementedError","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntorch.set_default_dtype(torch.double)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KineticMixingFeaturizer(SmilesFeaturizer):\n    def __init__(self, features=\"spange_descriptors\", mixed=False):\n        self.mixed = mixed\n        self.featurizer = load_features(features)\n        self.feats_dim = self.featurizer.shape[1] + 2 + 3\n\n    def featurize(self, X, flip=False):\n        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n\n        # --- KINETIC FEATURES ---\n        temp_c = X_vals[:, 1:2]\n        time_m = X_vals[:, 0:1]\n\n        temp_k = temp_c + 273.15\n        inv_temp = 1000.0 / temp_k\n        log_time = np.log(time_m + 1e-6)\n        interaction = inv_temp * log_time\n\n        X_kinetic = torch.tensor(\n            np.hstack([X_vals, inv_temp, log_time, interaction])\n        )\n\n        X_kinetic = X_kinetic + 0.01 * torch.randn_like(X_kinetic)\n\n        # --- CHEMICAL FEATURES ---\n        if self.mixed:\n            A = torch.tensor(self.featurizer.loc[X[\"SOLVENT A NAME\"]].values)\n            B = torch.tensor(self.featurizer.loc[X[\"SOLVENT B NAME\"]].values)\n            pct = torch.tensor(X[\"SolventB%\"].values.reshape(-1, 1))\n\n            if flip:\n                X_chem = B * (1 - (1 - pct)) + A * (1 - pct)\n            else:\n                X_chem = A * (1 - pct) + B * pct\n        else:\n            X_chem = torch.tensor(self.featurizer.loc[X[\"SOLVENT NAME\"]].values)\n\n        return torch.cat([X_kinetic, X_chem], dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLPInternal(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm1d(input_dim),\n            nn.Linear(input_dim, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(128, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n\n            nn.Linear(64, 3),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SymmetricBaggedModel(nn.Module, BaseModel):\n    def __init__(self, data=\"single\"):\n        super().__init__()\n        self.data_type = data\n        self.featurizer = KineticMixingFeaturizer(\n            mixed=(data == \"full\")\n        )\n\n        self.n_models = 9\n        self.models = nn.ModuleList()\n\n    def train_model(self, X_train, y_train):\n        X_std = self.featurizer.featurize(X_train, flip=False)\n        y_vals = torch.tensor(y_train.values)\n\n        if self.data_type == \"full\":\n            X_flip = self.featurizer.featurize(X_train, flip=True)\n            X_all = torch.cat([X_std, X_flip], dim=0)\n            y_all = torch.cat([y_vals, y_vals], dim=0)\n        else:\n            X_all = X_std\n            y_all = y_vals\n\n        input_dim = X_all.shape[1]\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        for _ in range(self.n_models):\n            model = MLPInternal(input_dim).to(device)\n            self.models.append(model)\n\n            dataset = TensorDataset(X_all, y_all)\n            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n            optimizer = torch.optim.Adam(\n                model.parameters(), lr=5e-4, weight_decay=1e-5\n            )\n\n            criterion = nn.SmoothL1Loss(beta=0.3)\n\n            model.train()\n            for _ in range(220):  # fewer epochs\n                for xb, yb in loader:\n                    xb, yb = xb.to(device), yb.to(device)\n                    optimizer.zero_grad()\n                    loss = criterion(model(xb), yb)\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    optimizer.step()\n\n    def predict(self, X):\n        device = next(self.models[0].parameters()).device\n\n        if self.data_type == \"full\":\n            X_std = self.featurizer.featurize(X, flip=False).to(device)\n            X_flip = self.featurizer.featurize(X, flip=True).to(device)\n\n            pred_sum = torch.zeros((len(X), 3)).to(device)\n            with torch.no_grad():\n                for model in self.models:\n                    model.eval()\n                    pred_sum += 0.5 * (model(X_std) + model(X_flip))\n\n            return (pred_sum / self.n_models).cpu()\n\n        else:\n            X_std = self.featurizer.featurize(X).to(device)\n            pred_sum = torch.zeros((len(X), 3)).to(device)\n            with torch.no_grad():\n                for model in self.models:\n                    model.eval()\n                    pred_sum += model(X_std)\n\n            return (pred_sum / self.n_models).cpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tqdm\n\nX, Y = load_data(\"single_solvent\")\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = SymmetricBaggedModel(data=\"single\")  # ONLY CHANGE\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X).numpy()\n\n    for row_idx, row in enumerate(predictions):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2],\n        })\n\nsubmission_single_solvent = pd.DataFrame(all_predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, Y = load_data(\"full\")\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = SymmetricBaggedModel(data=\"full\")  # ONLY CHANGE\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X).numpy()\n\n    for row_idx, row in enumerate(predictions):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2],\n        })\n\nsubmission_full_data = pd.DataFrame(all_predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}