{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a5c9c8",
   "metadata": {},
   "source": [
    "# Evolver Loop 69 Analysis\n",
    "\n",
    "Goals:\n",
    "1. Confirm the CVâ†’LB linear mapping from all *completed* submissions.\n",
    "2. Audit mixture representation: compare linear blending vs pairwise interaction features in *training CV* (proxy for ability to model mixture nonlinearity).\n",
    "3. Prototype an **applicability-domain (AD) correction**: distance-based shrinkage to baseline, trained **cross-fitted** (no leakage).\n",
    "\n",
    "We will not tune hyperparameters; we want a structural change likely to reduce LB intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Recompute CV->LB mapping from the completed LB entries provided in the prompt.\n",
    "hist = [\n",
    "    (0.0111, 0.0982),\n",
    "    (0.0123, 0.1065),\n",
    "    (0.0105, 0.0972),\n",
    "    (0.0104, 0.0969),\n",
    "    (0.0097, 0.0946),\n",
    "    (0.0093, 0.0932),\n",
    "    (0.0092, 0.0936),\n",
    "    (0.0090, 0.0913),\n",
    "    (0.0087, 0.0893),\n",
    "    (0.0085, 0.0887),\n",
    "    (0.0083, 0.0877),\n",
    "    (0.0098, 0.0970),\n",
    "]\n",
    "cv = np.array([a for a,b in hist])\n",
    "lb = np.array([b for a,b in hist])\n",
    "print('n paired:', len(cv))\n",
    "\n",
    "A = np.vstack([cv, np.ones_like(cv)]).T\n",
    "slope, intercept = np.linalg.lstsq(A, lb, rcond=None)[0]\n",
    "pred = slope*cv + intercept\n",
    "r2 = 1 - ((lb-pred)**2).sum()/((lb-lb.mean())**2).sum()\n",
    "print('Fit: LB = %.3f * CV + %.4f | R2=%.4f' % (slope, intercept, r2))\n",
    "\n",
    "TARGET = 0.0347\n",
    "req_cv = (TARGET - intercept)/slope\n",
    "print('Required CV for target given this fit:', req_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "DATA_PATH='/home/data'\n",
    "\n",
    "single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv')\n",
    "drfp = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv')\n",
    "acs = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv')\n",
    "smiles = pd.read_csv(f'{DATA_PATH}/smiles_lookup.csv')\n",
    "\n",
    "print(single.shape, full.shape)\n",
    "print('spange', spange.shape, 'drfp', drfp.shape, 'acs', acs.shape, 'smiles', smiles.shape)\n",
    "print('single solvents:', single['SOLVENT NAME'].nunique())\n",
    "print('full ramps:', full[['SOLVENT A NAME','SOLVENT B NAME']].drop_duplicates().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed858b3",
   "metadata": {},
   "source": [
    "## Mixture representation audit\n",
    "\n",
    "We compare two featurizations for **full-data** rows:\n",
    "\n",
    "1. **Blended (status quo):** `(1-p)A + pB`\n",
    "2. **Pairwise + interactions:** concat `[A, B, p, p(1-p), (A-B), A*B]` (and we still include Arrhenius features)\n",
    "\n",
    "If pairwise/interactions improves CV even with a simple model, it suggests mixture nonlinearity is real signal worth adding to the main pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Use Spange descriptors\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv').set_index('SOLVENT NAME')\n",
    "\n",
    "\n",
    "def arrhenius_feats(df):\n",
    "    T = df['Temperature'].values.astype(float)\n",
    "    t = df['Residence Time'].values.astype(float)\n",
    "    Tk = T + 273.15\n",
    "    invT = 1.0 / Tk\n",
    "    lnt = np.log(np.clip(t, 1e-6, None))\n",
    "    return np.vstack([t, T, invT, lnt, invT * lnt]).T\n",
    "\n",
    "\n",
    "def full_feats_blended(df):\n",
    "    p = (df['SolventB%'].values.astype(float) / 100.0).reshape(-1, 1)\n",
    "    A = spange_df.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange_df.loc[df['SOLVENT B NAME']].values\n",
    "    mix = (1 - p) * A + p * B\n",
    "    return np.hstack([arrhenius_feats(df), p, mix])\n",
    "\n",
    "\n",
    "def full_feats_pairwise(df):\n",
    "    p = (df['SolventB%'].values.astype(float) / 100.0).reshape(-1, 1)\n",
    "    A = spange_df.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange_df.loc[df['SOLVENT B NAME']].values\n",
    "    inter = np.hstack([A, B, p, p * (1 - p), (A - B), (A * B)])\n",
    "    return np.hstack([arrhenius_feats(df), inter])\n",
    "\n",
    "\n",
    "Y_full = full[['Product 2', 'Product 3', 'SM']].values\n",
    "ramps = full['SOLVENT A NAME'].astype(str) + '_' + full['SOLVENT B NAME'].astype(str)\n",
    "\n",
    "for name, featurizer in [('blended', full_feats_blended), ('pairwise', full_feats_pairwise)]:\n",
    "    gkf = GroupKFold(n_splits=13)\n",
    "    preds = np.zeros_like(Y_full)\n",
    "    for tr, te in gkf.split(full, Y_full, groups=ramps):\n",
    "        Xtr = featurizer(full.iloc[tr])\n",
    "        Xte = featurizer(full.iloc[te])\n",
    "\n",
    "        sc = StandardScaler().fit(Xtr)\n",
    "        Xtr = sc.transform(Xtr)\n",
    "        Xte = sc.transform(Xte)\n",
    "\n",
    "        # Simple model to isolate feature signal\n",
    "        for k in range(3):\n",
    "            m = Ridge(alpha=1.0)\n",
    "            m.fit(Xtr, Y_full[tr, k])\n",
    "            preds[te, k] = m.predict(Xte)\n",
    "\n",
    "    mse = ((Y_full - preds) ** 2).mean()\n",
    "    print(name, 'full-data Ridge CV MSE:', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b48272",
   "metadata": {},
   "source": [
    "## Cross-fitted Applicability-Domain (AD) shrinkage prototype\n",
    "\n",
    "Within each outer fold:\n",
    "- Train a base model on the train fold.\n",
    "- Compute a **distance-to-training** score for test samples (kNN in solvent-pair space).\n",
    "- On the train fold only, get **OOF predictions** and a **kNN-label baseline**.\n",
    "- Learn a **monotonic mapping** `alpha = f(distance)` with isotonic regression.\n",
    "- Apply: `pred_final = (1-alpha)*pred_model + alpha*pred_knn`\n",
    "\n",
    "This is designed to reduce catastrophic OOD errors (i.e., attack the LB intercept) and is implementable in the template-compliant wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# AD correction for FULL data only, using blended featurization + Ridge.\n",
    "\n",
    "def solventpair_vector(df):\n",
    "    p = (df['SolventB%'].values.astype(float)/100.0).reshape(-1,1)\n",
    "    A = spange.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange.loc[df['SOLVENT B NAME']].values\n",
    "    return np.hstack([A,B,p,p*(1-p),(A-B),(A*B)])\n",
    "\n",
    "Y = full[['Product 2','Product 3','SM']].values\n",
    "ramps = full['SOLVENT A NAME'].astype(str) + '_' + full['SOLVENT B NAME'].astype(str)\n",
    "\n",
    "gkf = GroupKFold(n_splits=13)\n",
    "preds_base = np.zeros_like(Y)\n",
    "preds_ad = np.zeros_like(Y)\n",
    "\n",
    "for tr, te in gkf.split(full, Y, groups=ramps):\n",
    "    train_df = full.iloc[tr].reset_index(drop=True)\n",
    "    test_df = full.iloc[te].reset_index(drop=True)\n",
    "\n",
    "    # base model\n",
    "    Xtr = full_feats_blended(train_df)\n",
    "    Xte = full_feats_blended(test_df)\n",
    "    scaler = StandardScaler().fit(Xtr)\n",
    "    Xtr_s = scaler.transform(Xtr); Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    for k in range(3):\n",
    "        m = Ridge(alpha=1.0)\n",
    "        m.fit(Xtr_s, Y[tr, k])\n",
    "        preds_base[te, k] = m.predict(Xte_s)\n",
    "\n",
    "    # distances\n",
    "    Vtr = solventpair_vector(train_df)\n",
    "    Vte = solventpair_vector(test_df)\n",
    "    Vsc = StandardScaler().fit(Vtr)\n",
    "    Vtr_s = Vsc.transform(Vtr); Vte_s = Vsc.transform(Vte)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=min(10, len(Vtr_s))).fit(Vtr_s)\n",
    "    d_te, nbr_te = nn.kneighbors(Vte_s)\n",
    "    d_te = d_te.mean(axis=1)\n",
    "\n",
    "    # Inner CV to learn alpha(d)\n",
    "    train_groups = train_df['SOLVENT A NAME'].astype(str) + '_' + train_df['SOLVENT B NAME'].astype(str)\n",
    "    n_splits_inner = min(5, len(train_groups.unique()))\n",
    "    inner = GroupKFold(n_splits=n_splits_inner)\n",
    "\n",
    "    oof_pred = np.zeros((len(train_df),3))\n",
    "    for tr2, va2 in inner.split(train_df, Y[tr], groups=train_groups):\n",
    "        Xtr2 = full_feats_blended(train_df.iloc[tr2])\n",
    "        Xva2 = full_feats_blended(train_df.iloc[va2])\n",
    "        sc2 = StandardScaler().fit(Xtr2)\n",
    "        Xtr2_s=sc2.transform(Xtr2); Xva2_s=sc2.transform(Xva2)\n",
    "        for k in range(3):\n",
    "            m2 = Ridge(alpha=1.0)\n",
    "            m2.fit(Xtr2_s, Y[tr][tr2, k])\n",
    "            oof_pred[va2,k] = m2.predict(Xva2_s)\n",
    "\n",
    "    # distances for train samples (exclude self)\n",
    "    nn2 = NearestNeighbors(n_neighbors=min(11, len(Vtr_s))).fit(Vtr_s)\n",
    "    d_tr, nbr_tr = nn2.kneighbors(Vtr_s)\n",
    "    if d_tr.shape[1] > 1:\n",
    "        d_tr = d_tr[:,1:].mean(axis=1)\n",
    "        nbr_tr = nbr_tr[:,1:]\n",
    "    else:\n",
    "        d_tr = d_tr.mean(axis=1)\n",
    "\n",
    "    # kNN baseline labels (train-only)\n",
    "    y_train = Y[tr]\n",
    "    baseline_tr = y_train[nbr_tr].mean(axis=1)\n",
    "    baseline_te = y_train[nbr_te].mean(axis=1)\n",
    "\n",
    "    # optimal alpha per train sample\n",
    "    eps=1e-12\n",
    "    bp = baseline_tr - oof_pred\n",
    "    num = (y_train - oof_pred)*bp\n",
    "    den = (bp*bp) + eps\n",
    "    a_opt = np.clip(num/den, 0, 1)\n",
    "    a_opt_scalar = a_opt.mean(axis=1)\n",
    "\n",
    "    iso = IsotonicRegression(y_min=0, y_max=1, increasing=True, out_of_bounds='clip')\n",
    "    iso.fit(d_tr, a_opt_scalar)\n",
    "    a_te = iso.predict(d_te)\n",
    "\n",
    "    preds_ad[te,:] = (1-a_te)[:,None]*preds_base[te,:] + a_te[:,None]*baseline_te\n",
    "\n",
    "mse_base = ((Y - preds_base)**2).mean()\n",
    "mse_ad = ((Y - preds_ad)**2).mean()\n",
    "print('FULL only: base Ridge blended MSE:', mse_base)\n",
    "print('FULL only: AD-corrected MSE:', mse_ad)\n",
    "print('delta:', mse_ad - mse_base)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
