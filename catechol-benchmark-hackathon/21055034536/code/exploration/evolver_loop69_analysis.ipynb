{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a5c9c8",
   "metadata": {},
   "source": [
    "# Evolver Loop 69 Analysis\n",
    "\n",
    "Goals:\n",
    "1. Confirm the CV→LB linear mapping from all *completed* submissions.\n",
    "2. Audit mixture representation: compare linear blending vs pairwise interaction features in *training CV* (proxy for ability to model mixture nonlinearity).\n",
    "3. Prototype an **applicability-domain (AD) correction**: distance-based shrinkage to baseline, trained **cross-fitted** (no leakage).\n",
    "\n",
    "We will not tune hyperparameters; we want a structural change likely to reduce LB intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fcd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "state = json.loads(Path('/home/code/session_state.json').read_text())\n",
    "submissions = state.get('submissions', [])\n",
    "print('submissions in state:', len(submissions))\n",
    "\n",
    "# Parse the user-provided list (some LB pending). We'll reconstruct from known ones here.\n",
    "paired = []\n",
    "# From prompt history (completed LB only)\n",
    "hist = [\n",
    "    (0.0111, 0.0982),\n",
    "    (0.0123, 0.1065),\n",
    "    (0.0105, 0.0972),\n",
    "    (0.0104, 0.0969),\n",
    "    (0.0097, 0.0946),\n",
    "    (0.0093, 0.0932),\n",
    "    (0.0092, 0.0936),\n",
    "    (0.0090, 0.0913),\n",
    "    (0.0087, 0.0893),\n",
    "    (0.0085, 0.0887),\n",
    "    (0.0083, 0.0877),\n",
    "    (0.0098, 0.0970),\n",
    "]\n",
    "cv = np.array([a for a,b in hist])\n",
    "lb = np.array([b for a,b in hist])\n",
    "print('n paired:', len(cv))\n",
    "\n",
    "# Linear regression with intercept\n",
    "A = np.vstack([cv, np.ones_like(cv)]).T\n",
    "slope, intercept = np.linalg.lstsq(A, lb, rcond=None)[0]\n",
    "pred = slope*cv + intercept\n",
    "r2 = 1 - ((lb-pred)**2).sum()/((lb-lb.mean())**2).sum()\n",
    "print('Fit: LB = %.3f * CV + %.4f | R2=%.4f' % (slope, intercept, r2))\n",
    "\n",
    "TARGET = 0.0347\n",
    "req_cv = (TARGET - intercept)/slope\n",
    "print('Required CV for target given this fit:', req_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "DATA_PATH='/home/data'\n",
    "\n",
    "single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv')\n",
    "drfp = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv')\n",
    "acs = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv')\n",
    "smiles = pd.read_csv(f'{DATA_PATH}/smiles_lookup.csv')\n",
    "\n",
    "print(single.shape, full.shape)\n",
    "print('spange', spange.shape, 'drfp', drfp.shape, 'acs', acs.shape, 'smiles', smiles.shape)\n",
    "print('single solvents:', single['SOLVENT NAME'].nunique())\n",
    "print('full ramps:', full[['SOLVENT A NAME','SOLVENT B NAME']].drop_duplicates().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed858b3",
   "metadata": {},
   "source": [
    "## Mixture representation audit\n",
    "\n",
    "We compare two featurizations for full-data rows:\n",
    "1. **Blended**: (1-p)A + pB (status quo)\n",
    "2. **Pairwise+interactions**: concat [A, B, p, p(1-p), (A-B), A*B] (+ numeric Arrhenius features)\n",
    "\n",
    "We don't need exact best model; we just want to see if pairwise features help *within* the same CV split, indicating mixture nonlinearity is present and exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Build solvent descriptor dicts\n",
    "spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv').set_index('solvent')\n",
    "# keep numeric columns only\n",
    "sp_cols = spange.columns\n",
    "\n",
    "def arrhenius_feats(df):\n",
    "    # basic kinetics-inspired transforms\n",
    "    T = df['Temperature'].values.astype(float)\n",
    "    t = df['Residence Time'].values.astype(float)\n",
    "    Tk = T + 273.15\n",
    "    invT = 1.0/Tk\n",
    "    lnt = np.log(np.clip(t, 1e-6, None))\n",
    "    return np.vstack([t, T, invT, lnt, invT*lnt]).T\n",
    "\n",
    "\n",
    "def full_feats_blended(df):\n",
    "    p = (df['SolventB%'].values.astype(float)/100.0).reshape(-1,1)\n",
    "    A = spange.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange.loc[df['SOLVENT B NAME']].values\n",
    "    mix = (1-p)*A + p*B\n",
    "    X = np.hstack([arrhenius_feats(df), p, mix])\n",
    "    return X\n",
    "\n",
    "\n",
    "def full_feats_pairwise(df):\n",
    "    p = (df['SolventB%'].values.astype(float)/100.0).reshape(-1,1)\n",
    "    A = spange.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange.loc[df['SOLVENT B NAME']].values\n",
    "    inter = np.hstack([\n",
    "        A, B, p, p*(1-p),\n",
    "        (A-B),\n",
    "        (A*B)\n",
    "    ])\n",
    "    X = np.hstack([arrhenius_feats(df), inter])\n",
    "    return X\n",
    "\n",
    "Y = full[['Product 2','Product 3','SM']].values\n",
    "ramps = full['SOLVENT A NAME'].astype(str) + '_' + full['SOLVENT B NAME'].astype(str)\n",
    "\n",
    "for name, featurizer in [('blended', full_feats_blended), ('pairwise', full_feats_pairwise)]:\n",
    "    gkf = GroupKFold(n_splits=13)\n",
    "    preds = np.zeros_like(Y)\n",
    "    for tr, te in gkf.split(full, Y, groups=ramps):\n",
    "        Xtr = featurizer(full.iloc[tr])\n",
    "        Xte = featurizer(full.iloc[te])\n",
    "        s = StandardScaler().fit(Xtr)\n",
    "        Xtr = s.transform(Xtr); Xte = s.transform(Xte)\n",
    "        # Simple baseline model to isolate feature signal\n",
    "        model = Ridge(alpha=1.0)\n",
    "        # fit 3 targets independently\n",
    "        for k in range(3):\n",
    "            model.fit(Xtr, Y[tr, k])\n",
    "            preds[te, k] = model.predict(Xte)\n",
    "    mse = ((Y - preds)**2).mean()\n",
    "    print(name, 'full-data Ridge CV MSE:', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b48272",
   "metadata": {},
   "source": [
    "## Cross-fitted Applicability-Domain (AD) shrinkage prototype\n",
    "\n",
    "We simulate an OOD scenario *within CV*: for each fold, compute distance of test samples' solvent descriptors to the training set (kNN in descriptor space). Learn a **distance→shrinkage** mapping using only training-fold cross-validation predictions (cross-fitted) to avoid leakage.\n",
    "\n",
    "This is a toy but checks feasibility + correct plumbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# We'll implement AD correction for FULL data only here, using blended featurization + Ridge.\n",
    "# Steps per outer fold:\n",
    "# 1) Train base model on train fold.\n",
    "# 2) Get base predictions on test fold.\n",
    "# 3) Compute distance for each test sample to training distribution (kNN on solvent-pair features only).\n",
    "# 4) Using inner CV on train fold, learn mapping d -> optimal alpha (shrinkage weight).\n",
    "\n",
    "\n",
    "def solventpair_vector(df):\n",
    "    # solvent-only vector for distance (no conditions): use pairwise solvent features incl pct\n",
    "    p = (df['SolventB%'].values.astype(float)/100.0).reshape(-1,1)\n",
    "    A = spange.loc[df['SOLVENT A NAME']].values\n",
    "    B = spange.loc[df['SOLVENT B NAME']].values\n",
    "    return np.hstack([A,B,p,p*(1-p),(A-B),(A*B)])\n",
    "\n",
    "Y = full[['Product 2','Product 3','SM']].values\n",
    "ramps = full['SOLVENT A NAME'].astype(str) + '_' + full['SOLVENT B NAME'].astype(str)\n",
    "\n",
    "gkf = GroupKFold(n_splits=13)\n",
    "preds_base = np.zeros_like(Y)\n",
    "preds_ad = np.zeros_like(Y)\n",
    "\n",
    "for tr, te in gkf.split(full, Y, groups=ramps):\n",
    "    train_df = full.iloc[tr].reset_index(drop=True)\n",
    "    test_df = full.iloc[te].reset_index(drop=True)\n",
    "\n",
    "    # base model\n",
    "    Xtr = full_feats_blended(train_df)\n",
    "    Xte = full_feats_blended(test_df)\n",
    "    scaler = StandardScaler().fit(Xtr)\n",
    "    Xtr_s = scaler.transform(Xtr); Xte_s = scaler.transform(Xte)\n",
    "\n",
    "    base_models = []\n",
    "    for k in range(3):\n",
    "        m = Ridge(alpha=1.0)\n",
    "        m.fit(Xtr_s, Y[tr, k])\n",
    "        base_models.append(m)\n",
    "        preds_base[te, k] = m.predict(Xte_s)\n",
    "\n",
    "    # distance model on solvent-pair vectors\n",
    "    Vtr = solventpair_vector(train_df)\n",
    "    Vte = solventpair_vector(test_df)\n",
    "    Vsc = StandardScaler().fit(Vtr)\n",
    "    Vtr_s = Vsc.transform(Vtr); Vte_s = Vsc.transform(Vte)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=min(10, len(Vtr_s))).fit(Vtr_s)\n",
    "    d_te, idx_te = nn.kneighbors(Vte_s)\n",
    "    d_te = d_te.mean(axis=1)\n",
    "\n",
    "    # Inner CV to learn alpha(d)\n",
    "    inner = GroupKFold(n_splits=min(5, len(np.unique(train_df['SOLVENT A NAME'].astype(str)+'_'+train_df['SOLVENT B NAME'].astype(str)))))\n",
    "    train_groups = train_df['SOLVENT A NAME'].astype(str) + '_' + train_df['SOLVENT B NAME'].astype(str)\n",
    "\n",
    "    oof_pred = np.zeros((len(train_df),3))\n",
    "    for tr2, va2 in inner.split(train_df, Y[tr], groups=train_groups):\n",
    "        Xtr2 = full_feats_blended(train_df.iloc[tr2])\n",
    "        Xva2 = full_feats_blended(train_df.iloc[va2])\n",
    "        sc2 = StandardScaler().fit(Xtr2)\n",
    "        Xtr2_s=sc2.transform(Xtr2); Xva2_s=sc2.transform(Xva2)\n",
    "        for k in range(3):\n",
    "            m2 = Ridge(alpha=1.0)\n",
    "            m2.fit(Xtr2_s, Y[tr][tr2, k])\n",
    "            oof_pred[va2,k] = m2.predict(Xva2_s)\n",
    "\n",
    "    # distances for train fold (to itself): use kNN within train vectors excluding self approx via n_neighbors=11 and drop first\n",
    "    nn2 = NearestNeighbors(n_neighbors=min(11, len(Vtr_s))).fit(Vtr_s)\n",
    "    d_tr, idx_tr = nn2.kneighbors(Vtr_s)\n",
    "    d_tr = d_tr[:,1:].mean(axis=1) if d_tr.shape[1]>1 else d_tr.mean(axis=1)\n",
    "\n",
    "    # define a baseline: nearest-neighbor mean label (kNN in solvent space) computed from train only\n",
    "    # for each sample in train and test, compute neighbor indices and take average Y of neighbors\n",
    "    nn_lbl = NearestNeighbors(n_neighbors=min(10, len(Vtr_s))).fit(Vtr_s)\n",
    "    _, nbr_tr = nn_lbl.kneighbors(Vtr_s)\n",
    "    _, nbr_te = nn_lbl.kneighbors(Vte_s)\n",
    "\n",
    "    y_train = Y[tr]\n",
    "    baseline_tr = y_train[nbr_tr].mean(axis=1)\n",
    "    baseline_te = y_train[nbr_te].mean(axis=1)\n",
    "\n",
    "    # For each train sample, the best alpha in [0,1] to combine oof_pred and baseline_tr\n",
    "    # alpha*baseline + (1-alpha)*pred\n",
    "    # Closed form per sample per target: minimize (y - ((1-a)p + a b))^2 -> a = clip((y-p)*(b-p)/((b-p)^2), 0,1)\n",
    "    eps=1e-12\n",
    "    bp = baseline_tr - oof_pred\n",
    "    num = (y_train - oof_pred)*bp\n",
    "    den = (bp*bp) + eps\n",
    "    a_opt = np.clip(num/den, 0, 1)\n",
    "    # Reduce to scalar alpha per sample by averaging targets\n",
    "    a_opt_scalar = a_opt.mean(axis=1)\n",
    "\n",
    "    # Learn monotonic mapping alpha(d)\n",
    "    iso = IsotonicRegression(y_min=0, y_max=1, increasing=True, out_of_bounds='clip')\n",
    "    iso.fit(d_tr, a_opt_scalar)\n",
    "    a_te = iso.predict(d_te)\n",
    "\n",
    "    # Apply correction on test\n",
    "    for k in range(3):\n",
    "        preds_ad[te,k] = (1-a_te)*preds_base[te,k] + a_te*baseline_te[:,k]\n",
    "\n",
    "mse_base = ((Y - preds_base)**2).mean()\n",
    "mse_ad = ((Y - preds_ad)**2).mean()\n",
    "print('FULL only: base Ridge blended MSE:', mse_base)\n",
    "print('FULL only: AD-corrected MSE:', mse_ad)\n",
    "print('delta:', mse_ad - mse_base)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
