{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be97f5bf",
   "metadata": {},
   "source": [
    "# Evolver Loop 72 Analysis\n",
    "\n",
    "Goals:\n",
    "1. Re-check CV↔LB relationship using **actual** submission history from `session_state.json`.\n",
    "2. Quantify how much of the gap could plausibly be reduced by **intercept-reduction** strategies.\n",
    "3. Prototype an **applicability-domain shrinkage** layer on top of the now template-safe A0 model outputs (no retraining): distance-to-training → alpha(x) → blend toward conservative baseline.\n",
    "4. Pull ideas from public kernels (esp. `mixall` and `ens-model`).\n",
    "\n",
    "We are NOT optimizing CV; we are trying to change the CV→LB mapping (reduce intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "state=json.loads(Path('/home/code/session_state.json').read_text())\n",
    "subs=pd.DataFrame(state.get('submissions', []))\n",
    "subs=subs.dropna(subset=['cv_score','lb_score'])\n",
    "subs=subs.sort_values('timestamp')\n",
    "subs[['experiment_id','cv_score','lb_score']].tail(), subs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0551a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X=subs[['cv_score']].values\n",
    "y=subs['lb_score'].values\n",
    "lr=LinearRegression().fit(X,y)\n",
    "pred=lr.predict(X)\n",
    "r2=lr.score(X,y)\n",
    "(lr.coef_[0], lr.intercept_, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(subs['cv_score'], subs['lb_score'])\n",
    "xs=np.linspace(subs['cv_score'].min(), subs['cv_score'].max(), 100)\n",
    "plt.plot(xs, lr.predict(xs.reshape(-1,1)))\n",
    "plt.xlabel('CV'); plt.ylabel('LB');\n",
    "plt.title(f'LB = {lr.coef_[0]:.2f}*CV + {lr.intercept_:.4f} (R2={r2:.3f})')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc28721",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=0.0347\n",
    "required_cv=(target - lr.intercept_)/lr.coef_[0]\n",
    "required_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dcb53",
   "metadata": {},
   "source": [
    "## Applicability-domain shrinkage prototype\n",
    "\n",
    "Idea: detect \"OOD-ness\" by distance of a test point to training distribution in **solvent descriptor space** (or learned embeddings), then blend prediction toward a conservative fallback.\n",
    "\n",
    "We can prototype with:\n",
    "- Features: solvent descriptors only (Spange + DRFP filtered + ACS PCA), plus mixture composition.\n",
    "- Distance: kNN average Euclidean after standardization.\n",
    "- Alpha mapping: isotonic regression from distance → optimal alpha in CV (minimizes MSE when blending).\n",
    "- Fallback: per-target mean of training folds.\n",
    "\n",
    "Then apply to fold predictions from A0-like model and evaluate CV effect. If CV worsens slightly but we expect LB intercept reduction, it may still help.\n",
    "\n",
    "We need fold predictions from a baseline model. For speed, we can fit a fast ridge baseline to generate predictions; in the executor we will implement the shrinkage inside the full model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d376b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "DATA_PATH='/home/data'\n",
    "INPUT_LABELS_FULL_SOLVENT=[\"Residence Time\",\"Temperature\",\"SOLVENT A NAME\",\"SOLVENT B NAME\",\"SolventB%\"]\n",
    "TARGET_LABELS=[\"Product 2\",\"Product 3\",\"SM\"]\n",
    "\n",
    "full=pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "X=full[INPUT_LABELS_FULL_SOLVENT].copy()\n",
    "Y=full[TARGET_LABELS].copy()\n",
    "\n",
    "spange=pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "drfp=pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "acs=pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "nz=drfp.var(); drfp=drfp[nz[nz>0].index]\n",
    "\n",
    "# simple mixture featurizer for distance only (no kinetics)\n",
    "def mix_desc(dfX):\n",
    "    A=dfX['SOLVENT A NAME']; B=dfX['SOLVENT B NAME']\n",
    "    pct=dfX['SolventB%'].values.reshape(-1,1)\n",
    "    A_sp=spange.loc[A].values; B_sp=spange.loc[B].values\n",
    "    A_dr=drfp.loc[A].values;   B_dr=drfp.loc[B].values\n",
    "    A_ac=acs.loc[A].values;    B_ac=acs.loc[B].values\n",
    "    desc=np.hstack([(1-pct)*A_sp+pct*B_sp, (1-pct)*A_dr+pct*B_dr, (1-pct)*A_ac+pct*B_ac, pct])\n",
    "    return desc\n",
    "\n",
    "D=mix_desc(X)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\",\"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\",\"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\",\"SOLVENT B NAME\"]] != solvent_pair).any(axis=1)\n",
    "        yield (train_idcs_mask.values, (~train_idcs_mask).values)\n",
    "\n",
    "splits=list(generate_leave_one_ramp_out_splits(X,Y))\n",
    "len(splits), sum(s[1].sum() for s in splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def cv_with_shrinkage(D, Y, splits, k=10):\n",
    "    mses=[]\n",
    "    for train_mask, test_mask in splits:\n",
    "        Dtr, Dte = D[train_mask], D[test_mask]\n",
    "        ytr, yte = Y.values[train_mask], Y.values[test_mask]\n",
    "\n",
    "        sc=StandardScaler().fit(Dtr)\n",
    "        Dtr_s=sc.transform(Dtr); Dte_s=sc.transform(Dte)\n",
    "\n",
    "        # baseline: fast per-target ridge on D (distance-only baseline)\n",
    "        preds=[]\n",
    "        for t in range(3):\n",
    "            m=Ridge(alpha=1.0).fit(Dtr_s, ytr[:,t])\n",
    "            preds.append(m.predict(Dte_s))\n",
    "        pred=np.vstack(preds).T\n",
    "\n",
    "        # distance to training\n",
    "        nn=NearestNeighbors(n_neighbors=min(k,len(Dtr_s))).fit(Dtr_s)\n",
    "        dist,_=nn.kneighbors(Dte_s)\n",
    "        d=dist.mean(axis=1)\n",
    "\n",
    "        # learn alpha mapping using train split itself via inner CV-free heuristic:\n",
    "        # Use isotonic on (d, |residual|)?? Instead, approximate alpha by correlating d with squared error on train using leave-one-out neighbors.\n",
    "        # Simple pragmatic: normalize d to [0,1] per fold and use fixed alpha curve.\n",
    "        d_norm=(d - d.min())/(d.max()-d.min()+1e-9)\n",
    "        alpha=np.clip(d_norm,0,1)\n",
    "\n",
    "        mu=ytr.mean(axis=0)\n",
    "        shrunk=(1-alpha)[:,None]*pred + alpha[:,None]*mu\n",
    "        mses.append(((yte - shrunk)**2).mean())\n",
    "    return float(np.mean(mses))\n",
    "\n",
    "mse_sh=cv_with_shrinkage(D,Y,splits,k=10)\n",
    "mse_nosh=cv_with_shrinkage(D,Y,splits,k=10)  # same because function always shrinks; placeholder\n",
    "mse_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70238abc",
   "metadata": {},
   "source": [
    "The prototype above is too crude; we need a proper mapping from distance→alpha calibrated on training folds (or an outer fold).\n",
    "\n",
    "Next step in executor: implement **two-stage**:\n",
    "- In each fold: split train into train_inner / cal_inner by group (solvent ramp).\n",
    "- Fit baseline model on train_inner.\n",
    "- For cal_inner: compute distances to train_inner; choose alpha per distance bin that minimizes MSE when blending to mean.\n",
    "- Fit monotone isotonic reg to map distance → alpha.\n",
    "- Apply to fold test.\n",
    "\n",
    "This is implementable inside a template-safe model without violating rules (hyperparameters fixed globally)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
