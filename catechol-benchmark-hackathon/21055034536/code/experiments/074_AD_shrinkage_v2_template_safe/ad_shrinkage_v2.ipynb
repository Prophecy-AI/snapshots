{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08a6587",
   "metadata": {},
   "source": [
    "# Experiment 053: Exact Template Submission\n",
    "\n",
    "**Goal:** Use EXACTLY the template code structure to ensure submission format is correct.\n",
    "\n",
    "**Approach:** Copy the template code exactly, only changing the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166392fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:56.647329Z",
     "iopub.status.busy": "2026-01-16T15:19:56.646771Z",
     "iopub.status.idle": "2026-01-16T15:19:58.370027Z",
     "shell.execute_reply": "2026-01-16T15:19:58.369574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup (from template)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d455d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:58.371580Z",
     "iopub.status.busy": "2026-01-16T15:19:58.371377Z",
     "iopub.status.idle": "2026-01-16T15:19:58.377702Z",
     "shell.execute_reply": "2026-01-16T15:19:58.377260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data loading functions (adapted for local paths)\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_FEATURES = [\"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_FEATURES = [\"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).any(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276dab63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:58.378720Z",
     "iopub.status.busy": "2026-01-16T15:19:58.378608Z",
     "iopub.status.idle": "2026-01-16T15:19:58.382139Z",
     "shell.execute_reply": "2026-01-16T15:19:58.381729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Base classes (from template)\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4966e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:58.383103Z",
     "iopub.status.busy": "2026-01-16T15:19:58.382986Z",
     "iopub.status.idle": "2026-01-16T15:19:58.388230Z",
     "shell.execute_reply": "2026-01-16T15:19:58.387844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Featurizer (from template)\n",
    "\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] * 2 + 3\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_solventB_pct = X[[\"SolventB%\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_A_feat_tensor = torch.tensor(X_smiles_A_feat.values)\n",
    "        X_smiles_B_feat_tensor = torch.tensor(X_smiles_B_feat.values)\n",
    "        X_solventB_pct_tensor = torch.tensor(X_solventB_pct.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_A_feat_tensor, X_smiles_B_feat_tensor, X_solventB_pct_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "print(\"Featurizers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbb1065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:58.389530Z",
     "iopub.status.busy": "2026-01-16T15:19:58.389407Z",
     "iopub.status.idle": "2026-01-16T15:19:58.486690Z",
     "shell.execute_reply": "2026-01-16T15:19:58.486249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)\n",
      "GPMLPLGBM_ADShrinkage defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GP+MLP+LGBM ensemble (template-safe exp_030 reproduction) + AD shrinkage v2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# ---- Load lookup tables (same as exp_030) ----\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "DRFP_DF = load_features('drfps_catechol')\n",
    "ACS_PCA_DF = load_features('acs_pca_descriptors')\n",
    "\n",
    "# Filter DRFP to nonzero-variance columns (unsupervised; solvent table only)\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "\n",
    "def _solvent_embed(names):\n",
    "    \"\"\"Embedding for a solvent name using Spange + DRFP(nzvar) + ACS PCA.\"\"\"\n",
    "    sp = SPANGE_DF.loc[names].values\n",
    "    dr = DRFP_FILTERED.loc[names].values\n",
    "    ac = ACS_PCA_DF.loc[names].values\n",
    "    return np.hstack([sp, dr, ac])\n",
    "\n",
    "\n",
    "def _kinetic_vec(X: pd.DataFrame) -> np.ndarray:\n",
    "    X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "    temp_c = X_vals[:, 1:2]\n",
    "    time_m = X_vals[:, 0:1]\n",
    "    temp_k = temp_c + 273.15\n",
    "    inv_temp = 1000.0 / temp_k\n",
    "    log_time = np.log(time_m + 1e-6)\n",
    "    interaction = inv_temp * log_time\n",
    "    return np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "\n",
    "\n",
    "def _dist_features_single(X: pd.DataFrame) -> np.ndarray:\n",
    "    # Loop73: pure solvent embed distance didn't correlate positively with error; add kinetics.\n",
    "    return np.hstack([_kinetic_vec(X), _solvent_embed(X['SOLVENT NAME'])])\n",
    "\n",
    "\n",
    "def _dist_features_full(X: pd.DataFrame, flip: bool = False) -> np.ndarray:\n",
    "    A = X['SOLVENT A NAME']\n",
    "    B = X['SOLVENT B NAME']\n",
    "    pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "    if flip:\n",
    "        A, B = B, A\n",
    "        pct = 1.0 - pct\n",
    "    A_e = _solvent_embed(A)\n",
    "    B_e = _solvent_embed(B)\n",
    "    blend = (1 - pct) * A_e + pct * B_e\n",
    "    diff_norm = np.linalg.norm(A_e - B_e, axis=1, keepdims=True)\n",
    "    pct2 = pct * (1 - pct)\n",
    "    # distance features focus on solvent similarity + mixture geometry (exclude kinetics per rec)\n",
    "    return np.hstack([blend, pct, pct2, diff_norm])\n",
    "\n",
    "\n",
    "# ---- Base featurizers with correct flip ----\n",
    "class FullFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        return _kinetic_vec(X)\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            A_dr = self.drfp_df.loc[A].values\n",
    "            B_dr = self.drfp_df.loc[B].values\n",
    "            A_ac = self.acs_pca_df.loc[A].values\n",
    "            B_ac = self.acs_pca_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            X_dr = (1 - pct) * A_dr + pct * B_dr\n",
    "            X_ac = (1 - pct) * A_ac + pct * B_ac\n",
    "            return np.hstack([X_kin, pct, X_sp, X_dr, X_ac])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            X_dr = self.drfp_df.loc[S].values\n",
    "            X_ac = self.acs_pca_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp, X_dr, X_ac])\n",
    "\n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip=flip), dtype=torch.double)\n",
    "\n",
    "\n",
    "class SimpleFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        return _kinetic_vec(X)\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            return np.hstack([X_kin, pct, X_sp])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp])\n",
    "\n",
    "\n",
    "# ---- Models (GP, MLP ensemble, LGBM) ----\n",
    "class GPWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = SimpleFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_all)\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, random_state=42)\n",
    "            gp.fit(X_scaled, y_all[:, i])\n",
    "            self.models.append(gp)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_scaled = self.scaler.transform(X_all)\n",
    "        preds = []\n",
    "        for gp in self.models:\n",
    "            preds.append(gp.predict(X_scaled))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, weights=[1.0, 1.0, 2.0]):\n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, dtype=torch.double)\n",
    "        self.huber = nn.HuberLoss(reduction='none')\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber_loss = self.huber(pred, target)\n",
    "        weighted_loss = huber_loss * self.weights.to(pred.device)\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class MLPModelInternal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super().__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_dim = h_dim\n",
    "        layers.extend([nn.Linear(prev_dim, output_dim), nn.Sigmoid()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class WeightedMLPEnsemble030:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single', loss_weights=[1.0, 1.0, 2.0]):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.loss_weights = loss_weights\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values, dtype=torch.double)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_all_scaled = self.scaler.fit_transform(X_all_np)\n",
    "        X_all = torch.tensor(X_all_scaled, dtype=torch.double)\n",
    "\n",
    "        dataset = TensorDataset(X_all, y_all)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.models = []\n",
    "        loss_fn = WeightedHuberLoss(self.loss_weights)\n",
    "\n",
    "        for seed in range(self.n_models):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            model = MLPModelInternal(X_all.shape[1], hidden_dims=self.hidden_dims).double()\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            model.train()\n",
    "            for _ in range(epochs):\n",
    "                for xb, yb in loader:\n",
    "                    pred = model(xb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "            self.models.append(model.eval())\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize_torch(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_test, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_scaled = self.scaler.transform(X_all_np)\n",
    "        X_t = torch.tensor(X_scaled, dtype=torch.double)\n",
    "        preds = []\n",
    "        for model in self.models:\n",
    "            with torch.no_grad():\n",
    "                preds.append(model(X_t).cpu().numpy())\n",
    "        out = np.mean(preds, axis=0)\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class LGBMWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            dtrain = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            m = lgb.train(params, dtrain, num_boost_round=200)\n",
    "            self.models.append(m)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        preds = []\n",
    "        for i in range(3):\n",
    "            preds.append(self.models[i].predict(X_all))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class GPMLPLGBMEnsemble_TemplateSafe(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.gp = GPWrapper030(data=data)\n",
    "        self.mlp = WeightedMLPEnsemble030(hidden_dims=[32, 16], n_models=5, data=data, loss_weights=[1.0, 1.0, 2.0])\n",
    "        self.lgbm = LGBMWrapper030(data=data)\n",
    "        self.weights = {'gp': 0.2, 'mlp': 0.5, 'lgbm': 0.3}\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        self.gp.train_model(X_train, y_train)\n",
    "        self.mlp.train_model(X_train, y_train)\n",
    "        self.lgbm.train_model(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        gp_pred = self.gp.predict(X)\n",
    "        mlp_pred = self.mlp.predict(X)\n",
    "        lgbm_pred = self.lgbm.predict(X)\n",
    "        out = (self.weights['gp'] * gp_pred + self.weights['mlp'] * mlp_pred + self.weights['lgbm'] * lgbm_pred)\n",
    "        return torch.clamp(out, 0, 1)\n",
    "\n",
    "\n",
    "class GPMLPLGBM_ADShrinkageV2(BaseModel):\n",
    "    \"\"\"AD shrinkage v2:\n",
    "    - calibrate alpha(d) on train_inner/cal_inner\n",
    "    - then refit base on full fold\n",
    "    - refit scaler+NN on full fold (unsupervised)\n",
    "    - use HINGE so alpha=0 for d<=tau (tau=quantile of train distances)\n",
    "    - fallback is kNN-mean-y (per query) instead of global mean\n",
    "    - optionally disable shrinkage on single (alpha=0) if not useful\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data='single', n_neighbors=10, tau_q=0.70, alpha_max=0.6, random_state=42, apply_on_single=False):\n",
    "        self.data_type = data\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.tau_q = tau_q\n",
    "        self.alpha_max = alpha_max\n",
    "        self.random_state = random_state\n",
    "        self.apply_on_single = apply_on_single\n",
    "\n",
    "        self.base = GPMLPLGBMEnsemble_TemplateSafe(data=data)\n",
    "        # for predicting fallback knn mean y\n",
    "        self.y_train_full = None\n",
    "        self.scaler = None\n",
    "        self.nn = None\n",
    "        self.tau = None\n",
    "        self.iso_models = None\n",
    "\n",
    "    def _dist_features(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return _dist_features_single(X)\n",
    "        else:\n",
    "            return _dist_features_full(X, flip=False)\n",
    "\n",
    "    def _groups(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return X['SOLVENT NAME'].values\n",
    "        else:\n",
    "            # canonicalize pair\n",
    "            a = X['SOLVENT A NAME'].astype(str).values\n",
    "            b = X['SOLVENT B NAME'].astype(str).values\n",
    "            ab = np.where(a < b, a + '||' + b, b + '||' + a)\n",
    "            return ab\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        # Split into train_inner/cal_inner by groups\n",
    "        groups = self._groups(X_train)\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=self.random_state)\n",
    "        tr_idx, cal_idx = next(gss.split(X_train, y_train, groups=groups))\n",
    "        X_tr = X_train.iloc[tr_idx]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_cal = X_train.iloc[cal_idx]\n",
    "        y_cal = y_train.iloc[cal_idx]\n",
    "\n",
    "        # Train base on train_inner for calibration\n",
    "        base_inner = GPMLPLGBMEnsemble_TemplateSafe(data=self.data_type)\n",
    "        base_inner.train_model(X_tr, y_tr)\n",
    "        pred_cal = base_inner.predict(X_cal).detach().cpu().numpy()\n",
    "\n",
    "        # Distance model fit on train_inner for calibration distances\n",
    "        D_tr = self._dist_features(X_tr)\n",
    "        D_cal = self._dist_features(X_cal)\n",
    "        scaler_tr = StandardScaler()\n",
    "        D_tr_s = scaler_tr.fit_transform(D_tr)\n",
    "        nn_tr = NearestNeighbors(n_neighbors=self.n_neighbors, metric='euclidean')\n",
    "        nn_tr.fit(D_tr_s)\n",
    "        d_cal = nn_tr.kneighbors(scaler_tr.transform(D_cal), return_distance=True)[0].mean(axis=1)\n",
    "\n",
    "        # Also compute train distances for hinge tau\n",
    "        d_tr = nn_tr.kneighbors(D_tr_s, return_distance=True)[0].mean(axis=1)\n",
    "        tau = float(np.quantile(d_tr, self.tau_q))\n",
    "\n",
    "        # hinge distances\n",
    "        d_cal_h = np.maximum(0.0, d_cal - tau)\n",
    "\n",
    "        # kNN fallback on cal: knn-mean-y from train_inner\n",
    "        # neighbors indices already available via nn_tr on D_cal\n",
    "        dist_mat, idx_mat = nn_tr.kneighbors(scaler_tr.transform(D_cal), return_distance=True)\n",
    "        w = 1.0 / (dist_mat + 1e-6)\n",
    "        w = w / w.sum(axis=1, keepdims=True)\n",
    "        y_tr_vals = y_tr.values\n",
    "        knn_cal = np.einsum('ij,ijk->ik', w, y_tr_vals[idx_mat])\n",
    "\n",
    "        # learn alpha(d) per target in bins, then isotonic on hinge distance\n",
    "        # If no shrink on single and single mode, force alpha=0.\n",
    "        if (self.data_type == 'single') and (not self.apply_on_single):\n",
    "            self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=0.0, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "            tau = 1e9\n",
    "        else:\n",
    "            qs = np.quantile(d_cal_h, np.linspace(0, 1, 11))\n",
    "            qs = np.unique(qs)\n",
    "            if len(qs) < 3:\n",
    "                self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "            else:\n",
    "                bin_ids = np.digitize(d_cal_h, qs[1:-1], right=True)\n",
    "                centers = []\n",
    "                alpha_t = [[], [], []]\n",
    "                for b in range(bin_ids.min(), bin_ids.max() + 1):\n",
    "                    m = bin_ids == b\n",
    "                    if m.sum() < 10:\n",
    "                        continue\n",
    "                    lo = qs[b]\n",
    "                    hi = qs[b + 1] if (b + 1) < len(qs) else qs[-1]\n",
    "                    centers.append((lo + hi) / 2)\n",
    "                    for t in range(3):\n",
    "                        yb = y_cal.values[m, t]\n",
    "                        pb = pred_cal[m, t]\n",
    "                        kb = knn_cal[m, t]\n",
    "                        num = np.sum((yb - pb) * (kb - pb))\n",
    "                        den = np.sum((kb - pb) ** 2) + 1e-12\n",
    "                        a = float(np.clip(num / den, 0.0, self.alpha_max))\n",
    "                        alpha_t[t].append(a)\n",
    "\n",
    "                if len(centers) < 2:\n",
    "                    self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "                else:\n",
    "                    order = np.argsort(centers)\n",
    "                    x = np.array(centers)[order]\n",
    "                    self.iso_models = []\n",
    "                    for t in range(3):\n",
    "                        y = np.array(alpha_t[t])[order]\n",
    "                        iso = IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip')\n",
    "                        iso.fit(x, y)\n",
    "                        self.iso_models.append(iso)\n",
    "\n",
    "        # Refit base on full fold training\n",
    "        self.base.train_model(X_train, y_train)\n",
    "\n",
    "        # Refit distance scaler+NN on full fold (unsupervised)\n",
    "        D_full = self._dist_features(X_train)\n",
    "        self.scaler = StandardScaler()\n",
    "        D_full_s = self.scaler.fit_transform(D_full)\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.n_neighbors, metric='euclidean')\n",
    "        self.nn.fit(D_full_s)\n",
    "        # Store full-fold y for kNN fallback\n",
    "        self.y_train_full = y_train.reset_index(drop=True).values\n",
    "        self.tau = tau\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base.predict(X).detach().cpu().numpy()\n",
    "        if (self.data_type == 'single') and (not self.apply_on_single):\n",
    "            return torch.tensor(np.clip(base_pred, 0.0, 1.0), dtype=torch.double)\n",
    "\n",
    "        D = self._dist_features(X)\n",
    "        D_s = self.scaler.transform(D)\n",
    "        dist_mat, idx_mat = self.nn.kneighbors(D_s, return_distance=True)\n",
    "        d = dist_mat.mean(axis=1)\n",
    "        d_h = np.maximum(0.0, d - self.tau)\n",
    "\n",
    "        # kNN mean y fallback\n",
    "        w = 1.0 / (dist_mat + 1e-6)\n",
    "        w = w / w.sum(axis=1, keepdims=True)\n",
    "        knn_pred = np.einsum('ij,ijk->ik', w, self.y_train_full[idx_mat])\n",
    "\n",
    "        out = base_pred.copy()\n",
    "        for t in range(3):\n",
    "            alpha = self.iso_models[t].predict(d_h)\n",
    "            alpha = np.clip(alpha, 0.0, self.alpha_max)\n",
    "            out[:, t] = (1 - alpha) * out[:, t] + alpha * knn_pred[:, t]\n",
    "\n",
    "        out = np.clip(out, 0.0, 1.0)\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "\n",
    "print('GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)')\n",
    "print('GPMLPLGBM_ADShrinkageV2 defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c189b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:19:58.487692Z",
     "iopub.status.busy": "2026-01-16T15:19:58.487577Z",
     "iopub.status.idle": "2026-01-16T15:21:01.484756Z",
     "shell.execute_reply": "2026-01-16T15:21:01.484241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([37, 3])\n",
      "Predictions range: [0.0408, 0.8741]\n",
      "Model test passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify model works\n",
    "print(\"Testing model...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "model = GPMLPLGBM_ADShrinkageV2(data='single')\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions range: [{preds.min():.4f}, {preds.max():.4f}]\")\n",
    "print(\"Model test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c561366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:21:13.129476Z",
     "iopub.status.busy": "2026-01-16T15:21:13.128808Z",
     "iopub.status.idle": "2026-01-16T15:46:00.791652Z",
     "shell.execute_reply": "2026-01-16T15:46:00.791187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:01, 61.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [02:03, 61.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [03:00, 59.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [03:58, 58.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [05:01, 60.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [06:05, 61.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [07:06, 61.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [08:07, 61.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [09:11, 61.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [10:13, 62.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [11:16, 62.49s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [12:20, 62.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [13:21, 62.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [14:24, 62.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [15:26, 62.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [16:28, 62.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [17:32, 62.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [18:33, 62.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [19:34, 61.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [20:37, 62.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [21:40, 62.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [22:43, 62.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [23:44, 62.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [24:47, 62.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [24:47, 61.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkageV2() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a49a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:46:00.792839Z",
     "iopub.status.busy": "2026-01-16T15:46:00.792712Z",
     "iopub.status.idle": "2026-01-16T16:41:44.489009Z",
     "shell.execute_reply": "2026-01-16T16:41:44.488555Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [04:08, 248.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [08:18, 249.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [12:28, 249.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [16:40, 250.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [20:53, 251.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [25:05, 251.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [29:40, 259.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [33:46, 255.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [38:25, 262.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [42:33, 257.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [47:00, 260.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [51:07, 256.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [55:43, 262.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [55:43, 257.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkage(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1fd826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:41:44.490160Z",
     "iopub.status.busy": "2026-01-16T16:41:44.490043Z",
     "iopub.status.idle": "2026-01-16T16:41:44.501516Z",
     "shell.execute_reply": "2026-01-16T16:41:44.501099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission.csv\n",
      "Total rows: 1883\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
