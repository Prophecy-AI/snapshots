{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08a6587",
   "metadata": {},
   "source": [
    "# Experiment 053: Exact Template Submission\n",
    "\n",
    "**Goal:** Use EXACTLY the template code structure to ensure submission format is correct.\n",
    "\n",
    "**Approach:** Copy the template code exactly, only changing the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166392fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:41.711736Z",
     "iopub.status.busy": "2026-01-16T14:16:41.711283Z",
     "iopub.status.idle": "2026-01-16T14:16:43.468066Z",
     "shell.execute_reply": "2026-01-16T14:16:43.467608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup (from template)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d455d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:43.469747Z",
     "iopub.status.busy": "2026-01-16T14:16:43.469569Z",
     "iopub.status.idle": "2026-01-16T14:16:43.475883Z",
     "shell.execute_reply": "2026-01-16T14:16:43.475463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data loading functions (adapted for local paths)\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_FEATURES = [\"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_FEATURES = [\"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).any(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276dab63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:43.477115Z",
     "iopub.status.busy": "2026-01-16T14:16:43.476997Z",
     "iopub.status.idle": "2026-01-16T14:16:43.480481Z",
     "shell.execute_reply": "2026-01-16T14:16:43.480099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Base classes (from template)\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4966e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:43.481617Z",
     "iopub.status.busy": "2026-01-16T14:16:43.481502Z",
     "iopub.status.idle": "2026-01-16T14:16:43.486602Z",
     "shell.execute_reply": "2026-01-16T14:16:43.486189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Featurizer (from template)\n",
    "\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] * 2 + 3\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_solventB_pct = X[[\"SolventB%\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_A_feat_tensor = torch.tensor(X_smiles_A_feat.values)\n",
    "        X_smiles_B_feat_tensor = torch.tensor(X_smiles_B_feat.values)\n",
    "        X_solventB_pct_tensor = torch.tensor(X_solventB_pct.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_A_feat_tensor, X_smiles_B_feat_tensor, X_solventB_pct_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "print(\"Featurizers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbb1065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:43.487706Z",
     "iopub.status.busy": "2026-01-16T14:16:43.487594Z",
     "iopub.status.idle": "2026-01-16T14:16:43.544037Z",
     "shell.execute_reply": "2026-01-16T14:16:43.543629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GP+MLP+LGBM ensemble (template-safe exp_030 reproduction) + AD mean-shrinkage\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# ---- Load lookup tables (same as exp_030) ----\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "DRFP_DF = load_features('drfps_catechol')\n",
    "ACS_PCA_DF = load_features('acs_pca_descriptors')\n",
    "\n",
    "# Filter DRFP to nonzero-variance columns (unsupervised; solvent table only)\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "\n",
    "def _solvent_embed(names):\n",
    "    \"\"\"Embedding for a solvent name using Spange + DRFP(nzvar) + ACS PCA.\"\"\"\n",
    "    sp = SPANGE_DF.loc[names].values\n",
    "    dr = DRFP_FILTERED.loc[names].values\n",
    "    ac = ACS_PCA_DF.loc[names].values\n",
    "    return np.hstack([sp, dr, ac])\n",
    "\n",
    "\n",
    "def _dist_features_single(X: pd.DataFrame) -> np.ndarray:\n",
    "    return _solvent_embed(X['SOLVENT NAME'])\n",
    "\n",
    "\n",
    "def _dist_features_full(X: pd.DataFrame, flip: bool = False) -> np.ndarray:\n",
    "    A = X['SOLVENT A NAME']\n",
    "    B = X['SOLVENT B NAME']\n",
    "    pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "    if flip:\n",
    "        A, B = B, A\n",
    "        pct = 1.0 - pct\n",
    "    A_e = _solvent_embed(A)\n",
    "    B_e = _solvent_embed(B)\n",
    "    blend = (1 - pct) * A_e + pct * B_e\n",
    "    return np.hstack([blend, pct])\n",
    "\n",
    "\n",
    "# ---- Feature engineering with correct flip ----\n",
    "class FullFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        # kinetic: [RT, TempC, invT, logRT, invT*logRT] => 5\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        return np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            A_dr = self.drfp_df.loc[A].values\n",
    "            B_dr = self.drfp_df.loc[B].values\n",
    "            A_ac = self.acs_pca_df.loc[A].values\n",
    "            B_ac = self.acs_pca_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            X_dr = (1 - pct) * A_dr + pct * B_dr\n",
    "            X_ac = (1 - pct) * A_ac + pct * B_ac\n",
    "            return np.hstack([X_kin, pct, X_sp, X_dr, X_ac])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            X_dr = self.drfp_df.loc[S].values\n",
    "            X_ac = self.acs_pca_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp, X_dr, X_ac])\n",
    "\n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip=flip), dtype=torch.double)\n",
    "\n",
    "\n",
    "class SimpleFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        return np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            return np.hstack([X_kin, pct, X_sp])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp])\n",
    "\n",
    "\n",
    "# ---- Models (GP, MLP ensemble, LGBM) ----\n",
    "class GPWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = SimpleFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_all)\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, random_state=42)\n",
    "            gp.fit(X_scaled, y_all[:, i])\n",
    "            self.models.append(gp)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_scaled = self.scaler.transform(X_all)\n",
    "        preds = []\n",
    "        for i, gp in enumerate(self.models):\n",
    "            preds.append(gp.predict(X_scaled))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, weights=[1.0, 1.0, 2.0]):\n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, dtype=torch.double)\n",
    "        self.huber = nn.HuberLoss(reduction='none')\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber_loss = self.huber(pred, target)\n",
    "        weighted_loss = huber_loss * self.weights.to(pred.device)\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class MLPModelInternal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super().__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_dim = h_dim\n",
    "        layers.extend([nn.Linear(prev_dim, output_dim), nn.Sigmoid()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class WeightedMLPEnsemble030:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single', loss_weights=[1.0, 1.0, 2.0]):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.loss_weights = loss_weights\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values, dtype=torch.double)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_all_scaled = self.scaler.fit_transform(X_all_np)\n",
    "        X_all = torch.tensor(X_all_scaled, dtype=torch.double)\n",
    "\n",
    "        dataset = TensorDataset(X_all, y_all)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.models = []\n",
    "        loss_fn = WeightedHuberLoss(self.loss_weights)\n",
    "\n",
    "        for seed in range(self.n_models):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            model = MLPModelInternal(X_all.shape[1], hidden_dims=self.hidden_dims).double()\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            model.train()\n",
    "            for _ in range(epochs):\n",
    "                for xb, yb in loader:\n",
    "                    pred = model(xb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "            self.models.append(model.eval())\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize_torch(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_test, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_scaled = self.scaler.transform(X_all_np)\n",
    "        X_t = torch.tensor(X_scaled, dtype=torch.double)\n",
    "        preds = []\n",
    "        for model in self.models:\n",
    "            with torch.no_grad():\n",
    "                preds.append(model(X_t).cpu().numpy())\n",
    "        out = np.mean(preds, axis=0)\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class LGBMWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            dtrain = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            m = lgb.train(params, dtrain, num_boost_round=200)\n",
    "            self.models.append(m)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        preds = []\n",
    "        for i in range(3):\n",
    "            preds.append(self.models[i].predict(X_all))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class GPMLPLGBMEnsemble_TemplateSafe(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.gp = GPWrapper030(data=data)\n",
    "        self.mlp = WeightedMLPEnsemble030(hidden_dims=[32, 16], n_models=5, data=data, loss_weights=[1.0, 1.0, 2.0])\n",
    "        self.lgbm = LGBMWrapper030(data=data)\n",
    "        self.weights = {'gp': 0.2, 'mlp': 0.5, 'lgbm': 0.3}\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        self.gp.train_model(X_train, y_train)\n",
    "        self.mlp.train_model(X_train, y_train)\n",
    "        self.lgbm.train_model(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        gp_pred = self.gp.predict(X)\n",
    "        mlp_pred = self.mlp.predict(X)\n",
    "        lgbm_pred = self.lgbm.predict(X)\n",
    "        out = (self.weights['gp'] * gp_pred + self.weights['mlp'] * mlp_pred + self.weights['lgbm'] * lgbm_pred)\n",
    "        return torch.clamp(out, 0, 1)\n",
    "\n",
    "\n",
    "class GPMLPLGBM_ADShrinkage(BaseModel):\n",
    "    \"\"\"Template-safe AD mean-shrinkage layer.\n",
    "\n",
    "    Trains base ensemble; calibrates alpha(distance) using group split within fold.\n",
    "    Prediction shrinks base prediction toward per-target mean mu_train with alpha from isotonic regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data='single', n_neighbors=10, n_bins=10, random_state=42):\n",
    "        self.data_type = data\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.n_bins = n_bins\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.base = GPMLPLGBMEnsemble_TemplateSafe(data=data)\n",
    "\n",
    "        self.mu_train = None\n",
    "        self.scaler = None\n",
    "        self.nn = None\n",
    "        self.iso_models = None\n",
    "\n",
    "    def _dist_features(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return _dist_features_single(X)\n",
    "        else:\n",
    "            return _dist_features_full(X, flip=False)\n",
    "\n",
    "    def _groups(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return X['SOLVENT NAME'].values\n",
    "        else:\n",
    "            return (X['SOLVENT A NAME'].astype(str) + '||' + X['SOLVENT B NAME'].astype(str)).values\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        # group split within fold\n",
    "        groups = self._groups(X_train)\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=self.random_state)\n",
    "        tr_idx, cal_idx = next(gss.split(X_train, y_train, groups=groups))\n",
    "        X_tr = X_train.iloc[tr_idx]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_cal = X_train.iloc[cal_idx]\n",
    "        y_cal = y_train.iloc[cal_idx]\n",
    "\n",
    "        # Train base on train_inner\n",
    "        base_inner = GPMLPLGBMEnsemble_TemplateSafe(data=self.data_type)\n",
    "        base_inner.train_model(X_tr, y_tr)\n",
    "        pred_cal = base_inner.predict(X_cal).detach().cpu().numpy()\n",
    "\n",
    "        # Distance model fit on train_inner\n",
    "        D_tr = self._dist_features(X_tr)\n",
    "        D_cal = self._dist_features(X_cal)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        D_tr_s = self.scaler.fit_transform(D_tr)\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.n_neighbors, metric='euclidean')\n",
    "        self.nn.fit(D_tr_s)\n",
    "\n",
    "        d_cal = self.nn.kneighbors(self.scaler.transform(D_cal), return_distance=True)[0].mean(axis=1)\n",
    "\n",
    "        # per-target mean for shrinkage baseline\n",
    "        self.mu_train = y_tr.values.mean(axis=0)\n",
    "\n",
    "        # compute optimal alpha per distance bin (per target) and fit isotonic\n",
    "        qs = np.quantile(d_cal, np.linspace(0, 1, self.n_bins + 1))\n",
    "        qs = np.unique(qs)\n",
    "        if len(qs) < 3:\n",
    "            # degenerate\n",
    "            self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=1.0, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "        else:\n",
    "            bin_ids = np.digitize(d_cal, qs[1:-1], right=True)\n",
    "            centers = []\n",
    "            alpha_t = [[], [], []]\n",
    "\n",
    "            for b in range(bin_ids.min(), bin_ids.max() + 1):\n",
    "                m = bin_ids == b\n",
    "                if m.sum() < 10:\n",
    "                    continue\n",
    "                lo = qs[b]\n",
    "                hi = qs[b + 1] if (b + 1) < len(qs) else qs[-1]\n",
    "                centers.append((lo + hi) / 2)\n",
    "                for t in range(3):\n",
    "                    yb = y_cal.values[m, t]\n",
    "                    pb = pred_cal[m, t]\n",
    "                    mu = self.mu_train[t]\n",
    "                    # best alpha for pred_shrunk = (1-a)*p + a*mu\n",
    "                    # minimize mse => closed-form\n",
    "                    num = np.sum((yb - pb) * (mu - pb))\n",
    "                    den = np.sum((mu - pb) ** 2) + 1e-12\n",
    "                    a = float(np.clip(num / den, 0.0, 1.0))\n",
    "                    alpha_t[t].append(a)\n",
    "\n",
    "            if len(centers) < 2:\n",
    "                self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=1.0, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "            else:\n",
    "                order = np.argsort(centers)\n",
    "                x = np.array(centers)[order]\n",
    "                self.iso_models = []\n",
    "                for t in range(3):\n",
    "                    y = np.array(alpha_t[t])[order]\n",
    "                    iso = IsotonicRegression(increasing=True, y_min=0.0, y_max=1.0, out_of_bounds='clip')\n",
    "                    iso.fit(x, y)\n",
    "                    self.iso_models.append(iso)\n",
    "\n",
    "        # Refit base on full fold training\n",
    "        self.base.train_model(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base.predict(X).detach().cpu().numpy()\n",
    "        D = self._dist_features(X)\n",
    "        d = self.nn.kneighbors(self.scaler.transform(D), return_distance=True)[0].mean(axis=1)\n",
    "        out = base_pred.copy()\n",
    "        for t in range(3):\n",
    "            alpha = self.iso_models[t].predict(d)\n",
    "            alpha = np.clip(alpha, 0.0, 1.0)\n",
    "            out[:, t] = (1 - alpha) * out[:, t] + alpha * self.mu_train[t]\n",
    "        out = np.clip(out, 0.0, 1.0)\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "\n",
    "print('GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)')\n",
    "print('GPMLPLGBM_ADShrinkage defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c189b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:16:43.545160Z",
     "iopub.status.busy": "2026-01-16T14:16:43.545051Z",
     "iopub.status.idle": "2026-01-16T14:17:19.379889Z",
     "shell.execute_reply": "2026-01-16T14:17:19.379337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([37, 3])\n",
      "Predictions range: [0.0200, 0.8841]\n",
      "Model test passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify model works\n",
    "print(\"Testing model...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "model = GPMLPLGBM_ADShrinkage(data='single')\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions range: [{preds.min():.4f}, {preds.max():.4f}]\")\n",
    "print(\"Model test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c561366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:17:27.707349Z",
     "iopub.status.busy": "2026-01-16T14:17:27.706858Z",
     "iopub.status.idle": "2026-01-16T14:31:25.182548Z",
     "shell.execute_reply": "2026-01-16T14:31:25.182079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:34, 34.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [01:09, 34.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:42, 33.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [02:15, 33.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [02:50, 34.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [03:25, 34.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [04:00, 34.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [04:35, 34.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [05:10, 34.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [05:45, 34.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [06:20, 34.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [06:56, 35.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [07:30, 34.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [08:05, 34.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [08:40, 34.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [09:15, 34.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [09:51, 35.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [10:27, 35.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [11:01, 35.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [11:37, 35.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [12:12, 35.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [12:48, 35.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [13:22, 35.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [13:57, 35.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [13:57, 34.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent predictions: 656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkage() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a49a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T14:31:25.183813Z",
     "iopub.status.busy": "2026-01-16T14:31:25.183685Z",
     "iopub.status.idle": "2026-01-16T15:05:12.286182Z",
     "shell.execute_reply": "2026-01-16T15:05:12.285720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [02:29, 149.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [05:02, 151.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [07:32, 151.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [10:01, 150.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [12:38, 152.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [15:08, 151.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [17:55, 156.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [20:24, 154.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [23:15, 159.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [25:45, 156.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [28:34, 160.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [31:01, 156.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [33:47, 159.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [33:47, 155.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data predictions: 1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkage(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1fd826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T15:05:12.287363Z",
     "iopub.status.busy": "2026-01-16T15:05:12.287245Z",
     "iopub.status.idle": "2026-01-16T15:05:12.299135Z",
     "shell.execute_reply": "2026-01-16T15:05:12.298747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission.csv\n",
      "Total rows: 1883\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
