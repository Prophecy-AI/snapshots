{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a72a56",
   "metadata": {},
   "source": [
    "# Experiment 044: Hybrid Model\n",
    "\n",
    "**Strategy:** Use baseline features for single solvents (CV 0.008194) and non-linear features for mixtures (CV 0.073776).\n",
    "\n",
    "**Why:** \n",
    "- Non-linear features improve mixture CV by 12.5% (0.084 → 0.074)\n",
    "- Non-linear features hurt single solvent CV by 9.8% (0.008194 → 0.008994)\n",
    "- Hybrid approach captures the best of both worlds\n",
    "\n",
    "**Goal:** Submit to verify if mixture improvements translate to LB improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd46cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:49:49.155178Z",
     "iopub.status.busy": "2026-01-15T09:49:49.154470Z",
     "iopub.status.idle": "2026-01-15T09:49:51.972777Z",
     "shell.execute_reply": "2026-01-15T09:49:51.972095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62446bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:49:51.975093Z",
     "iopub.status.busy": "2026-01-15T09:49:51.974761Z",
     "iopub.status.idle": "2026-01-15T09:49:52.041305Z",
     "shell.execute_reply": "2026-01-15T09:49:52.040734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: 13 features\n",
      "DRFP: 2048 features\n",
      "Single solvent: 656 samples\n",
      "Full data: 1227 samples\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[[\"SM\", \"Product 2\", \"Product 3\"]]\n",
    "    return X, Y\n",
    "\n",
    "# Load feature lookup tables\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "drfp_df = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "\n",
    "SPANGE_COLS = [c for c in spange_df.columns if c != 'solvent smiles']\n",
    "DRFP_COLS = [c for c in drfp_df.columns if str(c).isdigit() or isinstance(c, int)]\n",
    "\n",
    "print(f'Spange: {len(SPANGE_COLS)} features')\n",
    "print(f'DRFP: {len(DRFP_COLS)} features')\n",
    "\n",
    "# Load data\n",
    "X_single, Y_single = load_data('single_solvent')\n",
    "X_full, Y_full = load_data('full')\n",
    "\n",
    "print(f'Single solvent: {len(X_single)} samples')\n",
    "print(f'Full data: {len(X_full)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84fa7e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:49:52.043244Z",
     "iopub.status.busy": "2026-01-15T09:49:52.043053Z",
     "iopub.status.idle": "2026-01-15T09:49:52.048373Z",
     "shell.execute_reply": "2026-01-15T09:49:52.047901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('MLPModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e53886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:49:52.050214Z",
     "iopub.status.busy": "2026-01-15T09:49:52.050043Z",
     "iopub.status.idle": "2026-01-15T09:49:52.071230Z",
     "shell.execute_reply": "2026-01-15T09:49:52.070708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridModel defined\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Model: baseline for single solvents, non-linear for mixtures\n",
    "class HybridModel:\n",
    "    \"\"\"Hybrid model that uses different feature extraction for single vs mixture data.\n",
    "    \n",
    "    For single solvents: baseline Spange + DRFP features (no interaction terms)\n",
    "    For mixtures: adds interaction and difference features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', gp_weight=0.15, mlp_weight=0.55, lgbm_weight=0.3):\n",
    "        self.data_type = data\n",
    "        self.gp_weight = gp_weight\n",
    "        self.mlp_weight = mlp_weight\n",
    "        self.lgbm_weight = lgbm_weight\n",
    "        \n",
    "        self.scaler = None\n",
    "        self.gp_models = []\n",
    "        self.mlp_models = []\n",
    "        self.lgbm_models = []\n",
    "    \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract features based on data type.\"\"\"\n",
    "        features_list = []\n",
    "        \n",
    "        for idx, row in X.iterrows():\n",
    "            # Kinetics features (5 features)\n",
    "            time_m = row['Residence Time']\n",
    "            temp_c = row['Temperature']\n",
    "            temp_k = temp_c + 273.15\n",
    "            \n",
    "            kinetics = np.array([\n",
    "                time_m,\n",
    "                temp_c,\n",
    "                1.0 / temp_k,\n",
    "                np.log(time_m + 1),\n",
    "                time_m / temp_k\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            if self.data_type == 'single':\n",
    "                # Single solvent: baseline features (no interaction terms)\n",
    "                solvent = row['SOLVENT NAME']\n",
    "                spange = spange_df.loc[solvent, SPANGE_COLS].values.astype(np.float32) if solvent in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "                drfp = drfp_df.loc[solvent, DRFP_COLS].values.astype(np.float32) if solvent in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "                \n",
    "                features = np.concatenate([kinetics, spange, drfp])\n",
    "            else:\n",
    "                # Mixture: add non-linear features\n",
    "                solvent_a = row['SOLVENT A NAME']\n",
    "                solvent_b = row['SOLVENT B NAME']\n",
    "                pct_b = row['SolventB%'] / 100.0\n",
    "                pct_a = 1 - pct_b\n",
    "                \n",
    "                # Get Spange descriptors\n",
    "                sp_a = spange_df.loc[solvent_a, SPANGE_COLS].values.astype(np.float32) if solvent_a in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "                sp_b = spange_df.loc[solvent_b, SPANGE_COLS].values.astype(np.float32) if solvent_b in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "                \n",
    "                # Linear mixing (baseline)\n",
    "                spange_linear = pct_a * sp_a + pct_b * sp_b\n",
    "                \n",
    "                # Non-linear features (interaction + difference)\n",
    "                interaction = sp_a * sp_b * pct_a * pct_b * 4  # Scaled interaction\n",
    "                difference = np.abs(sp_a - sp_b)  # Absolute difference\n",
    "                \n",
    "                # DRFP features (linear mixing)\n",
    "                dr_a = drfp_df.loc[solvent_a, DRFP_COLS].values.astype(np.float32) if solvent_a in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "                dr_b = drfp_df.loc[solvent_b, DRFP_COLS].values.astype(np.float32) if solvent_b in drfp_df.index else np.zeros(len(DRFP_COLS), dtype=np.float32)\n",
    "                drfp = pct_a * dr_a + pct_b * dr_b\n",
    "                \n",
    "                features = np.concatenate([kinetics, spange_linear, interaction, difference, drfp])\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        return np.array(features_list, dtype=np.float32)\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs=200):\n",
    "        X_feat = self._get_features(X_train)\n",
    "        y_np = y_train.values.astype(np.float32)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # GP feature count (kinetics + spange + interaction + difference for mixtures)\n",
    "        if self.data_type == 'single':\n",
    "            gp_feat_count = 18  # 5 kinetics + 13 spange\n",
    "        else:\n",
    "            gp_feat_count = 18 + 13 + 13  # 5 kinetics + 13 spange + 13 interaction + 13 difference\n",
    "        gp_feat_count = min(gp_feat_count, X_scaled.shape[1])\n",
    "        \n",
    "        # Train GP models (one per target)\n",
    "        self.gp_models = []\n",
    "        for i in range(3):\n",
    "            kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42)\n",
    "            gp.fit(X_scaled[:, :gp_feat_count], y_np[:, i])\n",
    "            self.gp_models.append(gp)\n",
    "        \n",
    "        # Train MLP models (ensemble of 3)\n",
    "        self.mlp_models = []\n",
    "        for _ in range(3):\n",
    "            model = MLPModel(X_scaled.shape[1], hidden_dims=[32, 16]).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            y_tensor = torch.tensor(y_np).to(device)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                for X_batch, y_batch in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(X_batch)\n",
    "                    weights = torch.tensor([1.0, 1.0, 2.0]).to(device)\n",
    "                    loss = (weights * (pred - y_batch)**2).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            model.eval()\n",
    "            self.mlp_models.append(model)\n",
    "        \n",
    "        # Train LGBM models (one per target)\n",
    "        self.lgbm_models = []\n",
    "        for i in range(3):\n",
    "            lgbm_model = lgb.LGBMRegressor(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=5,\n",
    "                num_leaves=31, random_state=42, verbose=-1\n",
    "            )\n",
    "            lgbm_model.fit(X_scaled, y_np[:, i])\n",
    "            self.lgbm_models.append(lgbm_model)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self._get_features(X_test)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        if self.data_type == 'single':\n",
    "            gp_feat_count = 18\n",
    "        else:\n",
    "            gp_feat_count = 18 + 13 + 13\n",
    "        gp_feat_count = min(gp_feat_count, X_scaled.shape[1])\n",
    "        \n",
    "        # GP predictions\n",
    "        gp_preds = np.zeros((len(X_test), 3))\n",
    "        for i, gp in enumerate(self.gp_models):\n",
    "            gp_preds[:, i] = gp.predict(X_scaled[:, :gp_feat_count])\n",
    "        \n",
    "        # MLP predictions\n",
    "        mlp_preds = []\n",
    "        for model in self.mlp_models:\n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(X_tensor).cpu().numpy()\n",
    "            mlp_preds.append(pred)\n",
    "        mlp_preds = np.mean(mlp_preds, axis=0)\n",
    "        \n",
    "        # LGBM predictions\n",
    "        lgbm_preds = np.zeros((len(X_test), 3))\n",
    "        for i, lgbm_model in enumerate(self.lgbm_models):\n",
    "            lgbm_preds[:, i] = lgbm_model.predict(X_scaled)\n",
    "        \n",
    "        # Ensemble\n",
    "        ensemble_preds = self.gp_weight * gp_preds + self.mlp_weight * mlp_preds + self.lgbm_weight * lgbm_preds\n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(ensemble_preds, dtype=torch.float32)\n",
    "\n",
    "print('HybridModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a61f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T09:50:05.597054Z",
     "iopub.status.busy": "2026-01-15T09:50:05.596323Z",
     "iopub.status.idle": "2026-01-15T10:09:05.347313Z",
     "shell.execute_reply": "2026-01-15T10:09:05.346723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid model on single solvent data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent CV MSE: 0.008597 +/- 0.007959\n",
      "Baseline (exp_035): CV = 0.008194\n"
     ]
    }
   ],
   "source": [
    "# Test the hybrid model on single solvent data (should match baseline)\n",
    "print(\"Testing hybrid model on single solvent data...\")\n",
    "print()\n",
    "\n",
    "all_solvents = sorted(X_single[\"SOLVENT NAME\"].unique())\n",
    "fold_mses_single = []\n",
    "\n",
    "for test_solvent in all_solvents:\n",
    "    mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "    \n",
    "    model = HybridModel(data='single')\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    \n",
    "    actuals = Y_single[~mask].values\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    fold_mses_single.append(mse)\n",
    "\n",
    "mean_mse_single = np.mean(fold_mses_single)\n",
    "std_mse_single = np.std(fold_mses_single)\n",
    "print(f\"Single solvent CV MSE: {mean_mse_single:.6f} +/- {std_mse_single:.6f}\")\n",
    "print(f\"Baseline (exp_035): CV = 0.008194\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b6703c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:09:27.787264Z",
     "iopub.status.busy": "2026-01-15T10:09:27.786381Z",
     "iopub.status.idle": "2026-01-15T10:35:52.719057Z",
     "shell.execute_reply": "2026-01-15T10:35:52.717743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hybrid model on mixture data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methanol_Ethylene Glycol [1,2-Ethanediol]: MSE = 0.013026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,1,3,3,3-Hexafluoropropan-2-ol_2-Methyltetrahydrofuran [2-MeTHF]: MSE = 0.490165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclohexane_IPA [Propan-2-ol]: MSE = 0.195959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water.Acetonitrile_Acetonitrile: MSE = 0.020792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetonitrile_Acetonitrile.Acetic Acid: MSE = 0.025743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Methyltetrahydrofuran [2-MeTHF]_Diethyl Ether [Ether]: MSE = 0.090010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,2,2-Trifluoroethanol_Water.2,2,2-Trifluoroethanol: MSE = 0.013782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMA [N,N-Dimethylacetamide]_Decanol: MSE = 0.012260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethanol_THF [Tetrahydrofuran]: MSE = 0.023127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dihydrolevoglucosenone (Cyrene)_Ethyl Acetate: MSE = 0.005159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTBE [tert-Butylmethylether]_Butanone [MEK]: MSE = 0.008241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tert-Butanol [2-Methylpropan-2-ol]_Dimethyl Carbonate: MSE = 0.009201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methyl Propionate_Ethyl Lactate: MSE = 0.166745\n",
      "\n",
      "Mixture CV MSE: 0.082631 +/- 0.132468\n",
      "Baseline mixture (exp_043): CV = 0.084319\n",
      "Non-linear mixture (exp_043): CV = 0.073776\n"
     ]
    }
   ],
   "source": [
    "# Test the hybrid model on mixture data (should be better than baseline)\n",
    "print(\"Testing hybrid model on mixture data...\")\n",
    "print()\n",
    "\n",
    "# Create ramp identifier\n",
    "X_full_copy = X_full.copy()\n",
    "X_full_copy['ramp'] = X_full_copy['SOLVENT A NAME'] + '_' + X_full_copy['SOLVENT B NAME']\n",
    "unique_ramps = X_full_copy['ramp'].unique()\n",
    "\n",
    "fold_mses_mixture = []\n",
    "\n",
    "for test_ramp in unique_ramps:\n",
    "    mask = X_full_copy['ramp'] != test_ramp\n",
    "    \n",
    "    model = HybridModel(data='full')\n",
    "    model.train_model(X_full[mask], Y_full[mask], epochs=150)\n",
    "    preds = model.predict(X_full[~mask])\n",
    "    \n",
    "    actuals = Y_full[~mask].values\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    fold_mses_mixture.append(mse)\n",
    "    print(f\"{test_ramp}: MSE = {mse:.6f}\")\n",
    "\n",
    "mean_mse_mixture = np.mean(fold_mses_mixture)\n",
    "std_mse_mixture = np.std(fold_mses_mixture)\n",
    "print(f\"\\nMixture CV MSE: {mean_mse_mixture:.6f} +/- {std_mse_mixture:.6f}\")\n",
    "print(f\"Baseline mixture (exp_043): CV = 0.084319\")\n",
    "print(f\"Non-linear mixture (exp_043): CV = 0.073776\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f814c004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T10:36:30.259526Z",
     "iopub.status.busy": "2026-01-15T10:36:30.258936Z",
     "iopub.status.idle": "2026-01-15T10:36:30.265090Z",
     "shell.execute_reply": "2026-01-15T10:36:30.264468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of Hybrid Model Results ===\n",
      "\n",
      "Single Solvent CV:\n",
      "  Hybrid model: 0.008597\n",
      "  Baseline (exp_035): 0.008194\n",
      "  Difference: 4.9%\n",
      "\n",
      "Mixture CV:\n",
      "  Hybrid model: 0.082631\n",
      "  Baseline (exp_043): 0.084319\n",
      "  Non-linear (exp_043): 0.073776\n",
      "  Difference vs baseline: -2.0%\n",
      "\n",
      "Combined CV (weighted): 0.056839\n",
      "\n",
      "Key Insight:\n",
      "The hybrid model provides modest improvement in mixture predictions (2%)\n",
      "but the HFIP outlier still dominates the error (MSE 0.49).\n"
     ]
    }
   ],
   "source": [
    "# Summary of hybrid model results\n",
    "print(\"=== Summary of Hybrid Model Results ===\")\n",
    "print()\n",
    "print(\"Single Solvent CV:\")\n",
    "print(f\"  Hybrid model: {mean_mse_single:.6f}\")\n",
    "print(f\"  Baseline (exp_035): 0.008194\")\n",
    "print(f\"  Difference: {(mean_mse_single - 0.008194) / 0.008194 * 100:.1f}%\")\n",
    "print()\n",
    "print(\"Mixture CV:\")\n",
    "print(f\"  Hybrid model: {mean_mse_mixture:.6f}\")\n",
    "print(f\"  Baseline (exp_043): 0.084319\")\n",
    "print(f\"  Non-linear (exp_043): 0.073776\")\n",
    "print(f\"  Difference vs baseline: {(mean_mse_mixture - 0.084319) / 0.084319 * 100:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Combined score (weighted by samples)\n",
    "n_single = len(X_single)\n",
    "n_full = len(X_full)\n",
    "total = n_single + n_full\n",
    "\n",
    "combined_mse = (n_single * mean_mse_single + n_full * mean_mse_mixture) / total\n",
    "print(f\"Combined CV (weighted): {combined_mse:.6f}\")\n",
    "print()\n",
    "print(\"Key Insight:\")\n",
    "print(\"The hybrid model provides modest improvement in mixture predictions (2%)\")\n",
    "print(\"but the HFIP outlier still dominates the error (MSE 0.49).\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
