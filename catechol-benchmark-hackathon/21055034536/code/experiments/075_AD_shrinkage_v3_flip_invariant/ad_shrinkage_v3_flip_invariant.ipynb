{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c08a6587",
   "metadata": {},
   "source": [
    "# Experiment 053: Exact Template Submission\n",
    "\n",
    "**Goal:** Use EXACTLY the template code structure to ensure submission format is correct.\n",
    "\n",
    "**Approach:** Copy the template code exactly, only changing the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166392fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:25.888620Z",
     "iopub.status.busy": "2026-01-16T16:54:25.888174Z",
     "iopub.status.idle": "2026-01-16T16:54:27.604638Z",
     "shell.execute_reply": "2026-01-16T16:54:27.604199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and setup (from template)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Data path for local execution\n",
    "DATA_PATH = \"/home/data\"\n",
    "\n",
    "print(\"Imports complete.\")\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d455d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:27.606308Z",
     "iopub.status.busy": "2026-01-16T16:54:27.606137Z",
     "iopub.status.idle": "2026-01-16T16:54:27.612269Z",
     "shell.execute_reply": "2026-01-16T16:54:27.611843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data loading functions (adapted for local paths)\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\n",
    "    \"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"\n",
    "]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_FEATURES = [\"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_FEATURES = [\"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    features = pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "    return features\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvents.\"\"\"\n",
    "    all_solvents = X[\"SOLVENT NAME\"].unique()\n",
    "    for solvent_name in sorted(all_solvents):\n",
    "        train_idcs_mask = X[\"SOLVENT NAME\"] != solvent_name\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Generate all leave-one-out splits across the solvent ramps.\"\"\"\n",
    "    all_solvent_ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    all_solvent_ramps = all_solvent_ramps.sort_values(by=[\"SOLVENT A NAME\", \"SOLVENT B NAME\"])\n",
    "    for _, solvent_pair in all_solvent_ramps.iterrows():\n",
    "        train_idcs_mask = (X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]] != solvent_pair).any(axis=1)\n",
    "        yield (\n",
    "            (X[train_idcs_mask], Y[train_idcs_mask]),\n",
    "            (X[~train_idcs_mask], Y[~train_idcs_mask]),\n",
    "        )\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276dab63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:27.613275Z",
     "iopub.status.busy": "2026-01-16T16:54:27.613160Z",
     "iopub.status.idle": "2026-01-16T16:54:27.616765Z",
     "shell.execute_reply": "2026-01-16T16:54:27.616348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base classes defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Base classes (from template)\n",
    "\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4966e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:27.617953Z",
     "iopub.status.busy": "2026-01-16T16:54:27.617645Z",
     "iopub.status.idle": "2026-01-16T16:54:27.622868Z",
     "shell.execute_reply": "2026-01-16T16:54:27.622460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizers defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Featurizer (from template)\n",
    "\n",
    "class PrecomputedFeaturizer(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "class PrecomputedFeaturizerMixed(SmilesFeaturizer):\n",
    "    def __init__(self, features='spange_descriptors'):\n",
    "        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n",
    "        self.features = features\n",
    "        self.featurizer = load_features(self.features)\n",
    "        self.feats_dim = self.featurizer.shape[1] * 2 + 3\n",
    "\n",
    "    def featurize(self, X):\n",
    "        X_numeric = X[INPUT_LABELS_NUMERIC]\n",
    "        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n",
    "        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n",
    "        X_solventB_pct = X[[\"SolventB%\"]]\n",
    "        X_numeric_tensor = torch.tensor(X_numeric.values)\n",
    "        X_smiles_A_feat_tensor = torch.tensor(X_smiles_A_feat.values)\n",
    "        X_smiles_B_feat_tensor = torch.tensor(X_smiles_B_feat.values)\n",
    "        X_solventB_pct_tensor = torch.tensor(X_solventB_pct.values)\n",
    "        X_out = torch.cat((X_numeric_tensor, X_smiles_A_feat_tensor, X_smiles_B_feat_tensor, X_solventB_pct_tensor), dim=1)\n",
    "        return X_out\n",
    "\n",
    "print(\"Featurizers defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdbb1065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:27.624275Z",
     "iopub.status.busy": "2026-01-16T16:54:27.624159Z",
     "iopub.status.idle": "2026-01-16T16:54:27.722818Z",
     "shell.execute_reply": "2026-01-16T16:54:27.722386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)\n",
      "GPMLPLGBM_ADShrinkageV2 defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GP+MLP+LGBM ensemble (template-safe exp_030 reproduction) + AD shrinkage v3 (flip-invariant full)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# ---- Load lookup tables (same as exp_030) ----\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "DRFP_DF = load_features('drfps_catechol')\n",
    "ACS_PCA_DF = load_features('acs_pca_descriptors')\n",
    "\n",
    "# Filter DRFP to nonzero-variance columns (unsupervised; solvent table only)\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "\n",
    "def _solvent_embed(names):\n",
    "    \"\"\"Embedding for a solvent name using Spange + DRFP(nzvar) + ACS PCA.\"\"\"\n",
    "    sp = SPANGE_DF.loc[names].values\n",
    "    dr = DRFP_FILTERED.loc[names].values\n",
    "    ac = ACS_PCA_DF.loc[names].values\n",
    "    return np.hstack([sp, dr, ac])\n",
    "\n",
    "\n",
    "def _kinetic_vec(X: pd.DataFrame) -> np.ndarray:\n",
    "    X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "    temp_c = X_vals[:, 1:2]\n",
    "    time_m = X_vals[:, 0:1]\n",
    "    temp_k = temp_c + 273.15\n",
    "    inv_temp = 1000.0 / temp_k\n",
    "    log_time = np.log(time_m + 1e-6)\n",
    "    interaction = inv_temp * log_time\n",
    "    return np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "\n",
    "\n",
    "def canonicalize_full_rows(X: pd.DataFrame):\n",
    "    \"\"\"Return canonical A,B,pct with lexicographic A<=B. If swapped, pct := 1-pct.\"\"\"\n",
    "    A = X['SOLVENT A NAME'].astype(str).values\n",
    "    B = X['SOLVENT B NAME'].astype(str).values\n",
    "    pct = X['SolventB%'].values.astype(np.float64).reshape(-1, 1)\n",
    "    swap = A > B\n",
    "    A_can = A.copy()\n",
    "    B_can = B.copy()\n",
    "    pct_can = pct.copy()\n",
    "    A_can[swap] = B[swap]\n",
    "    B_can[swap] = A[swap]\n",
    "    pct_can[swap] = 1.0 - pct[swap]\n",
    "    return A_can, B_can, pct_can\n",
    "\n",
    "\n",
    "def _dist_features_single(X: pd.DataFrame) -> np.ndarray:\n",
    "    # keep kinetic+embed; shrink disabled by default\n",
    "    return np.hstack([_kinetic_vec(X), _solvent_embed(X['SOLVENT NAME'])])\n",
    "\n",
    "\n",
    "def _dist_features_full(X: pd.DataFrame) -> np.ndarray:\n",
    "    # flip-invariant via canonicalization\n",
    "    A_can, B_can, pct = canonicalize_full_rows(X)\n",
    "    A_e = _solvent_embed(A_can)\n",
    "    B_e = _solvent_embed(B_can)\n",
    "    blend = (1 - pct) * A_e + pct * B_e\n",
    "    diff_norm = np.linalg.norm(A_e - B_e, axis=1, keepdims=True)\n",
    "    pct2 = pct * (1 - pct)\n",
    "    return np.hstack([blend, pct, pct2, diff_norm])\n",
    "\n",
    "\n",
    "# ---- Base featurizers with correct flip (unchanged) ----\n",
    "class FullFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        return _kinetic_vec(X)\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            A_dr = self.drfp_df.loc[A].values\n",
    "            B_dr = self.drfp_df.loc[B].values\n",
    "            A_ac = self.acs_pca_df.loc[A].values\n",
    "            B_ac = self.acs_pca_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            X_dr = (1 - pct) * A_dr + pct * B_dr\n",
    "            X_ac = (1 - pct) * A_ac + pct * B_ac\n",
    "            return np.hstack([X_kin, pct, X_sp, X_dr, X_ac])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            X_dr = self.drfp_df.loc[S].values\n",
    "            X_ac = self.acs_pca_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp, X_dr, X_ac])\n",
    "\n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip=flip), dtype=torch.double)\n",
    "\n",
    "\n",
    "class SimpleFeaturizer030:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.feats_dim = 5 + self.spange_df.shape[1]\n",
    "        if self.mixed:\n",
    "            self.feats_dim = 5 + 1 + self.spange_df.shape[1]\n",
    "\n",
    "    def _kinetic(self, X):\n",
    "        return _kinetic_vec(X)\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_kin = self._kinetic(X)\n",
    "        if self.mixed:\n",
    "            A = X['SOLVENT A NAME']\n",
    "            B = X['SOLVENT B NAME']\n",
    "            pct = X['SolventB%'].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                A, B = B, A\n",
    "                pct = 1.0 - pct\n",
    "            A_sp = self.spange_df.loc[A].values\n",
    "            B_sp = self.spange_df.loc[B].values\n",
    "            X_sp = (1 - pct) * A_sp + pct * B_sp\n",
    "            return np.hstack([X_kin, pct, X_sp])\n",
    "        else:\n",
    "            S = X['SOLVENT NAME']\n",
    "            X_sp = self.spange_df.loc[S].values\n",
    "            return np.hstack([X_kin, X_sp])\n",
    "\n",
    "\n",
    "# ---- Models (GP, MLP ensemble, LGBM) ----\n",
    "class GPWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = SimpleFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_all)\n",
    "        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, random_state=42)\n",
    "            gp.fit(X_scaled, y_all[:, i])\n",
    "            self.models.append(gp)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_scaled = self.scaler.transform(X_all)\n",
    "        preds = []\n",
    "        for gp in self.models:\n",
    "            preds.append(gp.predict(X_scaled))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, weights=[1.0, 1.0, 2.0]):\n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, dtype=torch.double)\n",
    "        self.huber = nn.HuberLoss(reduction='none')\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        huber_loss = self.huber(pred, target)\n",
    "        weighted_loss = huber_loss * self.weights.to(pred.device)\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class MLPModelInternal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super().__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_dim = h_dim\n",
    "        layers.extend([nn.Linear(prev_dim, output_dim), nn.Sigmoid()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class WeightedMLPEnsemble030:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single', loss_weights=[1.0, 1.0, 2.0]):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.loss_weights = loss_weights\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "        self.scaler = None\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values, dtype=torch.double)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_all_scaled = self.scaler.fit_transform(X_all_np)\n",
    "        X_all = torch.tensor(X_all_scaled, dtype=torch.double)\n",
    "\n",
    "        dataset = TensorDataset(X_all, y_all)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.models = []\n",
    "        loss_fn = WeightedHuberLoss(self.loss_weights)\n",
    "\n",
    "        for seed in range(self.n_models):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            model = MLPModelInternal(X_all.shape[1], hidden_dims=self.hidden_dims).double()\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            model.train()\n",
    "            for _ in range(epochs):\n",
    "                for xb, yb in loader:\n",
    "                    pred = model(xb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                    optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "            self.models.append(model.eval())\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize_torch(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_test, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        X_all_np = X_all.cpu().numpy()\n",
    "        X_scaled = self.scaler.transform(X_all_np)\n",
    "        X_t = torch.tensor(X_scaled, dtype=torch.double)\n",
    "        preds = []\n",
    "        for model in self.models:\n",
    "            with torch.no_grad():\n",
    "                preds.append(model(X_t).cpu().numpy())\n",
    "        out = np.mean(preds, axis=0)\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class LGBMWrapper030:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = FullFeaturizer030(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': 42\n",
    "        }\n",
    "        self.models = []\n",
    "        for i in range(3):\n",
    "            dtrain = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            m = lgb.train(params, dtrain, num_boost_round=200)\n",
    "            self.models.append(m)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_std = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "        else:\n",
    "            X_all = X_std\n",
    "        preds = []\n",
    "        for i in range(3):\n",
    "            preds.append(self.models[i].predict(X_all))\n",
    "        out = np.vstack(preds).T\n",
    "        if self.data_type == 'full':\n",
    "            n = len(X_std)\n",
    "            out = 0.5 * (out[:n] + out[n:])\n",
    "        return torch.tensor(np.clip(out, 0, 1), dtype=torch.double)\n",
    "\n",
    "\n",
    "class GPMLPLGBMEnsemble_TemplateSafe(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.gp = GPWrapper030(data=data)\n",
    "        self.mlp = WeightedMLPEnsemble030(hidden_dims=[32, 16], n_models=5, data=data, loss_weights=[1.0, 1.0, 2.0])\n",
    "        self.lgbm = LGBMWrapper030(data=data)\n",
    "        self.weights = {'gp': 0.2, 'mlp': 0.5, 'lgbm': 0.3}\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        self.gp.train_model(X_train, y_train)\n",
    "        self.mlp.train_model(X_train, y_train)\n",
    "        self.lgbm.train_model(X_train, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        gp_pred = self.gp.predict(X)\n",
    "        mlp_pred = self.mlp.predict(X)\n",
    "        lgbm_pred = self.lgbm.predict(X)\n",
    "        out = (self.weights['gp'] * gp_pred + self.weights['mlp'] * mlp_pred + self.weights['lgbm'] * lgbm_pred)\n",
    "        return torch.clamp(out, 0, 1)\n",
    "\n",
    "\n",
    "class GPMLPLGBM_ADShrinkageV3(BaseModel):\n",
    "    \"\"\"AD shrinkage v3: same as v2 but flip-invariant full distance/fallback via canonicalize_full_rows.\"\"\"\n",
    "\n",
    "    def __init__(self, data='single', n_neighbors=10, tau_q=0.70, alpha_max=0.6, random_state=42, apply_on_single=False):\n",
    "        self.data_type = data\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.tau_q = tau_q\n",
    "        self.alpha_max = alpha_max\n",
    "        self.random_state = random_state\n",
    "        self.apply_on_single = apply_on_single\n",
    "\n",
    "        self.base = GPMLPLGBMEnsemble_TemplateSafe(data=data)\n",
    "        self.y_train_full = None\n",
    "        self.scaler = None\n",
    "        self.nn = None\n",
    "        self.tau = None\n",
    "        self.iso_models = None\n",
    "\n",
    "    def _dist_features(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return _dist_features_single(X)\n",
    "        else:\n",
    "            return _dist_features_full(X)\n",
    "\n",
    "    def _groups(self, X: pd.DataFrame):\n",
    "        if self.data_type == 'single':\n",
    "            return X['SOLVENT NAME'].values\n",
    "        else:\n",
    "            A_can, B_can, _ = canonicalize_full_rows(X)\n",
    "            return np.array([a + '||' + b for a, b in zip(A_can, B_can)])\n",
    "\n",
    "    def train_model(self, X_train, y_train, device=None, verbose=False):\n",
    "        groups = self._groups(X_train)\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=self.random_state)\n",
    "        tr_idx, cal_idx = next(gss.split(X_train, y_train, groups=groups))\n",
    "        X_tr = X_train.iloc[tr_idx]\n",
    "        y_tr = y_train.iloc[tr_idx]\n",
    "        X_cal = X_train.iloc[cal_idx]\n",
    "        y_cal = y_train.iloc[cal_idx]\n",
    "\n",
    "        base_inner = GPMLPLGBMEnsemble_TemplateSafe(data=self.data_type)\n",
    "        base_inner.train_model(X_tr, y_tr)\n",
    "        pred_cal = base_inner.predict(X_cal).detach().cpu().numpy()\n",
    "\n",
    "        D_tr = self._dist_features(X_tr)\n",
    "        D_cal = self._dist_features(X_cal)\n",
    "        scaler_tr = StandardScaler()\n",
    "        D_tr_s = scaler_tr.fit_transform(D_tr)\n",
    "        nn_tr = NearestNeighbors(n_neighbors=self.n_neighbors, metric='euclidean')\n",
    "        nn_tr.fit(D_tr_s)\n",
    "        d_cal = nn_tr.kneighbors(scaler_tr.transform(D_cal), return_distance=True)[0].mean(axis=1)\n",
    "\n",
    "        d_tr = nn_tr.kneighbors(D_tr_s, return_distance=True)[0].mean(axis=1)\n",
    "        tau = float(np.quantile(d_tr, self.tau_q))\n",
    "        d_cal_h = np.maximum(0.0, d_cal - tau)\n",
    "\n",
    "        # kNN fallback on cal (from train_inner)\n",
    "        dist_mat, idx_mat = nn_tr.kneighbors(scaler_tr.transform(D_cal), return_distance=True)\n",
    "        w = 1.0 / (dist_mat + 1e-6)\n",
    "        w = w / w.sum(axis=1, keepdims=True)\n",
    "        y_tr_vals = y_tr.values\n",
    "        knn_cal = np.einsum('ij,ijk->ik', w, y_tr_vals[idx_mat])\n",
    "\n",
    "        if (self.data_type == 'single') and (not self.apply_on_single):\n",
    "            self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=0.0, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "            tau = 1e9\n",
    "        else:\n",
    "            qs = np.quantile(d_cal_h, np.linspace(0, 1, 11))\n",
    "            qs = np.unique(qs)\n",
    "            if len(qs) < 3:\n",
    "                self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "            else:\n",
    "                bin_ids = np.digitize(d_cal_h, qs[1:-1], right=True)\n",
    "                centers = []\n",
    "                alpha_t = [[], [], []]\n",
    "                for b in range(bin_ids.min(), bin_ids.max() + 1):\n",
    "                    m = bin_ids == b\n",
    "                    if m.sum() < 10:\n",
    "                        continue\n",
    "                    lo = qs[b]\n",
    "                    hi = qs[b + 1] if (b + 1) < len(qs) else qs[-1]\n",
    "                    centers.append((lo + hi) / 2)\n",
    "                    for t in range(3):\n",
    "                        yb = y_cal.values[m, t]\n",
    "                        pb = pred_cal[m, t]\n",
    "                        kb = knn_cal[m, t]\n",
    "                        num = np.sum((yb - pb) * (kb - pb))\n",
    "                        den = np.sum((kb - pb) ** 2) + 1e-12\n",
    "                        a = float(np.clip(num / den, 0.0, self.alpha_max))\n",
    "                        alpha_t[t].append(a)\n",
    "\n",
    "                if len(centers) < 2:\n",
    "                    self.iso_models = [IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip').fit([0.0, 1.0], [0.0, 0.0]) for _ in range(3)]\n",
    "                else:\n",
    "                    order = np.argsort(centers)\n",
    "                    x = np.array(centers)[order]\n",
    "                    self.iso_models = []\n",
    "                    for t in range(3):\n",
    "                        y = np.array(alpha_t[t])[order]\n",
    "                        iso = IsotonicRegression(increasing=True, y_min=0.0, y_max=self.alpha_max, out_of_bounds='clip')\n",
    "                        iso.fit(x, y)\n",
    "                        self.iso_models.append(iso)\n",
    "\n",
    "        # Refit base on full fold\n",
    "        self.base.train_model(X_train, y_train)\n",
    "\n",
    "        # Refit NN on full fold (canonicalized for full)\n",
    "        D_full = self._dist_features(X_train)\n",
    "        self.scaler = StandardScaler()\n",
    "        D_full_s = self.scaler.fit_transform(D_full)\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.n_neighbors, metric='euclidean')\n",
    "        self.nn.fit(D_full_s)\n",
    "        self.y_train_full = y_train.reset_index(drop=True).values\n",
    "        self.tau = tau\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base.predict(X).detach().cpu().numpy()\n",
    "        if (self.data_type == 'single') and (not self.apply_on_single):\n",
    "            return torch.tensor(np.clip(base_pred, 0.0, 1.0), dtype=torch.double)\n",
    "\n",
    "        D = self._dist_features(X)\n",
    "        D_s = self.scaler.transform(D)\n",
    "        dist_mat, idx_mat = self.nn.kneighbors(D_s, return_distance=True)\n",
    "        d = dist_mat.mean(axis=1)\n",
    "        d_h = np.maximum(0.0, d - self.tau)\n",
    "\n",
    "        w = 1.0 / (dist_mat + 1e-6)\n",
    "        w = w / w.sum(axis=1, keepdims=True)\n",
    "        knn_pred = np.einsum('ij,ijk->ik', w, self.y_train_full[idx_mat])\n",
    "\n",
    "        out = base_pred.copy()\n",
    "        for t in range(3):\n",
    "            alpha = self.iso_models[t].predict(d_h)\n",
    "            alpha = np.clip(alpha, 0.0, self.alpha_max)\n",
    "            out[:, t] = (1 - alpha) * out[:, t] + alpha * knn_pred[:, t]\n",
    "\n",
    "        out = np.clip(out, 0.0, 1.0)\n",
    "        return torch.tensor(out, dtype=torch.double)\n",
    "\n",
    "\n",
    "print('GPMLPLGBMEnsemble_TemplateSafe defined. (Flip bug fixed)')\n",
    "print('GPMLPLGBM_ADShrinkageV3 defined (flip-invariant full distance/fallback).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c189b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:54:27.723974Z",
     "iopub.status.busy": "2026-01-16T16:54:27.723859Z",
     "iopub.status.idle": "2026-01-16T16:55:30.581472Z",
     "shell.execute_reply": "2026-01-16T16:55:30.580984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "Single solvent data: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([37, 3])\n",
      "Predictions range: [0.0200, 0.8841]\n",
      "Model test passed!\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify model works\n",
    "print(\"Testing model...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent data: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "# Test one fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "model = GPMLPLGBM_ADShrinkageV3(data='single')\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Predictions range: [{preds.min():.4f}, {preds.max():.4f}]\")\n",
    "print(\"Model test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c561366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T16:55:39.574293Z",
     "iopub.status.busy": "2026-01-16T16:55:39.573668Z",
     "iopub.status.idle": "2026-01-16T17:20:28.501206Z",
     "shell.execute_reply": "2026-01-16T17:20:28.500649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [01:00, 60.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [02:01, 60.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [02:59, 59.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [03:57, 58.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [05:00, 60.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [06:03, 61.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [07:04, 61.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [08:04, 60.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [09:08, 61.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [10:11, 62.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [11:14, 62.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [12:17, 62.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [13:18, 62.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [14:21, 62.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [15:24, 62.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [16:27, 62.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [17:31, 63.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [18:32, 62.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [19:33, 62.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [20:36, 62.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [21:40, 62.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [22:43, 62.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [23:45, 62.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [24:48, 62.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [24:48, 62.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkageV2() # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a49a70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T17:20:28.502872Z",
     "iopub.status.busy": "2026-01-16T17:20:28.502690Z",
     "iopub.status.idle": "2026-01-16T18:16:42.989914Z",
     "shell.execute_reply": "2026-01-16T18:16:42.989469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [04:08, 248.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [08:18, 249.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [12:28, 249.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [16:39, 250.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [20:53, 251.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [25:06, 252.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [29:42, 259.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [34:04, 260.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [38:45, 266.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [42:53, 261.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [47:33, 266.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [51:38, 260.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [56:14, 264.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [56:14, 259.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = GPMLPLGBM_ADShrinkageV2(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1fd826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:16:42.991156Z",
     "iopub.status.busy": "2026-01-16T18:16:42.990969Z",
     "iopub.status.idle": "2026-01-16T18:16:43.002733Z",
     "shell.execute_reply": "2026-01-16T18:16:43.002311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to /home/submission/submission.csv\n",
      "Total rows: 1883\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "print(f\"Submission saved to /home/submission/submission.csv\")\n",
    "print(f\"Total rows: {len(submission)}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
