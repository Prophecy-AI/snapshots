{
  "query": "How to handle 0 and 1 values in Logit transform for regression targets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**How to deal with 0\u202fand\u202f1 values when you want to use a logit\u2011transform on a proportion (or probability) target**\u202f([stats.stackexchange\u202f\u2013\u202fempirical logit](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data)\u202f;\u202f[logit\u2011link handling of 0/1](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data)\u202f;\u202f[logistic\u2011regression\u202f\u2013\u202f0\u202fand\u202f1 replacement](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1))  \n\n1. **Use a binomial GLM (logit link) directly** \u2013 you do not need to compute the logit of the response yourself; `glm(..., family = binomial(link = \"logit\"))` (or `statsmodels.Logit`) internally handles exact 0 and 1 outcomes without producing infinities. This is the standard way to model binary or proportion data. ([Result\u202f6]).\n\n2. **If a linear\u2011model on a transformed target is required, apply an \u201cempirical logit\u201d**:  \n   - Choose a small constant \u03b5 (e.g.,\u202f0.5\u202f/\u202fn, where *n* is the denominator of the proportion) and replace each proportion *p* by  \n\n     \\[\n     p^{*}= \\frac{y + \\varepsilon}{n + 2\\varepsilon}\n     \\]\n\n     where *y* is the count of successes.  \n   - This forces every value into the open interval (0,\u202f1) and avoids \u00b1\u221e after the logit. ([Result\u202f3]).\n\n3. **A practical shortcut is to bound the proportions away from the limits**:  \n   - Set  \n\n     \\[\n     p^{*}= \\frac{p\\,(N-1)+0.5}{N}\n     \\]\n\n     where *N* is the total number of trials for that observation.  \n   - For a single\u2011observation proportion (i.e., *N*\u202f=\u202f1) this reduces to replacing 0\u202f\u2192\u202f0.5\u202f/\u202f2\u202f=\u202f0.25 and 1\u202f\u2192\u202f(1\u202f\u00d7\u202f0\u202f+\u202f0.5)/2\u202f=\u202f0.75, or, more commonly, replace 0\u202f\u2192\u202f0.05 and 1\u202f\u2192\u202f0.95 as a rough rule of thumb. Both approaches keep the transformed values finite. ([Result\u202f5]).\n\n4. **Compute the logit on the adjusted values**:  \n\n   \\[\n   \\text{logit}(p^{*}) = \\log\\!\\left(\\frac{p^{*}}{1-p^{*}}\\right)\n   \\]\n\n   Use this continuous variable in ordinary least\u2011squares or other linear\u2011model frameworks.\n\n5. **Consider beta regression (or zero\u2011one\u2011inflated beta models) when the data are proportions in (0,\u202f1) but you have many boundary values**. Beta regression naturally models the (0,\u202f1) range; the zero\u2011one\u2011inflated version explicitly accounts for excess 0\u2019s and 1\u2019s. This avoids any ad\u2011hoc adjustment altogether. (Implied by the discussion of alternatives to the logit transform.)\n\n**Summary** \u2013 For most regression tasks keep the response in its original 0/1 form and let a binomial GLM handle it. If you must linearise the target, add a tiny constant (empirical logit) or bound the values away from 0 and 1 before applying the logit, and then proceed with standard linear modelling.",
      "url": ""
    },
    {
      "title": "Empirical logit transformation on percentage data",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Empirical logit transformation on percentage data](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked9 years, 10 months ago\n\nModified [8 years, 10 months ago](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data?lastactivity)\n\nViewed\n9k times\n\n10\n\n$\\\\begingroup$\n\nI have already used the logit transform on my outcome variables (which are displayed in percentages). However, this obviously gives me -INF values and since my data includes a lot of zeros in some instances, this makes it hard to analyse.\n\nI have now tried an empirical logit transform, adding the smallest non-zero promotion to the numerator and denominator of my variables to remove the -INF values (as suggested in [http://www.esajournals.org/doi/abs/10.1890/10-0340.1](http://www.esajournals.org/doi/abs/10.1890/10-0340.1)).\n\nHowever, now my data are very non-normal again. I have tried experimenting with error terms to add to the logit transform but since have had no luck.\n\nIs there any way I can find a value to add to my transformation to ensure normality?\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [data-transformation](https://stats.stackexchange.com/questions/tagged/data-transformation)\n- [proportion](https://stats.stackexchange.com/questions/tagged/proportion)\n- [logit](https://stats.stackexchange.com/questions/tagged/logit)\n\n[Share](https://stats.stackexchange.com/q/109702)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/109702/edit)\n\nFollow\n\nasked Jul 28, 2014 at 15:16\n\n[![user3237820's user avatar](https://www.gravatar.com/avatar/765093648c9cdf35dea0a7894550eb7c?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/52533/user3237820)\n\n[user3237820](https://stats.stackexchange.com/users/52533/user3237820) user3237820\n\n56055 silver badges1313 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- 3\n\n\n\n\n\n$\\\\begingroup$A worthy option for your consideration is a generalized linear model. Please search our site for threads on GLMs. If you still want to transform the response, then search for threads about transformations, logarithms, and regression: many of them explicitly discuss whether and how to add a \"start value\" to the data before re-expressing them.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedJul 30, 2014 at 19:13\n\n- $\\\\begingroup$If your outcome variables are 'displayed in percentages' this suggests that they aren't originally percentages. Presumably they are counts. @whuber is suggesting starting instead with a logistic (or multinomial logistic) regression model, for which even conditional normality is not a requirement.$\\\\endgroup$\n\n\u2013\u00a0[conjugateprior](https://stats.stackexchange.com/users/1739/conjugateprior)\n\nCommentedJul 30, 2014 at 22:36\n\n- $\\\\begingroup$Thanks for these. However, although I do have the raw counts, the particular research I am carrying out means that I would expect that the raw counts would increase with my predictor variables. Therefore, percentages are giving me a more reliable measure in my analyses.$\\\\endgroup$\n\n\u2013\u00a0[user3237820](https://stats.stackexchange.com/users/52533/user3237820)\n\nCommentedJul 31, 2014 at 21:27\n\n- $\\\\begingroup$As per @whuber... Are you sure this isn't a binomial process? I would pick a trials count for each record and model it as success and failures. glm binomial supports this. If it isn't binomial in nature, perhaps try inverse hyperbolic sine transformations [worthwhile.typepad.com/worthwhile\\_canadian\\_initi/2011/07/\u2026](http://worthwhile.typepad.com/worthwhile_canadian_initi/2011/07/a-rant-on-inverse-hyperbolic-sine-transformations.html)$\\\\endgroup$\n\n\u2013\u00a0[Chris](https://stats.stackexchange.com/users/70282/chris)\n\nCommentedDec 11, 2017 at 18:27\n\n\n[Add a comment](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data)\u00a0\\|\n\n## 2 Answers 2\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n16\n\n$\\\\begingroup$\n\nI've had luck with setting epsilon to half of the smallest non-zero value and replacing all 0 values with epsilon and all 1 values with 1-epsilon. Then apply the logit transformation.\n\nThis method keeps the original form of the logit transformation, but allows 1 and 0 to be transformed to values that match the overall shape of the intended transformation (note the black dots in the figure at raw=0 and 1). In particular, it preserves the quality that 0.5 is transformed to 0, and the rest of the values are symmetric.\n\nOn the other hand, adding the smallest non-zero value as described in the paper changes the shape of the curve and destroys the symmetry.\n\n![Comparing two methods of ways to adjust the logit transformation to deal with zeros](https://i.sstatic.net/HN0Kk.png)\n\n[Share](https://stats.stackexchange.com/a/110037)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/110037/edit)\n\nFollow\n\nanswered Jul 30, 2014 at 19:09\n\n[![emudrak's user avatar](https://www.gravatar.com/avatar/546303cf4e7bcf804eee6373bc2006fb?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/40571/emudrak)\n\n[emudrak](https://stats.stackexchange.com/users/40571/emudrak) emudrak\n\n32222 silver badges1313 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Welcome to our site! This thoughtful, well-illustrated post is a useful contribution.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedJul 30, 2014 at 19:14\n\n- $\\\\begingroup$Thanks! I will give this a go, although I have been experimenting with zero/one inflated beta regression too.$\\\\endgroup$\n\n\u2013\u00a0[user3237820](https://stats.stackexchange.com/users/52533/user3237820)\n\nCommentedJul 31, 2014 at 21:28\n\n- $\\\\begingroup$Is that right that the empirical logit isn't centered at 0? This equation `log((0.5 + eps)/(1 - 0.5 + eps))` always gives me 0 no matter what eps is. I also plotted logit vs empirical logit curves and empirical logit is symmetric as well, but the curve is a bit less steep as you get toward the edges for large eps. Using a smaller eps can correct this, but then the points near 0 and 1 will get increasingly pulled closer to +/- infinity. Still, it behaves pretty well for values like 1e-3 to 1e-5$\\\\endgroup$\n\n\u2013\u00a0[CHP](https://stats.stackexchange.com/users/41028/chp)\n\nCommentedMay 10, 2019 at 9:14\n\n\n[Add a comment](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data)\u00a0\\|\n\n2\n\n$\\\\begingroup$\n\nOne approach, which would solve the problem you are having, is to use a robust regression method on the raw, untransformed values. For example, in R, you could do the following:\n\n```\nexample = data.frame(outcome = c(0,0,0.3,0.7,1),\n                     predictor = c('left','left','left','right','right'))\nm = glm(outcome ~ predictor,example,family=quasibinomial())\nsummary(m)\n\n```\n\n[Share](https://stats.stackexchange.com/a/162899)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/162899/edit)\n\nFollow\n\nanswered Jul 23, 2015 at 18:48\n\n[![HaberdashPI's user avatar](https://www.gravatar.com/avatar/2bb471d1185a19b1e574b797b520dcd6?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/83118/haberdashpi)\n\n[HaberdashPI](https://stats.stackexchange.com/users/83118/haberdashpi) HaberdashPI\n\n12111 bronze badge\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](htt...",
      "url": "https://stats.stackexchange.com/questions/109702/empirical-logit-transformation-on-percentage-data"
    },
    {
      "title": "How does the logit link handle binomial (1/0) data?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How does the logit link handle binomial (1/0) data?](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked8 years, 9 months ago\n\nModified [2 years, 11 months ago](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data?lastactivity)\n\nViewed\n2k times\n\n3\n\n$\\\\begingroup$\n\nI have a data set that contains a continuous explanatory variable and a set of responses as binary success and failures. For example,\n\n```\nrequire(stats)\n\ntest.data <- data.frame(variable = runif(1000,100,200))\nmake.data <- function(x){\n  if(runif(1,0,1) <= ((x + runif(1,-50,50) - 100)/100)){1} else {0}\n}\ntest.data$response <- sapply(test.data$variable, make.data)\n\nhead(test.data)\n#  variable response\n#1 171.4345        1\n#2 186.9876        0\n#3 122.4847        0\n#4 189.0977        1\n#5 109.0487        0\n#6 157.7554        1\n\n```\n\nIt's easy enough to run a glm on this data and get valid results, e.g.\n\n```\nglm.test <- glm(response ~ variable, data = test.data, family = binomial(\"logit\"))\n\n```\n\nSomehow, the embedded glm logit link function seems to be able to account for entirely zero and entirely one values. If I was to perform the link function manually, e.g.\n\n```\nlogit_func <- make.link(\"logit\")$linkfun\n\ntest.data$link_response <- sapply(test.data$response, logit_func)\n\n```\n\nFor obvious reasons I get a returned array of +Inf and -Inf.\n\n```\nhead(test.data)\n#  variable response link_response\n#1 185.1213        1           Inf\n#2 150.7970        1           Inf\n#3 178.1121        0          -Inf\n#4 127.2224        1           Inf\n#5 132.4209        0          -Inf\n#6 195.1341        1           Inf\n\n```\n\nSo my questions is, what is the embedded glm link function doing which the standard logit link function not doing? How could I emulate the embedded glm link function?\n\n- [r](https://stats.stackexchange.com/questions/tagged/r)\n- [generalized-linear-model](https://stats.stackexchange.com/questions/tagged/generalized-linear-model)\n- [logit](https://stats.stackexchange.com/questions/tagged/logit)\n- [link-function](https://stats.stackexchange.com/questions/tagged/link-function)\n\n[Share](https://stats.stackexchange.com/q/168908)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/168908/edit)\n\nFollow\n\n[edited Aug 26, 2015 at 18:48](https://stats.stackexchange.com/posts/168908/revisions)\n\n[![gung - Reinstate Monica's user avatar](https://i.sstatic.net/TWeCs.png?s=64)](https://stats.stackexchange.com/users/7290/gung-reinstate-monica)\n\n[gung - Reinstate Monica](https://stats.stackexchange.com/users/7290/gung-reinstate-monica)\n\n146k8989 gold badges401401 silver badges710710 bronze badges\n\nasked Aug 26, 2015 at 16:29\n\n[![IanCognito's user avatar](https://www.gravatar.com/avatar/e21d250288ff5abf3fe4bdc14a9be84a?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/83157/iancognito)\n\n[IanCognito](https://stats.stackexchange.com/users/83157/iancognito) IanCognito\n\n14355 bronze badges\n\n$\\\\endgroup$\n\n1\n\n- 7\n\n\n\n\n\n$\\\\begingroup$There's an excellent discussion of what link functions do [here](http://stats.stackexchange.com/a/30909/17230). They're not transformations applied to the response.$\\\\endgroup$\n\n\u2013\u00a0[Scortchi - Reinstate Monica](https://stats.stackexchange.com/users/17230/scortchi-reinstate-monica) \u2666\n\nCommentedAug 26, 2015 at 16:36\n\n\n[Add a comment](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data)\u00a0\\|\n\n## 3 Answers 3\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n4\n\n$\\\\begingroup$\n\n$\\\\newcommand{\\\\variable}{\\\\rm variable}$The link function is link to parameter of the distribution (in this example is $p$ of Bernoulli distribution) to the linear score $\\\\eta$ (in this example is $b\\_0+b\\_1\\\\times\\\\variable$)\n\n$\\\\log(p\\_i/(1-p\\_i))=b\\_0+b\\_1\\\\times\\\\variable$\n\nThen such $p$ derives the outcome of $0$ and $1$ by the binomial probability function $p\\_i^{y\\_i}(1-p\\_i)^{1-y\\_i}$\n\nThe link function is not the link from or to the response directly.\n\n[Share](https://stats.stackexchange.com/a/168916)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/168916/edit)\n\nFollow\n\n[edited Aug 26, 2015 at 20:05](https://stats.stackexchange.com/posts/168916/revisions)\n\n[![gung - Reinstate Monica's user avatar](https://i.sstatic.net/TWeCs.png?s=64)](https://stats.stackexchange.com/users/7290/gung-reinstate-monica)\n\n[gung - Reinstate Monica](https://stats.stackexchange.com/users/7290/gung-reinstate-monica)\n\n146k8989 gold badges401401 silver badges710710 bronze badges\n\nanswered Aug 26, 2015 at 17:08\n\n[![Vincent's user avatar](https://i.sstatic.net/91T0a.jpg?s=64)](https://stats.stackexchange.com/users/29187/vincent)\n\n[Vincent](https://stats.stackexchange.com/users/29187/vincent) Vincent\n\n2,48611 gold badge1515 silver badges1515 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nIn case of logistic regression, you have a response variable $y\\_i$ that is 0/1 and (in the univariate case) one explanatory variable.\n\nSo you have, for each case in your sample, a binary outcome $y\\_i$ and a value for $x\\_i$.\n\nThe **idea is that the outcome 0/1 is the outcome of a Bernoulli variable with success probability $p\\_i$ that depends on $x\\_i$.**\n\nFor the case $i$ the probability that the outcome is $y\\_i \\\\in \\\\{0,1\\\\}$ is $p\\_i^{y\\_i}(1-p\\_i)^{1-y\\_i}$. Indeed, the probability of $y\\_i=1$ is $p\\_i$, obtained by substituting $y\\_i=1$ in $p\\_i^{y\\_i}(1-p\\_i)^{1-y\\_i}$, and the probability of $y\\_i=0$ is $1-p\\_i$, obtained by substituting $y\\_i=0$ in $p\\_i^{y\\_i}(1-p\\_i)^{1-y\\_i}$.\n\nIt is **further assumed that the probability $p\\_i$ depends on $x\\_i$**, i.e. $p\\_i(x\\_i)$. So for case $i$, the probability to observe the outcome $y\\_i$ is equal to $p\\_i(x\\_i)^{y\\_i}(1-p\\_i(x\\_i))^{1-y\\_i}$.\n\nThe probability to observe all the $y\\_i$ for all cases in your sample is thus ( _if all the observations are independent_)\n\n$\\\\prod\\_{i=1}^n p\\_i(x\\_i)^{y\\_i}(1-p\\_i(x\\_i))^{1-y\\_i}$.\n\n**This is the place where the link function comes in:** It is assumed that the dependence of $p\\_i$ on $x\\_i$ has a very **particular functional form namely**:\n\n$p\\_i(x\\_i)=\\\\frac{1}{1+e^{-(\\\\beta\\_0+\\\\beta\\_1x\\_i)}}$.\n\nSo the probability to observe all $y\\_i$ for all cases in the sample is given by:\n\n$\\\\prod\\_{i=1}^n \\\\left(\\\\frac{1}{1+e^{-(\\\\beta\\_0+\\\\beta\\_1x\\_i)}}\\\\right)^{y\\_i}\\\\left(1-\\\\frac{1}{1+e^{-(\\\\beta\\_0+\\\\beta\\_1x\\_i)}}\\\\right)^{1-y\\_i}$.\n\nNote that the parameters $\\\\beta\\_0$ and $\\\\beta\\_1$ are what we are looking for and if we see this probability as a fuction of these unknown parameters, then we get the likelihood function:\n\n$L(\\\\beta\\_0, \\\\beta\\_1)=\\\\prod\\_{i=1}^n \\\\left(\\\\frac{1}{1+e^{-(\\\\beta\\_0+\\\\beta\\_1x\\_i)}}\\\\right)^{y\\_i}\\\\left(1-\\\\frac{1}{1+e^{-(\\\\beta\\_0+\\\\beta\\_1x\\_i)}}\\\\right)^{1-y\\_i}$.\n\nAs the $x\\_i$ and $y\\_i$ are known from our sample, we can find the values $\\\\hat{\\\\beta}\\_i$ that maximise the likelihood.\n\n**So, as you observed (and as @Scortchi said), you can not simply transform the zero and one ,** but **instead you have to model each case in your sample as a Bernoulli variable with a success probability that depends on $x\\_i$ and estimate the parameters by maximum likelihood estimation**.\n\n[Share](https://stats.stackexchange.com/a/168925)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/168925/edit)\n\nFollow\n\n[edited Jul 2, 2021 at 16:30](https://stats.stackexchange.com/posts/168...",
      "url": "https://stats.stackexchange.com/questions/168908/how-does-the-logit-link-handle-binomial-1-0-data"
    },
    {
      "title": "Logistic Regression in R and How to deal with 0 and 1",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Logistic Regression in R and How to deal with 0 and 1](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked8 years, 9 months ago\n\nModified [8 years, 9 months ago](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1?lastactivity)\n\nViewed\n3k times\n\n5\n\n$\\\\begingroup$\n\nIn helping us understand how to fit a logistic regression in `R`, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method.\n\nThen we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite **different**.\n\nMy question is in `glm` function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?\n\n- [r](https://stats.stackexchange.com/questions/tagged/r)\n- [logistic](https://stats.stackexchange.com/questions/tagged/logistic)\n- [generalized-linear-model](https://stats.stackexchange.com/questions/tagged/generalized-linear-model)\n- [logit](https://stats.stackexchange.com/questions/tagged/logit)\n\n[Share](https://stats.stackexchange.com/q/173568)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/173568/edit)\n\nFollow\n\nasked Sep 21, 2015 at 23:48\n\n[![LaTeXFan's user avatar](https://www.gravatar.com/avatar/546303cf4e7bcf804eee6373bc2006fb?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/36575/latexfan)\n\n[LaTeXFan](https://stats.stackexchange.com/users/36575/latexfan) LaTeXFan\n\n1,30611 gold badge1414 silver badges2828 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- $\\\\begingroup$I am wondering if you replace 0 and 1 with 0.05 and 0.95 what link function will you use then?$\\\\endgroup$\n\n\u2013\u00a0[Deep North](https://stats.stackexchange.com/users/61705/deep-north)\n\nCommentedSep 22, 2015 at 0:51\n\n- $\\\\begingroup$@DeepNorth For logistic regression, obviously the link function is logit. In particular, log(p/1-p).$\\\\endgroup$\n\n\u2013\u00a0[LaTeXFan](https://stats.stackexchange.com/users/36575/latexfan)\n\nCommentedSep 22, 2015 at 0:55\n\n- $\\\\begingroup$while logit link is a transformation for binomial distribution and for binomial distribution there are only 1s and 0s.$\\\\endgroup$\n\n\u2013\u00a0[Deep North](https://stats.stackexchange.com/users/61705/deep-north)\n\nCommentedSep 22, 2015 at 1:01\n\n\n[Add a comment](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n7\n\n$\\\\begingroup$\n\nThat is very strange advice, I am forced to wonder who in the world advanced it.\n\nThe correct way to fit a logistic regression leaves the zeros and ones alone, and determines the parameters that minimize the log likelihood function:\n\n$$ f(\\\\beta) = \\\\sum\\_i y\\_i \\\\log(p\\_i) + (1 - y\\_i) \\\\log(1 - p\\_i) $$\n\nWhere $p\\_i$ is shorthand for\n\n$$ p\\_i = \\\\frac{e^{\\\\beta \\\\cdot x\\_i}}{1 + e^{\\\\beta \\\\cdot x\\_i} }$$\n\nThe exponents are vector dot products and $p\\_i$ is a function of the parameter vector $\\\\beta$. The $y\\_i$s in this expression are either $0$ or $1$, and it's pleasant to notice that this causes each term to be equal to either\n\n$$ \\\\log(p\\_i) $$\n\nor\n\n$$ \\\\log(1 - p\\_i) $$\n\nGenerally, yes, this expression is minimized using a method called iteratively re-weighted least squares, which is itself derived from Newton's classical method for minimizing non-linear functions.\n\nR's `glm` function does exactly this. No response replacement in sight.\n\n[Share](https://stats.stackexchange.com/a/173571)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/173571/edit)\n\nFollow\n\nanswered Sep 22, 2015 at 1:04\n\n[![Matthew Drury's user avatar](https://i.sstatic.net/NCIeU.jpg?s=64)](https://stats.stackexchange.com/users/74500/matthew-drury)\n\n[Matthew Drury](https://stats.stackexchange.com/users/74500/matthew-drury) Matthew Drury\n\n36k44 gold badges114114 silver badges143143 bronze badges\n\n$\\\\endgroup$\n\n6\n\n- $\\\\begingroup$Could you give a reference on how to solve this minimisation problem using the iteratively re-weighted least squares, please?$\\\\endgroup$\n\n\u2013\u00a0[LaTeXFan](https://stats.stackexchange.com/users/36575/latexfan)\n\nCommentedSep 22, 2015 at 1:27\n\n- $\\\\begingroup$It's the kind of thing that's worth working out for yourself, but here's a reference that I like: [win-vector.com/blog/2011/09/\u2026](http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/)$\\\\endgroup$\n\n\u2013\u00a0[Matthew Drury](https://stats.stackexchange.com/users/74500/matthew-drury)\n\nCommentedSep 22, 2015 at 1:43\n\n- $\\\\begingroup$Thanks. BTW, I do not think the idea in the question strange. The idea of link function is to transform the original response variable (0, 1) so that after transformation we obtain linear structure. Hence, the name GLM.$\\\\endgroup$\n\n\u2013\u00a0[LaTeXFan](https://stats.stackexchange.com/users/36575/latexfan)\n\nCommentedSep 22, 2015 at 2:05\n\n- 1\n\n\n\n\n\n$\\\\begingroup$You're welcome. I don't think your explanation of a glm is quite right. I do not believe in any cases does the algorithm for fitting a glm transform the original response variable (or, none that I have in my working memory). The structure equation for a glm is $g(E(y \\\\mid x)) = \\\\beta \\\\cdot x$, it is the conditional expectation that is transformed, not $y$ itself.$\\\\endgroup$\n\n\u2013\u00a0[Matthew Drury](https://stats.stackexchange.com/users/74500/matthew-drury)\n\nCommentedSep 22, 2015 at 2:08\n\n- 2\n\n\n\n\n\n$\\\\begingroup$In GLM you transform the conditional mean, not the dependent variable. This is why a log link function can deal with 0s in the dependent variable and a logit link function can deal with 0s and 1s in the dependent variable. For the former see: [dx.doi.org/10.1162/rest.88.4.641](http://dx.doi.org/10.1162/rest.88.4.641), for the latter see [dx.doi.org/10.1002/\u2026](http://dx.doi.org/10.1002/%28SICI%291099-1255%28199611%2911:6%3C619::AID-JAE418%3E3.0.CO;2-1)$\\\\endgroup$\n\n\u2013\u00a0[Maarten Buis](https://stats.stackexchange.com/users/23853/maarten-buis)\n\nCommentedSep 22, 2015 at 8:13\n\n\n\\|\u00a0[Show **1** more comment](https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1)\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f173568%2flogistic-regression-in-r-and-how-to-deal-with-0-and-1%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [r](https://stats.stackexchange.com/questions/tagged/r) - [logistic](https://stats.stackexchange.com/questions/tagged/logistic) - [generalized-linear-model](https://stats.stackexchange.com/questions/tagged/generalized-linear-model) - [logit](https://stats.stackexchange.com/questions/tagged/logit)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](htt...",
      "url": "https://stats.stackexchange.com/questions/173568/logistic-regression-in-r-and-how-to-deal-with-0-and-1"
    },
    {
      "title": "How to Handle 0 and 1 in Logit Transformation?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How to Handle 0 and 1 in Logit Transformation? \\[closed\\]](https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked1 year, 1 month ago\n\nModified [1 year, 1 month ago](https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation?lastactivity)\n\nViewed\n776 times\n\n4\n\n$\\\\begingroup$\n\nI am planning to analyze experimental data using statistical methods, and I intend to perform analysis on repeated measurements using GEE (Generalized Estimating Equations) or RM ANOVA. Some of the measurements are all in proportion values (the number of individuals performing a specific behavior in each replicate).\nPreviously, I used the arcsine square root transformation, but some papers claim that this method is not reasonable, mentioning that the logit transformation is more rational.\nTherefore, I want to analyze the data after logit transformation, but the data contains both 0 and 1. These values are represented as negative and positive infinity, respectively, making subsequent statistical analysis impossible.\nIs there a way to perform logit transformation in this case? Or should I abandon this method and stick with the previous arcsine square root transformation?\n\n- [logistic](https://stats.stackexchange.com/questions/tagged/logistic)\n- [repeated-measures](https://stats.stackexchange.com/questions/tagged/repeated-measures)\n- [data-transformation](https://stats.stackexchange.com/questions/tagged/data-transformation)\n\n[Share](https://stats.stackexchange.com/q/642465)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/642465/edit)\n\nFollow\n\nasked Mar 13, 2024 at 7:00\n\n[![soobinism's user avatar](https://lh3.googleusercontent.com/a-/AOh14GjX_Ke1pbYC4BbozVmL3h4Je6x03_gqlhDwYEG0=k-s64)](https://stats.stackexchange.com/users/409222/soobinism)\n\n[soobinism](https://stats.stackexchange.com/users/409222/soobinism) soobinism\n\n4111 bronze badge\n\n$\\\\endgroup$\n\n6\n\n- 7\n\n\n\n\n\n$\\\\begingroup$Why do you want to transform your outcome, rather than use a GLM with an appropriate link for the linear predictor? For example, a binomial GLM would not be able to _predict_ a probability of exactly 0 or 1 for the reasons you mention, but that's rarely an issue in practice, and you can use your data as-is. I'm also not sure why you would treat such data as a proportion, thereby discarding the size of the denominator? This sounds like a classical example of a binomial experiment.$\\\\endgroup$\n\n\u2013\u00a0[PBulls](https://stats.stackexchange.com/users/353280/pbulls)\n\nCommentedMar 13, 2024 at 9:18\n\n- $\\\\begingroup$It would help us to know your response variable, your explanatory variables, and what you are trying to test. You say you have repeated measures : what is repeated ?$\\\\endgroup$\n\n\u2013\u00a0[CaroZ](https://stats.stackexchange.com/users/131178/caroz)\n\nCommentedMar 13, 2024 at 11:07\n\n- $\\\\begingroup$I do not think the bast approach is to transform your data. I would simply use a Generalized Linear Mixed Model for a binomial distribution.$\\\\endgroup$\n\n\u2013\u00a0[CaroZ](https://stats.stackexchange.com/users/131178/caroz)\n\nCommentedMar 13, 2024 at 11:11\n\n- $\\\\begingroup$Not quite understanding why you want Logit. For example, if the model is linear after log-log transformation where the original equation becomes $Y\\\\approx P0\\\\,X1^{P1}X2^{P2}...Xn^{Pn}(S=1,2)^{Ps}$ then you can encode a binary variable for example, \"For these fits, values of S equal to 1 for female and 2 for male were assigned so that lnS exists and so that regressions using ln S would find appropriate scaling exponents.\" DOI: 10.1097/01.mnm.0000237988.52572.2c$\\\\endgroup$\n\n\u2013\u00a0[Carl](https://stats.stackexchange.com/users/99274/carl)\n\nCommentedMar 14, 2024 at 1:52\n\n- $\\\\begingroup$Here is a more specific outline of the experimental procedure: I observed the behavior of the target organisms and recorded it. There were 8 individuals in each jar, and I calculated the ratio of individuals showing the behavior by dividing by 8. Since there were a total of 12 jars, n=12. This behavior was observed twice a day for 14 days, resulting in 28 measurements of behavior. I compare the ratio of behavior occurrence between the control and experimental groups.$\\\\endgroup$\n\n\u2013\u00a0[soobinism](https://stats.stackexchange.com/users/409222/soobinism)\n\nCommentedMar 14, 2024 at 5:42\n\n\n\\|\u00a0[Show **1** more comment](https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation)\n\n## 3 Answers 3\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n5\n\n$\\\\begingroup$\n\nI would agree with the Caroz. If you have the number of individuals that completed a task, and you have the total number of individuals that tried the task in each replicate, you can easily use a Binomial observation model in a Generalized Linear Modeling framework. This will allow you to specifically ask targeted questions while accounting for repeated measures and other confounders. [This lecture by Richard McElreath provides a very useful introductio](https://www.youtube.com/watch?v=nPi5yGbfxuo) n (in fact, the entire series is well worth your time).\n\nIf you stick with proportions and use a Beta regression, you will not only have difficulties with the `1` s and `0` s, but you will lose the uncertainty associated with the number of individuals that tried the task. For example, a if `1 / 4` successfully completed the task, your proportion of `0.25` should be considered _more uncertain_ than if `4 / 12` were successful. Treating your data as proportions and either using a Beta regression or some other form of regression will provide less useful inferences because your estimates won't consider these underlying uncertainties\n\n[Share](https://stats.stackexchange.com/a/642483)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/642483/edit)\n\nFollow\n\n[edited Mar 13, 2024 at 23:08](https://stats.stackexchange.com/posts/642483/revisions)\n\nanswered Mar 13, 2024 at 11:22\n\n[![Nicholas Clark's user avatar](https://i.sstatic.net/LByEE.jpg?s=64)](https://stats.stackexchange.com/users/409044/nicholas-clark)\n\n[Nicholas Clark](https://stats.stackexchange.com/users/409044/nicholas-clark) Nicholas Clark\n\n1,13122 silver badges1111 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- 1\n\n\n\n\n\n$\\\\begingroup$What is above vs. below changes over time. Consider referring more specifically.$\\\\endgroup$\n\n\u2013\u00a0[Richard Hardy](https://stats.stackexchange.com/users/53690/richard-hardy)\n\nCommentedMar 13, 2024 at 17:25\n\n- $\\\\begingroup$Thanks. Have edited now$\\\\endgroup$\n\n\u2013\u00a0[Nicholas Clark](https://stats.stackexchange.com/users/409044/nicholas-clark)\n\nCommentedMar 13, 2024 at 23:09\n\n\n[Add a comment](https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation)\u00a0\\|\n\n3\n\n$\\\\begingroup$\n\nYou don't need to transform your data or use a generalized linear mixed model. You can use a linear model (yes, even with bounded outcomes) and adjust your standard errors to account for clustering in the experiment (i.e., by participant). A cluster-robust standard error also adjusts for heteroscedasticity and the fact that the outcome variable is not normally distributed within groups. This can be done using GEE as you have suggested without any modification to your outcome variable. I addition, the coefficients in your model can be directly interpretable as differences in means on the scale of your outcome variable.\n\nNote that this approach can be problematic when the experiment is imbalanced, i.e., there are different numbers of observations in each cell of the design, or when you are additionally adjusting f...",
      "url": "https://stats.stackexchange.com/questions/642465/how-to-handle-0-and-1-in-logit-transformation"
    },
    {
      "title": "How does one deal with lots of 1's when applying the logit function?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [How does one deal with lots of 1's when applying the logit function?](https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked2 years, 11 months ago\n\nModified [2 years, 11 months ago](https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function?lastactivity)\n\nViewed\n345 times\n\n0\n\n$\\\\begingroup$\n\nI have a dataframe containing many feature columns, one of them being `Score`, with values between 0 and 1 (if it helps: it represents the difficulty of a test. The closer to 1, the easier the test). I created an extra column that applies the logit function to each of these values.\n\nThe reason why I did this transformation is to do a logistic regression to predict the difficulty. However, many values will be `inf` because in the input there are values such as 0 and 1 (most of them are 1). What do I do with them? Should I apply the logistic function (inverse of logit) to 0 and 1's? Should I just eliminate them?\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [logistic](https://stats.stackexchange.com/questions/tagged/logistic)\n\n[Share](https://stats.stackexchange.com/q/534227)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/534227/edit)\n\nFollow\n\nasked Jul 12, 2021 at 8:43\n\n[![n.mathfreak's user avatar](https://www.gravatar.com/avatar/24a74ab99a56831f927f4c5fa79d4e5e?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/327301/n-mathfreak)\n\n[n.mathfreak](https://stats.stackexchange.com/users/327301/n-mathfreak) n.mathfreak\n\n1122 bronze badges\n\n$\\\\endgroup$\n\n8\n\n- $\\\\begingroup$You're predicting difficulty, based on difficulty? This is confusing.$\\\\endgroup$\n\n\u2013\u00a0[Gijs](https://stats.stackexchange.com/users/137593/gijs)\n\nCommentedJul 12, 2021 at 8:56\n\n- 4\n\n\n\n\n\n$\\\\begingroup$Logit or logistic regression is in my experience always implemented by a dedicated routine or through some more general routine such as code for generalized linear models. It is not equivalent to regression following a logit transform: in fact for the classic case with a binary outcome with values 0 and 1 logit doesn't yield any finite values at all. For your case with a more nearly continuous proportion as outcome you should **not** drop any values, but just use an appropriate routine in your unnamed software environment.$\\\\endgroup$\n\n\u2013\u00a0[Nick Cox](https://stats.stackexchange.com/users/22047/nick-cox)\n\nCommentedJul 12, 2021 at 9:34\n\n- 2\n\n\n\n\n\n$\\\\begingroup$Indeed; your response or outcome is bounded and much of the point of logit regression is to respect that and also that it has a particular variance structure, so that for example as the mean approaches 0 or 1 so also the variance approaches zero. But as said logit regression is not regression on a logit-transfomed outcome. Some people would be happy in practice with a linear model fit to the untransformed data. Otherwise interest in applying a logit model to a continuous proportion is common in practice, but often only discussed in the context of generalized linear models.$\\\\endgroup$\n\n\u2013\u00a0[Nick Cox](https://stats.stackexchange.com/users/22047/nick-cox)\n\nCommentedJul 12, 2021 at 10:03\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Otherwise put, linear regression on the original data might work not too badly, although the ideal conditions for such a regression are unlikely to be met closely; logit regression using a generalized linear model routine is the alternative. Logit transformation of a continuous proportion is problematic because zeros and ones don't fit that recipe.$\\\\endgroup$\n\n\u2013\u00a0[Nick Cox](https://stats.stackexchange.com/users/22047/nick-cox)\n\nCommentedJul 12, 2021 at 10:06\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I would start with the book by Dobson and Barnett. I can't recall if they cover your application.$\\\\endgroup$\n\n\u2013\u00a0[Nick Cox](https://stats.stackexchange.com/users/22047/nick-cox)\n\nCommentedJul 12, 2021 at 10:39\n\n\n\\|\u00a0[Show **3** more comments](https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function)\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nWhat you seems to want is sometimes called _fractional logistic regression_, there are many Questions on this site, start with [What is the difference between logistic regression and Fractional response regression?](https://stats.stackexchange.com/questions/216122/what-is-the-difference-between-logistic-regression-and-fractional-response-regre) and search this site.\n\nBut very short:\n\n- You should **NOT** transform the scores, use them directly as the outcome variable. Logistic regression will transform implicitly their expectation, not the scores themselves.\n\n- Since your data is not really binomial, the likelihood function is not strictly correct, so use [quasi-likelihood](https://stats.stackexchange.com/questions/tagged/quasi-likelihood)\n\n- With R you would use something like\n\n\n```\nmod0 <- glm(score ~  ., family=quasibinomial, data=your_df)\n\n```\n\n[Share](https://stats.stackexchange.com/a/534307)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/534307/edit)\n\nFollow\n\nanswered Jul 12, 2021 at 20:16\n\n[![kjetil b halvorsen's user avatar](https://i.sstatic.net/Nri78.jpg?s=64)](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen)\n\n[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666kjetil b halvorsen\n\n80.4k3131 gold badges196196 silver badges638638 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$Thank you! I'm working in Python and I found and used this reference: [statcompute.wordpress.com/2012/12/16/\u2026](https://statcompute.wordpress.com/2012/12/16/fractional-logit-model-with-python/). But I am getting a negative pseudo R^2, Log-Likelihood, LL-Null and LLR p-value. Is this something to worry about?$\\\\endgroup$\n\n\u2013\u00a0[n.mathfreak](https://stats.stackexchange.com/users/327301/n-mathfreak)\n\nCommentedJul 13, 2021 at 6:50\n\n- $\\\\begingroup$I have no experience with pseudo-R2. Negative loglik is as expected, but a negative p-value is impossible! Better for you to include the results (with some plots) in an edit to the Q$\\\\endgroup$\n\n\u2013\u00a0[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\nCommentedJul 13, 2021 at 15:18\n\n\n[Add a comment](https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f534227%2fhow-does-one-deal-with-lots-of-1s-when-applying-the-logit-function%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [regression](https://stats.stackexchange.com/questions/tagged/regression) - [logistic](https://stats.stackexchange.com/questions/tagged/logistic)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange....",
      "url": "https://stats.stackexchange.com/questions/534227/how-does-one-deal-with-lots-of-1s-when-applying-the-logit-function"
    },
    {
      "title": "When y= 1, logit is infinity. How can you regress that? Yet somehow, that's logistic regression",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [When y= 1, logit is infinity. How can you regress that? Yet somehow, that's logistic regression](https://stats.stackexchange.com/questions/69820/when-y-1-logit-is-infinity-how-can-you-regress-that-yet-somehow-thats-logi)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked10 years, 9 months ago\n\nModified [3 years ago](https://stats.stackexchange.com/questions/69820/when-y-1-logit-is-infinity-how-can-you-regress-that-yet-somehow-thats-logi?lastactivity)\n\nViewed\n9k times\n\n9\n\n$\\\\begingroup$\n\nLogistic regression essentially means taking the logit of your response proportions, p, and then doing standard regression. Consider the case where one p is 1.\n\nlogit(1) = log(1/(1-1)) = infinity. How can you do regression with infinity?\n\nThat is, if one of your observed proportions p is 1, then you are trying to find the line that minimises the sum of squared differences from a set of points that include infinity. How does this not drive the regression line to always have infinite slope?\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [logistic](https://stats.stackexchange.com/questions/tagged/logistic)\n\n[Share](https://stats.stackexchange.com/q/69820)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/69820/edit)\n\nFollow\n\nasked Sep 12, 2013 at 5:40\n\n[![Alex Holcombe's user avatar](https://www.gravatar.com/avatar/8747720601cc56109b8399a11f20c9da?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/1640/alex-holcombe)\n\n[Alex Holcombe](https://stats.stackexchange.com/users/1640/alex-holcombe) Alex Holcombe\n\n53911 gold badge77 silver badges99 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- 11\n\n\n\n\n\n$\\\\begingroup$You have mischaracterized logistic regression: one does not take the logits of the data but instead models the data as outcomes of random variables whose _parameters_ are expressed in terms of logits. For more information see _inter alia_ the [Wikipedia article on generalized linear models](http://en.wikipedia.org/wiki/Generalized_linear_model).$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedSep 12, 2013 at 5:44\n\n- $\\\\begingroup$OK, thanks! I misunderstood how the link function is used, and thus misunderstood glms. I'll be looking for a more basic tutorial than what I see in wikipedia.$\\\\endgroup$\n\n\u2013\u00a0[Alex Holcombe](https://stats.stackexchange.com/users/1640/alex-holcombe)\n\nCommentedSep 12, 2013 at 6:16\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I have offered an answer, Alex, to help bridge that gap.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedSep 12, 2013 at 16:03\n\n\n[Add a comment](https://stats.stackexchange.com/questions/69820/when-y-1-logit-is-infinity-how-can-you-regress-that-yet-somehow-thats-logi)\u00a0\\|\n\n## 3 Answers 3\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/69820/when-y-1-logit-is-infinity-how-can-you-regress-that-yet-somehow-thats-logi?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n12\n\n$\\\\begingroup$\n\nThe question characterizes logistic regression as\n\n$$\\\\text{logit}(y) = \\\\beta\\_0 + \\\\beta\\_1 x + \\\\varepsilon$$\n\nand proposes to fit this model using least squares. It points out that because $y$ is a binary ($0$-$1$) variable, $\\\\text{logit}(y)$ is undefined (or should be considered infinite), which is--to say the least--problematic!\n\nThe resolution of this conundrum is to avoid taking the logit of $y$ but instead apply its _inverse,_ the _logistic function_\n\n$$f(x) = \\\\frac{1}{1 + \\\\exp(-x)},$$\n\nto the right hand side. Because $y$ on the left hand side still is a random variable with possible outcomes $0$ and $1$, it must be a Bernoulli variable: that is, what we need to know about it is the _chance that $y=1$,_ written $\\\\Pr(y=1).$ Therefore we make another attempt in the form\n\n$$\\\\Pr(y=1) = f(\\\\beta\\_0 + \\\\beta\\_1 x).$$\n\nThis is an example of a [generalized linear model](http://en.wikipedia.org/wiki/Generalized_linear_model). Its parameters $\\\\beta\\_0$ and $\\\\beta\\_1$ are typically (but not necessarily) found using Maximum Likelihood.\n\nTo understand this better, many people find it instructive to _create_ synthetic datasets according to this model (instead of analyzing actual data, where the true model is unknown). We will look at how that might be coded in `R`, which is well suited to expressing and simulating statistical models. First, though, let's inspect its results.\n\n![Figure](https://i.sstatic.net/vFOUe.png)\n\nThe data are shown as jittered points (they have been randomly shifted slightly in the horizontal direction to resolve overlaps). The _true_ underlying probability function is plotted in solid red. The probability function fit using Maximum Likliehood is plotted in dashed gray.\n\nYou can see that where the red curve is high--which means the chance of $y=1$ is high--most of the data are $1$'s, whereas where the red curve drops to low levels, most of the data are $0$'s. _The height of the curve stipulates the chance that the response will be a $1$._ In logistic regression, the _curve_ usually has the sigmoidal shape of the logistic function, while the data are always either at $y=1$ or $y=0$.\n\nReading over the code, which is written for expressive clarity, will help make these descriptions precise.\n\n```\n#\n# Synthesize some data.\n#\nset.seed(17)                        # Allows results to be reproduced exactly\nn <- 8                              # Number of distinct x values\nk <- 4                              # Number of independent obs's for each x\nx <- rep(1:n, 4)                    # Independent values\nbeta <- c(3, -1)                    # True parameters\nlogistic <- function(x) 1 / (1 + exp(-x))\nprobability <- function(x, b) logistic(b[1] + b[2]*x)\ny <- rbinom(n*k, size=1, prob=probability(x, beta))   # Simulated data\n#\n# Fit the data using a logistic regression.\n#\nsummary(fit <- glm(y ~ x, family=binomial(link=\"logit\")))\n#\n# Plot the data, the true underlying probability function, and the fitted one.\n#\njitter <- runif(n*k, -1/3, 1/3)     # Displaces points to resolve overlaps\nplot(x+jitter, y, type=\"p\", xlab=\"x\", ylab=\"y\", main=\"Data with true and fitted models\")\ncurve(probability(x, beta), col=\"Red\", lwd=2, add=TRUE)\ncurve(probability(x, coef(fit)), col=\"Gray\", lwd=2, lty=2, add=TRUE)\n\n```\n\n[Share](https://stats.stackexchange.com/a/69873)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/69873/edit)\n\nFollow\n\n[edited Sep 26, 2013 at 22:04](https://stats.stackexchange.com/posts/69873/revisions)\n\nanswered Sep 12, 2013 at 16:03\n\n[![whuber's user avatar](https://www.gravatar.com/avatar/d64b4ff6c88982c9c131beeaa536973b?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/919/whuber)\n\n[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666whuber\n\n327k6161 gold badges760760 silver badges1.3k1.3k bronze badges\n\n$\\\\endgroup$\n\n3\n\n- $\\\\begingroup$I do not understand \"Because y on the left hand side\", which left hand side? There is above only a logit(y) and logit(1) or logit(0) is infininity. However in the shown plot there is only a range from 0 to 1. Still confusing, how the logit is regressed.$\\\\endgroup$\n\n\u2013\u00a0[granular\\_bastard](https://stats.stackexchange.com/users/269684/granular-bastard)\n\nCommentedDec 21, 2020 at 18:30\n\n- $\\\\begingroup$@granular the only equation preceding that remark has a left side and a right side.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedDec 21, 2020 at 18:46\n\n- $\\\\begingroup$Nothertheless, still do not understand the answer and a new posed question that is similar was deleted. And there is a bad equation style in commentaries. So we try to fulfill $1=\\\\frac{1}{1+\\\\text{exp}(-(\\\\beta\\_0+\\\\beta\\_1x))}$ if the dependent variable is 1 and $0=\\\\f...",
      "url": "https://stats.stackexchange.com/questions/69820/when-y-1-logit-is-infinity-how-can-you-regress-that-yet-somehow-thats-logi"
    },
    {
      "title": "Interesting Logistic Regression Idea - Problem: Data not currently in 0/1 form. Any solutions?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Interesting Logistic Regression Idea - Problem: Data not currently in 0/1 form. Any solutions?](https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked8 years, 10 months ago\n\nModified [8 years, 10 months ago](https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form/164127?lastactivity)\n\nViewed\n2k times\n\n3\n\n$\\\\begingroup$\n\nI am attempting to conduct a logistic regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. My response variable (service points) is binary in the sense that it can have only two outcomes for each observation - a success (service point win) or a failure (service point loss).\n\nI have an issue with my data: For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:\n\n250 matches, each with about 70 dependent variable observations (service points). So for each match I currently have the two variables: Total\\_Service\\_Points\\_Played **and** Total\\_Service\\_Points\\_Won.\n\nEg - Match 1: Total\\_Service\\_Points\\_Played: 70 ; Total\\_Service\\_Points\\_Won: 47\n\nSo my data isn't in 1's and 0's. Is there a way I can implement a logistic regression with my dependent variable observations in their current form? Is there any simple transformation that comes to mind?\n\nWhat springs to mind for me is to flesh out my match data into 1's and 0's. So following on from Match 1 above I would have: 47 1's followed by 26 0's . My data doesn't provide information as to what sequence these 1's and 0's arrived in, but since the depdendent variable observations are i.i.d this won't cause an issue? Correct me if I'm wrong please. Another issue posed by this technique would be the massive increase in my data - from 250 observations as a ratio (service point wins/service points played) to 250\\*70=17500 observations or more.\n\nAs a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values < 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about?\n\n- [r](https://stats.stackexchange.com/questions/tagged/r)\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [logistic](https://stats.stackexchange.com/questions/tagged/logistic)\n- [generalized-linear-model](https://stats.stackexchange.com/questions/tagged/generalized-linear-model)\n\n[Share](https://stats.stackexchange.com/q/164120)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/164120/edit)\n\nFollow\n\n[edited Jul 31, 2015 at 13:56](https://stats.stackexchange.com/posts/164120/revisions)\n\nStevie Kvothe\n\nasked Jul 31, 2015 at 10:51\n\n[![Stevie Kvothe's user avatar](https://www.gravatar.com/avatar/19d2ec9a2b3b2e71c9bb5f50b16bc76b?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/83421/stevie-kvothe)\n\n[Stevie Kvothe](https://stats.stackexchange.com/users/83421/stevie-kvothe) Stevie Kvothe\n\n5377 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- 1\n\n\n\n\n\n$\\\\begingroup$What are your explanatory variables ?$\\\\endgroup$\n\n\u2013\u00a0user83346\n\nCommentedJul 31, 2015 at 11:00\n\n- 3\n\n\n\n\n\n$\\\\begingroup$Look about r glm function. It tells that response can be two column matrix with columns having counts of successes and failures.$\\\\endgroup$\n\n\u2013\u00a0[Analyst](https://stats.stackexchange.com/users/28732/analyst)\n\nCommentedJul 31, 2015 at 11:03\n\n- $\\\\begingroup$@fcoppens my explanatory variables are : court surface (categorical with 3 types) and the log of opposition world ranking points. Any ideas?$\\\\endgroup$\n\n\u2013\u00a0[Stevie Kvothe](https://stats.stackexchange.com/users/83421/stevie-kvothe)\n\nCommentedJul 31, 2015 at 11:09\n\n\n[Add a comment](https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form/164127)\u00a0\\|\n\n## 3 Answers 3\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n9\n\n$\\\\begingroup$\n\nIf you're fitting with `glm`, you can use the win rate as your DV and use the `weights` option to specify the number of \"trials\" each rate observation is based on. From `?glm`:\n\n> For a binomial GLM prior weights are used to give the number of\n> trials when the response is the proportion of successes\n\nSo your call to `glm` would look something like this:\n\n```\nglm(Total_Service_Points_Won/Total_Service_Points_Played ~ ... ,\n    family = binomial(link=logit), weights = Total_Service_Points_Played)\n\n```\n\n[Share](https://stats.stackexchange.com/a/164122)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/164122/edit)\n\nFollow\n\nanswered Jul 31, 2015 at 11:18\n\n[![Mikko Marttila's user avatar](https://i.sstatic.net/0Zvu2.jpg?s=64)](https://stats.stackexchange.com/users/83560/mikko-marttila)\n\n[Mikko Marttila](https://stats.stackexchange.com/users/83560/mikko-marttila) Mikko Marttila\n\n20511 silver badge33 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Thanks. That works out nicely and the code supplied saved me a lot of time. As a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values < 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about?$\\\\endgroup$\n\n\u2013\u00a0[Stevie Kvothe](https://stats.stackexchange.com/users/83421/stevie-kvothe)\n\nCommentedJul 31, 2015 at 13:36\n\n- $\\\\begingroup$Stevie: no, there's no problem. If you had a wider range of variation of mean of response you could get more accuracy in estimates, but it's not a problem at all$\\\\endgroup$\n\n\u2013\u00a0[Glen\\_b](https://stats.stackexchange.com/users/805/glen-b)\n\nCommentedJul 31, 2015 at 17:33\n\n\n[Add a comment](https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form/164127)\u00a0\\|\n\n5\n\n$\\\\begingroup$\n\nProvided your predictors are constant during a match, it shouldn't matter. Suppose you are applying a generalized linear model of the form $\\\\exp(\\\\eta y - \\\\psi(\\\\eta))$ to some grouped responses, $y\\_{11},\\\\ldots,y\\_{1n\\_1},\\\\ldots,y\\_{N1},\\\\ldots,y\\_{Nn\\_N}$. Here there are $N$ groups (250 matches in your case) with $n\\_i$ observations (about 70 in your case) in the $ith$ group, and $\\\\eta()$ is (in your case) the logistic function. The likelihood is then\n\n$\\\\Pi\\_{i=1}^N\\\\Pi\\_{j=1}^{n\\_i} \\\\exp(\\\\eta\\_i y\\_{ij} - \\\\psi(\\\\eta\\_i))=\n\\\\Pi\\_{i=1}^N \\\\exp(\\\\Sigma\\_{j=1}^{n\\_i}\\\\eta\\_i y\\_{ij}-n\\_i\\\\psi(\\\\eta\\_i)) =\n\\\\Pi\\_{i=1}^N \\\\exp(n\\_i(\\\\eta\\_i y\\_{i.}-\\\\psi(\\\\eta\\_i))).$\n\nThe last equation is just the likelihood of an exponential family with sufficient statistic $y\\_{i.}=\\\\Sigma y\\_{ij}/n\\_i$ (the group match averages in your case). You can optimize the likelihood using this grouped statistic as well. So the likelihood equation is the same whether you use the group averages or any sequence of 0s, 1s that give the same group averages. In particular, the same co...",
      "url": "https://stats.stackexchange.com/questions/164120/interesting-logistic-regression-idea-problem-data-not-currently-in-0-1-form/164127"
    },
    {
      "title": "how to use sklearn when target variable is a proportion",
      "text": "##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [how to use sklearn when target variable is a proportion](https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked7 years, 1 month ago\n\nModified [5 years, 8 months ago](https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion?lastactivity)\n\nViewed\n9k times\n\n9\n\nThere are standard ways of predicting proportions such as logistic regression (without thresholding) and beta regression. There have already been discussions about this:\n\n[http://scikit-learn-general.narkive.com/4dSCktaM/using-logistic-regression-on-a-continuous-target-variable](http://scikit-learn-general.narkive.com/4dSCktaM/using-logistic-regression-on-a-continuous-target-variable)\n\n[http://scikit-learn-general.narkive.com/lLVQGzyl/beta-regression](http://scikit-learn-general.narkive.com/lLVQGzyl/beta-regression)\n\nI cannot tell if there exists a work-around within the `sklearn` framework.\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [scikit-learn](https://stackoverflow.com/questions/tagged/scikit-learn)\n\n[Share](https://stackoverflow.com/q/44234682)\n\nFollow\n\nasked May 29, 2017 at 4:38\n\n[![m33lky's user avatar](https://www.gravatar.com/avatar/3cc2734ba24f30f09c72e216a8c72b2b?s=64&d=identicon&r=PG)](https://stackoverflow.com/users/130111/m33lky)\n\n[m33lky](https://stackoverflow.com/users/130111/m33lky) m33lky\n\n7,2151010 gold badges4242 silver badges4848 bronze badges\n\n[Add a comment](https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n21\n\nThere exists a workaround, but it is not intrinsically _within_ the `sklearn` framework.\n\nIf you have a proportional target variable (value range 0-1) you run into two basic difficulties with scikit-learn:\n\n- Classifiers (such as logistic regression) deal with class labels as target variables only. As a workaround you could simply threshold your probabilities to 0/1 and interpret them as class labels, but you would lose a lot of information.\n- Regression models (such as linear regression) do not restrict the target variable. You can train them on proportional data, but there is no guarantee that the output on unseen data will be restricted to the 0/1 range. However, in this situation, there is a powerful work-around (below).\n\nThere are different ways to mathematically formulate logistic regression. One of them is the [generalized linear model](https://en.wikipedia.org/wiki/Logistic_regression#As_a_generalized_linear_model), which basically defines the logistic regression as a normal linear regression on logit-transformed probabilities. Normally, this approach requires sophisticated mathematical optimization because the probabilities are unknown and need to be estimated along with the regression coefficients.\n\nIn your case, however, the probabilities are known. This means you can simply transform them with `y = log(p / (1 - p))`. Now they cover the full range from `-oo` to `oo` and can serve as the target variable for a [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model \\[\\*\\]. Of course, the model output then needs to be transformed again to result in probabilities `p = 1 / (exp(-y) + 1)`.\n\n```\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nclass LogitRegression(LinearRegression):\n\n    def fit(self, x, p):\n        p = np.asarray(p)\n        y = np.log(p / (1 - p))\n        return super().fit(x, y)\n\n    def predict(self, x):\n        y = super().predict(x)\n        return 1 / (np.exp(-y) + 1)\n\nif __name__ == '__main__':\n    # generate example data\n    np.random.seed(42)\n    n = 100\n    x = np.random.randn(n).reshape(-1, 1)\n    noise = 0.1 * np.random.randn(n).reshape(-1, 1)\n    p = np.tanh(x + noise) / 2 + 0.5\n\n    model = LogitRegression()\n    model.fit(x, p)\n\n    print(model.predict([[-10], [0.0], [1]]))\n    # [[  2.06115362e-09]\n    #  [  5.00000000e-01]\n    #  [  8.80797078e-01]]\n\n```\n\n- There are also numerous other alternatives. Some non-linear regression models can work naturally in the 0-1 range. For example [Random Forest Regressors](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) will never exceed the target variables' range they were trained with. Simply put probabilities in and you will get probabilities out. Neural networks with appropriate output activation functions ( `tanh`, I guess) will also work well with probabilities, but if you want to use those there are more specialized libraries than sklearn.\n\n\\[\\*\\] You could in fact plug in _any_ [linear](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) regression model which can make the method more powerful, but then it no longer is exactly equivalent to logistic regression.\n\n[Share](https://stackoverflow.com/a/44236089)\n\nFollow\n\n[edited Oct 31, 2018 at 9:34](https://stackoverflow.com/posts/44236089/revisions)\n\nanswered May 29, 2017 at 6:43\n\n[![MB-F's user avatar](https://i.sstatic.net/LD2l4.png?s=64)](https://stackoverflow.com/users/3005167/mb-f)\n\n[MB-F](https://stackoverflow.com/users/3005167/mb-f) MB-F\n\n23.4k55 gold badges6666 silver badges121121 bronze badges\n\n18\n\n- 1\n\n\n\n\n\nCould you please explain what should be done about training / test data which contains a probability of 0 or 1? y is -inf and div by zero in these cases.\n\n\u2013\u00a0[Jake Drew](https://stackoverflow.com/users/1533306/jake-drew)\n\nCommentedMar 1, 2018 at 6:03\n\n- 1\n\n\n\n\n\n@JakeDrew The easiest solution would be to replace _0_ with _e_ and _1_ with _1-e_, where _e_ is a very small number. (You can also sanitize the probabilities with `p = p * e + 0.5 * e`). I guess `e = 1e-16` would work well.\n\n\u2013\u00a0[MB-F](https://stackoverflow.com/users/3005167/mb-f)\n\nCommentedMar 1, 2018 at 7:01\n\n- Thanks for the fast response! I was experimenting exactly as you suggested earlier. I found that for the range p = (0, 1) using .009 and .991 for values of 0 and 1 produce an 10-fold cv MAE = 0.059 or 5.9%. Using p = 9e-16 on the same data drives the MAE up to 0.2266 or 22.6%. The precision of e seems to have a huge impact on the mean absolute error. When y = np.log(p / (1 - p)) and p=0.991 then y=6.9. When p= 9e-16, y=36.7. Perhaps I am over-fitting to my own dataset?\n\n\u2013\u00a0[Jake Drew](https://stackoverflow.com/users/1533306/jake-drew)\n\nCommentedMar 1, 2018 at 8:07\n\n- 1\n\n\n\n\n\np/(1-p) in the answer should be log(p/(1-p))\n\n\u2013\u00a0[Hossein](https://stackoverflow.com/users/5617507/hossein)\n\nCommentedOct 31, 2018 at 6:26\n\n- 1\n\n\n\n\n\n@penelope I don't know any official term, but unofficially I would start by calling the whole thing a [Generalized Linear Model (GLM)](https://en.wikipedia.org/wiki/Generalized_linear_model). Then I would proceed by noting that these different linear models are actually only different algorithms for fitting a linear model. Depending on the used algorithm or it's properties I would construct a final name like _Regularized GLM_, _Sparse GLM_, _Ridge GLM_, _Lasso GLM_, etc.\n\n\u2013\u00a0[MB-F](https://stackoverflow.com/users/3005167/mb-f)\n\nCommentedJun 18, 2019 at 19:49\n\n\n\\|\u00a0[Sh...",
      "url": "https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
    }
  ]
}