## Current Status
- Best CV: 0.008465 (exp_026 - MLP+LGBM Ensemble)
- Recent: 
  - exp_031 (CatBoost Top 10): 0.009984 (18% worse)
  - exp_032 (CatBoost Logit): 0.013903 (39% worse)
- Conclusion: Logit transform failed. Top 10 features was too aggressive (underfitting).

## Response to Evaluator
- Evaluator suggested "Top 20 features".
- I will try **All Spange (13) + Kinetics (4) + SolventB% (1) = 18 features**.
- This is a middle ground between the underfitting Top 10 and the potentially overfitting 145 features.
- I will also implement **Clipping [0,1]** as requested.

## Recommended Approaches
1. **CatBoost with All Spange + Kinetics (18 Features)**
   - Features: 
     - Kinetics: `Time`, `Temp`, `1000/T`, `1000/T*ln(t)`
     - Spange: All 13 descriptors (linearly mixed for mixtures)
     - Composition: `SolventB%` (0 for single, actual % for mixed)
   - Model: CatBoostRegressor (RMSE loss, depth=6, lr=0.05, iterations=1000)
   - Post-processing: Clip predictions to [0, 1]
   - Validation: LOO (Single) + LORO (Full)
   - **Important**: For single solvent data, manually add `SolventB%` column with all zeros.

## What NOT to Try
- Logit transform (proven to degrade performance)
- Reducing features further (Top 10 was too few)
- Complex Deep Learning (failed previously)

## Validation Notes
- Compare strictly against exp_031 (0.0099) and exp_026 (0.0084).
- If CV < 0.0095, this is a strong candidate for ensembling.
- **DO NOT SUBMIT** (0 submissions remaining). Focus on CV improvement.