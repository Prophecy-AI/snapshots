{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24719cb4",
   "metadata": {},
   "source": [
    "# Loop 34 Analysis: Understanding the CV-LB Gap\n",
    "\n",
    "**Current State:**\n",
    "- Best CV: 0.008465 (exp_026)\n",
    "- Best LB: 0.0887 (exp_026)\n",
    "- Target: 0.0347\n",
    "- CV-LB Gap: ~10.5x\n",
    "\n",
    "**Key Question:** Why is there such a large gap? What can we do to reduce it?\n",
    "\n",
    "**Hypotheses to Test:**\n",
    "1. Distribution shift between train/test solvents\n",
    "2. Overfitting to specific solvents in CV\n",
    "3. The test set has fundamentally different characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "# Load all data\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('Single Solvent Data:')\n",
    "print(f'  Shape: {df_single.shape}')\n",
    "print(f'  Solvents: {df_single[\"SOLVENT NAME\"].nunique()}')\n",
    "print(f'  Samples per solvent: {df_single.groupby(\"SOLVENT NAME\").size().describe()}')\n",
    "\n",
    "print('\\nFull Data (Mixtures):')\n",
    "print(f'  Shape: {df_full.shape}')\n",
    "print(f'  Unique ramps: {df_full.groupby([\"SOLVENT A NAME\", \"SOLVENT B NAME\"]).ngroups}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05888b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distributions\n",
    "print('=== Target Distribution Analysis ===')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'\\n{col}:')\n",
    "    print(f'  Single: mean={df_single[col].mean():.4f}, std={df_single[col].std():.4f}, min={df_single[col].min():.4f}, max={df_single[col].max():.4f}')\n",
    "    print(f'  Full:   mean={df_full[col].mean():.4f}, std={df_full[col].std():.4f}, min={df_full[col].min():.4f}, max={df_full[col].max():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship from submissions\n",
    "submissions = [\n",
    "    ('exp_000', 0.0111, 0.0982),\n",
    "    ('exp_001', 0.0123, 0.1065),\n",
    "    ('exp_003', 0.0105, 0.0972),\n",
    "    ('exp_005', 0.0104, 0.0969),\n",
    "    ('exp_006', 0.0097, 0.0946),\n",
    "    ('exp_007', 0.0093, 0.0932),\n",
    "    ('exp_009', 0.0092, 0.0936),\n",
    "    ('exp_012', 0.0090, 0.0913),\n",
    "    ('exp_024', 0.0087, 0.0893),\n",
    "    ('exp_026', 0.0085, 0.0887),\n",
    "]\n",
    "\n",
    "cv_scores = [s[1] for s in submissions]\n",
    "lb_scores = [s[2] for s in submissions]\n",
    "\n",
    "# Linear fit\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv_scores, lb_scores)\n",
    "\n",
    "print('=== CV-LB Relationship ===')\n",
    "print(f'Linear fit: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nTo hit target LB = 0.0347:')\n",
    "print(f'  Required CV = (0.0347 - {intercept:.4f}) / {slope:.2f} = {(0.0347 - intercept) / slope:.6f}')\n",
    "\n",
    "# But this gives negative CV, which is impossible\n",
    "if (0.0347 - intercept) / slope < 0:\n",
    "    print('\\n⚠️ WARNING: Linear extrapolation suggests target is unreachable!')\n",
    "    print('This means we need a FUNDAMENTALLY DIFFERENT approach, not incremental CV improvement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25342c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches haven't been tried?\n",
    "print('=== UNEXPLORED APPROACHES ===')\n",
    "print('''\n",
    "1. **Quantile Regression** - Predict median instead of mean\n",
    "   - Could reduce impact of outliers\n",
    "   - CatBoost supports this natively\n",
    "\n",
    "2. **Beta Regression** - Model yields as Beta distribution\n",
    "   - Natural for [0,1] bounded data\n",
    "   - Handles heteroscedasticity\n",
    "\n",
    "3. **Mixture of Experts** - Different models for different solvent types\n",
    "   - Alcohols vs Esters vs Others\n",
    "   - Could capture different kinetics\n",
    "\n",
    "4. **Feature Selection via Permutation Importance**\n",
    "   - Remove features that hurt generalization\n",
    "   - Focus on most robust features\n",
    "\n",
    "5. **Adversarial Validation**\n",
    "   - Identify which features cause train/test shift\n",
    "   - Remove or transform those features\n",
    "\n",
    "6. **Ensemble with Different Feature Sets**\n",
    "   - Model 1: Only Spange (13 features)\n",
    "   - Model 2: Only DRFP (122 features)\n",
    "   - Model 3: Only Kinetics (5 features)\n",
    "   - Blend predictions\n",
    "\n",
    "7. **Temperature/Time Stratified CV**\n",
    "   - Ensure CV folds have similar T/t distributions\n",
    "   - May reduce CV-LB gap\n",
    "\n",
    "8. **Pseudo-Labeling**\n",
    "   - Use model predictions on test set as soft labels\n",
    "   - Retrain with augmented data\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a pattern in which solvents are harder to predict\n",
    "# Load Spange descriptors\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "print('=== Solvent Analysis ===')\n",
    "print(f'Solvents in Spange lookup: {len(SPANGE_DF)}')\n",
    "print(f'Solvents in single data: {df_single[\"SOLVENT NAME\"].nunique()}')\n",
    "print(f'\\nSolvents in single data:')\n",
    "print(sorted(df_single['SOLVENT NAME'].unique()))\n",
    "print(f'\\nSolvents in Spange lookup:')\n",
    "print(sorted(SPANGE_DF.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53fc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance of targets per solvent\n",
    "print('=== Per-Solvent Target Variance ===')\n",
    "solvent_stats = df_single.groupby('SOLVENT NAME')[['Product 2', 'Product 3', 'SM']].agg(['mean', 'std'])\n",
    "print(solvent_stats.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab50cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The CV-LB gap is ~10x\n",
    "# This is NOT normal. Typical gaps are 1.1-1.5x\n",
    "# \n",
    "# Possible causes:\n",
    "# 1. Test set has solvents NOT in training (out-of-distribution)\n",
    "# 2. Test set has different T/t ranges\n",
    "# 3. Test set has different mixture compositions\n",
    "# 4. Our CV scheme is too optimistic (leakage?)\n",
    "\n",
    "print('=== CRITICAL INSIGHT ===')\n",
    "print('''\n",
    "The CV-LB gap of ~10x is ABNORMAL.\n",
    "\n",
    "Our CV (LOO/LORO) should be pessimistic, not optimistic.\n",
    "Yet LB is 10x worse than CV.\n",
    "\n",
    "This suggests:\n",
    "1. The test set has FUNDAMENTALLY DIFFERENT characteristics\n",
    "2. OR our model is overfitting to training solvents in a way CV doesn't catch\n",
    "\n",
    "The linear fit shows:\n",
    "- LB = 4.22 * CV + 0.0533\n",
    "- Intercept = 0.0533 > Target = 0.0347\n",
    "\n",
    "This means even with PERFECT CV (0.0), we'd still get LB = 0.0533 > 0.0347!\n",
    "\n",
    "IMPLICATION: We need to REDUCE THE INTERCEPT, not just improve CV.\n",
    "The intercept represents the \"irreducible\" gap - likely due to distribution shift.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a23f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What could reduce the intercept?\n",
    "print('=== STRATEGIES TO REDUCE CV-LB INTERCEPT ===')\n",
    "print('''\n",
    "1. **Simpler Models** - Less capacity = less overfitting to train distribution\n",
    "   - Already tried: [32,16] MLP is best\n",
    "   - Could try: Linear models with regularization\n",
    "\n",
    "2. **Feature Engineering for Robustness**\n",
    "   - Use only features that are stable across distributions\n",
    "   - Remove features that are highly correlated with specific solvents\n",
    "\n",
    "3. **Domain Adaptation**\n",
    "   - If we knew which solvents are in test, we could adapt\n",
    "   - Without that, use techniques like CORAL, MMD\n",
    "\n",
    "4. **Ensemble Diversity**\n",
    "   - Different models may have different biases\n",
    "   - Averaging could cancel out some bias\n",
    "\n",
    "5. **Calibration**\n",
    "   - Post-hoc calibration using validation set\n",
    "   - Platt scaling, isotonic regression\n",
    "\n",
    "6. **Conservative Predictions**\n",
    "   - Shrink predictions toward mean\n",
    "   - Reduces variance at cost of bias\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what the top LB score (0.01727) implies\n",
    "target_lb = 0.01727\n",
    "best_lb = 0.0887\n",
    "\n",
    "print('=== GAP TO TOP SOLUTION ===')\n",
    "print(f'Our best LB: {best_lb:.4f}')\n",
    "print(f'Top LB: {target_lb:.4f}')\n",
    "print(f'Gap: {best_lb - target_lb:.4f} ({(best_lb - target_lb) / target_lb * 100:.1f}% worse)')\n",
    "print(f'\\nTo match top LB, we need to reduce error by {(best_lb - target_lb) / best_lb * 100:.1f}%')\n",
    "print(f'\\nThis is a MASSIVE gap. The top solution must have:')\n",
    "print('  1. A fundamentally different approach')\n",
    "print('  2. Domain knowledge we are missing')\n",
    "print('  3. Better handling of distribution shift')\n",
    "print('  4. Or access to information we don\\'t have')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
