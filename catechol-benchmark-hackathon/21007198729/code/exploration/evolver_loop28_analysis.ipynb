{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b46a12",
   "metadata": {},
   "source": [
    "# Loop 28 Analysis: Understanding the CV-LB Gap\n",
    "\n",
    "**Current State:**\n",
    "- Best CV: 0.008465 (exp_026)\n",
    "- Best LB: 0.0887 (exp_026)\n",
    "- Target: 0.01727\n",
    "- CV-LB ratio: ~10.5x\n",
    "- Linear fit: LB = 4.22*CV + 0.0533 (R²=0.96)\n",
    "\n",
    "**Critical Insight:**\n",
    "The intercept (0.0533) is 3x higher than target (0.01727). This means even with CV=0, predicted LB would be 0.0533.\n",
    "\n",
    "**Latest Experiment (exp_027):**\n",
    "- Tested simpler features (23 vs 145) - FAILED\n",
    "- CV 0.009150 (8.09% worse than exp_026)\n",
    "- DRFP features ARE valuable\n",
    "\n",
    "**Key Question:**\n",
    "What can we do to REDUCE the CV-LB gap, not just improve CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d70ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:18.571966Z",
     "iopub.status.busy": "2026-01-14T07:48:18.571354Z",
     "iopub.status.idle": "2026-01-14T07:48:19.725249Z",
     "shell.execute_reply": "2026-01-14T07:48:19.724721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All submissions:\n",
      "     id       cv      lb\n",
      "exp_000 0.011081 0.09816\n",
      "exp_001 0.012297 0.10649\n",
      "exp_003 0.010501 0.09719\n",
      "exp_005 0.010430 0.09691\n",
      "exp_006 0.009749 0.09457\n",
      "exp_007 0.009262 0.09316\n",
      "exp_009 0.009192 0.09364\n",
      "exp_012 0.009004 0.09134\n",
      "exp_024 0.008689 0.08929\n",
      "exp_026 0.008465 0.08875\n",
      "\n",
      "Linear fit: LB = 4.2168 * CV + 0.05334\n",
      "R² = 0.9618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# All 10 submissions with CV and LB scores\n",
    "submissions = [\n",
    "    {'id': 'exp_000', 'cv': 0.011081, 'lb': 0.09816},\n",
    "    {'id': 'exp_001', 'cv': 0.012297, 'lb': 0.10649},\n",
    "    {'id': 'exp_003', 'cv': 0.010501, 'lb': 0.09719},\n",
    "    {'id': 'exp_005', 'cv': 0.01043, 'lb': 0.09691},\n",
    "    {'id': 'exp_006', 'cv': 0.009749, 'lb': 0.09457},\n",
    "    {'id': 'exp_007', 'cv': 0.009262, 'lb': 0.09316},\n",
    "    {'id': 'exp_009', 'cv': 0.009192, 'lb': 0.09364},\n",
    "    {'id': 'exp_012', 'cv': 0.009004, 'lb': 0.09134},\n",
    "    {'id': 'exp_024', 'cv': 0.008689, 'lb': 0.08929},\n",
    "    {'id': 'exp_026', 'cv': 0.008465, 'lb': 0.08875},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('All submissions:')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Linear fit\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "print(f'\\nLinear fit: LB = {slope:.4f} * CV + {intercept:.5f}')\n",
    "print(f'R² = {r_value**2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0efa218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.727080Z",
     "iopub.status.busy": "2026-01-14T07:48:19.726790Z",
     "iopub.status.idle": "2026-01-14T07:48:19.732665Z",
     "shell.execute_reply": "2026-01-14T07:48:19.732198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Residual Analysis ===\n",
      "exp_000: Residual = -0.00191 (better than predicted)\n",
      "exp_001: Residual = 0.00129 (worse than predicted)\n",
      "exp_003: Residual = -0.00043 (better than predicted)\n",
      "exp_005: Residual = -0.00041 (better than predicted)\n",
      "exp_006: Residual = 0.00012 (worse than predicted)\n",
      "exp_007: Residual = 0.00076 (worse than predicted)\n",
      "exp_009: Residual = 0.00154 (worse than predicted)\n",
      "exp_012: Residual = 0.00003 (worse than predicted)\n",
      "exp_024: Residual = -0.00069 (better than predicted)\n",
      "exp_026: Residual = -0.00029 (better than predicted)\n",
      "\n",
      "Best residual: exp_000 (-0.00191)\n",
      "Worst residual: exp_009 (0.00154)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the residuals - which experiments deviate from the linear fit?\n",
    "print('=== Residual Analysis ===')\n",
    "predicted_lb = slope * cv + intercept\n",
    "residuals = lb - predicted_lb\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(f'{row[\"id\"]}: Residual = {residuals[i]:.5f} ({\"better\" if residuals[i] < 0 else \"worse\"} than predicted)')\n",
    "\n",
    "print(f'\\nBest residual: {df.iloc[residuals.argmin()][\"id\"]} ({residuals.min():.5f})')\n",
    "print(f'Worst residual: {df.iloc[residuals.argmax()][\"id\"]} ({residuals.max():.5f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f5d840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.734320Z",
     "iopub.status.busy": "2026-01-14T07:48:19.734153Z",
     "iopub.status.idle": "2026-01-14T07:48:19.741603Z",
     "shell.execute_reply": "2026-01-14T07:48:19.741134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiments with Negative Residuals (Better Generalization) ===\n",
      "exp_000: CV=0.011081, LB=0.09816, Residual=-0.00191\n",
      "exp_003: CV=0.010501, LB=0.09719, Residual=-0.00043\n",
      "exp_005: CV=0.010430, LB=0.09691, Residual=-0.00041\n",
      "exp_024: CV=0.008689, LB=0.08929, Residual=-0.00069\n",
      "exp_026: CV=0.008465, LB=0.08875, Residual=-0.00029\n",
      "\n",
      "=== Experiments with Positive Residuals (Worse Generalization) ===\n",
      "exp_001: CV=0.012297, LB=0.10649, Residual=0.00129\n",
      "exp_006: CV=0.009749, LB=0.09457, Residual=0.00012\n",
      "exp_007: CV=0.009262, LB=0.09316, Residual=0.00076\n",
      "exp_009: CV=0.009192, LB=0.09364, Residual=0.00154\n",
      "exp_012: CV=0.009004, LB=0.09134, Residual=0.00003\n"
     ]
    }
   ],
   "source": [
    "# What's special about experiments with negative residuals (better than predicted)?\n",
    "print('=== Experiments with Negative Residuals (Better Generalization) ===')\n",
    "for i, row in df.iterrows():\n",
    "    if residuals[i] < 0:\n",
    "        print(f'{row[\"id\"]}: CV={row[\"cv\"]:.6f}, LB={row[\"lb\"]:.5f}, Residual={residuals[i]:.5f}')\n",
    "\n",
    "print('\\n=== Experiments with Positive Residuals (Worse Generalization) ===')\n",
    "for i, row in df.iterrows():\n",
    "    if residuals[i] > 0:\n",
    "        print(f'{row[\"id\"]}: CV={row[\"cv\"]:.6f}, LB={row[\"lb\"]:.5f}, Residual={residuals[i]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f18e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.743000Z",
     "iopub.status.busy": "2026-01-14T07:48:19.742836Z",
     "iopub.status.idle": "2026-01-14T07:48:19.749837Z",
     "shell.execute_reply": "2026-01-14T07:48:19.749354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Approach Analysis ===\n",
      "\n",
      "Better generalization (negative residuals):\n",
      "  exp_000: MLP [128,128,64], Spange only, HuberLoss, 3 models\n",
      "  exp_003: MLP [256,128,64], Spange+DRFP, HuberLoss, 5 models\n",
      "  exp_005: MLP [256,128,64], Spange+DRFP, HuberLoss, 15 models\n",
      "  exp_024: MLP [32,16] + LightGBM, Spange+DRFP+ACS_PCA\n",
      "  exp_026: MLP [32,16] + LightGBM, Spange+DRFP+ACS_PCA, Weighted Loss [1,1,2]\n",
      "\n",
      "Worse generalization (positive residuals):\n",
      "  exp_001: LightGBM, Spange only\n",
      "  exp_006: MLP [64,32], Spange+DRFP, HuberLoss, 5 models\n",
      "  exp_007: MLP [32,16], Spange+DRFP, HuberLoss, 5 models\n",
      "  exp_009: Ridge Regression, Spange+DRFP\n",
      "  exp_012: MLP [32,16] + LightGBM ensemble, Spange+DRFP\n"
     ]
    }
   ],
   "source": [
    "# What approaches were used in experiments with better generalization?\n",
    "print('=== Approach Analysis ===')\n",
    "approaches = {\n",
    "    'exp_000': 'MLP [128,128,64], Spange only, HuberLoss, 3 models',\n",
    "    'exp_001': 'LightGBM, Spange only',\n",
    "    'exp_003': 'MLP [256,128,64], Spange+DRFP, HuberLoss, 5 models',\n",
    "    'exp_005': 'MLP [256,128,64], Spange+DRFP, HuberLoss, 15 models',\n",
    "    'exp_006': 'MLP [64,32], Spange+DRFP, HuberLoss, 5 models',\n",
    "    'exp_007': 'MLP [32,16], Spange+DRFP, HuberLoss, 5 models',\n",
    "    'exp_009': 'Ridge Regression, Spange+DRFP',\n",
    "    'exp_012': 'MLP [32,16] + LightGBM ensemble, Spange+DRFP',\n",
    "    'exp_024': 'MLP [32,16] + LightGBM, Spange+DRFP+ACS_PCA',\n",
    "    'exp_026': 'MLP [32,16] + LightGBM, Spange+DRFP+ACS_PCA, Weighted Loss [1,1,2]',\n",
    "}\n",
    "\n",
    "print('\\nBetter generalization (negative residuals):')\n",
    "for i, row in df.iterrows():\n",
    "    if residuals[i] < 0:\n",
    "        print(f'  {row[\"id\"]}: {approaches[row[\"id\"]]}')\n",
    "\n",
    "print('\\nWorse generalization (positive residuals):')\n",
    "for i, row in df.iterrows():\n",
    "    if residuals[i] > 0:\n",
    "        print(f'  {row[\"id\"]}: {approaches[row[\"id\"]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310ad347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.751766Z",
     "iopub.status.busy": "2026-01-14T07:48:19.751291Z",
     "iopub.status.idle": "2026-01-14T07:48:19.757017Z",
     "shell.execute_reply": "2026-01-14T07:48:19.756559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pattern Analysis ===\n",
      "\n",
      "Observations:\n",
      "1. exp_000 (Spange only, simpler) has negative residual\n",
      "2. exp_003, exp_005 (larger models) have negative residuals\n",
      "3. exp_009 (Ridge Regression) has positive residual\n",
      "4. exp_007 (simpler MLP) has positive residual\n",
      "5. exp_024, exp_026 (best CV) have negative residuals\n",
      "\n",
      "Conclusion:\n",
      "- No clear pattern between model complexity and generalization\n",
      "- The residuals are small (RMSE ~0.001) - the linear fit is very tight\n",
      "- The CV-LB gap is STRUCTURAL, not due to specific model choices\n",
      "- The gap is likely due to evaluation procedure differences\n"
     ]
    }
   ],
   "source": [
    "# Key insight: What's the pattern?\n",
    "print('=== Pattern Analysis ===')\n",
    "print('\\nObservations:')\n",
    "print('1. exp_000 (Spange only, simpler) has negative residual')\n",
    "print('2. exp_003, exp_005 (larger models) have negative residuals')\n",
    "print('3. exp_009 (Ridge Regression) has positive residual')\n",
    "print('4. exp_007 (simpler MLP) has positive residual')\n",
    "print('5. exp_024, exp_026 (best CV) have negative residuals')\n",
    "\n",
    "print('\\nConclusion:')\n",
    "print('- No clear pattern between model complexity and generalization')\n",
    "print('- The residuals are small (RMSE ~0.001) - the linear fit is very tight')\n",
    "print('- The CV-LB gap is STRUCTURAL, not due to specific model choices')\n",
    "print('- The gap is likely due to evaluation procedure differences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974cac5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.758373Z",
     "iopub.status.busy": "2026-01-14T07:48:19.758215Z",
     "iopub.status.idle": "2026-01-14T07:48:19.764368Z",
     "shell.execute_reply": "2026-01-14T07:48:19.763897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hypothesis: Different CV Scheme ===\n",
      "\n",
      "Our CV scheme:\n",
      "- Single solvents: Leave-one-solvent-out (24 folds)\n",
      "- Mixtures: Leave-one-ramp-out (13 folds)\n",
      "- Total: 37 folds\n",
      "\n",
      "Possible LB CV scheme:\n",
      "- GroupKFold (5 folds) as seen in \"mixall\" kernel\n",
      "- Different random seed\n",
      "- Different data ordering\n",
      "\n",
      "Key insight from \"mixall\" kernel:\n",
      "- Uses GroupKFold(n_splits=5) instead of Leave-One-Out\n",
      "- This is a DIFFERENT CV scheme!\n",
      "- Our local CV may not match the LB evaluation\n"
     ]
    }
   ],
   "source": [
    "# What if the evaluation uses a different CV scheme?\n",
    "print('=== Hypothesis: Different CV Scheme ===')\n",
    "print('\\nOur CV scheme:')\n",
    "print('- Single solvents: Leave-one-solvent-out (24 folds)')\n",
    "print('- Mixtures: Leave-one-ramp-out (13 folds)')\n",
    "print('- Total: 37 folds')\n",
    "\n",
    "print('\\nPossible LB CV scheme:')\n",
    "print('- GroupKFold (5 folds) as seen in \"mixall\" kernel')\n",
    "print('- Different random seed')\n",
    "print('- Different data ordering')\n",
    "\n",
    "print('\\nKey insight from \"mixall\" kernel:')\n",
    "print('- Uses GroupKFold(n_splits=5) instead of Leave-One-Out')\n",
    "print('- This is a DIFFERENT CV scheme!')\n",
    "print('- Our local CV may not match the LB evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e568e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.766011Z",
     "iopub.status.busy": "2026-01-14T07:48:19.765855Z",
     "iopub.status.idle": "2026-01-14T07:48:19.771365Z",
     "shell.execute_reply": "2026-01-14T07:48:19.770896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNEXPLORED Approaches ===\n",
      "\n",
      "XGBoost/CatBoost ensemble:\n",
      "  Rationale: Different tree algorithms may generalize differently\n",
      "\n",
      "Stacking meta-learner:\n",
      "  Rationale: Train a meta-model on base predictions\n",
      "\n",
      "Higher SM weights [1,1,3] or [1,1,4]:\n",
      "  Rationale: SM is still the bottleneck\n",
      "\n",
      "Learned loss weights (homoscedastic):\n",
      "  Rationale: Kendall et al. uncertainty weighting\n",
      "\n",
      "Consistency constraint (SM+P2+P3≈1):\n",
      "  Rationale: Physical constraint for regularization\n",
      "\n",
      "Different CV scheme (GroupKFold):\n",
      "  Rationale: May match LB evaluation better\n",
      "\n",
      "Adversarial validation:\n",
      "  Rationale: Identify features causing distribution shift\n",
      "\n",
      "Domain adaptation techniques:\n",
      "  Rationale: Handle distribution shift explicitly\n"
     ]
    }
   ],
   "source": [
    "# What approaches remain unexplored?\n",
    "print('=== UNEXPLORED Approaches ===')\n",
    "unexplored = [\n",
    "    ('XGBoost/CatBoost ensemble', 'Different tree algorithms may generalize differently'),\n",
    "    ('Stacking meta-learner', 'Train a meta-model on base predictions'),\n",
    "    ('Higher SM weights [1,1,3] or [1,1,4]', 'SM is still the bottleneck'),\n",
    "    ('Learned loss weights (homoscedastic)', 'Kendall et al. uncertainty weighting'),\n",
    "    ('Consistency constraint (SM+P2+P3≈1)', 'Physical constraint for regularization'),\n",
    "    ('Different CV scheme (GroupKFold)', 'May match LB evaluation better'),\n",
    "    ('Adversarial validation', 'Identify features causing distribution shift'),\n",
    "    ('Domain adaptation techniques', 'Handle distribution shift explicitly'),\n",
    "]\n",
    "\n",
    "for approach, rationale in unexplored:\n",
    "    print(f'\\n{approach}:')\n",
    "    print(f'  Rationale: {rationale}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268131c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.772939Z",
     "iopub.status.busy": "2026-01-14T07:48:19.772780Z",
     "iopub.status.idle": "2026-01-14T07:48:19.778329Z",
     "shell.execute_reply": "2026-01-14T07:48:19.777876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Priority Ranking ===\n",
      "\n",
      "1. HIGHEST PRIORITY: XGBoost/CatBoost Ensemble\n",
      "   - We only have MLP + LightGBM\n",
      "   - XGBoost and CatBoost are different algorithms\n",
      "   - May capture different patterns\n",
      "   - Easy to implement\n",
      "\n",
      "2. HIGH PRIORITY: Higher SM Weights [1,1,3]\n",
      "   - SM is still the hardest target\n",
      "   - Weighted loss [1,1,2] improved all targets\n",
      "   - More aggressive weighting may help further\n",
      "\n",
      "3. MEDIUM PRIORITY: Stacking Meta-Learner\n",
      "   - Train a simple model on base predictions\n",
      "   - Can learn optimal combination weights\n",
      "   - May improve generalization\n",
      "\n",
      "4. LOWER PRIORITY: Consistency Constraint\n",
      "   - SM + P2 + P3 ≈ 1 (mass balance)\n",
      "   - Physical constraint for regularization\n",
      "   - May improve predictions near boundaries\n"
     ]
    }
   ],
   "source": [
    "# Priority ranking based on potential impact\n",
    "print('=== Priority Ranking ===')\n",
    "print('\\n1. HIGHEST PRIORITY: XGBoost/CatBoost Ensemble')\n",
    "print('   - We only have MLP + LightGBM')\n",
    "print('   - XGBoost and CatBoost are different algorithms')\n",
    "print('   - May capture different patterns')\n",
    "print('   - Easy to implement')\n",
    "\n",
    "print('\\n2. HIGH PRIORITY: Higher SM Weights [1,1,3]')\n",
    "print('   - SM is still the hardest target')\n",
    "print('   - Weighted loss [1,1,2] improved all targets')\n",
    "print('   - More aggressive weighting may help further')\n",
    "\n",
    "print('\\n3. MEDIUM PRIORITY: Stacking Meta-Learner')\n",
    "print('   - Train a simple model on base predictions')\n",
    "print('   - Can learn optimal combination weights')\n",
    "print('   - May improve generalization')\n",
    "\n",
    "print('\\n4. LOWER PRIORITY: Consistency Constraint')\n",
    "print('   - SM + P2 + P3 ≈ 1 (mass balance)')\n",
    "print('   - Physical constraint for regularization')\n",
    "print('   - May improve predictions near boundaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1d781c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T07:48:19.779883Z",
     "iopub.status.busy": "2026-01-14T07:48:19.779731Z",
     "iopub.status.idle": "2026-01-14T07:48:19.784500Z",
     "shell.execute_reply": "2026-01-14T07:48:19.784023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RECOMMENDATION ===\n",
      "\n",
      "Given:\n",
      "- 3 submissions remaining\n",
      "- CV-LB gap is ~10x (structural, not model-specific)\n",
      "- Target 0.01727 is 5x better than best LB 0.0887\n",
      "\n",
      "Strategy:\n",
      "1. Try XGBoost/CatBoost ensemble (new model diversity)\n",
      "2. Try higher SM weights [1,1,3] (target the bottleneck)\n",
      "3. Try stacking meta-learner (optimal combination)\n",
      "\n",
      "Key insight:\n",
      "The CV-LB gap is the fundamental problem.\n",
      "We need approaches that GENERALIZE better, not just improve CV.\n",
      "New model types (XGBoost, CatBoost) may have different generalization properties.\n"
     ]
    }
   ],
   "source": [
    "# Final recommendation\n",
    "print('=== FINAL RECOMMENDATION ===')\n",
    "print('\\nGiven:')\n",
    "print('- 3 submissions remaining')\n",
    "print('- CV-LB gap is ~10x (structural, not model-specific)')\n",
    "print('- Target 0.01727 is 5x better than best LB 0.0887')\n",
    "\n",
    "print('\\nStrategy:')\n",
    "print('1. Try XGBoost/CatBoost ensemble (new model diversity)')\n",
    "print('2. Try higher SM weights [1,1,3] (target the bottleneck)')\n",
    "print('3. Try stacking meta-learner (optimal combination)')\n",
    "\n",
    "print('\\nKey insight:')\n",
    "print('The CV-LB gap is the fundamental problem.')\n",
    "print('We need approaches that GENERALIZE better, not just improve CV.')\n",
    "print('New model types (XGBoost, CatBoost) may have different generalization properties.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
