{
  "query": "What techniques can reduce the gap between cross-validation and test performance when there is a systematic bias or intercept in the relationship?",
  "results": [
    {
      "title": "ANSWER",
      "text": "When the relationship between cross\u2011validation (CV) error and true test error contains a systematic bias\u2014often manifesting as an intercept shift\u2014several practical remedies can shrink the gap. First, embed all data\u2011dependent steps (feature scaling, imputation, dimensionality reduction, etc.) inside each CV fold so that the CV estimate is not artificially optimistic; unsupervised preprocessing performed before the split is a known source of bias\u202f([Oxford Academic](https://academic.oup.com/jrsssb/article/84/4/1474/7073256)). Second, use a **nested CV** or a separate validation set to tune hyper\u2011parameters, then evaluate the final model on an outer CV loop; this prevents the \u201cdouble\u2011dip\u201d that inflates performance and reduces selection\u2011bias variance\u202f([JMLR](https://jmlr.csail.mit.edu/papers/v11/cawley10a.html)).  \n\nA more direct bias\u2011correction is to estimate the CV\u2011induced downward bias and adjust the reported error. Tibshirani\u202fet\u202fal. (2009) propose a simple estimator that re\u2011uses the CV folds to compute a bias term with virtually no extra computation, yielding a calibrated error closer to the true test error\u202f([arXiv](https://ui.adsabs.harvard.edu/abs/2009arXiv0908.2904T/abstract)). Complementary approaches such as the **bootstrap .632+** estimator or the **1\u2011SE rule** (selecting the simplest model within one standard error of the CV minimum) also temper over\u2011optimism and shrink the intercept gap\u202f([BMC Bioinformatics](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91)).  \n\nFinally, after the model is trained, **calibrate its predictions** (e.g., via Platt scaling or isotonic regression) on a held\u2011out calibration set; this explicitly corrects any systematic offset in the predicted probabilities, aligning the CV\u2011based performance with the true test performance\u202f([Oxford Academic](https://academic.oup.com/jrsssb/article/84/4/1474/7073256)). Combining these steps\u2014proper within\u2011fold preprocessing, nested CV or external validation, bias\u2011adjusted error estimates, and post\u2011hoc calibration\u2014provides a robust toolkit for narrowing the CV\u2011test performance gap when a systematic bias or intercept is present.",
      "url": ""
    },
    {
      "title": "On the Cross-Validation Bias due to Unsupervised Preprocessing",
      "text": "[Skip to Main Content](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#skipNav)\n\nAdvertisement\n\n[![Oxford Academic](https://oup.silverchair-cdn.com/UI/app/svg/umbrella/oxford-academic-logo.svg)](https://academic.oup.com/)\n\n[Journals](https://academic.oup.com/journals)\n\n[Books](https://academic.oup.com/books)\n\n- [_Search Menu_](javascript:;)\n- [![Information](https://oup.silverchair-cdn.com/UI/app/svg/i.svg)](https://academic.oup.com/pages/information)\n- [![Account](https://oup.silverchair-cdn.com/UI/app/svg/account.svg)](javascript:;)\n- [_Menu_](javascript:;)\n- [![Information](https://oup.silverchair-cdn.com/UI/app/svg/i.svg)](https://academic.oup.com/pages/information)\n- [![Account](https://oup.silverchair-cdn.com/UI/app/svg/account.svg)](javascript:;)\n- [Sign in through your institution](javascript:;)\n\n\nNavbar Search Filter\nJournal of the Royal Statistical Society Series B: Statistical MethodologyThis issueRSS JournalsMathematicsNeuroscienceProbability and StatisticsScience and MathematicsBooksJournalsOxford AcademicMobile Enter search term\n[Search](javascript:;)\n\n- [Issues](https://academic.oup.com/jrsssb/issue)\n- [More content](javascript:;)\n  - [Advance Articles](https://academic.oup.com/jrsssb/advance-articles)\n  - [Datasets](https://academic.oup.com/jrsssb/pages/datasets)\n- [Submit](javascript:;)\n  - [Author Guidelines](https://academic.oup.com/jrsssb/pages/general-instructions)\n  - [Submission Site](https://mc.manuscriptcentral.com/jrssb)\n  - [Open Access Policy](https://academic.oup.com/journals/pages/open_access)\n  - [Self-Archiving Policy](https://academic.oup.com/journals/pages/self_archiving_policy_b)\n  - [Why publish with Series B?](https://academic.oup.com/jrsssb/pages/why-publish)\n- [Purchase](https://academic.oup.com/jrsssb/subscribe)\n- [Alerts](https://academic.oup.com/pages/using-the-content/email-alerts)\n- [About](javascript:;)\n  - [About the Journal of the Royal Statistical Society Series B: Statistical Methodology](https://academic.oup.com/jrsssb/pages/about)\n  - [About The Royal Statistical Society](https://rss.org.uk/)\n  - [Editorial Board](https://academic.oup.com/jrsssb/pages/editorial-board)\n  - [Advertising & Corporate Services](http://www.oupmediainfo.com/#!/)\n  - [Twitter](https://twitter.com/RoyalStatSoc)\n- [Journals on Oxford Academic](https://academic.oup.com/journals)\n- [Books on Oxford Academic](https://academic.oup.com/books)\n\n[![Journal of the Royal Statistical Society Series B: Statistical Methodology](https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/jrsssb/jrsssb_title2009040467.svg)](https://academic.oup.com/jrsssb)\n\n[![The Royal Statistical Society](https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/jrsssb/jrsssb_h1-965359043.svg)](https://rss.org.uk/)\n\nArticle Navigation\n\nClose mobile search navigation\n\nArticle Navigation\n\n[![Issue Cover](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/jrsssb/Issue/84/4/1/m_cover.jpeg?Expires=1723189826&Signature=IDlkHXc2wenBdtq7brouZch0honkYaE91DLwtnBMlqOiZ7fervq9G8EBJVgTBFb9YznbugwrW9wiD0C9SeV9H5Mcuzx5VWpQRaWC9wSfhZHEjfwU2hVr4Yt3NjGGVvJjxeUqvo1rngdtn6V2DVHl9zS7iPEmGqgBZjWjtR6jMUJXQ-QYJ-vuj7~-TlnODLGLs4ZIRISkCGuY6b9zVrqqos7b0DmZFt4pqHvPx7G439Xwe~8~bjYDdRqjzzX4VnA7SfMYur1qgSdrGYXTfHuGx~glBKu6-hBrF0lwTmsMb4kiWtGdDsRhKrg-OoZ8TS6-ZatKfu2S15ikBcbTj-r4xg__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)\\\n\\\nVolume 84\\\n\\\nIssue 4\\\n\\\nSeptember 2022](https://academic.oup.com/jrsssb/issue/84/4)\n\n### Article Contents\n\n- [Abstract](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804575)\n\n- [1\\. INTRODUCTION](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804578)\n\n- [2\\. Notation and definitions](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804598)\n\n- [3\\. BASIC PROPERTIES OF THE BIAS](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804622)\n\n- [4\\. MAIN EXAMPLE: FEATURE SELECTION FOR HIGH-DIMENSIONAL LINEAR REGRESSION](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804628)\n\n- [5\\. ADDITIONAL EXAMPLES](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804654)\n\n- [6\\. POTENTIAL IMPACT ON MODEL SELECTION](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804704)\n\n- [7\\. ON GENERIC UPPER BOUNDS OF THE BIAS](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804710)\n\n- [8\\. CONCLUSION](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804735)\n\n- [REPRODUCIBILITY](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804739)\n\n- [ACKNOWLEDGEMENTS](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804741)\n\n- [REFERENCES](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804743)\n\n- [APPENDIX. TECHNICAL PROOFS](https://academic.oup.com/jrsssb/article/84/4/1474/7073256#397804745)\n\n\n- [< Previous](https://academic.oup.com/jrsssb/article/84/4/1446/7073272)\n- [Next >](https://academic.oup.com/jrsssb/article/84/4/1503/7073289)\n\nArticle Navigation\n\nArticle Navigation\n\nJournal Article\n\n# On the Cross-Validation Bias due to Unsupervised Preprocessing\n\nAmit Moscovich,\n\nAmit Moscovich\n\nDepartment of Statistics and Operations Research\n\nTel Aviv University\n\n,\n\nTel Aviv\n\n,\n\nIsrael\n\n**Correspondence** Amit Moscovich, Department of Statistics and Operations Research, Tel Aviv University, Tel Aviv, Israel. Email: [mosco@tauex.tau.ac.il](mailto:mosco@tauex.tau.ac.il)\n\nSearch for other works by this author on:\n\n[Oxford Academic](https://academic.oup.com/jrsssb/search-results?f_Authors=Amit+Moscovich)\n\n[Google Scholar](http://scholar.google.com/scholar?q=author:%22Moscovich Amit%22)\n\nSaharon Rosset\n\nSaharon Rosset\n\nDepartment of Statistics and Operations Research\n\nTel Aviv University\n\n,\n\nTel Aviv\n\n,\n\nIsrael\n\nSearch for other works by this author on:\n\n[Oxford Academic](https://academic.oup.com/jrsssb/search-results?f_Authors=Saharon+Rosset)\n\n[Google Scholar](http://scholar.google.com/scholar?q=author:%22Rosset Saharon%22)\n\n**Funding information** Israel Science Foundation, ISF 1804/16\n\n[Author Notes](javascript:;)\n\n_Journal of the Royal Statistical Society Series B: Statistical Methodology_, Volume 84, Issue 4, September 2022, Pages 1474\u20131502, [https://doi.org/10.1111/rssb.12537](https://doi.org/10.1111/rssb.12537)\n\nPublished:\n\n14 June 2022\n\n[Article history](javascript:;)\n\nReceived:\n\n11 April 2020\n\nAccepted:\n\n09 March 2022\n\nPublished:\n\n14 June 2022\n\n- [![pdf](https://oup.silverchair-cdn.com/UI/app/svg/pdf.svg)PDF](https://academic.oup.com/jrsssb/article-pdf/84/4/1474/49462949/jrsssb_84_4_1474.pdf)\n- [Split View](javascript:;)\n- [Views](javascript:;)\n  - [Article contents](javascript:;)\n  - [Figures & tables](javascript:;)\n  - [Video](javascript:;)\n  - [Audio](javascript:;)\n  - [Supplementary Data](javascript:;)\n- [Cite](https://academic.oup.com/jrsssb/article/84/4/1474/7073256)\n\n\n\n### Cite\n\n\n\n\n\nAmit Moscovich, Saharon Rosset, On the Cross-Validation Bias due to Unsupervised Preprocessing, _Journal of the Royal Statistical Society Series B: Statistical Methodology_, Volume 84, Issue 4, September 2022, Pages 1474\u20131502, [https://doi.org/10.1111/rssb.12537](https://doi.org/10.1111/rssb.12537)\n\n\n\n\n\nSelect FormatSelect format.ris (Mendeley, Papers, Zotero).enw (EndNote).bibtex (BibTex).txt (Medlars, RefWorks)Download citation\n\n[Close](javascript:;)\n\n- [_Permissions Icon_ Permissions](https://s100.copyright.com/AppDispatchServlet?publisherName=OUP&publication=1467-9868&title=On%20the%20Cross-Validation%20Bias%20due%20to%20Unsupervised%20Preprocessing&publicationDate=2022-06-14&volumeNum=84&issueNum=4&author=Moscovich%2C%20Amit%3B%20Rosset%2C%20Saharon&startPage=1474&endPage=1502&contentId=10.1111%2Frssb.12537&oa=CC%20BY&copyright=%C2%A9%202022%20The%20Authors.%20Journal%20of%20the%20Royal%20Statistical%20Society%3A%20Series%20B%20%28Statistical%20Methodology%29%20published%20by%20John%20Wiley%20%26%20Sons%20Ltd%20on%20behalf%20of%20Royal%20Statistical%20Society.&orderBeanReset=True)\n\n- [_Share Icon_ Share](javascript:;)\n  - [Facebook]...",
      "url": "https://academic.oup.com/jrsssb/article/84/4/1474/7073256"
    },
    {
      "title": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation",
      "text": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\n[![](https://jmlr.csail.mit.edu/img/jmlr.jpg)](https://jmlr.csail.mit.edu/)\n[Home Page](https://jmlr.csail.mit.edu/)\n[Papers](https://jmlr.csail.mit.edu/papers)\n[Submissions](https://jmlr.csail.mit.edu/author-info.html)\n[News](https://jmlr.csail.mit.edu/news.html)\n[Editorial Board](https://jmlr.csail.mit.edu/editorial-board.html)\n[Open Source Software](https://jmlr.csail.mit.edu/mloss)\n[Proceedings (PMLR)](https://proceedings.mlr.press/)\n[Transactions (TMLR)](https://jmlr.csail.mit.edu/tmlr)\n[Search](https://jmlr.csail.mit.edu/search-jmlr.html)\n[Statistics](https://jmlr.csail.mit.edu/stats.html)\n[Login](https://jmlr.csail.mit.edu/manudb)\n[Frequently Asked Questions](https://jmlr.csail.mit.edu/faq.html)\n[Contact Us](https://jmlr.csail.mit.edu/contact.html)\n[![RSS Feed](https://jmlr.csail.mit.edu/img/RSS.gif)](https://jmlr.csail.mit.edu/jmlr.xml)\n## On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation\n***Gavin C. Cawley, Nicola L. C. Talbot***; 11(70):2079\u22122107, 2010.\n### Abstract\nModel selection strategies for machine learning algorithms typically involve\nthe numerical optimisation of an appropriate model selection criterion, often\nbased on an estimator of generalisation performance, such as*k*-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.\n[abs][[pdf](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf)][[bib](https://jmlr.csail.mit.edu/papers/v11/cawley10a.bib)]\n\u00a9[JMLR](https://www.jmlr.org)2010. ([edit](https://github.com/JmlrOrg/v11/tree/master/cawley10a),[beta](http://jmlr.org/beta/papers/v11/cawley10a.html))|",
      "url": "https://jmlr.csail.mit.edu/papers/v11/cawley10a.html"
    },
    {
      "title": "Bias in error estimation when using cross-validation for model selection",
      "text": "Bias in error estimation when using cross-validation for model selection | BMC Bioinformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://bmcbioinformatics.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/1471-2105-7-91?)\n# Bias in error estimation when using cross-validation for model selection\n* Research article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:23 February 2006\n* Volume\u00a07, article\u00a0number91, (2006)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://bmcbioinformatics.biomedcentral.com/content/pdf/10.1186/1471-2105-7-91.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/12859?as=webp)BMC Bioinformatics](https://bmcbioinformatics.biomedcentral.com/journal/12859)[Aims and scope](https://bmcbioinformatics.biomedcentral.com/journal/12859/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/12859/3)\nBias in error estimation when using cross-validation for model selection\n[Download PDF](https://bmcbioinformatics.biomedcentral.com/content/pdf/10.1186/1471-2105-7-91.pdf)\n* [Sudhir Varma](#auth-Sudhir-Varma-Aff1)[1](#Aff1)&amp;\n* [Richard Simon](#auth-Richard-Simon-Aff1)[1](#Aff1)\n* 51kAccesses\n* 1632Citations\n* 62Altmetric\n* 3Mentions\n* [Explore all metrics](https://bmcbioinformatics.biomedcentral.com/article/10.1186/1471-2105-7-91/metrics)\n## Abstract\n### Background\nCross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.\n### Results\nWe used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these \"null\" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With \"null\" and \"non null\" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error.\nThe CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the \"null\" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30% on 18.5% of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30% on 38% of \"null\" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance.\nThe nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for \"null\" and \"non-null\" data distributions.\n### Conclusion\nWe show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-024-06630-y/MediaObjects/10994_2024_6630_Fig1_HTML.png)\n### [Reducing cross-validation variance through seed blocking in hyperparameter tuning](https://link.springer.com/10.1007/s10994-024-06630-y?fromPaywallRec=false)\nArticle17 February 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42113-018-0011-7/MediaObjects/42113_2018_11_Fig1_HTML.png)\n### [Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection](https://link.springer.com/10.1007/s42113-018-0011-7?fromPaywallRec=false)\nArticleOpen access27 September 2018\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10687-024-00495-z/MediaObjects/10687_2024_495_Fig1_HTML.png)\n### [Cross-validation on extreme regions](https://link.springer.com/10.1007/s10687-024-00495-z?fromPaywallRec=false)\nArticle03 September 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Bayesian Inference](https://bmcbioinformatics.biomedcentral.com/subjects/bayesian-inference)\n* [Learning algorithms](https://bmcbioinformatics.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://bmcbioinformatics.biomedcentral.com/subjects/machine-learning)\n* [Non-parametric Inference](https://bmcbioinformatics.biomedcentral.com/subjects/non-parametric-inference)\n* [Parametric Inference](https://bmcbioinformatics.biomedcentral.com/subjects/parametric-inference)\n* [Statistical Learning](https://bmcbioinformatics.biomedcentral.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=12859)\nAvoid common mistakes on your manuscript.\n## Background\nThe unique characteristics of microarray data have stimulated the development of a multitude of analysis methods. Microarray data is distinguished by very small numbers of samples compared to the number of features measured. Most previous machine learning methods have been developed on data where the opposite holds true; the number of samples is much larger than the number of features. As a result, such analysis methods have to be modified for microarray datasets.\nAn example is the common paradigm of splitting the data-set into training and test data. The training data is used for selecting features and training a classifier. Once a final classifier has been specified, it can be used to predict the classes of the test samples. The mean error on a sufficiently large (ideally infinite) test dataset gives the*true error*of the classifier.\nWhen the number of samples*n*is small, it is important to ensure that the data used to test the classifier is not part of the data used to train it. Testing the classifier on the same samples that were used to train it gives the*re-substitution estimate*of the true error, which is known to give falsely low (usually zero) error estimates for small*n*.\nWith microarray data, splitting the sample into large training and test sets is usually not feasible since the number of samples is so small. Cross-validation (CV) is one solution to the lack of sufficiently large training and testing sets [[1](https://bmcbioinformatics.biomedcentral.com/article/10.1186/1471-2105-7-91#ref-CR1)], where, instead of testing a f...",
      "url": "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-91"
    },
    {
      "title": "",
      "text": "<div><div>\n<p>\n</p><h2>ADS</h2>\n<p></p>\n<div>\n<article>\n<div>\n<ul>\n;\n<li><a href=\"https://ui.adsabs.harvard.edu/search/?q=author%3A%22Tibshirani%2C+Robert%22\">Tibshirani, Robert</a>\n</li>\n</ul>\n</div>\n<div>\n<h4>Abstract</h4>\n<p>\nTuning parameters in supervised learning problems are often estimated by cross-validation. The minimum value of the cross-validation error can be biased downward as an estimate of the test error at that same value of the tuning parameter. We propose a simple method for the estimation of this bias that uses information from the cross-validation process. As a result, it requires essentially no additional computation. We apply our bias estimate to a number of popular classifiers in various settings, and examine its performance.\n</p>\n</div>\n<br/>\n<dl>\n<dt>Publication:</dt>\n<dd>\n<p>arXiv e-prints</p>\n</dd>\n<dt>Pub Date:</dt>\n<dd>August 2009</dd>\n<dt>DOI:</dt>\n<dd>\n<p>\n<a href=\"https://ui.adsabs.harvard.edu/link_gateway/2009arXiv0908.2904T/doi:10.48550/arXiv.0908.2904\">10.48550/arXiv.0908.2904</a>\n<i></i>\n</p>\n</dd>\n<dt>arXiv:</dt>\n<dd>\n<span>\n<a href=\"https://ui.adsabs.harvard.edu/link_gateway/2009arXiv0908.2904T/arXiv:0908.2904\">arXiv:0908.2904</a>\n<i></i>\n</span>\n</dd>\n<dt>Bibcode:</dt>\n<dd>\n<a href=\"https://ui.adsabs.harvard.edu/abs/2009arXiv0908.2904T/abstract\">\n2009arXiv0908.2904T\n</a>\n<i></i>\n</dd>\n<dt>Keywords:</dt>\n<dd>\n<ul>\n<li>Statistics - Applications</li>\n</ul>\n</dd>\n<dt>E-Print:</dt>\n<dd>\nPublished in at http://dx.doi.org/10.1214/08-AOAS224 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org)\n</dd>\n</dl>\n</article>\n</div>\n</div></div>",
      "url": "https://ui.adsabs.harvard.edu/abs/2009arXiv0908.2904T/abstract"
    },
    {
      "title": "From Bias to Cross-Validation: A Comprehensive Guide to Machine ...",
      "text": "<div><article><div><div><a href=\"https://medium.com/@mroko001?source=post_page---byline--30f3b7e86c21---------------------------------------\"><div><p></p></div></a></div><h2>Introduction</h2><p>In the ever-evolving world of machine learning, creating a model is just the beginning of your journey. The true test lies in ensuring that your model performs well not just on the data it was trained on, but on new, unseen data. This crucial step is where validation techniques come into play. Whether you\u2019re a beginner just starting out or an experienced practitioner looking to refine your skills, understanding these techniques is essential for developing robust and reliable machine learning models.</p><h2>The Importance of Validation: Beyond the Basics</h2><p>Before we dive into specific techniques, let\u2019s explore why validation is so crucial in machine learning, and what it really means for your models.</p><ol><li><strong>Assessing True Performance</strong>: Imagine you\u2019re teaching a child to recognize different types of fruit. If you only show them apples and oranges during their lessons, how confident would you be that they could identify a mango or a kiwi? Similarly, validation helps us estimate how well our model will perform on new, unseen data. It\u2019s our way of testing whether the model has truly learned general patterns or has just memorized the examples we\u2019ve shown it.</li><li><strong>Avoiding Overfitting</strong>: Overfitting is like memorizing the answers to a specific test rather than understanding the underlying concepts. A model that overfits performs\u2026</li></ol></div></article></div>",
      "url": "https://medium.com/%40mroko001/from-bias-to-cross-validation-a-comprehensive-guide-to-machine-learning-validation-techniques-30f3b7e86c21"
    },
    {
      "title": "Practical Considerations and Applied Examples of Cross-Validation ...",
      "text": "Practical Considerations and Applied Examples of Cross-Validation for Model Development and Evaluation in Health Care: Tutorial - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.2196/49023)\n* [](pdf/ai_v2i1e49023.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![JMIR AI logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jmirai.png)\nJMIR AI\n. 2023 Dec 18;2:e49023. doi:[10.2196/49023](https://doi.org/10.2196/49023)\n# Practical Considerations and Applied Examples of Cross-Validation for Model Development and Evaluation in Health Care: Tutorial\n[Drew Wilimitis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wilimitis D\"[Author]>)\n### Drew Wilimitis,BS\n1Vanderbilt University Medical Center, Vanderbilt University, Nashville, TN, United States\nFind articles by[Drew Wilimitis](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wilimitis D\"[Author]>)\n1,[Colin G Walsh](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Walsh CG\"[Author]>)\n### Colin G Walsh,MA, MD\n1Vanderbilt University Medical Center, Vanderbilt University, Nashville, TN, United States\nFind articles by[Colin G Walsh](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Walsh CG\"[Author]>)\n1,\u2709\nEditors:Bradley Malin,Khaled El Emam\nReviewed by:Urjoshi Sinha,Silvia Figini\n* Author information\n* Article notes\n* Copyright and License information\n1Vanderbilt University Medical Center, Vanderbilt University, Nashville, TN, United States\n\u2709Corresponding Author: Colin G Walshcolin.walsh@vumc.org\n\u2709Corresponding author.\nReceived 2023 May 15; Revision requested 2023 Sep 5; Revised 2023 Sep 19; Accepted 2023 Sep 28; Collection date 2023.\n\u00a9Drew Wilimitis, Colin G Walsh. Originally published in JMIR AI (https://ai.jmir.org), 18.12.2023.\nThis is an open-access article distributed under the terms of the Creative Commons Attribution License ([https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR AI, is properly cited. The complete bibliographic information, a link to the original publication on[https://www.ai.jmir.org/](https://www.ai.jmir.org/), as well as this copyright and license information must be included.\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC11041453\u00a0\u00a0PMID:[38875530](https://pubmed.ncbi.nlm.nih.gov/38875530/)\n## Abstract\nCross-validation remains a popular means of developing and validating artificial intelligence for health care. Numerous subtypes of cross-validation exist. Although tutorials on this validation strategy have been published and some with applied examples, we present here a practical tutorial comparing multiple forms of cross-validation using a widely accessible, real-world electronic health care data set: Medical Information Mart for Intensive Care-III (MIMIC-III). This tutorial explored methods such as K-fold cross-validation and nested cross-validation, highlighting their advantages and disadvantages across 2 common predictive modeling use cases: classification (mortality) and regression (length of stay). We aimed to provide readers with reproducible notebooks and best practices for modeling with electronic health care data. We also described sets of useful recommendations as we demonstrated that nested cross-validation reduces optimistic bias but comes with additional computational challenges. This tutorial might improve the community\u2019s understanding of these important methods while catalyzing the modeling community to apply these guides directly in their work using the published code.\n**Keywords:**predictive modeling, cross-validation, tutorial, model development, risk detection, clinical decision-making, electronic health care, eHealth data, health care data, data validation, artificial intelligence, AI\n## Background\nBy learning complex statistical relationships from historical data, predictive models enable automated and scalable risk detection and prognostication, which might inform clinical decision-making. Although relatively few have been implemented in clinical use compared with the number developed, predictive models are increasingly being deployed and tested in clinical trials. The importance of predictive modeling is on the rise, with increasing attention from regulatory bodies such as the US Food and Drug Administration. Efforts to standardize the steps in model development and validation include statements such as transparent reporting of a multivariable prediction model for individual prognosis or diagnosis and multiple published guidelines on deployment and governance [[1](#ref1)-[3](#ref3)]. However, the mode in a critical step in model development, the validation strategy, remains a simple \u201choldout\u201d or \u201ctest-train split,\u201d which has been shown to introduce bias, fail to generalize, and hinder clinical utility [[4](#ref4)-[6](#ref6)].\nBroadly, validation consists of either internal validation, which should be reported alongside model development, or external validation, in which a developed model is tested in an unseen data set in a new setting [[7](#ref7),[8](#ref8)]. A newer concept of \u201cinternal-external\u201d validation has also been suggested for studies with multisite data [[9](#ref9)]. Most published models evaluate performance metrics by splitting the available data set into an independent \u201choldout\u201d or \u201ctest set,\u201d consisting of unforeseen samples excluded from model training. Such held-out sets are often selected randomly, for example, \u201c80% training and 20% testing,\u201d from the data in the original model development setting. In contrast to holdout validation, cross-validation and resampling methods such as bootstrapping can be used to produce less biased estimates of the true out-of-sample performance (ie, the ability to generalize to new samples). Although cross-validation is a widely used and extensively studied statistical method, many variations of cross-validation exist with respective strengths and weaknesses, distinct use cases for model development and performance estimation that are often misapplied, and domain-specific considerations necessary for effective health care implementation [[10](#ref10),[11](#ref11)].\nCross-validation surveys with practical examples, such as those involving microarray and neurologic data, have been published [[12](#ref12),[13](#ref13)]. However, gaps in comprehensive tutorials including complete codesets with relevant ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11041453"
    },
    {
      "title": "Cross Validation in Machine Learning: Techniques and Best Practices",
      "text": "<div><article><div><p>Building a<a href=\"https://www.udacity.com/course/intro-to-machine-learning--ud120\"> machine learning model</a> can feel exhilarating. You feed it data, tweak its parameters, and watch the performance metrics climb. Achieving a high score on the data the model trained on feels like success. But beware \u2013 this initial success can sometimes be fool\u2019s gold. The real test of a <a href=\"https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230\">machine learning model</a> isn\u2019t how well it performs on data it has already seen, but how well it generalizes to new, unseen data. A model might simply memorize the training data, learning its specific quirks and noise rather than the underlying patterns. This phenomenon, known as overfitting, leads to models that look great in development but fail spectacularly in the real world. Think of it like a student who crams for one specific test by memorizing the answers \u2013 they might ace that test, but they haven\u2019t truly learned the subject and will likely fail a different test on the same material.</p><figure><figcaption><em>Source: <a href=\"https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\">AWS documentation</a></em></figcaption></figure><p>So, how do we get a realistic estimate of our model\u2019s true capabilities before deploying it? This is where cross-validation (CV) becomes an indispensable tool in the machine learning practitioner\u2019s toolkit. Cross-validation is a robust resampling technique used to assess how the results of a model will generalize to an independent dataset. It provides a more accurate estimate of the model\u2019s ability to perform on new data by evaluating its performance across multiple subsets during training, giving an unbiased estimate of the generalization error \u2013 a measure of how well the model predicts future observations. By doing this, CV helps build more robust and reliable models. Averaging across multiple evaluations, as CV does, provides a much more trustworthy picture of the model\u2019s true potential.</p><h2><strong>Beyond the Simple Split: What Cross-Validation Really Does</strong></h2><p>Cross-validation systematically partitions data into multiple subsets (\u201cfolds\u201d). It iteratively trains the model on some folds and evaluates it on a remaining held-out fold, repeating until each fold has served as the test set. This contrasts with the traditional train/test split, where data is divided once (e.g., 80% train, 20% test). While simpler and faster, a single split\u2019s performance estimate can be unreliable, highly dependent on the specific split, and inefficient with data usage, as a portion is never used for training.</p><p>CV overcomes these issues. By averaging performance across folds, it offers a more robust, stable estimate of model performance, less prone to the randomness of a single split. Every data point contributes to both training and validation, maximizing data utility, which is crucial for smaller datasets. Furthermore, CV provides insight into performance variability (e.g., standard deviation across folds), highlighting model stability\u2014a detail missed by a single split.</p><p>It\u2019s key to distinguish that \u201cvalidation\u201d sets in CV are from the main training data. A final, untouched \u201ctest set\u201d should be reserved for a single, unbiased evaluation of the chosen model after all CV and tuning.</p><h2><strong>A Tour Through the Cross-Validation Toolkit: Finding the Right Technique</strong></h2><p>Cross-validation isn\u2019t a monolithic concept; it\u2019s a family of techniques. Think of them as specialized tools in a toolbox, each designed for specific types of data or modeling challenges. Let\u2019s explore some of the most common and useful ones.</p><h3><strong>K-Fold Cross-Validation: The Trusty Workhorse</strong></h3><p>The most common type is K-Fold CV. The process is straightforward:</p><ol><li>Divide the dataset randomly into K non-overlapping subsets (folds) of roughly equal size.</li><li>For each fold i from 1 to K:</li></ol><ul><li>Use fold i as the test set.</li><li>Use the remaining K-1 folds as the training set.</li><li>Train the model on the training set and evaluate it on the test set.</li></ul><ol><li>Average the evaluation scores from the K iterations to get the final performance estimate.</li></ol><figure><figcaption><em>Source: <a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\">scikit-learn documentation</a></em></figcaption></figure><p><strong>Pros:</strong> K-Fold offers a good balance between computational cost and obtaining a reliable performance estimate for many standard machine learning problems. It uses all data for both training and validation and generally provides a less biased estimate than a simple train/test split.</p><p><strong>Cons:</strong> More computationally intensive than a single split. Standard K-Fold assumes data points are independent and identically distributed (IID), unsuitable for time-series data if shuffling is involved. It can struggle with imbalanced datasets.</p><p><strong>Implementation:</strong></p><p>Scikit-learn provides KFold and the convenient cross_val_score function.</p><pre><code>\n<p><span>import</span><span> numpy </span><span>as</span><span> np</span><span>\n</span><span>from</span><span> sklearn.model_selection </span><span>import</span><span> KFold, cross_val_score</span><span>\n</span><span>from</span><span> sklearn.linear_model </span><span>import</span><span> LogisticRegression</span><span>\n</span><span>from</span><span> sklearn.datasets </span><span>import</span><span> make_classification</span><span>\n</span><span>\n</span><span># Generate sample classification data</span><span>\n</span><span>X, y = make_classification(n_samples=</span><span>100</span><span>, n_features=</span><span>20</span><span>, n_informative=</span><span>15</span><span>, n_redundant=</span><span>5</span><span>, random_state=37)</span><span>\n</span><span>\n</span><span># Initialize the model</span><span>\n</span><span>model = LogisticRegression()</span><span>\n</span><span>\n</span><span># Initialize KFold (e.g., 5 folds, shuffle for randomness, set random_state for reproducibility)</span><span>\n</span><span># Note: Shuffling is often good for IID data but MUST be False for time series.</span><span>\n</span><span>kf = KFold(n_splits=</span><span>5</span><span>, shuffle=</span><span>True</span><span>, random_state=37)</span><span>\n</span><span>\n</span><span># Evaluate the model using cross_val_score</span><span>\n</span><span>scores = cross_val_score(model, X, y, scoring=</span><span>\u2018accuracy\u2019</span><span>, cv=kf, n_jobs=-</span><span>1</span><span>)</span><span>\n</span><span>\n</span><span># Print the results</span><span>\n</span><span>print(</span><span>f\u201dScores for each fold: </span><span>{scores}</span><span>\u201c</span><span>)</span><span>\n</span><span>print(</span><span>f\u201dAverage Accuracy: </span><span>{np.mean(scores):</span><span>.4</span><span>f}</span><span> (+/- </span><span>{np.std(scores):</span><span>.4</span><span>f}</span><span>)\u201d</span><span>)</span><span></span></p>\n</code></pre><p>Key parameters for KFold include n_splits (the \u2018K\u2019), shuffle (whether to randomize order before splitting), and random_state (to ensure reproducibility when shuffling). The choice of K involves a trade-off: higher K means larger training folds (reducing bias) but potentially increases variance and computational cost. K=5 or K=10 are popular compromises.</p><h3><strong>Stratified K-Fold Cross-Validation: Ensuring Fairness for Imbalanced Data</strong></h3><p>Standard K-Fold can falter with imbalanced datasets (e.g., fraud detection). Random splitting might create folds with few or no minority class instances, leading to skewed evaluations. Stratified K-Fold ensures each fold preserves the original dataset\u2019s class proportions.</p><figure></figure><figure><figcaption><em>Source: <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html...",
      "url": "https://www.udacity.com/blog/2025/05/cross-validation-in-machine-learning-techniques-and-best-practices.html"
    },
    {
      "title": "Question about the gap between training set metrics and (cross ...",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Question about the gap between training set metrics and (cross)validation metrics, overfitting and underfitting](https://stats.stackexchange.com/questions/354569/question-about-the-gap-between-training-set-metrics-and-crossvalidation-metric)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked5 years, 11 months ago\n\nModified [3 years, 9 months ago](https://stats.stackexchange.com/questions/354569/question-about-the-gap-between-training-set-metrics-and-crossvalidation-metric?lastactivity)\n\nViewed\n1k times\n\n0\n\n$\\\\begingroup$\n\nI have two questions (they can be solved by one answer). They are related to comparing the training metrics vs the validation metrics for each iteration / levels of regularization parameters. I am sorry for posting a long message but I am trying to explain my thoughts as clear as I can.\n\nGenerally from what I know we want to stop training when the validation metrics reach their min/max values (thus the so-called callback functions that are built-in in so many algorithms).\n\nQuestion number 1: What happens if the validation metrics plateau? Should I relax my regularization parameters a bit because I am underfitting? (as to allow for a U shape in the validation metrics graph? U for logloss inverse U for AUC etc.) What if it still is stuck in the same value?. In that case what happens if I keep training without reaching an outrageous gap? This is closely related to my second question:\n\nQuestion number 2: We generally want our training and validation metrics to hold relatively the same values, is that correct? However, what happens if for some reason the validation set and the training set are not representive of each other 100%? More specifically can there be some cases where the model is trained correctly (thus higher training metrics) and not overfit (i am not talking about huge gaps), while these cases are missing in the validation set (thus the validation metrics plateau)?I am asking because In real world, sometimes it is hard to gather huge amounts of data, thus some cases might not be well-represented. So even by performing k-fold validation n times, I may still be depending on the luck of the split; enough cases in the training set and enough of them in the test set. By keeping a very close gap in this situation I am running the risk of underfitting my model, is that correct?\n\nIf my way of thinking is not flawed, would a good solution be to allow for slightly higher metrics in the training set?\n\nPS. I do not want to touch the test-set yet.\n\n- [cross-validation](https://stats.stackexchange.com/questions/tagged/cross-validation)\n- [error](https://stats.stackexchange.com/questions/tagged/error)\n\n[Share](https://stats.stackexchange.com/q/354569)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/354569/edit)\n\nFollow\n\n[edited Jul 5, 2018 at 7:35](https://stats.stackexchange.com/posts/354569/revisions)\n\nAlex G\n\nasked Jul 5, 2018 at 7:16\n\n[![Alex G's user avatar](https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=64)](https://stats.stackexchange.com/users/213648/alex-g)\n\n[Alex G](https://stats.stackexchange.com/users/213648/alex-g) Alex G\n\n122 bronze badges\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/354569/question-about-the-gap-between-training-set-metrics-and-crossvalidation-metric)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/354569/question-about-the-gap-between-training-set-metrics-and-crossvalidation-metric?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\n> Question number 1: What happens if the validation metrics plateau?\n\nA plateau in the validation metrics means that additional variables, parameters or parameter tweaks (depending on what you are tracking in validation) are not improving performance but not degrading it either. They are providing no additional information.\n\n> Should I relax my regularization parameters a bit because I am underfitting? (as to allow for a U shape in the validation metrics graph? U for logloss inverse U for AUC etc.)\n\nThat would be a reasonable option to explore to ensure your restrictions are not too harsh. However, since you would only know this after exposing the validation set then you will no longer be testing independent of model tuning, it will be a validation set for your model limits. This would mean that if you want independent validation of your final model you will need to have a further independent test set.\n\n> What if it still is stuck in the same value?.\n\nA typical response is Ockham's razor, keep the simplest version that gives the _required level of performance_.\n\n> In that case what happens if I keep training without reaching an outrageous gap?\n\nBy Outrageous I assume you mean that the difference is negligible for all practical purposes? This would be practical significance rather than statistical significance, and would require you to define what an 'outrageous gap' or minimally practical difference would be. Then apply Ockham's razor same as if the plateau is completely flat.\n\n> Question number 2: We generally want our training and validation metrics to hold relatively the same values, is that correct?\n\nThat is the ideal, it provides some confidence that the model is stable transferring from the training scenario to an independent implementation.\n\n> However, what happens if for some reason the validation set and the training set are not representive of each other 100%?\n\nThey never will be 100%, even if it is one data collection subsequently split into two sets. Based on stochastic sampling you would expect there to be some differences between the two sets. Ideally you want the differences to be unbiased. Statistics is not about conclusions but risk assessment and management, the difference you encounter between the training and validation sets gives you a first measure of risk associated with deploying your model in new data.\n\n> More specifically can there be some cases where the model is trained correctly (thus higher training metrics) and not overfit (i am not talking about huge gaps), while these cases are missing in the validation set (thus the validation metrics plateau)?\n\nOverfitting is always with respect to samples not included in the training. The value of CV during training is we can internally assess the risk of overfitting by holding out a sub set of samples on multiple iterations, to get a first estimate of overfitting. But what is overfitting? Over fitting is using irrelevant noise in the model that boosts training performance because it explains noise that is specific to that set of data making it appear to describe it better. The danger is that in new data it will be irrelevant and so will provide random and unpredictable deviations from the true value. If the test data simply does not show a sub-group of variation that was present in the training set you will indeed see a plateau in the regions associated with that unrepresented variation. The data may be irrelevant to your test data but may still be relevant to other sets of data. It may say more about the suitability of your test set than you model. The main concern with overfitting is that it really has fitted to noise and you see a drop in performance since the noise is not reproducible in any new data.\n\n> I am asking because In real world, sometimes it is hard to gather huge amounts of data, thus some cases might not be well-represented. So even by performing k-fold validation n times, I may still be depending on the luck of the split; enough cases in the training set and enough of them in the test set. By keeping a very close gap in this situation I am running the risk of underfitting my model,...",
      "url": "https://stats.stackexchange.com/questions/354569/question-about-the-gap-between-training-set-metrics-and-crossvalidation-metric"
    }
  ]
}