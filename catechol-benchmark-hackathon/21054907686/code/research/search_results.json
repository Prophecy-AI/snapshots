{
  "query": "How can I improve predictions when there is a systematic gap between cross-validation and test performance in chemistry machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "A common cause of a persistent gap between cross\u2011validation (CV) scores and true test performance is that the CV protocol does not reflect the distributional shift encountered when the model is deployed.  Recent work on materials\u2011discovery models shows that overly simplistic random splits can give overly optimistic error estimates, while standardized splits that respect chemical similarity (e.g., scaffold, time\u2011 or \u201cstep\u2011forward\u201d splits) produce performance that aligns much better with external test sets\u30101\u2020https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d\u3011\u30109\u2020https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006\u3011.  Adopting such realistic CV schemes, together with careful checks for data leakage (e.g., ensuring that the same molecule or closely related analogues do not appear in both training and validation folds), is the first step to narrowing the systematic bias.\n\nBeyond the split strategy, quantifying and correcting for uncertainty can further improve predictions.  Probabilistic models such as DIONYSUS have been shown to retain calibration and generalizability on low\u2011data chemical sets when combined with proper uncertainty estimation techniques\u30103\u2020https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b\u3011.  Ensembling, conformal prediction, and explicit aleatoric/epistemic decomposition (as described in the uncertainty\u2011characterisation literature) provide reliable confidence intervals and can be used to bias\u2011correct predictions on out\u2011of\u2011distribution test data\u30108\u2020https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/63ecf74d1d2d1840638a8b75/original/characterizing-uncertainty-in-machine-learning-for-chemistry.pdf\u3011.  Calibration techniques (e.g., isotonic regression or temperature scaling) and domain\u2011of\u2011applicability metrics further ensure that the model\u2019s confidence matches observed errors\u30105\u2020https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5\u3011.\n\nFinally, the \u201cfit\u2011vs\u2011noise\u201d perspective reminds us that small, noisy chemistry datasets limit achievable accuracy; augmenting data, using richer molecular representations, and explicitly modelling experimental error can reduce the systematic gap\u30102\u2020https://pubs.rsc.org/en/content/articlehtml/2025/fd/d4fd00091a\u3011.  By combining realistic CV splits, robust uncertainty quantification, calibration, and careful handling of data quality, you can bring CV estimates into closer agreement with true test performance and obtain more trustworthy chemical predictions.",
      "url": ""
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    },
    {
      "title": "Are we fitting data or noise? Analysing the predictive power of commonly used datasets in drug-, materials-, and molecular-discovery",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/fd/d4fd00091a)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/fd/d4fd00087k)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/fd/d4fd00101j)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D4FD00091A](https://doi.org/10.1039/D4FD00091A)\n(Paper)\n[Faraday Discuss.](https://doi.org/10.1039/1364-5498/1991), 2025, **256**, 304-321\n\n# Are we fitting data or noise? Analysing the predictive power of commonly used datasets in drug-, materials-, and molecular-discovery\n\nDaniel\nCrusius\na,\nFlaviu\nCipcigan\nb and Philip C.\nBiggin\n\\*aaDepartment of Biochemistry, University of Oxford, South Parks Road, Oxford OX1 3QU, UK. E-mail: [philip.biggin@bioch.ox.ac.uk](mailto:philip.biggin@bioch.ox.ac.uk)bIBM Research Europe, The Hartree Centre STFC Laboratory, Sci-Tech Daresbury, Warrington WA4 4AD, UK\n\nReceived\n6th May 2024\n, Accepted 4th June 2024\n\nFirst published on 4th June 2024\n\n## Abstract\n\nData-driven techniques for establishing quantitative structure property relations are a pillar of modern materials and molecular discovery. Fuelled by the recent progress in deep learning methodology and the abundance of new algorithms, it is tempting to chase benchmarks and incrementally build ever more capable machine learning (ML) models. While model evaluation has made significant progress, the intrinsic limitations arising from the underlying experimental data are often overlooked. In the chemical sciences data collection is costly, thus datasets are small and experimental errors can be significant. These limitations of such datasets affect their predictive power, a fact that is rarely considered in a quantitative way. In this study, we analyse commonly used ML datasets for regression and classification from drug discovery, molecular discovery, and materials discovery. We derived maximum and realistic performance bounds for nine such datasets by introducing noise based on estimated or actual experimental errors. We then compared the estimated performance bounds to the reported performance of leading ML models in the literature. Out of the nine datasets and corresponding ML models considered, four were identified to have reached or surpassed dataset performance limitations and thus, they may potentially be fitting noise. More generally, we systematically examine how data range, the magnitude of experimental error, and the number of data points influence dataset performance bounds. Alongside this paper, we release the Python package NoiseEstimator and provide a web-based application for computing realistic performance bounds. This study and the resulting tools will help practitioners in the field understand the limitations of datasets and set realistic expectations for ML model performance. This work stands as a reference point, offering analysis and tools to guide development of future ML models in the chemical sciences.\n\n## 1 Introduction\n\nMachine learning (ML) models are widely used tools in the fields of chemistry, drug discovery, molecular science, and materials-discovery. [1\u20134](https://pubs.rsc.org/pubs.rsc.org#cit1) These models aid the development of quantitative structure activity relations (QSAR) or quantitative structure property relations (QSPR), which can be used to predict various properties such as bioactivity, physicochemical characteristics, reaction data, or quantum mechanical properties. [5\u20139](https://pubs.rsc.org/pubs.rsc.org#cit5) The focus of the ML community and literature is often on state-of-the-art algorithms. However, the recent and past successes of ML models in biology and chemistry are not only due to algorithmic advancements, but also because of increasing amounts of data, either deposited to databases or laboriously curated from existing literature. [10\u201313](https://pubs.rsc.org/pubs.rsc.org#cit10) Assessing the variability in experimental data is important, [14](https://pubs.rsc.org/pubs.rsc.org#cit14) but ML applications in chemistry are also often limited by the high cost and presence of experimental noise in the data. This challenge is recognised but not always accounted for when evaluating ML model performance and uncertainty. [15](https://pubs.rsc.org/pubs.rsc.org#cit15)\n\nThe ML literature distinguishes two types of uncertainty: aleatoric and epistemic. [16\u201318](https://pubs.rsc.org/pubs.rsc.org#cit16) Aleatoric uncertainty arises due to random or systematic noise in the data. ML models are capable of fitting noise perfectly, [19](https://pubs.rsc.org/pubs.rsc.org#cit19) therefore it is important to consider the aleatoric limit, a maximum performance limit of ML models due to noise in the underlying data. The aleatoric limit primarily refers to the evaluation or test set data: it has been shown that performance of ML models trained on noisy data can potentially surpass the expected performance due to noise in the training set, if evaluated on a noise-free dataset. [18](https://pubs.rsc.org/pubs.rsc.org#cit18) Nonetheless, in practice, training and test datasets usually have comparable noise levels, and this effect most likely remains hidden. Epistemic uncertainty, on the other hand, is uncertainty due to limited expressiveness of a model, known as model bias; and suboptimal parameter choice, often referred to as model variance. [17](https://pubs.rsc.org/pubs.rsc.org#cit17)\n\nIn this study, we specifically focus on how aleatoric uncertainty, or experimental noise, can limit ML model performance. We extend the method by Brown et al. to define performance bounds for common datasets in chemistry and materials, distinguishing between experimental noise (\u03c3E) and prediction noise (\u03c3pred). Assuming a perfect model (\u03c3pred = 0), we obtain the aleatoric limit or maximum performance bound. When incorporating non-zero model prediction noise \u03c3pred, which could arise from model bias, model variance, or noise in the training dataset, we also identify a realistic performance bound.\n\nThe method of Brown derives performance bounds by computing performance metrics between a set of data points and the same set with added noise. If the added noise matches the size of the underlying experimental error, the method reveals limits of model accuracy that should not be surpassed.\n\nWe investigate the impact of data range, experimental error, and dataset size on these performance bounds. We then examine nine ML datasets from biological, chemical, and materials science domains, estimate performance bounds based on experimental errors, and compare to reported performance of leading ML models.\n\n## 2 Results and discussion\n\nIn Section 2.1, we analyse the general influence of dataset properties, such as the data range, the size of experimental errors, and the number of data points on the maximum and realistic performance bounds of datasets used for ML models. Utilising synthetic datasets, we specifically investigate how Gaussian noise, applied at one and two levels, affects these bounds. This analysis is the foundation for Section 2.2, where we compare estimated performance bounds of nine real-world ML datasets to reported performance of leading ML models. This allows us to distinguish between datasets where ML models have reached the limit of performance due to experimental error, and datasets where there is still room for ML model improvement.\n\n### 2.1 Impact of data range, experimental error, and number of datapoints on realistic and maximum performance bounds\n\nIn the following, we investigate the effect of data range, magnitude of experimental error, and dataset size on performance bounds using the method developed by Brown et al. [20](https://pubs.rsc.org/pubs.rsc.org#cit20) described in detail in Section 4.1 and extended by us to classification datasets. We define two types of performance bounds: a maximum performance bound where we only assume...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/fd/d4fd00091a"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Assessing the calibration in toxicological in vitro models with conformal prediction",
      "text": "Search all BMC articles\n\nSearch\n\nAssessing the calibration in toxicological in vitro models with conformal prediction\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-021-00511-5.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-021-00511-5.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-021-00511-5.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-021-00511-5.epub)\n\n- Research article\n- [Open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 29 April 2021\n\n# Assessing the calibration in toxicological in vitro models with conformal prediction\n\n- [Andrea Morger](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Andrea-Morger-Aff1) [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff1),\n- [Fredrik Svensson](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Fredrik-Svensson-Aff2) [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff2),\n- [Staffan Arvidsson McShane](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Staffan-Arvidsson_McShane-Aff3) [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff3),\n- [Niharika Gauraha](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Niharika-Gauraha-Aff3-Aff4) [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff3), [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff4),\n- [Ulf Norinder](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Ulf-Norinder-Aff3-Aff5-Aff6) [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff3), [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff5), [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff6),\n- [Ola Spjuth](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Ola-Spjuth-Aff3) [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff3)[na1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#na1) &\n- \u2026\n- [Andrea Volkamer](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#auth-Andrea-Volkamer-Aff1)[ORCID: orcid.org/0000-0002-3760-580X](http://orcid.org/0000-0002-3760-580X)[1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#Aff1)[na1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#na1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a013**, Article\u00a0number:\u00a035 (2021)\n[Cite this article](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#citeas)\n\n- 2789 Accesses\n\n- 11 Citations\n\n- 3 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5/metrics)\n\n\n## Abstract\n\nMachine learning methods are widely used in drug discovery and toxicity prediction. While showing overall good performance in cross-validation studies, their predictive power (often) drops in cases where the query samples have drifted from the training data\u2019s descriptor space. Thus, the assumption for applying machine learning algorithms, that training and test data stem from the same distribution, might not always be fulfilled. In this work, conformal prediction is used to assess the calibration of the models. Deviations from the expected error may indicate that training and test data originate from different distributions. Exemplified on the Tox21 datasets, composed of chronologically released Tox21Train, Tox21Test and Tox21Score subsets, we observed that while internally valid models could be trained using cross-validation on Tox21Train, predictions on the external Tox21Score data resulted in higher error rates than expected. To improve the prediction on the external sets, a strategy exchanging the calibration set with more recent data, such as Tox21Test, has successfully been introduced. We conclude that conformal prediction can be used to diagnose data drifts and other issues related to model calibration. The proposed improvement strategy\u2014exchanging the calibration data only\u2014is convenient as it does not require retraining of the underlying model.\n\n## Introduction\n\nMachine learning (ML) methods are ubiquitous in drug discovery and toxicity prediction \\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR1), [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR2)\\]. In silico toxicity prediction is typically used to guide toxicity testing in early phases of drug design \\[ [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR3)\\]. With more high-quality standardised data available, the (potential) impact of ML methods in regulatory toxicology is growing \\[ [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR4)\\]. The collection of available toxicity data is increasing, thanks in part to high-throughput screening programs such as ToxCast \\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR5)\\] and Tox21 \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR6), [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR7)\\], but also with public-private partnerships such as the eTOX and eTRANSAFE projects, which focus on the sharing of (confidential) toxicity data and ML models across companies \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR8), [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR9)\\]. In any case, no matter which underlying data and ML method is used, it is essential to know or assess if the ML model can be reliably used to make predictions on a new dataset.\n\nHence, validation of ML models is crucial to assess their predictivity. Several groups investigated random vs. rational selection of optimal test/training sets, e.g. using cluster- or activity-based splits, with the goal of better reflecting the true predictive power of established models \\[ [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR10), [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR12), [13](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR13), [14](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR14)\\]. Martin et al. \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR11)\\] showed that rational selection of training and test sets\u2014compared to random splits\u2014generated better statistical results on the (internal) test sets. However, the performance of both types of regression models on the\u2014artificially created\u2014external evaluation set was comparable.\n\nThus, further metrics to define the applicability domain (AD), the domain in which an ML classifier can reliably be applied \\[ [15](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR15), [16](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR16), [17](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR18), [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR20), [21](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5#ref-CR21)\\], are needed. Besides traditional metrics accounting for chemical space coverage, Sheridan \\[ [20](https://...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00511-5"
    },
    {
      "title": "",
      "text": "Characterizing Uncertainty in Machine Learning for Chemistry\nEsther Heid,1, 2, \u2217 Charles J. McGill,1, 3, \u2217 Florence H. Vermeire,1, 4 and William H. Green1, \u2020\n1Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, United States\n2\nInstitute of Materials Chemistry, TU Wien, 1060 Vienna, Austria\n3Department of Chemical and Life Science Engineering,\nVirginia Commonwealth University, Richmond, Virginia 23284, United States\n4Department of Chemical Engineering, KU Leuven, Celestijnenlaan 200F, B-3001 Leuven, Belgium\nCharacterizing uncertainty in machine learning models has recently gained interest in the context\nof machine learning reliability, robustness, safety, and active learning. Here, we separate the total\nuncertainty into contributions from noise in the data (aleatoric) and shortcomings of the model\n(epistemic), further dividing epistemic uncertainty into model bias and variance contributions. We\nsystematically address the influence of noise, model bias, and model variance in the context of\nchemical property predictions, where the diverse nature of target properties and the vast chemical\nchemical space give rise to many different distinct sources of prediction error. We demonstrate\nthat different sources of error can each be significant in different contexts and must be individually\naddressed during model development. Through controlled experiments on datasets of molecular\nproperties, we show important trends in model performance associated with the level of noise in\nthe dataset, size of the dataset, model architecture, molecule representation, ensemble size, and\ndataset splitting. In particular, we show that 1) noise in the test set can limit a model\u2019s observed\nperformance when the actual performance is much better, 2) using size-extensive model aggregation\nstructures is crucial for extensive property prediction, 3) ensembling is a reliable tool for uncertainty\nquantification and improvement specifically for the contribution of model variance, and 4) evalu\u0002ations of cross-validation models understate their performance. We develop general guidelines on\nhow to improve an underperforming model when falling into different uncertainty contexts.\nI. INTRODUCTION\nMachine learning models for chemical applications\nsuch as predicting molecular and reaction proper\u0002ties are becoming not only increasingly popular, but\nalso increasingly accurate, for example for quantum\u0002mechanical properties,1\u20133 biological effects,4\u20136 physico\u0002chemical properties,7\u201311, reaction yields,12\u201314 or reaction\nrates and barriers.15\u201319 Also, promising developments\nin the field of retrosynthesis20\u201324 and forward reaction\nprediction,25\u201328 have been made.\nHowever, despite the increase in accuracy, many ma\u0002chine learning models fail in real-world applications.29,30\nThis can be due to a lack of generalization, lack of abil\u0002ity to filter out erroneous predictions for edge cases, or\nbecause the employed training and test sets are simply\nnot reflective of the application of interest, so that the\ndeveloped model is suboptimal for the proposed task.\nPoor choice of test set can overestimate, or more com\u0002monly, underestimate the actual errors that a user will\nencounter when the model is applied. Optimizing a\nmediocre model can be tedious, time-consuming, and of\u0002ten unfruitful. Moreover, the model architectures, input\nrepresentations, and dataset characteristics for chemical\napplications differ considerably from other fields of re\u0002search, so that following general guidelines for optimizing\nmachine learning models often fails to produce accurate\nmodels for molecular and reaction properties. To opti\u0002mize a model in a targeted and efficient manner, it is\nimperative to understand and identify possible sources\nof error and uncertainty in a model.\nThe separation of the total uncertainty into aleatoric\n(data-dependent, noise-induced, irreducible) and epis\u0002temic (model-dependent, reducible) contributions31 has\nrecently received increasing attention.32\u201334 The aleatoric\nuncertainty is often referred to as the irreducible com\u0002ponent of uncertainty that cannot be overcome by im\u0002provements to the model. Reduction in aleatoric un\u0002certainty can instead come from improvements in the\ndata itself, such as adding repeat measurements or re\u0002moving erroneous entries. In contrast, epistemic uncer\u0002tainty characterizes the reducible uncertainty caused by\nmissing knowledge and can be decreased as the model\nis improved.35 The epistemic uncertainty can further be\nsplit into uncertainty arising from the choice of model\n(architecture, representation, and featurization) and the\nambiguity of parameter optimization once a model is\nchosen.35 In this work, we follow the convention36,37 of\ncalling the former model bias and the latter variance,\nbut different other names are sometimes used in the lit\u0002erature, such as model uncertainty and approximation\nuncertainty.35 The difference between reducible and irre\u0002ducible uncertainty can become blurred in these consider\u0002ations, especially for different model architectures, differ\u0002ent representations, and different data and test sets.32,35\nSmall dataset sizes contribute to both bias and variance\ncomponents of epistemic uncertainty because they cause\nsome ambiguity in the optimal model parameters due to\nsparsity in some regions, but also hinder the model con\u0002vergence to a meaningful minimum generally. The size or\nnature of the data may additionally influence the choice\nof model architecture or machine learning method, pro\u0002viding a further avenue by which aspects of the data can\n1\nfeed into epistemic uncertainty.\nMany approaches toward characterizing the uncer\u0002tainty of a prediction exist, such as mean-variance\nestimation,38 Bayesian approaches,39 ensembling,40\u201343\nevidential learning,44 and conformal predictions,45\namongst many others.35,46,47 Most approaches tackle\naleatoric uncertainty, as well as those parts of the epis\u0002temic uncertainty that are associated with the ambigu\u0002ity of the model parameters. However, uncertainty from\nmodel bias is usually omitted.35 Even when the aleatoric\nerror is low and plenty of data is available for training,\nmodel bias can still prove to be significant. Model bias\ncan have many forms and causes, among them limited\nflexibility of the model, limited data coverage, incomplete\nfeature representation of the input data, poor training\nconvergence to an appropriate model, and poor general\u0002izability of training to the test set or to actual applica\u0002tions. We discuss how, especially in chemical systems,\nuncertainty from model bias can be a large contribution\ntoward the error in a model\u2019s prediction.\nDespite the many works on characterizing uncertainty,\nlittle advice exists on how to optimize a sub-optimal\nmodel once the sources of uncertainty are known. Fur\u0002thermore, the circumstances under which the epistemic\nuncertainty modeled by ensembling is actually indicative\nof the true error are not well researched yet, despite its\npopularity.46 We therefore studied the performance of se\u0002lected deep learning models on chemical prediction tasks\nwhere we systematically vary noise in the input data,\nthe number of datapoints, the chosen model architecture,\nmolecular representation, and the number of models in\nan ensemble. To this aim, we rely not only on literature\ndatasets, but also construct a new, noise-free, chemically\nmeaningful dataset. In the discussion, we then put for\u0002ward general guidelines for how to detect and circum\u0002vent model errors caused by noise, bias, and variance.\nWe pay particular attention to predictions of physico\u0002chemical targets, since we find some of the sources of\nuncertainty to be specific to chemistry.\nII. METHODS\nA. Datasets\nIn this work, a synthetic dataset was constructed for\nmolecular enthalpy at 298 K in units of kcal/mol as cal\u0002culated from group additivity coefficients, based on the\nBenson group-increment theory48. The dataset was de\u0002sired to have characteristics well-suited to the analysis\nof error...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/63ecf74d1d2d1840638a8b75/original/characterizing-uncertainty-in-machine-learning-for-chemistry.pdf"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Cross-validation pitfalls when selecting and assessing regression and classification models",
      "text": "Cross-validation pitfalls when selecting and assessing regression and classification models | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/1758-2946-6-10?)\n# Cross-validation pitfalls when selecting and assessing regression and classification models\n* Methodology\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:29 March 2014\n* Volume\u00a06, article\u00a0number10, (2014)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nCross-validation pitfalls when selecting and assessing regression and classification models\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n* [Damjan Krstajic](#auth-Damjan-Krstajic-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3),\n* [Ljubomir J Buturovic](#auth-Ljubomir_J-Buturovic-Aff3)[3](#Aff3),\n* [David E Leahy](#auth-David_E-Leahy-Aff4)[4](#Aff4)&amp;\n* \u2026* [Simon Thomas](#auth-Simon-Thomas-Aff5)[5](#Aff5)Show authors\n* 150kAccesses\n* 902Citations\n* 42Altmetric\n* 1Mention\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10/metrics)\n## Abstract\n### Background\nWe address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.\n### Methods\nWe describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case.\n### Results\nWe show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models.\n### Conclusions\nWe demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-024-06630-y/MediaObjects/10994_2024_6630_Fig1_HTML.png)\n### [Reducing cross-validation variance through seed blocking in hyperparameter tuning](https://link.springer.com/10.1007/s10994-024-06630-y?fromPaywallRec=false)\nArticle17 February 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-31041-7?as&#x3D;webp)\n### [Enhancement of Cross Validation Using Hybrid Visual and Analytical Means with Shannon Function](https://link.springer.com/10.1007/978-3-030-31041-7_29?fromPaywallRec=false)\nChapter\u00a9 2020\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42979-022-01051-x/MediaObjects/42979_2022_1051_Figa_HTML.png)\n### [An Efficient Ridge Regression Algorithm with Parameter Estimation for Data Analysis in Machine Learning](https://link.springer.com/10.1007/s42979-022-01051-x?fromPaywallRec=false)\nArticle23 February 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Compound Screening](https://jcheminf.biomedcentral.com/subjects/compound-screening)\n* [Linear Models and Regression](https://jcheminf.biomedcentral.com/subjects/linear-models-and-regression)\n* [Learning algorithms](https://jcheminf.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Molecular Target Validation](https://jcheminf.biomedcentral.com/subjects/molecular-target-validation)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Background\nAllen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)], Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] and Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)], independently introduced cross-validation as a way of estimating parameters for predictive models in order to improve predictions. Allen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)] proposed the PRESS (Prediction Sum of Squares) criteria, equivalent to leave-one-out cross-validation, for problems with selection of predictors and suggested it for general use. Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] suggested the use of leave-one-out cross-validation for estimating model parameters and for assessing their predictive error. It is important to note that Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] was the first to clearly differentiate between the use of cross-validation to select the model (\u201ccross-validatory choice\u201d) and to assess the model (\u201ccross-validatory assessment\u201d). Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)] introduced the Predictive Sample Reuse Method, a method equivalent to V-fold cross-validation, arguing that it improves predictive performance of the cross-validatory choice, at a cost of introducing pseudo-randomness in the process. Since then, cross-validation, with its different varieties, has been investigated extensively and, due to its universality, gained popularity in statistical modelling.\nIn an ideal situation we would have enough data to train and validate our models (training samples) and have separate data for assessing the quality of our model (test samples). Both training and test samples would need to be sufficiently large and diverse in order to be represenatitive. However such data rich situations are rare in life sciences, including QSAR. A major problem with selection and assessment of models is that we usually only have information from the training samples, and it is therefore not feasible to calculate a test error. However, even though we cannot calculate the test error, it is possible to estimate the expected test error using training samples. It can be shown that the expected test error is the...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10"
    },
    {
      "title": "Reliable estimation of externally validated prediction errors for QSAR models",
      "text": "Search all BMC articles\n\nSearch\n\nReliable estimation of externally validated prediction errors for QSAR models\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/1758-2946-5-S1-P33.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/1758-2946-5-S1-P33.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/1758-2946-5-S1-P33.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/1758-2946-5-S1-P33.epub)\n\nVolume 5 Supplement 1\n\n## [8th German Conference on Chemoinformatics: 26 CIC-Workshop](https://jcheminf.biomedcentral.com/articles/supplements/volume-5-supplement-1)\n\n- Poster presentation\n- [Open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 22 March 2013\n\n# Reliable estimation of externally validated prediction errors for QSAR models\n\n- [D\u00e9sir\u00e9e Baumann](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#auth-D_sir_e-Baumann-Aff1) [1](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#Aff1) &\n- [Knut Baumann](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#auth-Knut-Baumann-Aff1) [1](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#Aff1)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a05**, Article\u00a0number:\u00a0P33 (2013)\n[Cite this article](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#citeas)\n\n- 1799 Accesses\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33/metrics)\n\n\nIn most cases of QSAR modelling the final model used to make predictions, is not known _a priori_ but has to be selected in a data driven fashion (e.g. selection of principal components, variable selection, selection of the best mathematical modelling technique). Reliable estimation of externally validated prediction errors under this model uncertainty is still a challenge in chemoinformatics. To fulfil the standards of external validation, the test data set has to be independent not only from model building but also from model selection.\n\nThere still is a controversy in the literature how the independent test data set should be chosen and how large it should be. For setting aside a test data set there are basically two different options: 1) a single test data set is set aside and 2) the test data are generated by repeatedly partitioning the available data into test and training set partitions - i.e. cross-validation. Since cross-validation uses the data more efficiently, it is to be preferred in particular for small data sets.\n\nThe aforementioned cross-validation step must not be confused with a cross-validation step that might be necessary to select the model! If model selection is also done by cross-validation two loops of cross-validation are necessary \\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#ref-CR1)\\]. In the inner loop, cross-validation is employed for model selection \\[ [2](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33#ref-CR2)\\] (also referred to as internal validation) while in the outer loop of cross-validation different test data sets are generated repeatedly that are used to assess the readily selected models (external validation).\n\nIn this contribution double cross-validation is evaluated for its ability to estimate prediction errors under model uncertainty. Depending on how double cross-validation is parameterized (test set size, number of repetitions), it either yields biased or highly variable estimates of the prediction error. The sources of bias and variability will be highlighted and recommendations are provided how to determine the test set size in order to obtain a favourable bias-variability trade-off.\n\n## References\n\n1. Filzmoser P, Liebmann B, Varmuza K: Repeated double cross-validation. J Chemometrics. 2009, 23: 160-171. 10.1002/cem.1225.\n\n[Article](https://doi.org/10.1002%2Fcem.1225) [CAS](https://jcheminf.biomedcentral.com/articles/cas-redirect/1:CAS:528:DC%2BD1MXltFaksLY%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Repeated%20double%20cross-validation&journal=J%20Chemometrics&doi=10.1002%2Fcem.1225&volume=23&pages=160-171&publication_year=2009&author=Filzmoser%2CP&author=Liebmann%2CB&author=Varmuza%2CK)\n\n2. Baumann K: Cross-validation as the objective function for variable selection. Trends Anal Chem. 2003, 22: 395-406. 10.1016/S0165-9936(03)00607-1.\n\n[Article](https://doi.org/10.1016%2FS0165-9936%2803%2900607-1) [CAS](https://jcheminf.biomedcentral.com/articles/cas-redirect/1:CAS:528:DC%2BD3sXks1WrsL8%3D) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Cross-validation%20as%20the%20objective%20function%20for%20variable%20selection&journal=Trends%20Anal%20Chem&doi=10.1016%2FS0165-9936%2803%2900607-1&volume=22&pages=395-406&publication_year=2003&author=Baumann%2CK)\n\n\n[Download references](https://citation-needed.springer.com/v2/references/10.1186/1758-2946-5-S1-P33?format=refman&flavour=references)\n\n## Author information\n\n### Authors and Affiliations\n\n1. Institut f\u00fcr Medizinische und Pharmazeutische Chemie, Technische Universit\u00e4t Braunschweig, Beethovenstra\u00dfe 55, D-38106, Braunschweig, Germany\n\nD\u00e9sir\u00e9e Baumann\u00a0&\u00a0Knut Baumann\n\n\nAuthors\n\n1. D\u00e9sir\u00e9e Baumann\n\n\n[View author publications](https://www.biomedcentral.com/search?query=author%23D%C3%A9sir%C3%A9e%20Baumann)\n\n\n\n\n\nYou can also search for this author in\n[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=D%C3%A9sir%C3%A9e%20Baumann) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22D%C3%A9sir%C3%A9e%20Baumann%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n\n2. Knut Baumann\n\n\n[View author publications](https://www.biomedcentral.com/search?query=author%23Knut%20Baumann)\n\n\n\n\n\nYou can also search for this author in\n[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Knut%20Baumann) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Knut%20Baumann%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n\n\n### Corresponding author\n\nCorrespondence to\n[D\u00e9sir\u00e9e Baumann](mailto:d.baumann@tu-bs.de).\n\n## Rights and permissions\n\n**Open Access** This article is distributed under the terms of the Creative Commons Attribution 2.0 International License ( [https://creativecommons.org/licenses/by/2.0](https://creativecommons.org/licenses/by/2.0)), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=Reliable%20estimation%20of%20externally%20validated%20prediction%20errors%20for%20QSAR%20models&author=D%C3%A9sir%C3%A9e%20Baumann%20et%20al&contentID=10.1186%2F1758-2946-5-S1-P33&copyright=Baumann%20and%20Baumann%3B%20licensee%20BioMed%20Central%20Ltd.&publication=1758-2946&publicationDate=2013-03-22&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)\n\n## About this article\n\n### Cite this article\n\nBaumann, D., Baumann, K. Reliable estimation of externally validated prediction errors for QSAR models.\n_J Cheminform_ **5**\n(Suppl 1), P33 (2013). https://doi.org/10.1186/1758-2946-5-S1-P33\n\n[Download citation](https://citation-needed.springer.com/v2/references/10.1186/1758-2946-5-S1-P33?format=refman&flavour=citation)\n\n- Published: 22 March 2013\n\n- DOI: https://doi.org/10.1186/1758-2946-5-S1-P33\n\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nGet shareable link\n\nSorry, a shareable link is not currently available for this article.\n\nCopy to clipboard\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Keywords\n\n- [Test Data](https://jcheminf.bio...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-5-S1-P33"
    }
  ]
}