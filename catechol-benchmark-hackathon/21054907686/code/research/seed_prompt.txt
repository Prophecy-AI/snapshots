## Current Status
- Best CV score: 0.008298 from exp_030 (GP 0.15 + MLP 0.55 + LGBM 0.30)
- Best LB score: 0.0877 from exp_030
- CV-LB gap: LB = 4.31*CV + 0.0525 (R²=0.95)
- Target: 0.0707 (24% improvement needed from best LB)
- Submissions remaining: 5

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The experiment was well-executed.
- Evaluator's top priority: Prediction Calibration (Isotonic Regression). AGREE - this directly addresses the intercept.
- Key concerns raised: 27 consecutive failures, need fundamentally different approach. AGREE - we need to change the CV-LB relationship, not just improve CV.
- Evaluator correctly identified that the target IS reachable (intercept 0.0525 < target 0.0707).
- Required CV to hit target: 0.00422 (49% improvement from current best).

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop58_analysis.ipynb` for CV-LB analysis
- Key patterns:
  1. CV-LB relationship is highly linear (R²=0.95) with intercept 0.0525
  2. exp_000 has best residual (-0.0022) - simpler model performed better on LB than expected
  3. The 'mixall' kernel uses GroupKFold(5) instead of Leave-One-Out(24) - not directly comparable
  4. DRFP features are important for CV but may hurt LB generalization

## Recommended Approaches

### Priority 1: CatBoost Ensemble (NEW - NOT YET TRIED)
**Rationale**: CatBoost is a fundamentally different gradient boosting implementation that:
- Handles categorical features natively (solvent names)
- Uses ordered boosting to reduce overfitting
- Has different inductive biases than XGBoost/LightGBM
- May have better out-of-distribution generalization

**Implementation**:
```python
from catboost import CatBoostRegressor

catboost_model = CatBoostRegressor(
    iterations=500,
    learning_rate=0.03,
    depth=6,
    l2_leaf_reg=3,
    random_seed=42,
    verbose=False
)
```

**Why this might work**: CatBoost's ordered boosting and symmetric trees may generalize better to unseen solvents.

### Priority 2: Quantile Regression (NEW - NOT YET TRIED)
**Rationale**: Training with quantile loss (median) instead of MSE may:
- Produce more robust predictions
- Be less sensitive to outliers
- Change the CV-LB relationship

**Implementation**:
```python
# Use quantile loss (median) instead of MSE
from lightgbm import LGBMRegressor

lgbm_quantile = LGBMRegressor(
    objective='quantile',
    alpha=0.5,  # median
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    random_state=42
)
```

**Why this might work**: Median predictions are more robust to outliers and may generalize better.

### Priority 3: Ensemble with Different Random Seeds
**Rationale**: The CV-LB relationship has variance. Training multiple models with different seeds and selecting the one with best LB (via submission) may help.

**Implementation**:
- Train 5 versions of exp_030 with different random seeds
- Submit the one with best CV (hoping for favorable LB variance)

### Priority 4: Feature Selection Based on LB Correlation
**Rationale**: Some features may hurt LB generalization even if they help CV.
- exp_000 (simpler features) had best residual (-0.0022)
- DRFP features may be overfitting to CV

**Implementation**:
- Try removing DRFP features from exp_030
- Keep only Spange + Arrhenius features
- But use the full GP + MLP + LGBM ensemble

## What NOT to Try
- Simpler Spange-only features (exp_057 failed badly - CV 0.023)
- XGBoost + RF ensemble (exp_056 failed - CV 0.014)
- Pure GP (exp_032 failed)
- Higher GP weight (exp_031, exp_035 failed)
- GNN architectures (exp_040, exp_052 failed)
- ChemBERTa (exp_041 failed)
- Calibration approaches (exp_042 failed)
- Similarity weighting (exp_037 failed)
- Domain adaptation approaches (exp_049, exp_050 failed)

## Validation Notes
- CV scheme: Leave-One-Solvent-Out (24 folds for single, 13 for mixtures)
- CV-LB relationship: LB = 4.31*CV + 0.0525
- To reach target LB 0.0707, need CV = 0.00422 (49% improvement)
- OR need to reduce the intercept (0.0525) through better generalization

## Submission Strategy
- 5 submissions remaining
- Use 2-3 for experiments that might change CV-LB relationship
- Save 2 for final attempts
- Focus on approaches that reduce the intercept, not just CV

## Key Insight from Research
Recent research on CV-test gaps in chemistry ML suggests:
1. Calibration techniques (isotonic regression, temperature scaling) can reduce systematic bias
2. Realistic CV splits that respect chemical similarity produce better LB estimates
3. Ensembling with uncertainty quantification helps
4. The aleatoric limit (noise in data) may be limiting performance

The path forward is NOT more model variations - it's changing the CV-LB relationship through:
1. Different model types (CatBoost) with different inductive biases
2. Different loss functions (quantile regression)
3. Feature selection that favors LB generalization over CV