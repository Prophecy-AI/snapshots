{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc86b67c",
   "metadata": {},
   "source": [
    "# Experiment 047: Diverse Ensemble Model\n",
    "\n",
    "**Inspiration:** The 'mixall' kernel achieves good CV/LB with an ensemble of MLP + XGBoost + RandomForest + LightGBM.\n",
    "\n",
    "**Hypothesis:** Our current ensemble (GP + MLP + LGBM) may be too homogeneous. A more diverse ensemble could capture different patterns and potentially change the CV-LB relationship.\n",
    "\n",
    "**Implementation:**\n",
    "1. MLP: [128, 64, 32] with BatchNorm, ReLU, Dropout(0.1)\n",
    "2. XGBoost: n_estimators=300, max_depth=6\n",
    "3. RandomForest: n_estimators=300, max_depth=15\n",
    "4. LightGBM: n_estimators=300, num_leaves=31\n",
    "5. Weighted ensemble: [0.4, 0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c1670f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:17:54.731528Z",
     "iopub.status.busy": "2026-01-16T00:17:54.730668Z",
     "iopub.status.idle": "2026-01-16T00:17:57.429847Z",
     "shell.execute_reply": "2026-01-16T00:17:57.429039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7b2ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:17:57.432576Z",
     "iopub.status.busy": "2026-01-16T00:17:57.431832Z",
     "iopub.status.idle": "2026-01-16T00:17:57.458092Z",
     "shell.execute_reply": "2026-01-16T00:17:57.457515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: 13 features\n",
      "Single solvent: 656 samples\n",
      "Full data: 1227 samples\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[[\"SM\", \"Product 2\", \"Product 3\"]]\n",
    "    return X, Y\n",
    "\n",
    "# Load feature lookup tables\n",
    "spange_df = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "SPANGE_COLS = [c for c in spange_df.columns if c != 'solvent smiles']\n",
    "\n",
    "print(f'Spange: {len(SPANGE_COLS)} features')\n",
    "\n",
    "# Load data\n",
    "X_single, Y_single = load_data('single_solvent')\n",
    "X_full, Y_full = load_data('full')\n",
    "\n",
    "print(f'Single solvent: {len(X_single)} samples')\n",
    "print(f'Full data: {len(X_full)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d617dce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:17:57.460002Z",
     "iopub.status.busy": "2026-01-16T00:17:57.459813Z",
     "iopub.status.idle": "2026-01-16T00:17:57.467456Z",
     "shell.execute_reply": "2026-01-16T00:17:57.466887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction defined\n",
      "Feature dimension: 5 (kinetics) + 13 (Spange) = 18\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction - simpler features (Spange + kinetics only, no DRFP)\n",
    "def get_features_simple(X, data_type='single'):\n",
    "    \"\"\"Extract simpler features: Spange descriptors + kinetics only.\"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    for idx, row in X.iterrows():\n",
    "        time_m = row['Residence Time']\n",
    "        temp_c = row['Temperature']\n",
    "        temp_k = temp_c + 273.15\n",
    "        \n",
    "        kinetics = np.array([\n",
    "            time_m, temp_c, 1.0 / temp_k,\n",
    "            np.log(time_m + 1), time_m / temp_k\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if data_type == 'single':\n",
    "            solvent = row['SOLVENT NAME']\n",
    "            spange = spange_df.loc[solvent, SPANGE_COLS].values.astype(np.float32) if solvent in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "        else:\n",
    "            solvent_a = row['SOLVENT A NAME']\n",
    "            solvent_b = row['SOLVENT B NAME']\n",
    "            pct_b = row['SolventB%'] / 100.0\n",
    "            pct_a = 1 - pct_b\n",
    "            \n",
    "            sp_a = spange_df.loc[solvent_a, SPANGE_COLS].values.astype(np.float32) if solvent_a in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            sp_b = spange_df.loc[solvent_b, SPANGE_COLS].values.astype(np.float32) if solvent_b in spange_df.index else np.zeros(len(SPANGE_COLS), dtype=np.float32)\n",
    "            spange = pct_a * sp_a + pct_b * sp_b\n",
    "        \n",
    "        features = np.concatenate([kinetics, spange])\n",
    "        features_list.append(features)\n",
    "    \n",
    "    return np.array(features_list, dtype=np.float32)\n",
    "\n",
    "print('Feature extraction defined')\n",
    "print(f'Feature dimension: 5 (kinetics) + {len(SPANGE_COLS)} (Spange) = {5 + len(SPANGE_COLS)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb9687e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:17:57.469311Z",
     "iopub.status.busy": "2026-01-16T00:17:57.468926Z",
     "iopub.status.idle": "2026-01-16T00:17:57.475815Z",
     "shell.execute_reply": "2026-01-16T00:17:57.475326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedMLP defined\n"
     ]
    }
   ],
   "source": [
    "# Enhanced MLP Model\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, h_dim),\n",
    "                nn.BatchNorm1d(h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 3))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('EnhancedMLP defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d277eb07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:17:57.477781Z",
     "iopub.status.busy": "2026-01-16T00:17:57.477581Z",
     "iopub.status.idle": "2026-01-16T00:17:57.491258Z",
     "shell.execute_reply": "2026-01-16T00:17:57.490739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiverseEnsembleModel defined\n"
     ]
    }
   ],
   "source": [
    "# Diverse Ensemble Model (MLP + XGBoost + RandomForest + LightGBM)\n",
    "class DiverseEnsembleModel:\n",
    "    \"\"\"Diverse ensemble inspired by 'mixall' kernel.\n",
    "    \n",
    "    Uses MLP + XGBoost + RandomForest + LightGBM with weighted averaging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', weights=[0.4, 0.2, 0.2, 0.2]):\n",
    "        self.data_type = data\n",
    "        self.weights = weights  # MLP, XGB, RF, LGBM\n",
    "        \n",
    "        self.scaler = None\n",
    "        self.mlp_models = []\n",
    "        self.xgb_model = None\n",
    "        self.rf_model = None\n",
    "        self.lgbm_model = None\n",
    "    \n",
    "    def train_model(self, X_train, y_train, epochs=200):\n",
    "        X_feat = get_features_simple(X_train, self.data_type)\n",
    "        y_np = y_train.values.astype(np.float32)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # Train MLP ensemble (3 models)\n",
    "        self.mlp_models = []\n",
    "        for _ in range(3):\n",
    "            model = EnhancedMLP(X_scaled.shape[1], hidden_dims=[128, 64, 32], dropout=0.1).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            y_tensor = torch.tensor(y_np).to(device)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                for X_batch, y_batch in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(X_batch)\n",
    "                    loss = nn.MSELoss()(pred, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            model.eval()\n",
    "            self.mlp_models.append(model)\n",
    "        \n",
    "        # Train XGBoost\n",
    "        self.xgb_model = MultiOutputRegressor(\n",
    "            xgb.XGBRegressor(\n",
    "                n_estimators=300, learning_rate=0.05, max_depth=6,\n",
    "                subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0\n",
    "            )\n",
    "        )\n",
    "        self.xgb_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # Train RandomForest\n",
    "        self.rf_model = MultiOutputRegressor(\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=300, max_depth=15, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        )\n",
    "        self.rf_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgbm_model = MultiOutputRegressor(\n",
    "            lgb.LGBMRegressor(\n",
    "                n_estimators=300, learning_rate=0.05, num_leaves=31,\n",
    "                random_state=42, verbose=-1\n",
    "            )\n",
    "        )\n",
    "        self.lgbm_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = get_features_simple(X_test, self.data_type)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # MLP predictions (average of ensemble)\n",
    "        mlp_preds = []\n",
    "        for model in self.mlp_models:\n",
    "            X_tensor = torch.tensor(X_scaled).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(X_tensor).cpu().numpy()\n",
    "            mlp_preds.append(pred)\n",
    "        mlp_preds = np.mean(mlp_preds, axis=0)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        xgb_preds = self.xgb_model.predict(X_scaled)\n",
    "        \n",
    "        # RandomForest predictions\n",
    "        rf_preds = self.rf_model.predict(X_scaled)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        lgbm_preds = self.lgbm_model.predict(X_scaled)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_preds = (self.weights[0] * mlp_preds + \n",
    "                          self.weights[1] * xgb_preds + \n",
    "                          self.weights[2] * rf_preds + \n",
    "                          self.weights[3] * lgbm_preds)\n",
    "        \n",
    "        ensemble_preds = np.clip(ensemble_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(ensemble_preds, dtype=torch.float32)\n",
    "\n",
    "print('DiverseEnsembleModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f042680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:18:15.326014Z",
     "iopub.status.busy": "2026-01-16T00:18:15.325770Z",
     "iopub.status.idle": "2026-01-16T00:28:31.617517Z",
     "shell.execute_reply": "2026-01-16T00:28:31.616738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing diverse ensemble on single solvent data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,1,3,3,3-Hexafluoropropan-2-ol: MSE = 0.040208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,2,2-Trifluoroethanol: MSE = 0.017506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Methyltetrahydrofuran [2-MeTHF]: MSE = 0.002296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetonitrile: MSE = 0.016375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acetonitrile.Acetic Acid: MSE = 0.025274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Butanone [MEK]: MSE = 0.003126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclohexane: MSE = 0.014711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMA [N,N-Dimethylacetamide]: MSE = 0.006389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decanol: MSE = 0.008167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diethyl Ether [Ether]: MSE = 0.012435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dihydrolevoglucosenone (Cyrene): MSE = 0.007206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimethyl Carbonate: MSE = 0.008959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethanol: MSE = 0.002158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethyl Acetate: MSE = 0.005683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethyl Lactate: MSE = 0.003473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethylene Glycol [1,2-Ethanediol]: MSE = 0.016504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPA [Propan-2-ol]: MSE = 0.008462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTBE [tert-Butylmethylether]: MSE = 0.004697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methanol: MSE = 0.003262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methyl Propionate: MSE = 0.000900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THF [Tetrahydrofuran]: MSE = 0.000591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water.2,2,2-Trifluoroethanol: MSE = 0.004290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water.Acetonitrile: MSE = 0.009525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tert-Butanol [2-Methylpropan-2-ol]: MSE = 0.003240\n",
      "\n",
      "=== Diverse Ensemble CV Results ===\n",
      "Mean MSE: 0.009393 +/- 0.008862\n",
      "Baseline (exp_030): CV = 0.008298\n"
     ]
    }
   ],
   "source": [
    "# Test diverse ensemble on single solvent data\n",
    "print(\"Testing diverse ensemble on single solvent data...\")\n",
    "print()\n",
    "\n",
    "all_solvents = sorted(X_single[\"SOLVENT NAME\"].unique())\n",
    "fold_mses = []\n",
    "\n",
    "for test_solvent in all_solvents:\n",
    "    mask = X_single[\"SOLVENT NAME\"] != test_solvent\n",
    "    \n",
    "    model = DiverseEnsembleModel(data='single', weights=[0.4, 0.2, 0.2, 0.2])\n",
    "    model.train_model(X_single[mask], Y_single[mask], epochs=150)\n",
    "    preds = model.predict(X_single[~mask])\n",
    "    \n",
    "    actuals = Y_single[~mask].values\n",
    "    mse = np.mean((actuals - preds.numpy())**2)\n",
    "    fold_mses.append(mse)\n",
    "    print(f\"{test_solvent}: MSE = {mse:.6f}\")\n",
    "\n",
    "mean_mse = np.mean(fold_mses)\n",
    "std_mse = np.std(fold_mses)\n",
    "print(f\"\\n=== Diverse Ensemble CV Results ===\")\n",
    "print(f\"Mean MSE: {mean_mse:.6f} +/- {std_mse:.6f}\")\n",
    "print(f\"Baseline (exp_030): CV = 0.008298\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c54b79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T00:29:10.677387Z",
     "iopub.status.busy": "2026-01-16T00:29:10.676492Z",
     "iopub.status.idle": "2026-01-16T00:29:10.683221Z",
     "shell.execute_reply": "2026-01-16T00:29:10.682520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of Diverse Ensemble Experiment ===\n",
      "\n",
      "Diverse Ensemble CV MSE: 0.009393 +/- 0.008862\n",
      "Baseline (exp_030): CV = 0.008298\n",
      "Improvement: -13.2%\n",
      "\n",
      "Per-solvent comparison (top 5 hardest):\n",
      "  HFIP: MSE = 0.040208 (was 0.096369 in baseline)\n",
      "  Cyclohexane: MSE = 0.014711 (was 0.198108 in baseline)\n",
      "  TFE: MSE = 0.017506 (was 0.041910 in baseline)\n",
      "  Acetonitrile.Acetic Acid: MSE = 0.025274\n",
      "  Ethylene Glycol: MSE = 0.016504\n",
      "\n",
      "Key Insight:\n",
      "The diverse ensemble with simpler features (18 vs 2066) does NOT improve CV.\n",
      "However, it significantly reduces error on Cyclohexane (0.014 vs 0.198).\n",
      "This suggests the DRFP features may be causing overfitting for some solvents.\n",
      "\n",
      "CONCLUSION: Diverse ensemble does NOT help overall CV.\n",
      "The baseline (exp_030) remains the best model.\n"
     ]
    }
   ],
   "source": [
    "# Summary of diverse ensemble experiment\n",
    "print(\"=== Summary of Diverse Ensemble Experiment ===\")\n",
    "print()\n",
    "print(f\"Diverse Ensemble CV MSE: {mean_mse:.6f} +/- {std_mse:.6f}\")\n",
    "print(f\"Baseline (exp_030): CV = 0.008298\")\n",
    "print(f\"Improvement: {(0.008298 - mean_mse) / 0.008298 * 100:.1f}%\")\n",
    "print()\n",
    "print(\"Per-solvent comparison (top 5 hardest):\")\n",
    "print(f\"  HFIP: MSE = 0.040208 (was 0.096369 in baseline)\")\n",
    "print(f\"  Cyclohexane: MSE = 0.014711 (was 0.198108 in baseline)\")\n",
    "print(f\"  TFE: MSE = 0.017506 (was 0.041910 in baseline)\")\n",
    "print(f\"  Acetonitrile.Acetic Acid: MSE = 0.025274\")\n",
    "print(f\"  Ethylene Glycol: MSE = 0.016504\")\n",
    "print()\n",
    "print(\"Key Insight:\")\n",
    "print(\"The diverse ensemble with simpler features (18 vs 2066) does NOT improve CV.\")\n",
    "print(\"However, it significantly reduces error on Cyclohexane (0.014 vs 0.198).\")\n",
    "print(\"This suggests the DRFP features may be causing overfitting for some solvents.\")\n",
    "print()\n",
    "print(\"CONCLUSION: Diverse ensemble does NOT help overall CV.\")\n",
    "print(\"The baseline (exp_030) remains the best model.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
