## Current Status
- Best CV score: 0.008601 from exp_022 (ACS PCA features) - NEW BEST!
- Best LB score: 0.0913 from exp_012 (MLP[32,16] + LightGBM 0.6/0.4)
- Target: 0.0333
- Gap to target: 2.74x (LB 0.0913 vs target 0.0333)
- CV-LB relationship: LB = 4.05*CV + 0.0551 (R²=0.948)
- Predicted LB for exp_022: 0.0900 (1.4% better than 0.0913)
- Submissions remaining: 4

## Response to Evaluator

**Technical verdict**: TRUSTWORTHY. The ACS PCA experiment executed correctly with verified CV score 0.008601.

**Evaluator's top priority**: Create submission-compliant notebook with ACS PCA features and submit. I FULLY AGREE.

**Key concerns raised**:
1. exp_022 notebook is NOT submission-compliant - MUST CREATE COMPLIANT VERSION
2. Per-target models still unexplored - WILL TRY AFTER SUBMISSION
3. Stacking meta-learner unexplored - LOWER PRIORITY

**My synthesis**: The evaluator is correct. We achieved the best CV score (0.008601) with ACS PCA features, but we cannot submit it because the notebook doesn't follow the competition template. The IMMEDIATE priority is to create a submission-compliant notebook and submit to verify LB improvement.

## Data Understanding

Reference notebooks:
- `experiments/019_acs_pca/acs_pca_ensemble.ipynb`: ACS PCA experiment (CV 0.008601)
- `experiments/013_compliant_ensemble/compliant_ensemble.ipynb`: Template for compliant notebook
- `exploration/evolver_loop23_analysis.ipynb`: Strategic analysis

Key findings:
1. **ACS PCA features (5 features)**: Improved CV by 4.47% (0.009004 → 0.008601)
2. **Feature composition**: Spange (13) + DRFP (122) + Arrhenius (5) + ACS PCA (5) = 145 features
3. **Single solvent improved more**: 21.2% (0.010429 → 0.008221) vs mixtures 2.2%
4. **CV-LB gap**: Linear fit predicts LB 0.0900 for CV 0.008601

## Recommended Approaches

**PRIORITY 1: CREATE SUBMISSION-COMPLIANT NOTEBOOK WITH ACS PCA (IMMEDIATE)**

The exp_022 notebook achieved best CV but is NOT submission-compliant. Must create compliant version:

1. Copy structure from `experiments/013_compliant_ensemble/compliant_ensemble.ipynb`
2. Add ACS_PCA_DF loading:
```python
ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)
```
3. Update featurizer to include ACS PCA features:
```python
class ACSPCAFeaturizer:
    def __init__(self, mixed=False):
        self.mixed = mixed
        self.spange_df = SPANGE_DF
        self.drfp_df = DRFP_FILTERED
        self.acs_pca_df = ACS_PCA_DF  # ADD THIS
        
    def featurize(self, X, flip=False):
        # ... existing code ...
        X_acs = self.acs_pca_df.loc[X["SOLVENT NAME"]].values  # ADD THIS
        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])  # ADD X_acs
```
4. Update model class name to `ACSPCAEnsemble`
5. Ensure last 3 cells match template EXACTLY (only change model definition line)
6. SUBMIT to verify LB improvement

**PRIORITY 2: Per-Target Models (After submission)**

Competition rules explicitly allow "different hyper-parameters for different objectives (e.g., for SM vs Product 1)".

Implementation:
- Train separate MLP[32,16] + LightGBM ensembles for each target
- SM has different distribution (mean 0.52, std 0.36) vs products (mean ~0.13, std ~0.14)
- Product 2 and Product 3 are highly correlated (0.923) - may benefit from shared model

**PRIORITY 3: Stacking Meta-Learner**

Instead of fixed weights (0.6 MLP, 0.4 LGBM), train Ridge regression on out-of-fold predictions:
```python
# Get OOF predictions from MLP and LGBM
oof_mlp = np.zeros((n_samples, 3))
oof_lgbm = np.zeros((n_samples, 3))

# Train meta-learner
from sklearn.linear_model import Ridge
meta = Ridge(alpha=1.0)
meta.fit(np.hstack([oof_mlp, oof_lgbm]), y_train)
```

**PRIORITY 4: Non-linear Mixture Encoding**

Try non-linear mixing for mixtures:
```python
# Current: linear interpolation
X_feat = A * (1 - pct) + B * pct

# Try: non-linear mixing with interaction term
X_feat = A * (1 - pct) + B * pct + 0.05 * A * B * pct * (1 - pct)
```

## What NOT to Try

1. **Attention mechanisms on tabular features** - EXHAUSTED. exp_021 showed 159% worse.
2. **Fragprints instead of DRFP** - EXHAUSTED. exp_020 showed 8.28% worse.
3. **Deep residual networks** - EXHAUSTED. exp_004 showed 5x worse.
4. **Very large ensembles (15+ models)** - EXHAUSTED. Only 0.7% improvement.
5. **Single-layer networks** - EXHAUSTED. [16] is too simple.

## Validation Notes

- Use leave-one-solvent-out CV for single solvents (24 folds)
- Use leave-one-ramp-out CV for mixtures (13 folds)
- Weighted average of single and full data MSE
- TTA for mixtures (average both orderings)
- CV-LB gap is ~10x - don't expect LB to match CV

## Template Compliance (CRITICAL)

The competition requires EXACT template structure:
- Last 3 cells must match template exactly
- Only allowed change: `model = MLPModel()` line can be replaced with new model definition
- Same hyperparameters across all folds (unless explainable rationale)

## Decision Threshold

- SUBMIT exp_022 (ACS PCA) immediately after creating compliant notebook
- If LB improves: Continue with per-target models
- If LB doesn't improve: The CV-LB gap has widened, focus on different approaches
- We have 4 submissions remaining - use them wisely

## Key Insight

The target (0.0333) requires a fundamentally different approach than what we've tried. The linear CV-LB relationship (LB = 4.05*CV + 0.0551) suggests that even CV=0 would give LB=0.0551 > target. However:

1. The linear fit is based on only 8 data points
2. The relationship may be non-linear at lower CV values
3. We haven't exhausted all tabular approaches yet

**DO NOT GIVE UP. The target IS reachable. Submit ACS PCA and continue exploring.**