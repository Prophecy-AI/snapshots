{
  "query": "How can tabular neural networks approximate graph neural network performance for molecular property prediction without using molecular graphs?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Tabular neural networks can come close to the accuracy of graph neural networks (GNNs) for molecular property prediction by first **flattening the molecular graph into a rich table of descriptors** and then applying a foundation\u2011model\u2011scale tabular learner.  In the \u201cBringing Graphs to the Table\u201d framework each molecule (or graph node) is represented as a single row whose columns contain both traditional feature vectors (e.g., atom\u2011type counts, physicochemical descriptors) and explicit structural statistics derived from the original graph (e.g., degree distributions, sub\u2011graph frequencies, shortest\u2011path histograms)\u202f([arXiv\u202f2509.07143](https://arxiv.org/pdf/2509.07143)).  This tabular view lets a large\u2011scale tabular foundation model (TFM) treat the problem as a standard supervised regression or classification task, using in\u2011context learning or fine\u2011tuning to capture the same relational patterns that a GNN would learn from edge information.  A complementary study, \u201cTurning Tabular Foundation Models into Graph Foundation Models,\u201d shows that converting a graph into a table of feature\u2011 and structure\u2011derived columns enables TFMs to act as **graph foundation models** without ever processing an explicit adjacency matrix\u202f([arXiv\u202f2508.20906](https://arxiv.org/abs/2508.20906)).\n\nBecause the tabular representation can embed **high\u2011level graph topology** (e.g., fragment connectivity, ring counts, bond\u2011type distributions) alongside **chemical fingerprints**, a deep MLP, transformer, or other tabular architecture can learn mappings from these engineered descriptors to target properties that rival GNNs trained on raw molecular graphs.  Empirically, zero\u2011shot node\u2011classification experiments using TFMs have achieved performance comparable to dedicated GNNs, suggesting that with sufficiently expressive tabular features and large\u2011scale pretraining, tabular models can **approximate GNN performance while avoiding the computational overhead of graph construction and message\u2011passing**\u202f([arXiv\u202f2509.07143](https://arxiv.org/pdf/2509.07143)).  Thus, the key steps are: (1) extract a comprehensive set of molecular descriptors that capture both local atom/bond information and global graph structure; (2) organize them into a flat table; and (3) train or prompt a powerful tabular neural network, which can then predict molecular properties with accuracy close to that of state\u2011of\u2011the\u2011art GNNs.",
      "url": ""
    },
    {
      "title": "Bringing Graphs to the Table: Zero-shot Node Classification via Tabular Foundation Models",
      "text": "<div>\n<div>\n<p><a href=\"https://www.cornell.edu/\"></a>\n</p>\n<div>\n<p><a href=\"https://confluence.cornell.edu/x/ALlRF\">We gratefully acknowledge support from<br/>\nthe Simons Foundation and member institutions.</a>\n</p></div>\n</div>\n<div>\n<h2><a href=\"https://arxiv.org/\">\n</a></h2>\n</div>\n</div>",
      "url": "https://arxiv.org/pdf/2509.07143"
    },
    {
      "title": "Turning Tabular Foundation Models into Graph Foundation Models",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2508.20906"
    },
    {
      "title": "Kolmogorov\u2013Arnold graph neural networks for molecular property ...",
      "text": "Kolmogorov\u2013Arnold graph neural networks for molecular property prediction | Nature Machine Intelligence\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Machine Intelligence](https://media.springernature.com/full/nature-cms/uploads/product/natmachintell/header-3a46d3127519f23b44cac085d4e82c58.svg)](https://www.nature.com/natmachintell)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42256-025-01087-7?error=cookies_not_supported&code=1ed9a7e8-d1e2-4109-be3d-534e404f54ff)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42256)\n* [RSS feed](https://www.nature.com/natmachintell.rss)\nKolmogorov\u2013Arnold graph neural networks for molecular property prediction\n[Download PDF](https://www.nature.com/articles/s42256-025-01087-7.pdf)\n[Download PDF](https://www.nature.com/articles/s42256-025-01087-7.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:11 August 2025# Kolmogorov\u2013Arnold graph neural networks for molecular property prediction\n* [Longlong Li](#auth-Longlong-Li-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3)[na1](#na1),\n* [Yipeng Zhang](#auth-Yipeng-Zhang-Aff3)[ORCID:orcid.org/0009-0009-2895-0220](https://orcid.org/0009-0009-2895-0220)[3](#Aff3)[na1](#na1),\n* [Guanghui Wang](#auth-Guanghui-Wang-Aff1)[1](#Aff1)&amp;\n* \u2026* [Kelin Xia](#auth-Kelin-Xia-Aff3)[ORCID:orcid.org/0000-0003-4183-0943](https://orcid.org/0000-0003-4183-0943)[3](#Aff3)Show authors\n[*Nature Machine Intelligence*](https://www.nature.com/natmachintell)**volume7**,pages1346\u20131354 (2025)[Cite this article](#citeas)\n* 40kAccesses\n* 21Citations\n* 35Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42256-025-01087-7/metrics)\n### Subjects\n* [Applied mathematics](https://www.nature.com/subjects/applied-mathematics)\n* [Computational models](https://www.nature.com/subjects/computational-models)\n## Abstract\nGraph neural networks (GNNs) have shown remarkable success in molecular property prediction as key models in geometric deep learning. Meanwhile, Kolmogorov\u2013Arnold networks (KANs) have emerged as powerful alternatives to multi-layer perceptrons, offering improved expressivity, parameter efficiency and interpretability. To combine the strengths of both frameworks, we propose Kolmogorov\u2013Arnold GNNs (KA-GNNs), which integrate KAN modules into the three fundamental components of GNNs: node embedding, message passing and readout. We further introduce Fourier-series-based univariate functions within KAN to enhance function approximation and provide theoretical analysis to support their expressiveness. Two architectural variants, KA-graph convolutional networks and KA-augmented graph attention networks, are developed and evaluated across seven molecular benchmarks. Experimental results show that KA-GNNs consistently outperform conventional GNNs in terms of both prediction accuracy and computational efficiency. Moreover, our models exhibit improved interpretability by highlighting chemically meaningful substructures. These findings demonstrate that KA-GNNs offer a powerful and generalizable framework for molecular data modelling, drug discovery and beyond.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-59439-1/MediaObjects/41467_2025_59439_Fig1_HTML.png)\n### [Using GNN property predictors as molecule generators](https://www.nature.com/articles/s41467-025-59439-1?fromPaywallRec=false)\nArticleOpen access08 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-023-00751-0/MediaObjects/42256_2023_751_Fig1_HTML.png)\n### [Calibrated geometric deep learning improves kinase\u2013drug binding predictions](https://www.nature.com/articles/s42256-023-00751-0?fromPaywallRec=false)\nArticle06 November 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-023-00654-0/MediaObjects/42256_2023_654_Fig1_HTML.png)\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\nArticleOpen access04 May 2023\n## Main\nEven with the huge successes in efficient experimental tools and state-of-the-art computational models, drug design and discovery is still a time-consuming and extremely costly process[1](https://www.nature.com/articles/s42256-025-01087-7#ref-CR1). Recently, artificial intelligence, in particular artificial intelligence for sciences, has demonstrated its enormous potential and great power in scientific data analysis. The success of AlphFold models in protein structure prediction has fundamentally changed molecular structural and property analysis, and ushered in a new era of drug design and discovery[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s42256-025-01087-7#ref-CR6). In general, all molecule-based artificial intelligence models fall into two categories, that is, molecular feature-based machine learning and end-to-end deep learning[7](https://www.nature.com/articles/s42256-025-01087-7#ref-CR7). The first category relies on molecular descriptors or fingerprints as input features for machine learning models. The key process is molecular featurization (or feature engineering), which extracts or generates molecular features from structural, physical, chemical or biological properties. Among them, structure-based descriptors or fingerprints, especially those derived from topological methods, have proved highly effective[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42256-025-01087-7#ref-CR11). Integrating these topology-based molecular features with machine learning models has led to notable success in various drug design stages, such as protein\u2013ligand binding affinity prediction[12](https://www.nature.com/articles/s42256-025-01087-7#ref-CR12),[13](https://www.nature.com/articles/s42256-025-01087-7#ref-CR13), protein mutation analysis[14](https://www.nature.com/articles/s42256-025-01087-7#ref-CR14),[15](https://www.nature.com/articles/s42256-025-01087-7#ref-CR15)and others[16](https://www.nature.com/articles/s42256-025-01087-7#ref-CR16),[17](https://www.nature.com/articles/s42256-025-01087-7#ref-CR17).\nThe second category includes end-to-end deep learning models that use various molecular representations, such as simplified molecular input line entry system strings, molecule-based images and volumetric data, or molecular graphs, and adopt architectures such as Transformers, two-dimensional (2D) or three-dimensional (3D) convolutional neural networks (CNNs) and geometric deep learning (GDL)[18](#ref-CR18),[19](#ref-CR19),[20](#ref-CR20),[21](#ref-CR21),[22](#ref-CR22),[23](https://www.nature.com/articles/s42256-025-01087-7#ref-CR23). In particular, GDL models, such as graph convolutional networks (GCNs)[24](https://www.nature.com/articles/s42256-025-01087-7#ref-CR24), graph autoencoders[25](https://www.nature.com/articles/s42256-025-01087-7#ref-CR25), graph transformers[26](https://www.nature.com/articles/s42256-025-01087-7#ref-CR26)and so on, have been widely used in molecular data analysis and drug design. However, traditional covalent-bond-based molecular graph representations have various limitations, while incorporating non-covalent ...",
      "url": "https://www.nature.com/articles/s42256-025-01087-7"
    },
    {
      "title": "A compact review of molecular property prediction with graph neural ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/drug-discovery-today-technologies\"><span><span></span></span></a></p><p><a href=\"https://www.sciencedirect.com/journal/drug-discovery-today-technologies/vol/37/suppl/C\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><h2>Abstract</h2><div><p>As graph neural networks are becoming more and more powerful and useful in the field of drug discovery, many pharmaceutical companies are getting interested in utilizing these methods for their own in-house frameworks. This is especially compelling for tasks such as the prediction of molecular properties which is often one of the most crucial tasks in computer-aided drug discovery workflows. The immense hype surrounding these kinds of algorithms has led to the development of many different types of promising architectures and in this review we try to structure this highly dynamic field of AI-research by collecting and classifying 80 GNNs that have been used to predict more than 20 molecular properties using 48 different datasets.</p></div></div><ul><li></li><li></li></ul><div><h2>Keywords</h2><p><span>AI</span></p><p><span>Deep-learning</span></p><p><span>Neural-networks</span></p><p><span>Graph neural-networks</span></p><p><span>Molecular representation</span></p><p><span>Molecular property</span></p><p><span>Drug discovery</span></p><p><span>Computational chemistry</span></p></div><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2020 The Authors. Published by Elsevier Ltd.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S1740674920300305"
    },
    {
      "title": "Molecular property prediction based on graph structure learning - PMC",
      "text": "[Skip to main content](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#main-content)\n\n**Official websites use .gov**\nA\n**.gov** website belongs to an official\ngovernment organization in the United States.\n\n**Secure .gov websites use HTTPS**\nA **lock** (\n\nLocked padlock icon\n) or **https://** means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n\nSearch PMC Full-Text ArchiveSearch in PMC\n\n- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n\n- ## PERMALINK\n\n\n\nCopy\n\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:\n[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nBioinformatics\n\n. 2024 May 6;40(5):btae304. doi: [10.1093/bioinformatics/btae304](https://doi.org/10.1093/bioinformatics/btae304)\n\n# Molecular property prediction based on graph structure learning\n\n[Bangyi Zhao](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhao%20B%22%5BAuthor%5D)\n\n### Bangyi Zhao\n\n1\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\nFind articles by [Bangyi Zhao](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhao%20B%22%5BAuthor%5D)\n\n1,2, [Weixia Xu](https://pubmed.ncbi.nlm.nih.gov/?term=%22Xu%20W%22%5BAuthor%5D)\n\n### Weixia Xu\n\n2\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\nFind articles by [Weixia Xu](https://pubmed.ncbi.nlm.nih.gov/?term=%22Xu%20W%22%5BAuthor%5D)\n\n2,2, [Jihong Guan](https://pubmed.ncbi.nlm.nih.gov/?term=%22Guan%20J%22%5BAuthor%5D)\n\n### Jihong Guan\n\n3\nDepartment of Computer Science and Technology, Tongji University, Shanghai 201804, China\n\nFind articles by [Jihong Guan](https://pubmed.ncbi.nlm.nih.gov/?term=%22Guan%20J%22%5BAuthor%5D)\n\n3,\u2709, [Shuigeng Zhou](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20S%22%5BAuthor%5D)\n\n### Shuigeng Zhou\n\n4\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\nFind articles by [Shuigeng Zhou](https://pubmed.ncbi.nlm.nih.gov/?term=%22Zhou%20S%22%5BAuthor%5D)\n\n4,\u2709\n\nEditor: Jonathan Wren\n\n- Author information\n- Article notes\n- Copyright and License information\n\n1\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\n2\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\n3\nDepartment of Computer Science and Technology, Tongji University, Shanghai 201804, China\n\n4\nShanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China\n\n\u2709\n\nCorresponding authors. Department of Computer Science and Technology, Tongji University, Shanghai 201804, China. E-mail:\u00a0jhguan@tongji.edu.cn\u00a0(J.G.); Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, Shanghai 200438, China. E-mail:\u00a0sgzhou@fudan.edu.cn\u00a0(S.Z.)\n\n2\n\nBangyi Zhao and Weixia Xu Equal contribution.\n\n#### Roles\n\n**Jonathan Wren**: Associate Editor\n\nReceived 2023 Dec 27; Revised 2024 Apr 6; Accepted 2024 May 3; Collection date 2024 May.\n\n\u00a9 The Author(s) 2024. Published by Oxford University Press.\n\nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License ( [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\n\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nPMCID: PMC11112045\u00a0\u00a0PMID: [38710497](https://pubmed.ncbi.nlm.nih.gov/38710497/)\n\n## Abstract\n\n### Motivation\n\nMolecular property prediction (MPP) is a fundamental but challenging task in the computer-aided drug discovery process. More and more recent works employ different graph-based models for MPP, which have achieved considerable progress in improving prediction performance. However, current models often ignore relationships between molecules, which could be also helpful for MPP.\n\n### Results\n\nFor this sake, in this article we propose a graph structure learning (GSL) based MPP approach, called GSL-MPP. Specifically, we first apply graph neural network (GNN) over molecular graphs to extract molecular representations. Then, with molecular fingerprints, we construct a molecule similarity graph (MSG). Following that, we conduct GSL on the MSG, i.e. molecule-level GSL, to get the final molecular embeddings, which are the results of fuzing both GNN encoded molecular representations and the relationships among molecules. That is, combining both intra-molecule and inter-molecule information. Finally, we use these molecular embeddings to perform MPP. Extensive experiments on 10 various benchmark datasets show that our method could achieve state-of-the-art performance in most cases, especially on classification tasks. Further visualization studies also demonstrate the good molecular representations of our method.\n\n### Availability and implementation\n\nSource code is available at [https://github.com/zby961104/GSL-MPP](https://github.com/zby961104/GSL-MPP).\n\n## 1 Introduction\n\nThe accurate prediction of molecular properties is a critical task in the field of drug discovery. By utilizing computational methods, this task can be accomplished with great efficiency, reducing both time and expense associated with identifying drug candidates. This is particularly important considering that the average cost of developing a new drug is currently estimated to be approximately $2.8 billion ( [Fleming 2018](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B4), [Wieder _et al._ 2020](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B26)) and the development period lasts a dozen of years, let alone the high risk of clinical failure ( [Sarkar _et al._ 2023](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B17)). Naturally, a molecule can be abstracted as a topological graph, where atoms are treated as nodes and bonds are viewed as edges. In the past few years, deep graph learning methods, especially various graph neural networks (GNNs) have been applied in this field, offering effective molecular graph representations for accurate molecular property prediction (MPP) ( [Duvenaud _et al._ 2015](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B3), [Song _et al._ 2020](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B19), [Sun _et al._, 2020](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B23)). In GNNs, nodes iteratively update their representations after aggregating information from their neighbors and a final graph-pooling layer will generate a graph representation for the molecule. Up to now, various message passing layers have been proposed and applied, including GAT ( [Veli\u010dkovi\u0107 _et al._ 2017](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B24)), MPNN ( [Gilmer _et al._ 2017](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B5)), and GIN ( [Xu _et al._ 2018](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B29)). And later studies further considered to integrate edge features into the passing messages in order to improve the expressive power of their models, like DMPNN ( [Yang _et al._ 2019](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B30)) and CMPNN ( [Song _et al._ 2020](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#btae304-B19)).\n\nDespite the considerable progress, most of the recent studies focus only on the message passing within individual molecules. The ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11112045"
    },
    {
      "title": "FragNet: A Graph Neural Network for Molecular Property Prediction ...",
      "text": "FragNet: A Graph Neural Network for Molecular Property Prediction with Four Layers of Interpretability\n[1]\\\\fnmGihan\\\\surPanapitiya\n[1]\\\\fnmEmily G\\\\surSaldanha\n[1]\\\\orgdivPacific Northwest National Laboratory,\\\\cityRichland,\\\\stateWA,\\\\postcode99354,\\\\countryUnited States\n# FragNet: A Graph Neural Network for Molecular Property Prediction with Four Layers of Interpretability\n[gihan.panapitiya@pnnl.gov](mailto:gihan.panapitiya@pnnl.gov)\\\\fnmPeiyuan\\\\surGao[peiyuan.gao@pnnl.gov](mailto:peiyuan.gao@pnnl.gov)\\\\fnmC Mark\\\\surMaupin[mark.maupin@pnnl.gov](mailto:mark.maupin@pnnl.gov)[emily.saldanha@pnnl.gov](mailto:emily.saldanha@pnnl.gov)\\*\n###### Abstract\nMolecular property prediction is a crucial step in many modern-day scientific applications including drug discovery and energy storage material design. Despite the availability of numerous machine learning models for this task, we are lacking in models that provide both high accuracies and interpretability of the predictions. We introduce the FragNet architecture, a graph neural network not only capable of achieving prediction accuracies comparable to the current state-of-the-art models, but also able to provide insight on four levels of molecular substructures. This model enables understanding of which atoms, bonds, molecular fragments, and molecular fragment connections are critical in the prediction of a given molecular property. The ability to interpret the importance of connections between fragments is of particular interest for molecules which have substructures that are not connected with regular covalent bonds. The interpretable capabilities of FragNet are key to gaining scientific insights from the model\u2019s learned patterns between molecular structure and molecular properties.\n###### keywords:\nGraph Neural Networks, Deep Learning, Attention, Model Interpretability\n## 1Introduction\nMolecular property prediction is a crucial component of material discovery and design in many modern scientific applications including drug design, energy storage material discovery, catalysis and agrochemicals. Despite the availability of a large number of machine learning models> [\n[> 1\n](https://arxiv.org/html/2410.12156v1#bib.bib1)> ]\nfor molecular property prediction, there is often a trade-off in the ability of models to provide highly accurate predictions versus their ability to provide interpretability of their predictions to enable scientific insights. In this work, we introduce a graph neural network architecture called FragNet which is not only capable of achieving prediction accuracies comparable to or exceeding the accuracies of the current state-of-the-art models but also has the ability to provide insight into four levels of molecular substructures which are the atoms, bonds, fragments and fragment connections. In other words, with this model, one can understand which atoms, bonds, molecular fragments and also which connections between molecular fragments play critical role in predicting a given molecular property. By enhancing the model with the ability to reason about the connections between fragments, we provide improved representations for molecules with substructures that are not connected with regular covalent bonds, such as salts and complexes.\nThere are several prior works which implement deep learning models which leverage attention mechanisms which can provide values for different types of molecular substructures. The AttentiveFP> [\n[> 2\n](https://arxiv.org/html/2410.12156v1#bib.bib2)> ]\nand MoGAT> [\n[> 3\n](https://arxiv.org/html/2410.12156v1#bib.bib3)> ]\nmodels provide atom level importance values. The work by Wu et al.> [\n[> 4\n](https://arxiv.org/html/2410.12156v1#bib.bib4)> ]\nprovides fragment level attention. However, their work is limited in the range of molecular properties on which it evaluates, as it only provides results for the ESOL dataset> [\n[> 5\n](https://arxiv.org/html/2410.12156v1#bib.bib5)> ]\nusing a random train-test split configuration. In contrast, our work provides prediction results for several benchmark datasets in MoleculeNet> [\n[> 6\n](https://arxiv.org/html/2410.12156v1#bib.bib6)> ]\nwhile using more challenging scaffold splitting method> [\n[> 7\n](https://arxiv.org/html/2410.12156v1#bib.bib7)> ]\n. This provides a more comprehensive and robust understanding of how the reasoning of the molecular property prediction models is affected by the target property.\nIn addition to demonstrating the strong property prediction performance of the FragNet model across this challenging set of tasks, we also demonstrate the utility of the multi-layered interpretability mechanisms of the through several case studies on a selection of property prediction tasks. We use model attention weights and contribution values to investigate the reasoning used by the model for both individual molecular predictions as well as aggregated across multiple predictions, to identify the key molecular constituents that drive property variations. To further validate the reasoning extracted from the models, we perform a comparative study of FragNet contribution scores with density functional theory (DFT) computations of electrostatic surface potentials. Finally, we develop and release a interactive browser application to make these types of interpretability studies accessible for other molecular property tasks.\n## 2Results\n### 2.1Model\n![Refer to caption](x1.png)Figure 1:FragNet\u2019s architecture and data representation. (a) Atom and Fragment graphs\u2019 edge features are learned from Bond and Fragment connection graphs respectively. b) Initial fragment features for the fragment graph are the summation of the updated atom features that compose the fragment. (c) Illustration of FragNet\u2019s message passing taking place between two non-covalently bonded substructures. Fragment-Fragment connections are also present between adjacent fragments in each non-covalently bonded structure of the compound.Table 1:FragNet performance measured with RMSE on regression tasks in the MoleculeNet benchmark compared with four state-of-the-art baselines. Best results in bold. Lower is better. Reported here aremean\u00b1plus-or-minus\\\\pm\u00b1standard deviationcorresponding to three random seeds. Except for SimSGT, results for other models were obtained from the work of reference> [\n[> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n.|Dataset|ESOL|LIPO|CEP|\nContextPred> [\n[> 9\n](https://arxiv.org/html/2410.12156v1#bib.bib9)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.196\u00b1plus-or-minus\\\\pm\u00b10.037|0.702\u00b1plus-or-minus\\\\pm\u00b10.020|1.243\u00b1plus-or-minus\\\\pm\u00b10.025|\nAttrMask> [\n[> 9\n](https://arxiv.org/html/2410.12156v1#bib.bib9)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.112\u00b1plus-or-minus\\\\pm\u00b10.048|0.730\u00b1plus-or-minus\\\\pm\u00b10.004|1.256\u00b1plus-or-minus\\\\pm\u00b10.000|\nJOAO> [\n[> 10\n](https://arxiv.org/html/2410.12156v1#bib.bib10)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.120\u00b1plus-or-minus\\\\pm\u00b10.019|0.708\u00b1plus-or-minus\\\\pm\u00b10.007|1.293\u00b1plus-or-minus\\\\pm\u00b10.003|\nGraphMVP> [\n[> 11\n](https://arxiv.org/html/2410.12156v1#bib.bib11)> , [> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.064\u00b1plus-or-minus\\\\pm\u00b10.045|0.691\u00b1plus-or-minus\\\\pm\u00b10.013|1.2228\u00b1plus-or-minus\\\\pm\u00b10.001|\nMole-BERT> [\n[> 8\n](https://arxiv.org/html/2410.12156v1#bib.bib8)> ]\n|1.015\u00b1plus-or-minus\\\\pm\u00b10.030|0.676\u00b1plus-or-minus\\\\pm\u00b10.017|1.232\u00b1plus-or-minus\\\\pm\u00b10.009|\nSimSGT> [\n[> 12\n](https://arxiv.org/html/2410.12156v1#bib.bib12)> ]\n|0.917\u00b1plus-or-minus\\\\pm\u00b10.028|0.670\u00b1plus-or-minus\\\\pm\u00b10.015|1.036\u00b1plus-or-minus\\\\pm\u00b10.022|\nFragNet|0.881\u00b1plus-or-minus\\\\pm\u00b10.011|0.682\u00b1plus-or-minus\\\\pm\u00b10.031|1.092\u00b1plus-or-minus\\\\pm\u00b10.031|\nTable 2:FragNet performance measured withmean\u00b1plus-or-minus\\\\pm\u00b1standard deviationAUC-ROC corresponding to three random seeds on classification tasks in the MoleculeNet benchmark compared with four state-of-the-art baselines. Best result in bold. Higher is better.|Dataset|Clintox|Sider|Tox21|\nContextPred> [\n[> 8\n](https://arxiv.or...",
      "url": "https://arxiv.org/html/2410.12156v1"
    },
    {
      "title": "Composite Graph Neural Networks for Molecular Property Prediction",
      "text": "Composite Graph Neural Networks for Molecular Property Prediction\n**\nNext Article in Journal\n[The Identification and Function of Linc01615 on Influenza Virus Infection and Antiviral Response](https://www.mdpi.com/1422-0067/25/12/6584)\nNext Article in Special Issue\n[Protein Language Models and Machine Learning Facilitate the Identification of Antimicrobial Peptides](https://www.mdpi.com/1422-0067/25/16/8851)\n**\nPrevious Article in Journal\n[Cadmium Alters the Metabolism and Perception of Abscisic Acid in*Pisum sativum*Leaves in a Developmentally Specific Manner](https://www.mdpi.com/1422-0067/25/12/6582)\nPrevious Article in Special Issue\n[Protein&ndash;Protein Interfaces: A Graph Neural Network Approach](https://www.mdpi.com/1422-0067/25/11/5870)\n## Journals\n[Active Journals](https://www.mdpi.com/about/journals)[Find a Journal](https://www.mdpi.com/about/journalfinder)[Journal Proposal](https://www.mdpi.com/about/journals/proposal)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n[## Topics\n](https://www.mdpi.com/topics)\n## Information\n[For Authors](https://www.mdpi.com/authors)[For Reviewers](https://www.mdpi.com/reviewers)[For Editors](https://www.mdpi.com/editors)[For Librarians](https://www.mdpi.com/librarians)[For Publishers](https://www.mdpi.com/publishing_services)[For Societies](https://www.mdpi.com/societies)[For Conference Organizers](https://www.mdpi.com/conference_organizers)\n[Open Access Policy](https://www.mdpi.com/openaccess)[Institutional Open Access Program](https://www.mdpi.com/ioap)[Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)[Editorial Process](https://www.mdpi.com/editorial_process)[Research and Publication Ethics](https://www.mdpi.com/ethics)[Article Processing Charges](https://www.mdpi.com/apc)[Awards](https://www.mdpi.com/awards)[Testimonials](https://www.mdpi.com/testimonials)\n[## Author Services\n](https://www.mdpi.com/authors/english)\n## Initiatives\n[Sciforum](https://sciforum.net)[MDPI Books](https://www.mdpi.com/books)[Preprints.org](https://www.preprints.org)[Scilit](https://www.scilit.com)[SciProfiles](https://sciprofiles.com)[Encyclopedia](https://encyclopedia.pub)[JAMS](https://jams.pub)[Proceedings Series](https://www.mdpi.com/about/proceedings)\n## About\n[Overview](https://www.mdpi.com/about)[Contact](https://www.mdpi.com/about/contact)[Careers](https://careers.mdpi.com)[News](https://www.mdpi.com/about/announcements)[Press](https://www.mdpi.com/about/press)[Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)\n## Notice\nYou can make submissions to other journals[here](https://susy.mdpi.com/user/manuscripts/upload).\n*clear*\n## Notice\nYou are accessing a machine-readable page. In order to be human-readable, please install an RSS reader.\nContinueCancel\n*clear*\nAll articles published by MDPI are made immediately available worldwide under an open access license. No special permission is required to reuse all or part of the article published by MDPI, including figures and tables. For articles published under an open access Creative Common CC BY license, any part of the article may be reused without permission provided that the original article is clearly cited. For more information, please refer to[https://www.mdpi.com/openaccess](https://www.mdpi.com/openaccess).\nFeature papers represent the most advanced research with significant potential for high impact in the field. A Feature Paper should be a substantial original Article that involves several techniques or approaches, provides an outlook for future research directions and describes possible research applications.\nFeature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive positive feedback from the reviewers.\nEditor\u2019s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world. Editors select a small number of articles recently published in the journal that they believe will be particularly interesting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the most exciting work published in the various research areas of the journal.\nOriginal Submission Date Received:.\n[![](https://pub.mdpi-res.com/img/design/mdpi-pub-logo-black-small1.svg?da3a8dcae975a41c?1767773881 \"MDPI Open Access Journals\")](https://www.mdpi.com/)\n* [Journals](https://www.mdpi.com/about/journals)\n* * [Active Journals](https://www.mdpi.com/about/journals)\n* [Find a Journal](https://www.mdpi.com/about/journalfinder)\n* [Journal Proposal](https://www.mdpi.com/about/journals/proposal)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [Topics](https://www.mdpi.com/topics)\n* [Information](https://www.mdpi.com/authors)\n* * [For Authors](https://www.mdpi.com/authors)\n* [For Reviewers](https://www.mdpi.com/reviewers)\n* [For Editors](https://www.mdpi.com/editors)\n* [For Librarians](https://www.mdpi.com/librarians)\n* [For Publishers](https://www.mdpi.com/publishing_services)\n* [For Societies](https://www.mdpi.com/societies)\n* [For Conference Organizers](https://www.mdpi.com/conference_organizers)\n* [Open Access Policy](https://www.mdpi.com/openaccess)\n* [Institutional Open Access Program](https://www.mdpi.com/ioap)\n* [Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines)\n* [Editorial Process](https://www.mdpi.com/editorial_process)\n* [Research and Publication Ethics](https://www.mdpi.com/ethics)\n* [Article Processing Charges](https://www.mdpi.com/apc)\n* [Awards](https://www.mdpi.com/awards)\n* [Testimonials](https://www.mdpi.com/testimonials)\n* [Author Services](https://www.mdpi.com/authors/english)\n* [Initiatives](https://www.mdpi.com/about/initiatives)\n* * [Sciforum](https://sciforum.net)\n* [MDPI Books](https://www.mdpi.com/books)\n* [Preprints.org](https://www.preprints.org)\n* [Scilit](https://www.scilit.com)\n* [SciProfiles](https://sciprofiles.com)\n* [Encyclopedia](https://encyclopedia.pub)\n* [JAMS](https://jams.pub)\n* [Proceedings Series](https://www.mdpi.com/about/proceedings)\n* [About](https://www.mdpi.com/about)\n* * [Overview](https://www.mdpi.com/about)\n* [Contact](https://www.mdpi.com/about/contact)\n* [Careers](https://careers.mdpi.com)\n* [News](https://www.mdpi.com/about/announcements)\n* [Press](https://www.mdpi.com/about/press)\n* [Blog](http://blog.mdpi.com/)\n[Sign In / Sign Up](https://www.mdpi.com/user/login)[Submit](< https://susy.mdpi.com/user/manuscripts/upload?journal=ijms\n>)\n*error\\_outline*You can access[the new MDPI.com website here](https://www.mdpi.com/redirect/new_site). Explore and share your feedback with us.\nSearchfor Articles:\nTitle / Keyword\nAuthor / Affiliation / Email\nJournal\nAll JournalsAccounting and AuditingAcousticsActa Microbiologica Hellenica (AMH)ActuatorsAdhesivesAdministrative SciencesAdolescentsAdvances in Respiratory Medicine (ARM)AerobiologyAerospaceAgricultureAgriEngineeringAgrochemicalsAgronomyAIAI ChemistryAI for EngineeringAI in EducationAI in MedicineAI MaterialsAI SensorsAirAlgorithmsAllergiesAlloysAnalogAnalyticaAnalyticsAnatomiaAnesthesia ResearchAnimalsAntibioticsAntibodiesAntioxidantsApplied BiosciencesApplied MechanicsApplied MicrobiologyApplied NanoApplied SciencesApplied System Innovation (ASI)AppliedChemAppliedMathAppliedPhysAquaculture JournalArchitectureArthropodaArtsAstronauticsAstronomyAtmosphereAtomsAudiology ResearchAutomationAxiomsBacteriaBatteriesBehavioral SciencesBeveragesBig Data and Cognitive Computing (BDCC)BioChemBioengineeringBiologicsBiologyBiology and Life Sciences ForumBiomassBiomechanicsBioMedBiomedicinesBioMedInformaticsBiomimeticsBiomoleculesBiophysicaBioresources and BioproductsBiosensorsBiosphereBioTechBirdsBlockchainsBrain SciencesBuildingsBusinessesC (Journal of Carbon Research)CancersCardiogeneticsCardiovascular MedicineCatalystsCellsCeramicsChallengesChemEngineeringChemistryChemistry ProceedingsChemosensorsChildrenChipsCivilEngClean Technologies (Clean Technol.)ClimateClin...",
      "url": "https://www.mdpi.com/1422-0067/25/12/6583"
    },
    {
      "title": "[PDF] Applying graph neural network models to molecular property ...",
      "text": "Artificial Intelligence Chemistry 2 (2024) 100050\nAvailable online 19 January 2024\n2949-7477/Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\nApplying graph neural network models to molecular property prediction \nusing high-quality experimental data\u2606 \nChen Qu *, Barry I. Schneider *, Anthony J. Kearsley , Walid Keyrouz , Thomas C. Allison \nNational Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, MD 20899, USA \nARTICLE INFO \nKeywords: \nKov\u00b4ats retention index \nBoiling point \nMass spectrum \nGraph neural network \nDeep learning \nABSTRACT \nGraph neural networks have been successfully applied to machine learning models related to molecules and \ncrystals, due to the similarity between a molecule/crystal and a graph. In this paper, we present three models that \nare trained with high-quality experimental data to predict three molecular properties (Kovats \u00b4 retention index, \nnormal boiling point, and mass spectrum), using the same GNN architecture. We show that graph representations \nof molecules, combined with deep learning methodologies and high-quality data sets, lead to accurate machine \nlearning models to predict molecular properties. \n1. Introduction \nIn recent years, the adoption and efficacy of machine learning (ML) \nand artificial intelligence (AI) have advanced rapidly due to more \npowerful compute hardware and software as well as the availability of \nlarge data sets. Breakthrough applications include AlphaGo [1], large \nlanguage models (such as the Generative Pre-training Transformer, \nGPT), and self-driving cars. ML methodologies have been applied to \nmolecular sciences and chemistry, such as material design and drug \ndiscovery [2\u20139], synthesis planning and reaction optimization [10\u201313], \nprotein structure prediction [14,15], and to a wide range of theoret\nical/computational chemistry targets [16\u201320]. Among these applica\ntions, predicting molecular properties stands out as a key component in \ndrug and materials design, and this is the subject of this paper. \nData play a central role in ML. Unfortunately, experimental data in \nthe physical sciences are often scarce and costly to assemble. (Data sets \npresented in this article are products of decades of curation and even \nlonger to measure.) There are several strategies to address data scarcity. \nThe first is to use theory and computation to generate data. For example, \ndensity functional theory can produce large sets of data rapidly (in some \ncases) to augment or supplement measurement data. However, accurate \nand reliable computations are still costly; the cost of \u201cgold-standard\u201d \ncoupled-cluster theory scales as N7\n, where N is the size of the electronic \nspace. This scaling makes the coupled cluster method prohibitively \nexpensive for molecules with more than tens of atoms. An alternate \nstrategy accepts the scarcity of relevant data and trains a model with \nsmall data sets. For example, active learning [21\u201323] can guide exper\niments (or expensive computations) in selecting the most informative \ndata points, thereby reducing the size of the data set needed for a suc\ncessful model. Transfer learning [24\u201326] uses a pre-trained model that is \nbased on abundant data for a related problem, and then tunes the model \nwith a small amount of data for the target problem. The quantity of \nexperimental data is not the only concern; the quality of experimental \ndata usually varies due to dependence on experimental conditions, in\nstrument precision, sample purity, and even the instrument operator. \nOnce a suitable problem has been identified and a dataset has been \nselected for training the ML model, it is necessary to select the features \n(i.e., input data) that will be used in the training process. Well-chosen \nfeatures may lead to a robust model, whereas other features may have \nlittle effect on the performance of a model. For molecular property \nprediction, a key question can be the representation of the structure of \nthe molecule or crystal [27]. When the 3D coordinates of the atoms in \nthe molecules are available (e.g., for applications in theoretical and \ncomputational chemistry), some widely used molecular representations \ninclude Coulomb matrix [28], bag-of-bonds [29], atom-centered sym\nmetry functions [30], and smooth overlap of atomic positions [31]. \nWhen 3D coordinates are not available, which is often true for experi\nmental data, the simplified molecular input line entry system (SMILES) \nrepresentation [32] or molecular fingerprints [33] can be used as the \ninput to various machine learning models. \nWe employed a 2D topological molecular graph as the representation \nfor molecular systems and a graph neural network (GNN) as these are \n\u2606 Official contribution of the National Institute of Standards and Technology; not subject to copyright in the United States. \n* Corresponding authors. \nE-mail addresses: chen.qu@nist.gov (C. Qu), barry.schneider@nist.gov (B.I. Schneider). \nContents lists available at ScienceDirect \nArtificial Intelligence Chemistry \njournal homepage: www.journals.elsevier.com/artificial-intelligence-chemistry \nhttps://doi.org/10.1016/j.aichem.2024.100050 \nReceived 28 September 2023; Received in revised form 15 December 2023; Accepted 11 January 2024 \nArtificial Intelligence Chemistry 2 (2024) 100050\n2\nmore closely aligned with the internal structure of chemical compounds \nwhere chemical bonds play a prominent role in the properties of these \ncompounds. Intuitively, it is easier for a GNN to learn from a chemical \ncompound that is itself represented as a graph. One advantage of a graph \nrepresentation is that it is a natural and expressive representation of a \nmolecule: nodes in the graph correspond to atomic centers and edges of \nthe graph correspond to chemical bonds. For each node and edge, a \nfeature vector containing attributes of the corresponding atom or bond, \nsuch as the atom type, hybridization, or bond order provides the input \ndata for the model. In the model described in this article, these feature \nvectors are updated during training using information from neighboring \natoms and bonds exchanged via message passing [34,35] between the \nnodes and edges of the molecular graph, giving the model additional \nflexibility to predict the molecular property of interest. Additional de\ntails of the GNN model are given in Section 3. \nHere we present applications of the GNN model trained on high\u0002quality experimental data sets curated by NIST [36\u201338]. In particular, \nwe predict Kovats \u00b4 retention indices [39], normal boiling points, and \nmass spectra. Before presenting the details of our GNN models and the \ndata libraries used to train these models, we first give some background \ninformation on Kov\u00b4ats retention indices, normal boiling points, and \nmass spectra. \nGas chromatography (GC) is an important analytical technique for \nthe separation and identification of chemical compounds. In a GC \nexperiment, a mixture of target substances, often unknowns, in a \ngaseous state and a carrier gas is passed through a chromatography \ncolumn. The time elapsed before the unknown compound passes \nthrough the column is indexed against the elution times of known \ncompounds; this index is called the retention index. It has been \ndemonstrated [39] that the retention index can be made independent of \nmany experimental factors such as column length, column diameter, and \nfilm thickness. This results in a dimensionless quantity known as the \nKovats \u00b4 retention index. \nGC is frequently used in combination with mass spectrometry (GC/ \nMS) as a means of enhancing the accuracy of identifying chemical \ncompounds. In this context, matching the retention index can signifi\ncantly improve the confidence in results generated by library searching \nversus use of the mass spectrum alone [40]. Therefore, accurate pre\ndiction of Kov\u00b4ats retention indices has considerable value and many \ntechniques have b...",
      "url": "https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=956419"
    }
  ]
}