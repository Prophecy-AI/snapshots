# Catechol Reaction Yield Prediction - Strategy Guide (Loop 4)

## Current Status
- Best CV score: 0.010986 from exp_001 (Tree-Based Ensemble)
- Best LB score: 0.0998 from exp_000 (MLP)
- CV-LB gap: ~9x (consistent across experiments)
- Target: 0.017270 (5.7x better than best public LB of 0.098)
- Submissions remaining: 3/5

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** for exp_002 (GP model). The execution was sound but results were worse than previous experiments.

**Evaluator's top priority**: Investigate the CV-LB gap and official metric. I AGREE this is critical but we've already analyzed this extensively. The gap appears consistent (~9x) across all experiments and public kernels. The LB metric is likely MSE but evaluated differently than our local CV.

**Key concerns raised**:
1. GP experiment regressed performance → AGREED. GPs are not the right approach for this problem. Moving on.
2. CV-LB gap unexplained → We've analyzed this. The gap is consistent with public kernels (~0.098 LB). Our models are competitive.
3. Target seems unreachable → DISAGREE. The target IS reachable. We need a breakthrough approach.

**My synthesis**: The GP approach failed. We need to focus on:
1. **Stacking ensemble** to leverage diversity between MLP and Trees
2. **High-dimensional features** (drfps, fragprints) that may capture chemistry better
3. **Per-target optimization** since SM has highest variance

## CRITICAL: Submission Structure Requirements
**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.
- The last three cells MUST remain unchanged except for the model definition line
- `model = MLPModel()` can be replaced with a new model definition
- Both single solvent and full (mixed) solvent tasks must be handled

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target statistics, CV structure
- `exploration/evolver_loop3_analysis.ipynb` - Analysis of CV-LB gap and prediction differences

**Key Data Facts:**
- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)
- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)
- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)
- **SM has highest variance** (std ~0.36) - this is the hardest target to predict
- **Targets do NOT sum to 1** (mean ~0.8) - there is mass loss in the reaction

## Recommended Approaches (Priority Order)

### PRIORITY 1: Stacking Ensemble (MLP + Trees)
**Rationale**: MLP and Trees make different predictions (mean diff ~0.03-0.06 per target). Combining them could reduce variance and improve generalization.

**Implementation**:
```python
class StackingEnsembleModel:
    def __init__(self, data='single'):
        self.data_type = data
        # MLP component (from exp_000)
        self.mlp = MLPWithArrhenius(data=data)
        # Tree component (from exp_001)  
        self.trees = TreeEnsembleModel(data=data)
    
    def train_model(self, X, Y):
        self.mlp.train_model(X, Y)
        self.trees.train_model(X, Y)
        
    def predict(self, X):
        pred_mlp = self.mlp.predict(X)
        pred_trees = self.trees.predict(X)
        # Simple average - can try weighted average later
        return (pred_mlp + pred_trees) / 2
```

**Key elements to include**:
- Arrhenius kinetics features (inv_temp, log_time, interaction)
- Spange + ACS PCA descriptors
- Symmetry TTA for mixed solvents
- Data augmentation for mixed solvents

### PRIORITY 2: High-Dimensional Features (drfps with PCA)
**Rationale**: drfps (2048 features) are differential reaction fingerprints that may capture chemistry better than spange (13 features).

**Implementation**:
- Load drfps_catechol_lookup.csv
- Apply PCA to reduce to 50-100 components
- Combine with Arrhenius kinetics features
- Use with tree-based model (handles high-dim well)

**Caution**: High-dimensional features may overfit. Use strong regularization.

### PRIORITY 3: Per-Target Optimization with Feature Selection
**Rationale**: SM has highest variance and dominates the error. Different features may work better for different targets.

**Implementation**:
- For SM: Use HistGradientBoosting with deeper trees
- For Products: Use ExtraTrees with more estimators
- Try different feature tables for different targets

## What NOT to Try
- **Gaussian Processes**: Already tried (exp_002), performed worse than trees
- **Simple hyperparameter tuning**: Marginal improvements won't bridge the 5.7x gap to target
- **More bagging**: Already using 3-7 models, diminishing returns

## Validation Notes
- CV scheme: Leave-one-solvent-out (24 folds) for single, leave-one-ramp-out (13 folds) for full
- CV-LB gap is ~9x - this is consistent with public kernels
- Focus on CV improvement - LB will follow if approach is sound

## Key Insights from Public Kernels
1. **Arrhenius Kinetics + TTA (LB 0.09831)**: Best public kernel uses inv_temp, log_time, interaction features with symmetry TTA and 7-model bagging
2. **Per-Target Ensemble (LB 0.11161)**: Different models for SM vs Products, weighted ensemble (0.65 ACS PCA + 0.35 Spange)
3. **Our best (LB 0.0998)**: Competitive with best public kernels

## Experiment Tracking
| ID | Model | CV Score | LB Score | Notes |
|----|-------|----------|----------|-------|
| exp_000 | MLP + Arrhenius + TTA | 0.011303 | 0.0998 | Baseline |
| exp_001 | Tree Ensemble | 0.010986 | 0.0999 | Best CV |
| exp_002 | Gaussian Process | 0.017921 | - | WORSE, not submitted |

## Next Experiment: Stacking Ensemble (MLP + Trees)
**Goal**: Combine the diversity of MLP and Tree-based models to reduce variance and improve generalization.

**Steps**:
1. Implement MLP component with Arrhenius features, BatchNorm, Dropout, HuberLoss
2. Implement Tree component with per-target models (HGB for SM, ExtraTrees for Products)
3. Average predictions from both models
4. Use symmetry TTA for mixed solvents

**Expected outcome**: CV improvement over individual models due to ensemble diversity.

If stacking doesn't improve CV significantly (< 2% improvement), try high-dimensional features (drfps with PCA) in the next experiment.