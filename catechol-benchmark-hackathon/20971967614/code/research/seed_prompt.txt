## Current Status
- Best CV score: 0.010986 from exp_001 (Tree-Based Per-Target Ensemble)
- Best LB score: 0.0998 from exp_000 (MLP baseline)
- CV-LB gap: 9x (0.011 CV vs 0.0998 LB)
- Target: 0.017270 (5-6x better than best public LB!)
- Remaining submissions: 4

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** Agree - the execution is sound.

**Evaluator's top priority: Submit exp_001 to verify CV-LB gap pattern.**
- AGREE. We need to understand if tree-based models have similar CV-LB gap.
- However, I've analyzed the gap and found a critical insight: the LB metric appears to be MAE, not MSE.
- If LB is MAE=0.0998, our CV MSE=0.011 gives estimated MAE~0.105, which is close!
- This explains the 9x gap and suggests our models ARE competitive.

**Key concerns raised:**
1. CV-LB gap not understood → ADDRESSED: Likely MAE vs MSE metric difference
2. Target score analysis → The target of 0.017270 is 5-6x better than best public LB (0.098). This is a HUGE gap that suggests either a fundamentally different approach or the target represents something different.
3. Per-fold variance → SM target has much higher variance (0.13-0.14) than Products (0.02). Focus on SM prediction.

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Full EDA, CV structure, target distributions
- `exploration/evolver_loop2_analysis.ipynb` - CV-LB gap analysis, metric investigation

**Key patterns to exploit:**
1. **SM dominates error**: SM variance is 6-7x higher than Products. Better SM prediction = better overall score.
2. **Solvent extrapolation is key**: Leave-one-solvent-out CV tests generalization to unseen solvents.
3. **Some solvents are outliers**: PCA shows some solvents are far from the cluster - these may be hard to predict.
4. **Arrhenius kinetics works**: Physics-informed features (inv_temp, log_time, interaction) are effective.

## Recommended Approaches

### Priority 1: Submit exp_001 to Kaggle (IMMEDIATE)
- Verify if tree-based approach has similar CV-LB gap
- This uses 1 submission but provides critical calibration data
- If LB is similar (~0.098), confirms our models are competitive

### Priority 2: Gaussian Process Model
**Why:** GPs excel at extrapolation with uncertainty quantification - exactly what we need for unseen solvents.
- Use GPyTorch or sklearn GaussianProcessRegressor
- Kernel: RBF + Matern for smooth + rough patterns
- Features: Arrhenius + spange descriptors
- Consider Deep Kernel Learning (DKL) for better feature learning

### Priority 3: Focus on SM Target
**Why:** SM has 6-7x higher variance than Products and dominates the error.
- Train specialized model for SM with more capacity
- Consider separate hyperparameters for SM vs Products
- Try regressor chain: predict SM first, use as input for Products

### Priority 4: Different Solvent Featurizations
**Why:** Current spange descriptors (13 features) may not capture all relevant chemistry.
- Try drfps_catechol (2048 features) - differential reaction fingerprints
- Try fragprints (2133 features) - fragment + fingerprint concatenation
- Use PCA to reduce dimensionality if needed
- Ensemble models with different featurizations

### Priority 5: Ensemble of Diverse Models
**Why:** Diversity in model types can capture different patterns.
- Combine: MLP + Trees + GP
- Use stacking with meta-learner
- Weight by per-target performance

## What NOT to Try

1. **More hyperparameter tuning on current approaches** - CV is already good (0.011), the gap is in LB metric
2. **Normalizing targets to sum to 1** - Targets don't sum to 1 due to mass loss
3. **GroupKFold with fewer folds** - Some kernels do this but it's not the official CV structure

## Validation Notes

- CV scheme: Leave-one-solvent-out (24 folds) for single, leave-one-ramp-out (13 folds) for full
- The LB metric appears to be MAE, not MSE - this explains the 9x CV-LB gap
- Our CV MSE of 0.011 corresponds to estimated MAE of ~0.105, close to LB of 0.0998
- Focus on approaches that improve extrapolation to unseen solvents

## CRITICAL CONSTRAINTS (MUST FOLLOW)

**MANDATORY SUBMISSION STRUCTURE:**
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same

**DATA CONTAMINATION RULES:**
- Pre-training on solvent mixture data to predict full solvent data = NOT ALLOWED
- Hyper-parameters must be SAME across every fold (unless clear rationale exists)
- Test: "If I had to predict a NEW unseen solvent, would this method work without results?"

## Next Experiment Suggestion

**Experiment 002: Gaussian Process with Arrhenius Features**
- Use sklearn GaussianProcessRegressor or GPyTorch
- Kernel: RBF + WhiteKernel for noise
- Features: Arrhenius (inv_temp, log_time, interaction) + spange descriptors
- Per-target GPs (3 separate models)
- This should provide better extrapolation to unseen solvents
