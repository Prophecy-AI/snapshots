# Catechol Reaction Yield Prediction - Strategy Guide (Loop 5)

## CRITICAL: Submission Structure Requirements
**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.
- The last three cells MUST remain unchanged except for the model definition line
- `model = MLPModel()` can be replaced with a new model definition
- Both single solvent and full (mixed) solvent tasks must be handled

## Current Status
- **Best CV score:** 0.0103 from exp_003 (Stacking Ensemble)
- **Best LB score:** 0.0949 from exp_003 (Stacking Ensemble)
- **CV-LB gap:** ~0.085 (LB is ~9x higher than CV MSE)
- **Target:** 0.017270 (5.5x better than current best LB)
- **Remaining submissions:** 2

## CRITICAL INSIGHT: CV Improvements ARE Translating to LB!
- exp_000: CV 0.0113 → LB 0.0998
- exp_001: CV 0.0110 → LB 0.0999 (worse LB despite better CV - noise)
- exp_003: CV 0.0103 → LB 0.0949 (BEST LB! 5% improvement)

**The stacking ensemble is working!** Continue improving CV to improve LB.

## Data Understanding
- Reference notebooks: `exploration/eda.ipynb`, `exploration/evolver_loop4_lb_feedback.ipynb`
- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)
- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)
- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)
- **SM contributes ~80% of total variance** - improving SM prediction is key!

## Available Features
- spange_descriptors (13 features) - currently used
- acs_pca_descriptors (5 features) - currently used in trees
- drfps_catechol (2048 features) - UNEXPLORED, high-dimensional
- fragprints (2133 features) - UNEXPLORED, high-dimensional

## Recommended Approaches (Priority Order)

### 1. EXPAND ENSEMBLE WITH MORE DIVERSE MODELS (HIGH PRIORITY)
The stacking ensemble improved LB by 5%. Add more diverse models:
- **Add LightGBM** - different gradient boosting implementation
- **Add XGBoost** - another GBDT variant
- **Try different ensemble weights** - not just 50/50, try 40/30/30 or learned weights

Reference: `research/kernels/lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb` uses MLP + XGBoost + RF + LightGBM

### 2. INCREASE MLP TRAINING (MEDIUM PRIORITY)
Top kernel uses 7 bagged MLPs with 300 epochs (vs our 3 models, 200 epochs):
- Increase n_models from 3 to 5-7
- Increase epochs from 200 to 300
- This alone improved LB from ~0.098 to 0.09831 in public kernel

### 3. HIGH-DIMENSIONAL FEATURES (MEDIUM PRIORITY)
drfps (2048) and fragprints (2133) are unexplored:
- These may capture chemistry better than low-dimensional descriptors
- Use PCA to reduce dimensionality (e.g., 50-100 components)
- Add as additional features to existing models

### 4. REGRESSOR CHAINS (EXPERIMENTAL)
Targets are correlated (chemical yields):
- Predict SM first (highest variance, 80% of error)
- Use SM prediction as input for Products
- This exploits target correlations

## What NOT to Try
- **Gaussian Processes** - exp_002 showed GP performed worse (CV 0.0179 vs 0.0103)
- **Simple averaging without diversity** - need model diversity for ensemble to work
- **Hyperparameter tuning only** - marginal improvements, need structural changes

## Validation Notes
- CV scheme: leave-one-solvent-out (24 folds) for single, leave-one-ramp-out (13 folds) for full
- LB metric is likely RMSE (CV RMSE ~0.101 matches LB ~0.095)
- CV improvements ARE translating to LB improvements (verified with exp_003)
- Target RMSE 0.017270 requires 5.5x improvement from current best LB

## Submission Strategy
- **2 submissions remaining** - use wisely
- Submit only when CV improves significantly (>5% improvement)
- Current best: CV 0.0103, LB 0.0949
- Next submission threshold: CV < 0.0098 (5% improvement)

## Key Implementation Notes

### Arrhenius Kinetics Features (ALREADY IMPLEMENTED)
```python
temp_k = temp_c + 273.15
inv_temp = 1000.0 / temp_k
log_time = np.log(time_m + 1e-6)
interaction = inv_temp * log_time
```

### Symmetry TTA for Mixed Solvents (ALREADY IMPLEMENTED)
```python
# Train on both A,B and B,A
# Predict on both, average results
```

### Multi-Model Ensemble (TO IMPLEMENT)
```python
# Combine MLP + XGBoost + LightGBM + Trees
# Use optimized weights (not just equal)
# Each model should use Arrhenius features and symmetry TTA
```

## Target Path
- Current LB: 0.0949
- Target: 0.017270
- Gap: 5.5x

To reach target, need breakthrough improvements:
1. More diverse ensemble (4+ model types)
2. Better features (high-dimensional drfps/fragprints with PCA)
3. Per-target optimization (focus on SM which has 80% of variance)
4. Regressor chains (exploit target correlations)