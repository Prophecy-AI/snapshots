# Catechol Reaction Yield Prediction - Strategy Guide (Loop 2)

## Current Status
- Best CV score: 0.0113 from exp_000 (Arrhenius Kinetics + MLP + TTA)
- Best LB score: 0.0998 (from exp_000)
- CV-LB gap: -0.0885 (MASSIVE - 9x worse on LB!)
- Target to beat: 0.017270
- Submissions remaining: 4/5

## Response to Evaluator

**Technical verdict was CONCERNS.** The evaluator correctly identified submission structure issues and lack of reproducibility seeds.

**CRITICAL INSIGHT:** Our LB score of 0.0998 is actually competitive with the best public kernel (0.098). The massive CV-LB gap suggests our local CV calculation differs from how Kaggle evaluates. The target of 0.017270 is 6x better than any public kernel, suggesting either:
1. A different metric interpretation
2. Significant room for improvement through novel approaches

## MANDATORY: Submission Structure Requirements

**CRITICAL - MUST FOLLOW EXACTLY:**
- The submission must follow the exact template structure from https://www.kaggle.com/code/josepablofolch/catechol-benchmark-hackathon-template
- Last 3 cells must match the template exactly
- Only the `model = MLPModel()` line can be changed to `model = YourModel(data='single')` or `model = YourModel(data='full')`
- Model class must have `train_model(X, Y)` and `predict(X)` methods
- predict() must return a tensor/array of shape [N, 3] with columns [Product 2, Product 3, SM]

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Feature distributions, target statistics
- `exploration/evolver_loop1_lb_feedback.ipynb` - CV-LB gap analysis

**Key Data Facts:**
- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)
- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)
- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)
- Targets do NOT sum to 1 (mean ~0.8) - mass loss in reaction
- Temperature range: 175-225Â°C, Residence Time: ~2-15 minutes

## Recommended Approaches (Priority Order)

### 1. PRIORITY: Tree-Based Per-Target Ensemble
Based on the "catechol strategy to get 0.11161" kernel (LB 0.111):
- **Different models per target:**
  - SM: HistGradientBoostingRegressor (max_depth=7, max_iter=700, lr=0.04)
  - Product 2 & 3: ExtraTreesRegressor (n_estimators=900, min_samples_leaf=2)
- **Feature ensemble:** Combine acs_pca_descriptors (5 features) and spange_descriptors (13 features)
- **Weighted average:** 0.65 * model1 + 0.35 * model2
- **Post-processing:** Clip predictions to [0, 1]

### 2. Multi-Model Ensemble (from "mixall" kernel)
Ensemble of MLP + XGBoost + RandomForest + LightGBM:
- StandardScaler preprocessing
- Weighted ensemble (can optimize via Optuna)
- Each model type captures different patterns

### 3. Physics-Informed Features (Keep Using)
The Arrhenius kinetics features are well-motivated:
```python
inv_temp = 1000 / (Temperature + 273.15)  # Inverse temperature
log_time = np.log(Residence_Time + 1e-6)  # Log of time
interaction = inv_temp * log_time          # Interaction term
```

### 4. Symmetry TTA for Mixed Solvents (Keep Using)
For mixed solvents, predict twice and average:
- Prediction 1: Input as (A, B, SolventB%)
- Prediction 2: Input as (B, A, 1-SolventB%)
- Final = (Pred1 + Pred2) / 2

## What NOT to Try
- Complex deep learning architectures (small data, risk of overfitting)
- High-dimensional features (drfps: 2048, fragprints: 2133) without PCA
- Normalizing targets to sum to 1 (they naturally don't)
- Chasing local CV improvements without LB validation

## Model Interface Template

```python
import torch
import numpy as np
import pandas as pd

class YourModel:
    def __init__(self, data='single'):
        self.data_type = data
        # Initialize your model components here
    
    def train_model(self, X_train, y_train):
        # X_train: DataFrame with columns:
        #   Single: ['Residence Time', 'Temperature', 'SOLVENT NAME']
        #   Full: ['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']
        # y_train: DataFrame with ['Product 2', 'Product 3', 'SM']
        pass
    
    def predict(self, X_test):
        # Returns tensor/array of shape [N, 3]
        # Columns must be [Product 2, Product 3, SM] in that order
        return torch.tensor(predictions)
```

## Validation Notes
- **DO NOT trust local CV** - it doesn't match LB
- Focus on LB score as the true measure of progress
- The target of 0.017270 is ambitious - try fundamentally different approaches
- Submit to verify progress on LB before extensive optimization

## Key Insights from Public Kernels

1. **Arrhenius kinetics kernel (LB 0.098):** Physics-informed features, symmetry TTA, bagging MLPs
2. **Per-target ensemble (LB 0.111):** Different models per target, tree-based, feature ensemble
3. **mixall kernel:** Ensemble of MLP + XGBoost + RF + LightGBM

## Next Experiment Recommendation

**Implement a tree-based per-target ensemble** following the "catechol strategy" approach:
1. Use HistGradientBoostingRegressor for SM target
2. Use ExtraTreesRegressor for Product 2 and Product 3
3. Ensemble predictions from acs_pca and spange descriptors
4. Add Arrhenius kinetics features
5. Clip predictions to [0, 1]

This approach is simpler, less prone to overfitting, and has shown good LB performance (0.111).