{
  "query": "What are the best techniques for multi-output regression when targets are correlated chemical yields?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Multi\u2011output regression aims to predict several continuous targets at once, and when those targets are chemically linked (e.g., yields of related reactions) exploiting their correlation can markedly improve accuracy.  The literature groups the most effective approaches into **problem\u2011transformation** methods\u2014where a standard univariate learner is applied to a reshaped target set\u2014and **algorithm\u2011adaptation** methods that modify the learning algorithm to handle multiple outputs directly\u202f([PDF\u202fsurvey](https://oa.upm.es/40804/1/INVE_MEM_2015_204213.pdf)).\n\nFor correlated chemical yields, the top\u2011performing techniques are:\n\n* **Regressor\u2011chain and stacking variants** that feed the prediction of one yield as an input to the next, thereby transferring information along the chain\u202f([Multi\u2011target SVR via correlation regressor chains](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946)).  \n* **Multi\u2011output support\u2011vector regression**, especially Bayesian formulations that model the joint covariance of the outputs and can incorporate prior knowledge about yield relationships\u202f([Multi\u2011Output Bayesian Support Vector Regression](https://www.mdpi.com/2227-7390/12/18/2923)).  \n* **Tree\u2011based multivariate ensembles** (multivariate Random Forests or Gradient Boosting) that use modified splitting criteria to maximise reduction of joint impurity across all yields; simulations show they outperform separate univariate trees when outputs are correlated\u202f([arXiv\u202f2201.05340](https://arxiv.org/abs/2201.05340)).  \n* **Curds\u202f&\u202fWhey** \u2013 a linear\u2011algebraic shrinkage method that decorrelates the response vector, fits ordinary least\u2011squares on the transformed targets, and then back\u2011transforms; it consistently reduces prediction error in the presence of response correlation\u202f([Breiman & Friedman, \u201cCurds & Whey\u201d](https://www.stat.berkeley.edu/~breiman/curds-whey-justtext.pdf)).  \n* **Neural\u2011network\u2011based extreme learning machines with correlation\u2011oriented subnetworks**, which explicitly separate positively and negatively correlated input attributes and have been applied to steady\u2011state chemical\u2011process monitoring\u202f([PNIAOS\u2011DPELM](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775)).  \n\nIn practice, a robust workflow for correlated chemical yields combines a **problem\u2011transformation** step (e.g., regressor chains) with a **strong multivariate learner** such as Bayesian SVR or a multivariate Random Forest, optionally followed by a post\u2011hoc linear correction like Curds\u202f&\u202fWhey.  This hybrid exploits both the statistical dependence among yields and the non\u2011linear relationships typical of chemical reaction data, delivering the most accurate and interpretable predictions.",
      "url": ""
    },
    {
      "title": "Multi-Output Bayesian Support Vector Regression Considering ...",
      "text": "Next Article in Journal\n\n[A Novel Ensemble Method of Divide-and-Conquer Markov Boundary Discovery for Causal Feature Selection](https://www.mdpi.com/2227-7390/12/18/2927)\n\nPrevious Article in Journal\n\n[TSPconv-Net: Transformer and Sparse Convolution for 3D Instance Segmentation in Point Clouds](https://www.mdpi.com/2227-7390/12/18/2926)\n\n## Journals\n\n[Active Journals](https://www.mdpi.com/about/journals) [Find a Journal](https://www.mdpi.com/about/journalfinder) [Journal Proposal](https://www.mdpi.com/about/journals/proposal) [Proceedings Series](https://www.mdpi.com/about/proceedings)\n\n[**Topics**](https://www.mdpi.com/topics)\n\n## Information\n\n[For Authors](https://www.mdpi.com/authors) [For Reviewers](https://www.mdpi.com/reviewers) [For Editors](https://www.mdpi.com/editors) [For Librarians](https://www.mdpi.com/librarians) [For Publishers](https://www.mdpi.com/publishing_services) [For Societies](https://www.mdpi.com/societies) [For Conference Organizers](https://www.mdpi.com/conference_organizers)\n\n[Open Access Policy](https://www.mdpi.com/openaccess) [Institutional Open Access Program](https://www.mdpi.com/ioap) [Special Issues Guidelines](https://www.mdpi.com/special_issues_guidelines) [Editorial Process](https://www.mdpi.com/editorial_process) [Research and Publication Ethics](https://www.mdpi.com/ethics) [Article Processing Charges](https://www.mdpi.com/apc) [Awards](https://www.mdpi.com/awards) [Testimonials](https://www.mdpi.com/testimonials)\n\n[**Author Services**](https://www.mdpi.com/authors/english)\n\n## Initiatives\n\n[Sciforum](https://sciforum.net) [MDPI Books](https://www.mdpi.com/books) [Preprints.org](https://www.preprints.org) [Scilit](https://www.scilit.com) [SciProfiles](https://sciprofiles.com) [Encyclopedia](https://encyclopedia.pub) [JAMS](https://jams.pub) [Proceedings Series](https://www.mdpi.com/about/proceedings)\n\n## About\n\n[Overview](https://www.mdpi.com/about) [Contact](https://www.mdpi.com/about/contact) [Careers](https://careers.mdpi.com) [News](https://www.mdpi.com/about/announcements) [Press](https://www.mdpi.com/about/press) [Blog](http://blog.mdpi.com/)\n\n[Sign In / Sign Up](https://www.mdpi.com/user/login)\n\n## Notice\n\nYou can make submissions to other journals\n[here](https://susy.mdpi.com/user/manuscripts/upload).\n\n_clear_\n\n## Notice\n\nYou are accessing a machine-readable page. In order to be human-readable, please install an RSS reader.\n\nContinueCancel\n\n_clear_\n\nAll articles published by MDPI are made immediately available worldwide under an open access license. No special\npermission is required to reuse all or part of the article published by MDPI, including figures and tables. For\narticles published under an open access Creative Common CC BY license, any part of the article may be reused without\npermission provided that the original article is clearly cited. For more information, please refer to\n[https://www.mdpi.com/openaccess](https://www.mdpi.com/openaccess).\n\nFeature papers represent the most advanced research with significant potential for high impact in the field. A Feature\nPaper should be a substantial original Article that involves several techniques or approaches, provides an outlook for\nfuture research directions and describes possible research applications.\n\nFeature papers are submitted upon individual invitation or recommendation by the scientific editors and must receive\npositive feedback from the reviewers.\n\nEditor\u2019s Choice articles are based on recommendations by the scientific editors of MDPI journals from around the world.\nEditors select a small number of articles recently published in the journal that they believe will be particularly\ninteresting to readers, or important in the respective research area. The aim is to provide a snapshot of some of the\nmost exciting work published in the various research areas of the journal.\n\nOriginal Submission Date Received: .\n\n[![mathematics-logo](https://pub.mdpi-res.com/img/journals/mathematics-logo.png?8600e93ff98dbf14)](https://www.mdpi.com/journal/mathematics)\n\n[Submit to this Journal](https://susy.mdpi.com/user/manuscripts/upload?form%5Bjournal_id%5D%3D154) [Review for this Journal](https://susy.mdpi.com/volunteer/journals/review) [Propose a Special Issue](https://www.mdpi.com/journalproposal/sendproposalspecialissue/mathematics)\n\n[\u25ba\u25bc\\\nArticle Menu](https://www.mdpi.com/2227-7390/12/18/2923)\n\n## Article Menu\n\n- [Academic Editor](https://www.mdpi.com/2227-7390/12/18/2923#academic_editors)\n\n\n\n\n[![](https://www.mdpi.com/profiles/1250467/thumb/Manuel_Alberto_M._Ferreira.jpg)Manuel Alberto M. Ferreira](https://sciprofiles.com/profile/1250467?utm_source=mdpi.com&utm_medium=website&utm_campaign=avatar_name)\n\n- [Subscribe SciFeed](https://www.mdpi.com/2227-7390/12/18/2923/scifeed_display)\n- [Recommended Articles](https://www.mdpi.com/2227-7390/12/18/2923)\n- [Related Info Link](https://www.mdpi.com/2227-7390/12/18/2923#related)\n\n\n- [Google Scholar](http://scholar.google.com/scholar?q=Multi-Output%20Bayesian%20Support%20Vector%20Regression%20Considering%20Dependent%20Outputs)\n\n- [More by Authors Links](https://www.mdpi.com/2227-7390/12/18/2923#authors)\n\n\n- on DOAJ\n\n\n- [Wang, Y.](http://doaj.org/search/articles?source=%7B%22query%22%3A%7B%22query_string%22%3A%7B%22query%22%3A%22%5C%22Yanlin%20Wang%5C%22%22%2C%22default_operator%22%3A%22AND%22%2C%22default_field%22%3A%22bibjson.author.name%22%7D%7D%7D)\n- [Cheng, Z.](http://doaj.org/search/articles?source=%7B%22query%22%3A%7B%22query_string%22%3A%7B%22query%22%3A%22%5C%22Zhijun%20Cheng%5C%22%22%2C%22default_operator%22%3A%22AND%22%2C%22default_field%22%3A%22bibjson.author.name%22%7D%7D%7D)\n- [Wang, Z.](http://doaj.org/search/articles?source=%7B%22query%22%3A%7B%22query_string%22%3A%7B%22query%22%3A%22%5C%22Zichen%20Wang%5C%22%22%2C%22default_operator%22%3A%22AND%22%2C%22default_field%22%3A%22bibjson.author.name%22%7D%7D%7D)\n\n- on Google Scholar\n\n\n- [Wang, Y.](http://scholar.google.com/scholar?q=Yanlin%20Wang)\n- [Cheng, Z.](http://scholar.google.com/scholar?q=Zhijun%20Cheng)\n- [Wang, Z.](http://scholar.google.com/scholar?q=Zichen%20Wang)\n\n- on PubMed\n\n\n- [Wang, Y.](http://www.pubmed.gov/?cmd=Search&term=Yanlin%20Wang)\n- [Cheng, Z.](http://www.pubmed.gov/?cmd=Search&term=Zhijun%20Cheng)\n- [Wang, Z.](http://www.pubmed.gov/?cmd=Search&term=Zichen%20Wang)\n\n/ajax/scifeed/subscribe\n\n[Article Views](https://www.mdpi.com/2227-7390/12/18/2923#metrics)\n\n[Citations-](https://www.mdpi.com/2227-7390/12/18/2923#metrics)\n\n- [Table of Contents](https://www.mdpi.com/2227-7390/12/18/2923#table_of_contents)\n\nAltmetric[_share_ Share](https://www.mdpi.com/2227-7390/12/18/2923) [_announcement_ Help](https://www.mdpi.com/2227-7390/12/18/2923) [_format\\_quote_ Cite](javascript:void(0);) [_question\\_answer_ Discuss in SciProfiles](https://sciprofiles.com/discussion-groups/public/10.3390/math12182923?utm_source=mpdi.com&utm_medium=publication&utm_campaign=discuss_in_sciprofiles)\n\n## Need Help?\n\n### Support\n\nFind support for a specific problem in the support section of our website.\n\n[Get Support](https://www.mdpi.com/about/contactform)\n\n### Feedback\n\nPlease let us know what you think of our products and services.\n\n[Give Feedback](https://www.mdpi.com/feedback/send)\n\n### Information\n\nVisit our dedicated information section to learn more about MDPI.\n\n[Get Information](https://www.mdpi.com/authors)\n\n_clear_\n\n## JSmol Viewer\n\n_clear_\n\n_first\\_page_\n\n[Download PDF](https://www.mdpi.com/2227-7390/12/18/2923/pdf?version=1726829433)\n\n_settings_\n\n[Order Article Reprints](https://www.mdpi.com/2227-7390/12/18/2923/reprints)\n\nFont Type:\n\n_Arial__Georgia__Verdana_\n\nFont Size:\n\nAaAaAa\n\nLine Spacing:\n\n_\uf034__\uf034__\uf034_\n\nColumn Width:\n\n_\uf035__\uf035__\uf035_\n\nBackground:\n\nOpen AccessFeature PaperArticle\n\n# Multi-Output Bayesian Support Vector Regression Considering Dependent Outputs\n\nby\n\nYanlin Wang\n\n![](https://www.mdpi.com/bundles/mdpisciprofileslink/img/unknown-user.png)Yanlin Wang\n\n[SciProfiles](https://sciprofiles.com/profile/3750665?utm_source=mdpi.com&utm_medium=website&ut...",
      "url": "https://www.mdpi.com/2227-7390/12/18/2923"
    },
    {
      "title": "[PDF] A survey on multi-output regression",
      "text": "A survey on multi-output regression\nHanen Borchani\u22171, Gherardo Varando2, Concha Bielza2, and Pedro\nLarra\u02dcnaga2\n1Machine Intelligence Group, Department of Computer Science,\nAalborg University, Selma Lagerl\u00a8ofs Vej 300, 9220, Denmark.\n2Computational Intelligence Group, Departamento de Inteligencia\nArtificial, Facultad de Inform\u00b4atica, Universidad Polit\u00b4ecnica de\nMadrid, Boadilla del Monte, 28660, Spain.\nAbstract\nIn recent years, a plethora of approaches have been proposed to deal\nwith the increasingly challenging task of multi-output regression. This pa\u0002per provides a survey on state-of-the-art multi-output regression methods,\nthat are categorized as problem transformation and algorithm adaptation\nmethods. In addition, we present the mostly used performance evalu\u0002ation measures, publicly available data sets for multi-output regression\nreal-world problems, as well as open-source software frameworks.\nKeywords Multi-output regression, problem transformation methods, algorithm\nadaptation methods, multi-target regression, performance evaluation measures.\n1 Introduction\nMulti-output regression, also known in the literature as multi-target 1\u20135, multi-variate\n6\u20138, or multi-response 9,10 regression, aims to simultaneously predict multiple real\u0002valued output/target variables. When the output variables are binary, the learning\nproblem is called multi-label classification 11\u201313. However, when the output variables\nare discrete (not necessarily binary), the learning problem is referred to as multi\u0002dimensional classification 14\n.\nSeveral applications for multi-output regression have been studied. They include\necological modeling to predict multiple target variables describing the condition or\nquality of the vegetation 3, chemometrics to infer concentrations of several analytes\nfrom multi-variate calibration using multi-variate spectral data 15, prediction of the\naudio spectrum of wind noise (represented by several sound pressure variables) of a\ngiven vehicle component 16, real-time prediction of multiple gas tank levels of the Linz\nDonawitz converter gas system17, simultaneous estimation of different biophysical pa\u0002rameters from remote sensing images 18, channel estimation through the prediction of\n\u2217Corresponding author: Hanen Borchani. E-mail: hanen@cs.aau.dk\n1\nseveral received signals 19, etc. In spite of their different backgrounds, these real-world\napplications give rise to many challenges such as missing data (i.e., when some feature/-\ntarget values are not observed), the presence of noise typically due to the complexity\nof the real domains, and most importantly, the multivariate nature and the compound\ndependencies between the multiple feature/target variables. In dealing with these\nchallenges, it has been proven that multi-output regression methods yield to a better\npredictive performance, in general, when compared against the single-output meth\u0002ods 15\u201317. Multi-output regression methods provide as well the means to effectively\nmodel the multi-output datasets by considering not only the underlying relationships\nbetween the features and the corresponding targets but also the relationships between\nthe targets, guaranteeing thereby a better representation and interpretability of the\nreal-world problems 3,18. A further advantage of the multi-target approaches is that\nthey may produce simpler models with a better computational efficiency 3.\nExisting methods for multi-output regression can be categorized as: a) problem\ntransformation methods (also known as local methods) that transform the multi-output\nproblem into independent single-output problems each solved using a single-output\nregression algorithm, and b) algorithm adaptation methods (also known as global or\nbig-bang methods) that adapt a specific single-output method (such as decision trees\nand support vector machines) to directly handle multi-output data sets. Algorithm\nadaptation methods are deemed to be more challenging since they usually aim not\nonly to predict the multiple targets but also to model and interpret the dependencies\namong these targets.\nNote here that the multi-task learning problem20\u201324 is related to the multi-output\nregression problem: it also aims to learn multiple related tasks (i.e., outputs) at the\nsame time. Commonly investigated issues in multi-task learning include modeling task\nrelatedness and the definition of similarity between jointly learned tasks, feature selec\u0002tion, and certainly, the development of efficient algorithms for learning and predicting\nseveral tasks simultaneously using different approaches, such as clustering, kernel re\u0002gression, neural networks, tree and graph structures, Bayesian model, etc. The main\ndifference between multi-output regression and multi-task problems is that tasks may\nhave different training sets and/or different descriptive features, in contrast to the\ntarget variables that share always the same data and/or descriptive features.\nThe remainder of this paper is organized as follows. In Section 2, the state-of-the\u0002art multi-output regression approaches are presented according to the categorization as\nproblem transformation and algorithm adaptation methods. In Section 3, we provide\na theoretical comparison of the different presented approaches. In Section 4, we dis\u0002cuss evaluation measures, and publicly available data sets for multi-output regression\nlearning problems are given in Section 5. Section 6 describes the open-source software\nframeworks available for multi-output regression methods, and finally, Section 7 sums\nup the paper with some conclusions and possible lines for future research.\n2 Multi-output regression\nLet us consider the training data set D of N instances containing a value assignment\nfor each variable X1, . . . , Xm, Y1, . . . , Yd, i.e., D = {(x\n(1)\n, y\n(1)), . . . , (x(N)\n, y\n(N)\n)}.\nEach instance is characterized by an input vector of m descriptive or predictive\nvariables x\n(l) = (x\n(l)\n1\n, . . . , x\n(l)\nj\n, . . . , x\n(l)\nm ) and an output vector of d target variables\ny\n(l) = (y\n(l)\n1\n, . . . , y\n(l)\ni\n, . . . , y\n(l)\nd\n), with i \u2208 {1, . . . , d}, j \u2208 {1, . . . , m}, and l \u2208 {1, . . . , N}.\nThe task is to learn a multi-target regression model from D consisting of finding a\n2\nfunction h that assigns to each instance, given by the vector x, a vector y of d target\nvalues:\nh : \u2126X1 \u00d7 . . . \u00d7 \u2126Xm \u2212\u2192 \u2126Y1 \u00d7 . . . \u00d7 \u2126Yd\nx = (x1, . . . , xm) 7\u2212\u2192 y = (y1, . . . , yd),\nwhere \u2126Xj and \u2126Yi denote the sample spaces of each predictive variable Xj , for all\nj \u2208 {1, . . . , m}, and each target variable Yi, for all i \u2208 {1, . . . , d}, respectively. Note\nthat, all target variables are considered to be continuous here. The learned multi-target\nmodel will be used afterwards to simultaneously predict the values {y\u02c6\n(N+1)\n, . . . , y\u02c6\n(N0)\n}\nof all target variables of the new incoming unlabeled instances {x\n(N+1)\n, . . . , x\n(N0)\n}.\nThroughout this section, we provide a survey on state-of-the-art multi-output re\u0002gression learning methods categorized as problem transformation methods (Section\n2.1) and algorithm adaptation methods (Section 2.2).\n2.1 Problem transformation methods\nThese methods are mainly based on transforming the multi-output regression prob\u0002lem into single-target problems, then building a model for each target, and finally\nconcatenating all the d predictions. The main drawback of these methods is that the\nrelationships among the targets are ignored, and the targets are predicted indepen\u0002dently, which may affect the overall quality of the predictions.\nRecently, Spyromitros-Xioufis et al. 4 proposed to extend well-known multi-label\nclassification transformation methods to deal with the multi-output regression problem\nand to also model the target dependencies. In particular, they introduced two novel\napproaches for multi-target regression, multi-target regressor stacking and regressor\nchains, inspired by popular and successful multi-label classification approaches.\nAs discussed in Spyromitros-Xioufis et al. 4, only approaches based on s...",
      "url": "https://oa.upm.es/40804/1/INVE_MEM_2015_204213.pdf"
    },
    {
      "title": "Positive and negative correlation input attributes oriented subnets based double parallel extreme learning machine (PNIAOS-DPELM) and its application to monitoring chemical processes in steady state",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0925231215002775)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0925231215002775/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#preview-section-snippets)\n- [References (51)](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#preview-section-references)\n- [Cited by (17)](https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/neurocomputing)\n\n## [Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing)\n\n[Volume 165](https://www.sciencedirect.com/journal/neurocomputing/vol/165/suppl/C), 1 October 2015, Pages 171-181\n\n[![Neurocomputing](https://ars.els-cdn.com/content/image/1-s2.0-S0925231215X00154-cov150h.gif)](https://www.sciencedirect.com/journal/neurocomputing/vol/165/suppl/C)\n\n# Positive and negative correlation input attributes oriented subnets based double parallel extreme learning machine (PNIAOS-DPELM) and its application to monitoring chemical processes in steady state\n\nAuthor links open overlay panelYanlinHe, ZhiQiangGeng, QunXiongZhu\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.neucom.2015.03.007](https://doi.org/10.1016/j.neucom.2015.03.007) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0925231215002775&orderBeanReset=true)\n\n## Abstract\n\n[Extreme learning machine](https://www.sciencedirect.com/topics/computer-science/extreme-learning-machine) (ELM) is an effective learning algorithm for single-hidden-layer feed-forward [neural networks](https://www.sciencedirect.com/topics/neuroscience/neural-network) (SLFNNs). Due to its easiness in theory and implementation, ELM has been widely used in many fields. In order to further enhance the [generalization performance](https://www.sciencedirect.com/topics/computer-science/generalization-performance) of ELM, a positive and negative correlation input attributes oriented subnets based double parallel extreme learning machine (PCNCIAOS-DPELM) is proposed in this paper. A salient feature in the PNIAOS-DPELM is that there are two special subnets. In one of the two subnets, the input attributes have a [positive correlation](https://www.sciencedirect.com/topics/computer-science/positive-correlation) to the outputs. In another subnet, the input attributes have a negative correlation to the outputs. The two kinds of input attributes can be obtained by separating the input attributes into two categories using the correlation coefficient analysis. Then according to the categories, the two subnets can be established. The two subnets are based on well-trained auto-associative [neural networks](https://www.sciencedirect.com/topics/computer-science/neural-network) (AANNs), which can extract the nonlinear information of the input attributes and remove the redundant information. An advantage in PNIAOS-DPELM is that the proper number of the nodes in the hidden layer can be determined. To test the validity of PNIAOS-DPELM, it is applied to monitoring three chemical processes in steady state. Meanwhile, ELM, double parallel ELM (DP-ELM), and ELM with kernel (ELMK) were developed for comparisons. Experimental results demonstrated that PNIAOS-DPELM could achieve better regression precision and have better stable ability than ELM, DP-ELM, and ELMK did during the generalization phase.\n\n## Introduction\n\nIn modern chemical processes, some key variables such as product qualities and other important process variables should be reliable and measured accurately \\[1\\]. The chemical processes were usually run in two states: in the dynamic state and in the steady state. Generally speaking, chemical processes are run in the steady state for producing products. Therefore, an accurate automatic control system is necessary for monitoring the processes in the steady state. Due to the increasing intensity and complexity of process operations in the chemical industry, the old predictors are neither sufficiently accurate nor reliable for utilization of intelligent control applications \\[2\\]. And the performance of the control systems is subject to the highly nonlinear relationships between the input variables and the output variables. In order to solve this problem, the implementation of advanced control strategy based on artificial neural networks (ANNs) soft sensors can remarkably enhance the intelligent control capabilities for the highly nonlinear processes \\[3\\], \\[4\\]. The successful applications of ANNs is due to three aspects: first, ANNs have good ability in nonlinear function approximators \\[5\\], \\[6\\]; second, ANNs have the ability to learn from some known samples and then predict the unknown ones; finally, no knowledge of the processes is necessary and needed for the development of the ANNs based models \\[7\\]. Additionally, ANNs based models can learn well from the process data collected from the distributed control systems in chemical industries \\[8\\].\n\nExtreme learning machine (ELM), proposed by Huang et al. \\[9\\], \\[10\\], \\[11\\], has been proved to be a competitive machine learning technique for single-hidden-layer feed-forward neural networks (SLFNNs). In ELM, the input weights and hidden layer biases can be randomly generated. However, the output weights are analytically determined by the Moore\u2013Penrose generalized inverse \\[12\\] of the hidden layer output matrix. Compared with the networks based on the gradient-based learning method (for example, the back propagation neural network (BPNN) and the radial basis function neural network (RBFNN)), ELM has an extremely fast learning speed \\[9\\]. In addition, ELM also can avoid many difficulties such as how to determine the parameters of the stopping criteria, the learning rate, and the learning epochs \\[9\\], \\[10\\], \\[11\\]. And the learning strategy of ELM has been successfully applied to the RBFNN \\[13\\], \\[14\\]. Due to these excellent characteristics, ELM has been successfully applied in many fields, such as classification \\[15\\], \\[16\\], \\[17\\], regression \\[18\\], \\[19\\], \\[20\\], recognition \\[21\\], \\[22\\], \\[23\\], function approximation \\[24\\], and prediction \\[25\\], \\[26\\].\n\nRecently ELM has attracted more and more tremendous attention and interest form researchers. And a lot of improved learning algorithms for enhancing ELM have been proposed \\[27\\], \\[28\\], \\[29\\], \\[30\\], \\[31\\], \\[32\\], \\[33\\], \\[34\\], \\[35\\], \\[36\\], \\[37\\], \\[38\\], \\[39\\], \\[40\\]. In the paper of Huang et al., an extreme learning machine with kernel (ELMK) which uses unknown kernel mappings instead of the known hidden layer mappings is proposed to avoid the problem of selecting the hidden layer nodes number \\[16\\]. In the paper of Yao et al., a double parallel ELM (DP-ELM) was proposed for improving the performance of ELM \\[41\\]. The double parallel structure can enable the output nodes to not only receive the information from the hidden layer nodes but also receive the direct information from the input layer nodes \\[42\\]. In traditional ELM and its many improved methods, however, the input attributes are tied together as a whole put into the network models, wh...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0925231215002775"
    },
    {
      "title": "Multi-target support vector regression via correlation regressor chains",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0020025517307946)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0020025517307946/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#preview-section-snippets)\n- [References (51)](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#preview-section-references)\n- [Cited by (121)](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/information-sciences)\n\n## [Information Sciences](https://www.sciencedirect.com/journal/information-sciences)\n\n[Volumes 415\u2013416](https://www.sciencedirect.com/journal/information-sciences/vol/415/suppl/C), November 2017, Pages 53-69\n\n[![Information Sciences](https://ars.els-cdn.com/content/image/1-s2.0-S0020025517X00212-cov150h.gif)](https://www.sciencedirect.com/journal/information-sciences/vol/415/suppl/C)\n\n# Multi-target support vector regression via correlation regressor chains\n\nAuthor links open overlay panelGabriellaMelkia, AlbertoCanoa, VojislavKecmana, Sebasti\u00e1nVenturabc\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.ins.2017.06.017](https://doi.org/10.1016/j.ins.2017.06.017) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0020025517307946&orderBeanReset=true)\n\n## Highlights\n\n- \u2022\nThree novel multi-target support vector regressor models are proposed.\n\n- \u2022\nThe first builds an independent single-target support vector regressor for each output variable.\n\n- \u2022\nThe second builds an ensemble of random chains using the ERCC methodology.\n\n- \u2022\nThe third calculates the targets correlation and builds a single regressor model using that chain.\n\n- \u2022\nResults show that using a single maximum correlation chain model obtains better performance than the ensemble of random chains while learning a single model.\n\n\n## Abstract\n\nMulti-target regression is a challenging task that consists of creating predictive models for problems with multiple continuous target outputs. Despite the increasing attention on multi-label classification, there are fewer studies concerning multi-target (MT) regression. The current leading MT models are based on ensembles of regressor chains, where random, differently ordered chains of the target variables are created and used to build separate regression models, using the previous target predictions in the chain. The challenges of building MT models stem from trying to capture and exploit possible correlations among the target variables during training. This paper presents three multi-target support vector regression models. The first involves building independent, single-target Support Vector Regression (SVR) models for each output variable. The second builds an ensemble of random chains using the first method as a base model. The third calculates the targets\u2019 correlations and forms a maximum correlation chain, which is used to build a single chained support vector regression model, improving the models\u2019 prediction performance while reducing the computational complexity. The experimental study evaluates and compares the performance of the three approaches with seven other state-of-the-art multi-target regressors on 24 multi-target datasets. The experimental results are then analyzed using non-parametric statistical tests. The results show that the maximum correlation SVR approach improves the performance of using ensembles of random chains.\n\n## Introduction\n\nIn supervised learning, _single-target_ (ST) models are trained to predict the value of a single, categorical or numeric, target attribute of a given example. In some cases, more than one target, or output, can be associated with a single sample input. These situations are handled by a generalization of ST learning, which involves predicting these multiple outputs concurrently, and is known as multi-target (MT) learning\u00a0\\[1\\], \\[5\\]. Specifically, MT learning includes _multi-target regression_ (MTR), which addresses the prediction of continuous targets, _multi-label classification_\u00a0\\[48\\] which focuses on binary targets, and _multi-dimensional classification_ which describes the prediction of discrete targets\u00a0\\[5\\], \\[38\\].\n\nMulti-target prediction has the capacity to generate models representing a wide variety of real-world applications, ranging from natural language processing\u00a0\\[25\\] to bioinformatics\u00a0\\[34\\]. Other application areas include ecology\u00a0\\[1\\], gene function prediction\u00a0\\[27\\], predicting the quality of vegetation\u00a0\\[22\\], \\[28\\], stock price index forecasting\u00a0\\[46\\], and operations research\u00a0\\[5\\], \\[23\\].\n\nConstructing models for these types of real-world problems presents many challenges, such as missing data, (due to targets not being observed or recorded), and noisy data (due to instrument, experimental or human error), and the curse of high dimensionality. Along with these challenges, the most difficult task is identifying relationships between the input data and its corresponding output value. In the context of multi-target modeling, multiple outputs must now be trained against, which inherently adds computational complexity. The targets may or may not be correlated, and the corresponding model must accommodate for both scenarios. However, a characteristic of the MT datasets used in these applications and elsewhere, is that they are generated by a single system, most likely indicating that the nature of the outputs captured has some structure\u00a0\\[23\\]. Even though modeling the multi-variate nature and complex relationships between the target variables is challenging\u00a0\\[5\\], they are more accurately represented by an MT model.\n\nSeveral base-line methods have been proposed for solving multi-target tasks such as Multi-Objective Random Forests\u00a0\\[28\\], Boosted Neural Networks\u00a0\\[22\\], Ensembles of Trees\u00a0\\[30\\], and many others. Support Vector Machines are a popular set of linear and non-linear supervised machine learning algorithms with a strong theoretical basis on Vapnik-Chervonenkis theory\u00a0\\[13\\]. It has previously been shown that they outperform most algorithms in terms of performance, scalability, and the ability to efficiently deal with outliers\u00a0\\[16\\], \\[26\\]. Input space dimensionality does not have an adverse effect on the model training time, and furthermore, the final model produced is sparse, allowing for quick predictions.\n\nThere are two main approaches for using such base-line methods in the context of MT learning. The first being _problem transformation_ methods, or _local_ methods, in which the multi-target problem is transformed into multiple single-target problems, each solved separately using classical methods, as described above. The second being _algorithm adaptation_ methods, or _global_, or _big-bang_ methods, that adapt existing single-target methods to predict all the target variables simultaneously\u00a0\\[5\\], \\[27\\]. Using _problem transformation_ algorithms for a domain of _t_ target variables, _t_ predictive models must be constructed, each predicting a single-target variable\u00a0\\[27\\]. Prediction for an unseen sample would be obtained by running each of the _t_ single-target models and concatenating their resu...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946"
    },
    {
      "title": "Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?",
      "text": "# Statistics > Machine Learning\n\n**arXiv:2201.05340** (stat)\n\n\\[Submitted on 14 Jan 2022\\]\n\n# Title:Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?\n\nAuthors: [Lena Schmid](https://arxiv.org/search/stat?searchtype=author&query=Schmid,+L), [Alexander Gerharz](https://arxiv.org/search/stat?searchtype=author&query=Gerharz,+A), [Andreas Groll](https://arxiv.org/search/stat?searchtype=author&query=Groll,+A), [Markus Pauly](https://arxiv.org/search/stat?searchtype=author&query=Pauly,+M)\n\nView a PDF of the paper titled Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?, by Lena Schmid and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2201.05340)\n\n> Abstract:Tree-based ensembles such as the Random Forest are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether we separately fit univariate models or directly follow a multivariate approach. For the latter, several possibilities exist that are, e.g. based on modified splitting or stopping rules for multi-output regression. In this work we compare these methods in extensive simulations to help in answering the primary question when to use multivariate ensemble techniques.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2201.05340](https://arxiv.org/abs/2201.05340) \\[stat.ML\\] |\n|  | (or [arXiv:2201.05340v1](https://arxiv.org/abs/2201.05340v1) \\[stat.ML\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2201.05340](https://doi.org/10.48550/arXiv.2201.05340)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Lena Schmid \\[ [view email](https://arxiv.org/show-email/6ebcdec5/2201.05340)\\]\n\n**\\[v1\\]**\nFri, 14 Jan 2022 08:44:25 UTC (341 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?, by Lena Schmid and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2201.05340)\n- [TeX Source](https://arxiv.org/src/2201.05340)\n- [Other Formats](https://arxiv.org/format/2201.05340)\n\n[![license icon](https://arxiv.org/icons/licenses/by-nc-nd-4.0.png)view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2201.05340&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2201.05340&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2022-01](https://arxiv.org/list/stat.ML/2022-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2201.05340?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2201.05340?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2201.05340?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2201.05340)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2201.05340)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2201.05340)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2201.05340&description=Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2201.05340&title=Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2201.05340) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2201.05340"
    },
    {
      "title": "justtext.dvi",
      "text": "Predicting Multivariate Responses in Multiple\nLinear Regression\nLeo Breiman\u0003Jerome H. Friedmany\nNovember 24, 1995\nAbstract\nWe look at the problem of predicting several response variables from\nthe same set of explanatory variables. The question is how to take advan\u0002tage of correlations between the response variables to improve predictive\naccuracy as compared to the usual procedure of doing individual regres\u0002sions of each response variable on the common set of predictor variables. A\nnew procedure is introduced called the curds & whey method. Its use can\nsubstantially reduce prediction errors when there are correlations between\nresponses while maintaining accuracy even if the responses are uncorrelated.\nIn extensive simulations, the new procedure is compared to several previ\u0002ously proposed methods for predicting multiple responses (including PLS)\nand exhibits superior accuracy. One version can be easily implemented in\nthe context of standard statistical packages.\n1. Introduction.\nIncreasingly, there are applications where several quantities are to be predicted us\u0002ing a common set of predictor variables. For instance, in a manufacturing process\nwe may want to predict various quality aspects of a product from the parameter\n\u0003Department of Statistics, University of California, Berkeley, CA 94720. Work partially\nsupported by the National Science Foundation, Grant No. DMS-9212419.\nyDepartment of Statistics and Stanford Linear Accelerator Center, Stanford University, Stan\u0002ford, CA 94305. Work partially supported by the Department of Energy, Contract No. DE\u0002AC03-76SF00515.\nsettings used in the manufacturing. Or, given the mass spectra of a sample, the\ngoal may be to predict the concentrations of several chemical constituents in the\nsample.\nSome years ago, the authors were involved in a pro ject trying to predict changes\nin the valuations of the stocks in 60 industry groups using over 100 econometric\nvariables as predictors. In our state of knowledge at that time, prediction equa\u0002tions for each one of the 60 groups were derived not using the data on the other 59\nresponses. However, the changes in the 60 groups were strongly correlated. If we\nknew then what we know now, we could have taken advantage of the correlations\nto produce more accurate predictors.\nTo give a simple example of the potential improvement in estimation, suppose\nthat the data is of the form fyn1; yn2; xngN\n1 where each xn = (xn1; \u0001 \u0001 \u0001; xnp) is a p\n- vector of predictor variables and there are two responses y1 and y2. Taking the\nusual path, we get predictors for y1; y2 by doing separate regressions on (x1; \u0001\u0001\u0001; xp).\nThat is, the estimated regression coe\u000ecients ^a1 = (^a11; \u0001 \u0001 \u0001; a^1p) and ^a2 = (^a21; \u0001 \u0001\n\u0001; a^2p) are solutions to\n^a1 = arg min\na\nX\nN\nn=1\n(yn1  a\nt\nxn)\n2\n^a2 = arg min\na\nX\nN\nn=1\n(yn2  a\nt\nxn)\n2\nwhere all variables have been centered. The prediction equations for y1 and y2\nare y^1(x) = y\u00161 + ^at\n1(x  \u0016x) and y^2(x) = y\u00162 + ^at2(x  \u0016x) where (\u0016yi; \u0016x) are the\ncorresponding sample means (before centering). Now suppose further that the\n(unknown) truth happens to be yn1 = b10 + btxn + \"n1 and yn2 = b20 + btxn + \"n2\nwhere f\"n1gN\n1 and f\"n2gN\n1 are independent i.i.d. N(0; \u001b2\n): Here y1 and y2 are\ncorrelated because they have the same dependence on the predictor variables,\nbtx. It is also clear that accuracy is improved for each of the two responses by\nusing the predictors y~i = y\u0016i + 1\n2\n(^y1  y\u00161) + 1\n2\n(^y2  y\u00162) (i = 1; 2); instead of y^1 and\ny^2 respectively.\n1.1. The curds and whey (C&W) procedure.\nIn general, if there are q responses y = (y1; \u0001 \u0001 \u0001; yq) with separate least squares\nregressions ^y = (^y1; \u0001 \u0001 \u0001; y^\nq), then the above example raises the possibility that if\nthe responses are correlated, we may be able to get a more accurate predictor y~i\n2\nof each yi by using a linear combination\ny~i = y\u0016i + X\nq\nk=1\nbik(^yk  y\u0016k); i = 1; \u0001 \u0001 \u0001; q (1.1)\nof the ordinary least squares (OLS) predictors\nybi = y\u0016i + X\np\nj=1\nabij (xj  x\u0016j ) , (1.2)\nfabijg\np\nj=1 = arg min\nfajg\np\n1\nX\nN\nn=1\n2\n4yni  y\u0016i  X\np\nj=1\naj(xnj  x\u0016j )\n3\n5\n2\n; (1.3)\nrather than with the least squares predictors themselves. Note that (1.1) (1.2)\nimply that the coe\u000ecients, but not the means, of the (OLS) estimates are modi\fed.\nTo simplify notation in all derivations that follow, we assume that the response\nand predictor variables are all centered by their corresponding training sample\nmeans fyi yi  y\u0016ig\nq\n1, fxj xj  x\u0016jg\np\n1. As a result, all response estimates\nare centered at the corresponding response sample means fy^i y^i  y\u0016ig\nq\n1, fy~i \ny~i  y\u0016ig\nq\n1; and reference the centered predictor variables.\nAssuming that (1.1) is an interesting possibility, the trick is to \fnd what fbikg\nto use. It turns out that there is a nearly optimal set of fbikg that are given by\nwhat we call the \\curds and whey\" (C&W) procedure. Using vector and matrix\nnotation for the respective (centered) quantities\nye = fyeig\nq\n1\n; yb = fybig\nq\n1\n; and B = [bik] 2 Rq\u0002q, (1.4)\n(1.1) can be expressed as\nye = Byc: (1.5)\nWe derive estimates of the matrix B that take the form B = T1DT where T is\nthe q \u0002 q matrix whose rows are the response canonical coordinates (see Section\n2.2) and D = diag(d1; \u0001 \u0001 \u0001; dq) is a diagonal matrix. Two prescriptions are derived\nfor calculating fdkg\nq\n1. A generalized cross-validation approach (Section 3.1) yields\na simple formula (3.12) (3.13). This works surprisingly well. Using regular (5 or\n10 - fold) cross-validation (Section 3.2) to obtain the fdkg\nq\n1 gives slightly better\nprediction.\n3\n1.2. Statistical background.\nThe curds and whey (C&W) procedure is a form of multivariate shrinking. It\ntransforms (T), shrinks (multiplies by D = fdkg\nq\n1), and then transforms back\n(T1). It derives its power by shrinking in the right coordinate system (canonical\ncoordinates), and can be viewed as a multivariate generalization of proportional\nshrinkage based on cross-validation [Stone (1974)].\nIn the case of a single response variable (q = 1) it is well known that the\nOLS estimate (1.2)(1.3) can be outperformed in terms of prediction accuracy by\nbiased (regularized) shrinkage estimates. Examples include proportional shrink\u0002age [James and Stein (1961), Stone (1974), Copas (1983) (1987)], ridge regression\n[Hoerl and Kennard (1970)], principal components regression [Massey (1965)], and\npartial least squares (\\PLS\") regression [Wold (1975)]. These results suggest that\nthere may be gains associated with treating the collection of responses as a vec\u0002tor valued variable in the context of a combined shrinkage estimation procedure.\nSuch procedures have been proposed: reduced rank regression [Izenman (1975)],\ntwo-block PLS [Wold (1975)], FICYREG [van der Merwe and Zidek (1980)], and\nmultivariate forms of ridge regression [Brown and Zidek (1980) (1982)]. How\u0002ever they have seen little use in statistical practice. An exception is two-block\nPLS which is widely applied in the \feld of chemometrics. C&W di\u000bers from\nthe methods cited in that it has roots in both a theoretical and a cross-validation\nfoundation. Furthermore, simulation results indicate that its performance exceeds\nthat of the several (previous) methods to which we have compared it.\n1.3. Outline of paper.\nIn Section 2 we assume (centered) predictors of the form (1.5). Taking the data to\nbe generated from linear models plus noise, we derive (under idealized conditions)\nthe optimal shrinkage matrix B\u0003 = T1DT where T is the canonical transfor\u0002mation and D is a diagonal \\shrinking\" matrix. However, due to the idealized\nsetting, the matrix D derived there underestimates the amount of shrinkage nec\u0002essary. Section 3 takes a cross-validation approach to estimation of the shrinkage\nfactors, derives a simple approximate formula, and then describes the V - fold\ncross-validation estimates of D.\nSection 4 gives a brief description of some other methods proposed in the lit\u0002erature for estimating multiple responses, and these are compared to C&W in the\nsimulation study covered in Section 5. In some \fe...",
      "url": "https://www.stat.berkeley.edu/~breiman/curds-whey-justtext.pdf"
    },
    {
      "title": "Multi-output prediction of dose\u2013response curves enables drug ...",
      "text": "Multi-output prediction of dose\u2013response curves enables drug repositioning and biomarker discovery - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1038/s41698-024-00691-x)\n* [](pdf/41698_2024_Article_691.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![NPJ Precision Oncology logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-npjprecisoncol.jpg)\nNPJ Precis Oncol\n. 2024 Sep 20;8:209. doi:[10.1038/s41698-024-00691-x](https://doi.org/10.1038/s41698-024-00691-x)\n# Multi-output prediction of dose\u2013response curves enables drug repositioning and biomarker discovery\n[Juan-Jos\u00e9 Giraldo Gutierrez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Gutierrez JJG\"[Author]>)\n### Juan-Jos\u00e9 Giraldo Gutierrez\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\nFind articles by[Juan-Jos\u00e9 Giraldo Gutierrez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Gutierrez JJG\"[Author]>)\n1,2,\u2709,#,[Evelyn Lau](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Lau E\"[Author]>)\n### Evelyn Lau\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Evelyn Lau](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Lau E\"[Author]>)\n3,#,[Subhashini Dharmapalan](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Dharmapalan S\"[Author]>)\n### Subhashini Dharmapalan\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Subhashini Dharmapalan](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Dharmapalan S\"[Author]>)\n2,3,[Melody Parker](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Parker M\"[Author]>)\n### Melody Parker\n4Nuffield Department of Clinical Medicine, University of Oxford, John Radcliffe Hospital, Oxford, UK\n5Big Data Institute at the Li Ka Shing Centre for Health Information and Discovery, University of Oxford, Oxford, UK\nFind articles by[Melody Parker](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Parker M\"[Author]>)\n4,5,[Yurui Chen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Chen Y\"[Author]>)\n### Yurui Chen\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n6Department of Mathematics, National University of Singapore, Singapore, Republic of Singapore\nFind articles by[Yurui Chen](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Chen Y\"[Author]>)\n3,6,[Mauricio A \u00c1lvarez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00c1lvarez MA\"[Author]>)\n### Mauricio A \u00c1lvarez\n7Department of Computer Science, The University of Manchester, Manchester, UK\nFind articles by[Mauricio A \u00c1lvarez](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00c1lvarez MA\"[Author]>)\n7,[Dennis Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang D\"[Author]>)\n### Dennis Wang\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n8Bioinformatics Institute (BII), Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\nFind articles by[Dennis Wang](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wang D\"[Author]>)\n1,2,3,8,\u2709\n* Author information\n* Article notes\n* Copyright and License information\n1National Heart and Lung Institute, Imperial College London, London, UK\n2Department of Computer Science, The University of Sheffield, Sheffield, UK\n3Institute for Human Development and Potential, Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n4Nuffield Department of Clinical Medicine, University of Oxford, John Radcliffe Hospital, Oxford, UK\n5Big Data Institute at the Li Ka Shing Centre for Health Information and Discovery, University of Oxford, Oxford, UK\n6Department of Mathematics, National University of Singapore, Singapore, Republic of Singapore\n7Department of Computer Science, The University of Manchester, Manchester, UK\n8Bioinformatics Institute (BII), Agency for Science Technology and Research (A\\*STAR), Singapore, Republic of Singapore\n\u2709Corresponding author.\n#\nContributed equally.\nReceived 2023 Nov 10; Accepted 2024 Aug 28; Collection date 2024.\n\u00a9The Author(s) 2024\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC11415488\u00a0\u00a0PMID:[39304771](https://pubmed.ncbi.nlm.nih.gov/39304771/)\n## Abstract\nDrug response prediction is hampered by uncertainty in the measures of response and selection of doses. In this study, we propose a probabilistic multi-output model to simultaneously predict all dose\u2013responses and uncover their biomarkers. By describing the relationship between genomic features and chemical properties to every response at every dose, our multi-output Gaussian Process (MOGP) models enable assessment of drug efficacy using any dose\u2013response metric. This approach was tested across two drug screening studies and ten cancer types. Kullback-leibler divergence measured the importance of each feature and identified*EZH2*gene as a novel biomarker of BRAF inhibitor response. We demonstrate the effectiveness of our MOGP models in accurately predicting dose\u2013responses in different cancer types and when there is a limited number of drug screening experiments for training. Our findings highlight the potential of MOGP models in enhancing drug develop...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11415488"
    },
    {
      "title": "Multi-output process identification - ScienceDirect.com",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0959152497000036)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0959152497000036/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036#preview-section-abstract)\n- [References (19)](https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036#preview-section-references)\n- [Cited by (31)](https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/journal-of-process-control)\n\n## [Journal of Process Control](https://www.sciencedirect.com/journal/journal-of-process-control)\n\n[Volume 7, Issue 4](https://www.sciencedirect.com/journal/journal-of-process-control/vol/7/issue/4), August 1997, Pages 269-282\n\n[![Journal of Process Control](https://ars.els-cdn.com/content/image/1-s2.0-S0959152424X00062-cov150h.gif)](https://www.sciencedirect.com/journal/journal-of-process-control/vol/7/issue/4)\n\n# Full paper  Multi-output process identification\n\nAuthor links open overlay panelBhupinder S.Dayal\u2217, John F.MacGregor\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/S0959-1524(97)00003-6](https://doi.org/10.1016/S0959-1524(97)00003-6) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0959152497000036&orderBeanReset=true)\n\n## Abstract\n\nIn model based control of multivariate processes, it has been common practice to identify a multi-input single-output (MISO) model for each output separately and then combine the individual models into a final MIMO model. If models for all outputs are independently parameterized then this approach is optimal. However, if there are common or correlated parameters among models for different output variables and/or correlated noise, then performing identification on all outputs simultaneously can lead to better and more robust models. In this paper, theoretical justifications for using multi-output identification for a multivariate process are presented and the potential benefits from using them are investigated via simulations on two process examples: a quality control example and an extractive distillation column. The identification of both the parsimonious transfer function models using multivariate prediction error methods, and of non-parsimonious finite impulse response (FIR) models using multivariate statistical regression methods such as partial least squares (PLS2), canonical correlation regression (CCR) and reduced rank regression (RRR) are considered. The multi-output identification results are compared to traditional single-output identification from several points of view: best predictions, closeness of the model to the true process, the precision of the identified models in frequency domain, stability robustness of the resulting model based control system, and multivariate control performance. The multi-output identification methods are shown to be superior to the single-output methods on the basis of almost all the criteria. Improvements in the prediction of individual outputs and in the closeness of the model to the true process are only marginal. The major benefits are in the stability and performance robustness of controllers based on the identified models. In this sense the multi-output identification methods are more \u2018control relevant\u2019.\n\nRecommended articles\n\n- P. Geladi _et al._\n\n\n### Anal. Chem. Acta.\n\n\n\n(1986)\n\n- G.E.P. Box _et al._\n\n\n### Biometrica\n\n\n\n(1965)\n\n- G.E.P. Box _et al._\n\n\n### Technometrics\n\n\n\n(1973)\n\n- D.W. Clarke\n\n### Generalized least squares estimation of parameters of a dynamic model\n\n- A.J. Burnham _et al._\n\n\n### J. Chemometrics\n\n\n\n(1996)\n\n- M.S. Bartlett\n\n\n### J. Roy. Statist. Soc.\n\n\n\n(1947)\n\n- L. Breiman _et al._\n\n### Predicting multivariate responses in multiple linear regression\n\n\n\n\n### J. Roy. Statist. Soc.\n\n\n\n(1996)\n\n- M.K.-S. Tso\n\n\n### J.R. Statist. Soc.\n\n\n\n(1981)\n\n- S. Wold\n\n\n### Technometrics\n\n\n\n(1978)\n\n\nThere are more references available in the full text version of this article.\n\n- ### [A multi-output two-stage locally regularized model construction method using the extreme learning machine](https://www.sciencedirect.com/science/article/pii/S0925231213010060)\n\n\n\n2014, Neurocomputing\n\n\n\n\n\n\n\nCitation Excerpt :\n\n\n\nThe modeling and identification of multi-input multi-output (MIMO) dynamic systems have been used in many industrial applications \\[1,2\\]. A conventional approach is to identify a multi-input single-output model for each output separately and then combine every individual model to produce a final MIMO model \\[2\\]. However, if there are common or correlated parameters for different output variables, then performing identification on all outputs simultaneously may lead to better and more robust models.\n\n\n\n\n\n\n\nShow abstract\n\n\n\n\n\n\n\nThis paper investigates the construction of linear-in-the-parameters (LITP) models for multi-output regression problems. Most existing stepwise forward algorithms choose the regressor terms one by one, each time maximizing the model error reduction ratio. The drawback is that such procedures cannot guarantee a sparse model, especially under highly noisy learning conditions. The main objective of this paper is to improve the sparsity and generalization capability of a model for multi-output regression problems, while reducing the computational complexity. This is achieved by proposing a novel multi-output two-stage locally regularized model construction (MTLRMC) method using the extreme learning machine (ELM). In this new algorithm, the nonlinear parameters in each term, such as the width of the Gaussian function and the power of a polynomial term, are firstly determined by the ELM. An initial multi-output LITP model is then generated according to the termination criteria in the first stage. The significance of each selected regressor is checked and the insignificant ones are replaced at the second stage. The proposed method can produce an optimized compact model by using the regularized parameters. Further, to reduce the computational complexity, a proper regression context is used to allow fast implementation of the proposed method. Simulation results confirm the effectiveness of the proposed technique.\n\n- ### [On experiment design for identification of Ill-conditioned systems](https://www.sciencedirect.com/science/article/pii/S1474667016418136)\n\n\n\n2014, IFAC Proceedings Volumes (IFAC-PapersOnline)\n\n\n\nShow abstract\n\n\n\n\n\n\n\nIn this paper experiment design for identification of ill-conditioned systems is studied. A short overview of recently proposed techniques is presented. These are mainly based on a singular value decomposition (SVD) of an estimated gain matrix. A summary of this approach with some extensions is given. Another approach is to find a D-optimal solution; the result is essentially the same as found by SVD methods. A result is that it is very important properly to excite the so-called low-gain direction of the system. The methods are motivated by the desire to guarantee integral controllability in model based control designs such as model predictive control (MPC). The dynamics of the process have not been a consideration in these works. However, it is well known from practical studies and simple models that high gains tend to be associated with slow dynamics and low gains with fast dynamics. For experiment design, it is useful to know how general this behaviour is. In this paper it is...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0959152497000036"
    }
  ]
}