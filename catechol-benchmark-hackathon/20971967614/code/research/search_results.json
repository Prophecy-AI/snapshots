{
  "query": "What is the evaluation metric for the Catechol Benchmark Hackathon Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The provided search results do not contain information about the evaluation metric used for the Catechol Benchmark Hackathon Kaggle competition. ([Kaggle\u202fdocs](https://www.kaggle.com/docs/competitions-setup), [Kaggle\u202fbenchmarks](https://www.kaggle.com/benchmarks))",
      "url": ""
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions-setup"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/benchmarks"
    },
    {
      "title": "NeurIPS 2025 San Diego Datasets & Benchmarks",
      "text": "NeurIPS 2025 San Diego Datasets &amp; BenchmarksNeurIPS 2025\n[Skip to yearly menu bar](#child-menu)[Skip to main content](#main)\n## Main Navigation\n[![conference_logo](https://neurips.cc/static/core/img/neurips-navbar-logo.svg)](https://neurips.cc/)\n* [NeurIPS](#)\n* [Help/FAQ](https://neurips.cc/FAQ)\n* [Contact NeurIPS](https://neurips.cc/Help/Contact)\n* [Code of Ethics](https://neurips.cc/Conferences/2023/EthicsGuidelines)\n* [Code of Conduct](https://neurips.cc/public/CodeOfConduct)\n* [Create Profile](https://neurips.cc/Profile/create)\n* [Journal To Conference Track](https://neurips.cc/public/JournalToConference)\n* [Diversity &amp; Inclusion](https://neurips.cc/public/DiversityInclusion)\n* [Proceedings](https://proceedings.neurips.cc/)\n* [Future Meetings](https://neurips.cc/Conferences/FutureMeetings)\n* [Press](https://neurips.cc/Conferences/2025/Press)\n* [Exhibitor Information](https://neurips.cc/Exhibitors/exhibitorinfo)\n* [Privacy Policy](https://neurips.cc/public/PrivacyPolicy)\n* [Downloads](https://neurips.cc/Downloads)\n* [My Stuff](https://neurips.cc/MyStuff)\n**\n[Login](https://neurips.cc/accounts/login?nextp=/careers/)\n* ![San Diego graphic](https://neurips.cc/media/Locations/15-san-diego.svg)San Diego\n* ![Mexico City graphic](https://neurips.cc/media/Locations/17-mexico-city.svg)[Mexico City](https://neurips.cc/virtual/2025/loc/mexico-city/events/datasets-benchmarks-2025)\n# San Diego Datasets &amp; Benchmarks\n**494 Events\n**\nPoster### [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://neurips.cc/virtual/2025/poster/121603)\nKuicai Dong &middot; CHANG YUJING &middot; Shijie Huang &middot; Yasheng Wang &middot; Ruiming Tang &middot; Yong Liu\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nDocument Visual Question Answering (DocVQA) faces dual challenges in processing lengthy multimodal documents (text, images, tables) and performing cross-modal reasoning. Current document retrieval-augmented generation (DocRAG) methods remain limited by their text-centric approaches, frequently missing critical visual information. The field also lacks robust benchmarks for assessing multimodal evidence selection and integration. We introduce MMDocRAG, a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains. Our framework introduces innovative metrics for evaluating multimodal quote selection and enables answers that interleave text with relevant visual elements. Through large-scale experiments with 60 VLM/LLM models and 14 retrieval systems, we identify persistent challenges in multimodal evidence retrieval, selection, and integration. Key findings reveal that advanced proprietary LVMs show superior performance than open-sourced alternatives. Also, they show moderate advantages using multimodal inputs over text-only inputs, while open-source alternatives show significant performance degradation. Notably, fine-tuned LLMs achieve substantial improvements when using detailed image descriptions. MMDocRAG establishes a rigorous testing ground and provides actionable insights for developing more robust multimodal DocVQA systems.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121603)\nPoster### [CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays](https://neurips.cc/virtual/2025/poster/121386)\nHyungyung Lee &middot; Geon Choi &middot; Jung-Oh Lee &middot; Hangyul Yoon &middot; Hyuk Hong &middot; Edward Choi\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nRecent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning.The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements.Even the strongest of 12 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121386)\nPoster### [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://neurips.cc/virtual/2025/poster/121553)\nAtharva Gundawar &middot; Som Sagar &middot; Ransalu Senanayake\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nVision-Language Models (VLMs) are increasingly pivotal for generalist robot manipulation, enabling tasks such as physical reasoning, policy generation, and failure detection. However, their proficiency in these high-level applications often assumes a deep understanding of low-level physical prerequisites, a capability that is largely unverified. To perform actions reliably, robots must comprehend intrinsic object properties (e.g., material, weight), action affordances (e.g., graspable, stackable), and physical constraints (e.g., stability, reachability, or an object's state like being closed). Despite their ubiquitous use in manipulation, we argue that off-the-shelf VLMs may lack this granular, physically-grounded understanding, as these specific prerequisites are often overlooked during training. Addressing this critical gap, we introduce PAC Bench, a comprehensive benchmark designed to systematically evaluate VLMs on their understanding of these core Properties, Affordances, and Constraints (PAC) from a task executability perspective. PAC Bench features a diverse dataset with more than 30,000 annotations, comprising 673 real-world images (115 object classes, 15 property types, 1\u20133 affordances defined per object class), 100 real-world humanoid view scenarios, and 120 unique simulated constraint scenarios across four tasks. Our evaluations reveal significant gaps in the ability of VLMs to grasp fundamental physical concepts, underscoring their current limitations for reliable robot manipulation and pointing to key areas that require targeted research. PAC Bench also serves as a standardized benchmark for rigorously evaluating the physical reasoning capabilities of VLMs guiding the development of more robust and physically grounded models for robot manipulation.\n[Show more](#)\n[View full details**](https://neurips.cc/virtual/2025/poster/121553)\nPoster### [CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation](https://neurips.cc/virtual/2025/poster/121706)\nXinran Wang &middot; Songyu Xu &middot; Shan Xiangxuan &middot; Yuxuan Zhang &middot; Muxi Diao &middot; Xueyan Duan &middot; Yanhua huang &middot; Kongming Liang &middot; Zhanyu Ma\n**Dec 3, 11:00 AM - 2:00 PM**Exhibit Hall C,D,E\nCinematography is a cornerstone of film production and appreciation, shaping mood, emotion, and narrative through visual elements such as camera movement, shot composition, and lighting. Despite recent progress in multimodal large language models (MLLMs) and video generation models, the capacity of current models to grasp and repro...",
      "url": "https://neurips.cc/virtual/2025/loc/san-diego/events/datasets-benchmarks-2025"
    },
    {
      "title": "Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "Authors: [Toby Boyne](https://arxiv.org/search/cs?searchtype=author&query=Boyne,+T), [Juan S. Campos](https://arxiv.org/search/cs?searchtype=author&query=Campos,+J+S), [Becky D. Langdon](https://arxiv.org/search/cs?searchtype=author&query=Langdon,+B+D), [Jixiang Qing](https://arxiv.org/search/cs?searchtype=author&query=Qing,+J), [Yilin Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+Y), [Shiqiang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Calvin Tsay](https://arxiv.org/search/cs?searchtype=author&query=Tsay,+C), [Ruth Misener](https://arxiv.org/search/cs?searchtype=author&query=Misener,+R), [Daniel W. Davies](https://arxiv.org/search/cs?searchtype=author&query=Davies,+D+W), [Kim E. Jelfs](https://arxiv.org/search/cs?searchtype=author&query=Jelfs,+K+E), [Sarah Boyall](https://arxiv.org/search/cs?searchtype=author&query=Boyall,+S), [Thomas M. Dixon](https://arxiv.org/search/cs?searchtype=author&query=Dixon,+T+M), [Linden Schrecker](https://arxiv.org/search/cs?searchtype=author&query=Schrecker,+L), [Jose Pablo Folch](https://arxiv.org/search/cs?searchtype=author&query=Folch,+J+P)\n\n[View PDF](https://arxiv.org/pdf/2506.07619) [HTML (experimental)](https://arxiv.org/html/2506.07619v1)\n\n> Abstract:Machine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n\n## Submission history\n\nFrom: Jose Pablo Folch \\[ [view email](https://arxiv.org/show-email/5932e2ca/2506.07619)\\]\n\n**\\[v1\\]**\nMon, 9 Jun 2025 10:34:14 UTC (339 KB)",
      "url": "https://arxiv.org/abs/2506.07619"
    },
    {
      "title": "ACNP 63rd Annual Meeting: Poster Abstracts P609-P914 - PMC",
      "text": "ACNP 63rd Annual Meeting: Poster Abstracts P609-P914 - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1038/s41386-024-02013-y)\n* [](pdf/41386_2024_Article_2013.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Neuropsychopharmacology logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-nppharm.gif)\nNeuropsychopharmacology\n. 2024 Dec 5;49(Suppl 1):418\u2013594. doi:[10.1038/s41386-024-02013-y](https://doi.org/10.1038/s41386-024-02013-y)\n# ACNP 63rd Annual Meeting: Poster Abstracts P609-P914\n* Article notes\n* Copyright and License information\nIssue date 2024 Dec.\n\u00a9The Author(s), under exclusive licence to American College of Neuropsychopharmacology 2024\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC11627190\n**December 8-11, 2024**\n**Phoenix, Arizona**\n**Sponsorship Statement: Publication of this supplement is sponsored by the ACNP**.\nOnly disclosures for presenting authors are listed. Asterisks in the author lists indicate presenter of the abstract at the annual meeting.\nAbstract numbers do not correlate to poster number assigned for presentation at the Annual Meeting.\n## P609. Determining Optimal Parameters for Noninvasive Thalamic Neuromodulation With Trancranial Focused Ultrasound\n### Ryan Ash\\*, Patricia Limon, Martin Scott, Morteza Mohammadjavadi, Kim Butts Pauly, Anthony Norcia\n#### Stanford University School of Medicine, Stanford, California, United States\n**Background:**Clinical neuroscience has led to a revolution in our understanding of how different neural circuits contribute to phenotypes of neuropsychiatric conditions. Unfortunately, the tools to modulate these circuits in the human brain and thereby ameliorate symptoms are limited by poor focality and depth penetration. Transcranial ultrasound stimulation (TUS) is an emerging tool to achieve noninvasive focal brain-wide neuromodulation with high focality (\u2009&lt;\u20091cm) and the ability to achieve high intensities in-depth. This technology is at an early stage of development, and many key optimizations are needed to accelerate clinical trials in psychiatry. One key need is to better understand how different sonication parameters relate to neuromodulatory effects. Key parameters include the pulse repetition frequency (PRF), intensity, and duty cycle (DC). Importantly, by varying these parameters it may be possible to have either predominantly suppressive/inhibitory effects or facilitatory/excitatory effects on neural activity and synaptic strength. Previous work by our group and others in large animal models (e.g. Fry et al Science 1958, Mohammadjavadi, Ash et al., Scientific Reports 2022) have demonstrated a suppression of EEG visual-evoked potentials (VEPs) when TUS is targeted to the visual thalamus (lateral geniculate nucleus). We are therefore adapting this paradigm into human, as an efficient testbed to evaluate TUS effects.\n**Methods:**We implemented steady-state visual evoked potential (ssVEP) measures of contrast-response (Ash, Norcia Psychophysiology 2023) and contrast increment detection psychophysics as robust neural and behavioral readouts of subcortical visual pathway function. We developed a robust neuroimaging and simulation pipeline to target LGN, and we are using a neuronavigated depth-steerable 4-element TUS transducer.\n**Results:**Our preliminary data suggests that VEPs can be reversibly suppressed with TUS to the human LGN. We are parametrically varying PRF, intensity, and DC to determine which parameters lead to the most robust suppressive and/or facilitatory online and offline effects.\n**Conclusions:**This work provides the foundation for a dissection of the roles of subcortical and deep cortical nuclei in cognition, emotion, and sensory processing in health and disease.\n**Keywords:**focused ultrasound, Electroencephalography (EEG), Thalamus, noninvasive brain stimulation, Visual plasticity\n**Disclosure:**Nothing to disclose.\n## P610. Neurocomputational Mechanisms of Cognitive Flexibility and its Modulation via Deep Brain Stimulation of the Ventral Capsule/Ventral Striatum\n### Jaejoong Kim\\*, Alik Widge\n#### The University of Minnesota, St Paul, Minnesota, United States\n**Background:**Flexible decision-making requires efficiently integrating new, relevant information into ongoing mental processes to enable context-appropriate behavior. Impaired cognitive flexibility during decision-making is a transdiagnostic symptom across multiple mental illnesses, including OCD, depression, and autism spectrum disorder, and is associated with poor treatment outcomes. However, the mechanisms behind cognitive flexibility, especially how new information is integrated with ongoing contextual information to enable adaptive behavior, remain unclear. Given this incomplete understanding, an effective treatment strategy to enhance cognitive flexibility is still a long way off. Here, we investigated a neurocomputational mechanism for encoding both new task-related and old contextual information in high-frequency neural oscillations and how this information is integrated through cross-frequency coupling between multiple regions to support flexible decision-making. Furthermore, we demonstrated how these local and global neural mechanisms supporting cognitive flexibility can be effectively modulated through deep brain stimulation of the ventral capsule/ventral striatum (VCVS DBS), which has been shown to enhance decision efficiency in previous studies.\n**Methods:**Twenty-one epilepsy patients without psychiatric diagnoses (study 1) and fourteen patients with MDD\u2009+\u2009/- OCD (study 2) performed the Multi-Source Interference Task with and without VCVS stimulation while undergoing intracranial EEG (study 1) and scalp EEG (study 2) recordings. A novel drift-diffusion model decomposing cognitive control into proactive control (context computation) and flexible control (new information computation) was applied to behavioral data. To investigate local mechanisms of encoding and integrating new and contextual information in the non-DBS state, we performed model-based local field potential (LFP) analyses on high-frequency oscillations (\u03b2-band: 15-30 Hz, low \u03b3-band: 30-50 Hz, high \u03b3-band: 70-150 Hz) in brain regions including the bilateral dorsolateral prefrontal cortex (dlPFC), lateral orbitofrontal cortex (lOFC), hippocampus, rostral anterior cingulate cortex (rACC), and dorsal anterior cingulate cortex (dACC) using cluster-based per...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11627190"
    },
    {
      "title": "Science - C.Thang Nguyen",
      "text": "Science - C.Thang Nguyen\n[Skip to content](#science)\n# Science[&para;](#science)\n* May 21, 2025\n* in[Science](./),[ML](../ml/)\n* 16 min read\n## [AI for Crystal Materials - models and benchmarks](../../2025/05/21/ai-for-crystal-materials---models-and-benchmarks/)\n# AI for Crystal Materials\uff1a models and benchmarks\nHere we have collected papers with the theme of \"AI for crystalline materials\" that have appeared at top machine learning conferences and journals (ICML, ICLR, NeurIPS, AAAI, NPJ, NC, etc.) in recent years. See[https://arxiv.org/abs/2408.08044](https://arxiv.org/abs/2408.08044)for details. We will keep this page updated.\n### [Crystalline Material Physicochemical Property Prediction](../../2025/05/21/ai-for-crystal-materials---models-and-benchmarks/#crystalline-material-physicochemical-property-prediction)\n|Method|Paper|\nSchNet|Schnet: A continuous-filter convolutional neural network for modeling quantum interactions (NeurIPS2017)[**Paper**](https://proceedings.neurips.cc/paper/2017/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html)([https://github.com/atomistic-machine-learning/schnetpack](https://github.com/atomistic-machine-learning/schnetpack))]|\nCGCNN|Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties (Physical Review Letters, 2018)[**Paper**](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301)([https://github.com/txie-93/cgcnn](https://github.com/txie-93/cgcnn))]|\nMEGNET|Graph networks as a universal machine learning framework for molecules and crystals (Chemistry of Materials, 2019)[**Paper**](https://pubs.acs.org/doi/10.1021/acs.chemmater.9b01294)([https://github.com/materialsvirtuallab/megnet](https://github.com/materialsvirtuallab/megnet))]|\nGATGNN|Graph convolutional neural networks with global attention for improved materials property prediction (Physical Chemistry Chemical Physics, 2020)[**Paper**](https://pubs.rsc.org/en/content/articlelanding/2020/cp/d0cp01474e/unauth)([https://github.com/superlouis/GATGNN](https://github.com/superlouis/GATGNN))]|\nALIGNN|Atomistic line graph neural network for improved materials property predictions (npj Computational Materials, 2021)[**Paper**](https://www.nature.com/articles/s41524-021-00650-1)([https://github.com/usnistgov/alignn](https://github.com/usnistgov/alignn))]|\nE(3)NN|Direct prediction of phonon density of states with Euclidean neural networks (Advanced Science, 2021)[**Paper**](https://onlinelibrary.wiley.com/doi/full/10.1002/advs.202004214)([https://github.com/zhantaochen/phonondos\\_e3nn](https://github.com/zhantaochen/phonondos_e3nn))]|\nECN|Equivariant networks for crystal structures (NeurIPS2022)[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2022/hash/1abed6ee581b9ceb4e2ddf37822c7fcb-Abstract-Conference.html)([https://github.com/oumarkaba/equivariant\\_crystal\\_networks](https://github.com/oumarkaba/equivariant_crystal_networks))]|\nMatformer|Periodic Graph Transformers for Crystal Material Property Prediction (NeurIPS2022)[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2022/hash/6145c70a4a4bf353a31ac5496a72a72d-Abstract-Conference.html)([https://github.com/YKQ98/Matformer](https://github.com/YKQ98/Matformer))]|\nPotNet|Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction (ICML2023)[**Paper**](https://proceedings.mlr.press/v202/lin23m.html)([https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet](https://github.com/divelab/AIRS/tree/main/OpenMat/PotNet))]|\nCrysGNN|Crysgnn: Distilling pre-trained knowledge to enhance property prediction for crystalline materials (AAAI2023)[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/25892)([https://github.com/kdmsit/crysgnn](https://github.com/kdmsit/crysgnn))]|\nETGNN|A general tensor prediction framework based on graph neural networks (The Journal of Physical Chemistry Letters, 2023) [[**Paper**](https://pubs.acs.org/doi/abs/10.1021/acs.jpclett.3c01200)]|\nDOSTransformer|Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer (NeurIPS2023)[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c23fdcb9f8e28af705a87de1375a705c-Abstract-Conference.html)([https://github.com/HeewoongNoh/DOSTransformer](https://github.com/HeewoongNoh/DOSTransformer))]|\nMOFTransformer|A multi-modal pre-training transformer for universal transfer learning in metal-organic frameworks (Nature Machine Intelligence, 2023)[**Paper**](https://www.nature.com/articles/s42256-023-00628-2)([https://github.com/hspark1212/MOFTransformer](https://github.com/hspark1212/MOFTransformer))]|\n-|Examining graph neural networks for crystal structures: Limitations and opportunities for capturing periodicity (Science Advances, 2023)[**Paper**](https://www.science.org/doi/full/10.1126/sciadv.adi3245)([https://github.com/shenggong1996/examining-GNN-for-crystal-periodicity/tree/master](https://github.com/shenggong1996/examining-GNN-for-crystal-periodicity/tree/master))]|\nSCANN|Towards understanding structure\u2013property relations in materials with interpretable deep learning (npj Computational Materials, 2023)[**Paper**](https://www.nature.com/articles/s41524-023-01163-9?fromPaywallRec=false)([https://github.com/sinhvt3421/scann--material](https://github.com/sinhvt3421/scann--material))]|\nFAENet|FAENet: Frame Averaging Equivariant GNN for Materials Modeling (ICML2023) [[**Paper**](https://proceedings.mlr.press/v202/duval23a.html)]|\nCEGANN|CEGANN: Crystal Edge Graph Attention Neural Network for multiscale classification of materials environment (npj Computational Materials, 2023)[**Paper**](https://www.nature.com/articles/s41524-023-00975-z)([https://github.com/sbanik2/CEGANN](https://github.com/sbanik2/CEGANN))]|\nDTNet|Dielectric tensor prediction for inorganic materials using latent information from preferred potential (npj Computational Materials, 2024)[**Paper**](https://www.nature.com/articles/s41524-024-01450-z)([https://github.com/pfnet-research/dielectric-pred](https://github.com/pfnet-research/dielectric-pred))]|\nGMTNet|A Space Group Symmetry Informed Network for O(3) Equivariant Crystal Tensor Prediction (ICML2024)[**Paper**](https://openreview.net/forum?id=BOFjRnJ9mX)([https://github.com/divelab/AIRS/tree/main/OpenMat/GMTNet](https://github.com/divelab/AIRS/tree/main/OpenMat/GMTNet))]|\nComFormer|Complete and Efficient Graph Transformers for Crystal Material Property Prediction (ICLR2024)[**Paper**](https://openreview.net/forum?id=BnQY9XiRAS)([https://github.com/divelab/AIRS/tree/main/OpenMat/ComFormer](https://github.com/divelab/AIRS/tree/main/OpenMat/ComFormer))]|\nCrystalformer|Crystalformer: infinitely connected attention for periodic structure encoding (ICLR2024)[**Paper**](https://openreview.net/forum?id=fxQiecl9HB)([https://github.com/omron-sinicx/crystalformer](https://github.com/omron-sinicx/crystalformer))]|\nCrystalformer|Conformal Crystal Graph Transformer with Robust Encoding of Periodic Invariance (AAAI2024) [[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/27781)]|\nCrysDiff|A Diffusion-Based Pre-training Framework for Crystal Property Prediction (AAAI2024) [[**Paper**](https://ojs.aaai.org/index.php/AAAI/article/view/28748)]|\n-|Structure-aware graph neural network based deep transfer learning framework for enhanced predictive analytics on diverse materials datasets (npj Computational Materials, 2024) [[**Paper**](https://www.nature.com/articles/s41524-023-01185-3?fromPaywallRec=false)]|\nUni-MOF|A comprehensive transformer-based approach for high-accuracy gas adsorption predictions in metal-organic frameworks (Nature Communications, 2024)[**Paper**](https://www.nature.com/articles/s41467-024-46276-x)([https://github.com/dptech-corp/Uni-MOF](https://github.com/dptech-corp/Uni-MOF))]|\nSODNet|Learning Superconductivity from Ordered and Disordered Material Structures (NeurIPS2024)[**Paper**](https://openreview.net/forum?id=iNYrB3ip9F#discussion)([https://gi...",
      "url": "https://thangckt.github.io/blog/category/science"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    },
    {
      "title": "LLM Benchmarks Explained: A Guide to Comparing the Best AI Models",
      "text": "LLM Benchmarks Explained: A Guide to Comparing the Best AI Models | DataCamp\n[Skip to main content](#main)\n# LLM Benchmarks Explained: A Guide to Comparing the Best AI Models\nCut through the hype. Learn to interpret LLM benchmarks, navigate open leaderboards, and run your own evaluations to find the best AI models for your needs.\nListContents\nDec 28, 2025\nContents\n## GroupTraining more people?\nGet your team access to the full DataCamp for business platform.\nNew AI models drop almost weekly:[Gemini 3](https://www.datacamp.com/blog/gemini-3),[Claude Opus 4.5](https://www.datacamp.com/blog/claude-opus-4-5),[GPT-5.2](https://www.datacamp.com/blog/gpt-5-2),[Mistral Large 3](https://www.datacamp.com/blog/mistral-3). Each release comes with benchmark scores and claims about being the best at something. The problem: most people have no idea what these numbers mean or how to compare them.\nLarge Language Model (LLM) benchmarks are standardized tests that measure how well models perform on specific tasks, from broad knowledge quizzes to complex coding challenges and multi-step reasoning problems. Understanding what each benchmark measures helps you cut through the marketing and pick the right model for your actual needs.\nThis guide breaks down the major benchmark categories, explains where to find current rankings, and shows you how to run your own evaluations. By the end, you'll know how to read a leaderboard and choose the AI that fits your use case.\nFor a deeper dive into how LLMs work under the hood, check out our[LLMs Concepts](https://www.datacamp.com/courses/large-language-models-llms-concepts)course.\n## What Is an LLM Benchmark?\nAn LLM benchmark is a standardized test that measures how well a language model handles a specific type of task. The same questions and scoring rubric are applied to every model that takes the test.\nThe numbers in model announcements come from a handful of popular tests. Each score tells a different story, and no single benchmark captures the full picture.\n### Why LLM benchmarks matter\nBenchmarks are important for three reasons:\n* **Comparing models:**When OpenAI drops GPT-5.2 and Anthropic releases Claude Opus 4.5 in the same month, benchmarks give us common ground. Otherwise, we're stuck with each company claiming victory based on cherry-picked examples.\n* **Tracking progress:**Run the same benchmark over time, and you can see whether models are actually getting better.[MMLU](https://www.datacamp.com/blog/what-is-mmlu)scores jumped from 70% in 2022 to over 90% in 2025.\n* **Spotting gaps:**A model might crush general knowledge questions but choke on multi-step math. Benchmarks surface these weaknesses.\n### Factors influencing LLM benchmark scores\nBenchmark scores reflect more than raw intelligence. Multiple factors shape the numbers you see on leaderboards.\nModel size is the obvious one.Parametersstore everything a model learns, and frontier models pack hundreds of billions of them. More parameters mean the model can handle more complex reasoning and hold more nuance, which pushes scores up.\nThe trade-off shows up duringinference, when the model actually generates responses: all those parameters need to fire in sequence, so bigger models are slower. A model might top every benchmark but take several seconds to answer.\n![LLM benchmark scores vs inference speed trade-off showing how larger models score higher but respond slower](https://media.datacamp.com/cms/1-c633b7.jpg)\nTraining duration is trickier. Each pass through the training data is called anepoch. Too few, and the model hasn't absorbed enough to score well. Too many and it starts memorizing examples instead of learning patterns that transfer to new questions. That's[overfitting](https://www.datacamp.com/blog/what-is-overfitting), and benchmark designers specifically try to catch it by including questions the model couldn't have seen during training.\nWith dozens of benchmarks in use today, it helps to group them by what they actually test.\n## LLM Benchmarks by Category\nBenchmarks cluster into a rough hierarchy. At the base, knowledge tests check what a model knows. Above that, reasoning benchmarks probe how well it thinks. At the top sit agentic and multimodal tests that measure whether AI can act in the real world or process information beyond text.\n![LLM benchmark hierarchy pyramid showing progression from knowledge to reasoning to coding and agentic to multimodal benchmarks](https://media.datacamp.com/cms/a5c5d815b3632c3d99143e168ad3a79b.png)\n### Knowledge and reasoning benchmarks\n#### MMLU\n[MMLU](https://www.datacamp.com/blog/what-is-mmlu)(Massive Multitask Language Understanding) covers 57 academic subjects from high school to professional level, spanning everything from abstract algebra to world religions. For years, it served as the go-to test for general knowledge, but frontier models now cluster above 88%, leaving little room to tell them apart.\n#### GPQA\nThat saturation pushed researchers toward harder tests.GPQA(Graduate-level Google-Proof Q&amp;A) asks 448 questions in biology, physics, and chemistry that domain experts designed to be unsearchable.\nThe benchmark has three difficulty tiers, with Diamond containing the hardest questions. Even with unlimited web access, non-experts score just 34%\u2014only 9% above the result you would expect from random guessing with four answer options. As of December 2025,[Gemini 3 Pro](https://www.datacamp.com/blog/gemini-3)leads GPQA Diamond at 92.6%.\n#### GDPVal\nOpenAI'sGDPval(Gross Domestic Product-valued) benchmark measures something different: real-world work output. It covers 44 occupations across sectors worth $3 trillion in annual economic activity, asking models to produce deliverables like legal briefs, slide decks, and engineering specs rather than answer multiple-choice questions. The recently released[GPT-5.2](https://www.datacamp.com/blog/gpt-5-2)is the leader in this regard.\n#### HellaSwag\nHellaSwagprobes common-sense reasoning by presenting everyday scenarios and asking models to pick the most plausible next sentence. A person cooking dinner reaches for a pan. What happens next?\nThe wrong answers were written specifically to fool AI: they use words that statistically fit the context but describe impossible outcomes (the pan floats away, the stove turns into a cat). Humans score 95.6% because we know how kitchens work. Models get tricked because they predict likely words, not likely events.\n#### Newer benchmarks\nThe newest benchmarks push difficulty further:\n* **FrontierMath**features never-before-published problems from research mathematicians, where even the best models score below 20%.\n* **[Humanity's Last Exam](https://www.datacamp.com/blog/what-is-humanitys-last-exam-ai-benchmark)**compiles 2,500 expert-level questions designed to resist guessing.\n* **MathArena**pulls problems from 2025 math competitions to guarantee zero training data overlap.\n### Coding and agentic benchmarks\n#### HumanEval\n[HumanEval](https://www.datacamp.com/tutorial/humaneval-benchmark-for-evaluating-llm-code-generation-capabilities)is the classic coding test: It contains 164 Python problems where models write functions from docstrings and are graded on whether the code passes unit tests. Most current frontier models score above 85%, so researchers created more difficult variants likeHumanEval+with more rigorous test cases.\n#### SWE-bench\nSWE-bench(Software Engineering Benchmark) moves beyond isolated functions. It drops models into real GitHub repositories and asks them to fix actual bugs. The model must navigate the codebase, understand the issue, and produce a working patch.\nSWE-bench Verifiedis a smaller, highly curated subset of the original SWE-bench, which filters for high-quality tasks vetted by human engineers. As of December 2025, Claude Opus 4.5 is the first model to break 80% in SWE-bench Verified (80.9%).\n#### GAIA\nGAIA(General AI Assistants) inverts the usual difficulty relationship. Its 466 tasks are trivially easy for hu...",
      "url": "https://datacamp.com/tutorial/llm-benchmarks"
    }
  ]
}