{
  "query": "What is the catechol_hackathon_metric evaluation metric in the Catechol Benchmark Hackathon Kaggle competition?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The provided sources do not specify what the\u202fcatechol_hackathon_metric\u202fevaluation metric is for the Catechol Benchmark Hackathon competition. ([openreview.net](https://openreview.net/pdf?id=6l8q74TabE))",
      "url": ""
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    },
    {
      "title": "Competitions - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/docs/competitions-setup"
    },
    {
      "title": "Checking your browser - reCAPTCHA",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/benchmarks"
    },
    {
      "title": "Comprehensive Guide to Ranking Evaluation Metrics",
      "text": "Comprehensive Guide to Ranking Evaluation Metrics | Towards Data Science\n[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)\nPublish AI, ML &amp; data-science insights to a global community of data professionals.\nSign in\n[Submit an Article](https://contributor.insightmediagroup.io/)\n* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)\n* [X](https://x.com/TDataScience)\nToggle Search\nSearch\n[Data Science](https://towardsdatascience.com/category/data-science/)\n# Comprehensive Guide to Ranking Evaluation Metrics\nExplore an abundant choice of metrics and find the best one for your problem\n[Vyacheslav Efimov](https://towardsdatascience.com/author/slavahead/)\nJul 29, 2023\n15 min read\nShare\n![](https://towardsdatascience.com/wp-content/uploads/2023/07/1Douc9pgrtzDWVKxYoZjubQ.png)## Introduction\nRanking is a problem in machine learning where the objective is to sort a list of documents for an end user in the most suitable way, so the most relevant documents appear on top. Ranking appears in several domains of data science, starting from recommender systems where an algorithm suggests a set of items for purchase and ending up with NLP search engines where by a given query, the system tries to return the most relevant search results.\nThe question which arises naturally is how to estimate the quality of a ranking algorithm. As in classical machine learning, there does not exist a single universal metric that would be suitable for any type of task. Why? Simply because every metric has its own application scope which depends on the nature of a given problem and data characteristics.\nThat is why it is crucial to be aware of all the main metrics to successfully tackle any machine learning problem. This is exactly what we are going to do in this article.\nNevertheless, before going ahead let us understand why certain popular metrics should not be normally used for ranking evaluation. By taking this information into consideration, it will be easier to understand the necessity of the existence of other, more sophisticated metrics.\n*Note*. The article and used formulas are based on the presentation on[offline evaluation from Ilya Markov](https://drive.google.com/drive/folders/19OfEsLME1IR7bGPzVo8Dh31emcPNiaP_).\n## Metrics\nThere are several types of information retrieval metrics that we are going to discuss in this article:\n![Different types of metrics](https://towardsdatascience.com/wp-content/uploads/2023/07/1jiIIeM0t15l7FEMbTJl1qg.png)Different types of metrics## Unranked metrics\nImagine a recommender system predicting ratings of movies and showing the most relevant films to users. Rating usually represents a positive real number. At first sight, a regression metric like*MSE*(*RMSE, MAE*, etc.) seems a reasonable choice to evaluate the quality of the system on a hold-out dataset.\n*MSE*takes all the predicted films into consideration and measures the average square error between true and predicted labels. However, end users are usually interested only in the top results which appear on the first page of a website. This indicates that they are not really interested in films with lower ratings appearing at the end of the search result which are also equally estimated by standard regression metrics.\nA simple example below demonstrates a pair of search results and measures the*MSE*value in each of them.\n![Error estimation for both queries shows that MSE is a bad metric for ranking. Green documents are relevant while red documents are irrelevant. The list of documents is shown in the order of predicted relevance (from left to right).](https://towardsdatascience.com/wp-content/uploads/2023/07/1Tb7Y7DxnpzOGbj2HKTIIKw.png)Error estimation for both queries shows that MSE is a bad metric for ranking. Green documents are relevant while red documents are irrelevant. The list of documents is shown in the order of predicted relevance (from left to right).\nThough the second search result has a lower*MSE*, the user will not be satisfied with such a recommendation. By first looking only at non-relevant items, the user will have to scroll up all the way down to find the first relevant item. That is why from the user experience perspective, the first search result is much better: the user is just happy with the top item and proceeds to it while not caring about others.\nThe same logic goes with classification metrics (*precision*,*recall*) which consider all items as well.\n![Precision and recall formulas](https://towardsdatascience.com/wp-content/uploads/2023/07/1O43GGH6t-YDQ8TSjm0bfsA.png)Precision and recall formulas\nWhat do all of described metrics have in common? All of them treat all items equally and do not consider any differentiation between high and low-relevant results. That is why they are called**unranked**.\nBy having gone through these two similar problematic examples above, the aspect we should focus on while designing a ranking metric seems more clear:\n> A ranking metric should put more weight on more relevant results while lowering or ignoring the less relevant ones.\n## Ranked metrics\n### Kendall Tau distance\n[Kendall Tau distance](<https://en.wikipedia.org/wiki/Kendall_tau_distance#:~:text=The Kendall tau rank distance,dissimilar the two lists are.>)is based on the number of rank inversions.\n> An **> invertion\n**> is a pair of documents (i, j) such as document i having a greater relevance than document j, appears after on the search result than j.\nKendall Tau distance calculates all the number of inversions in the ranking. The lower the number of inversions, the better the search result is. Though the metric might look logical, it still has a downside which is demonstrated in the example below.\n![Despite fewer number of inversions, the second ranking is still worse, from the user perspective](https://towardsdatascience.com/wp-content/uploads/2023/07/1rLRCq6otud_HZObB5MhrxQ.png)Despite fewer number of inversions, the second ranking is still worse, from the user perspective\nIt seems like the second search result is better with only 8 inversions versus 9 in the first one. Similarly to the*MSE*example above, the user is only interested in the first relevant result. By going through several non-relevant search results in the second case, the user experience will be worse than in the first case.\n### Precision@k &amp; Recall@k\nInstead of usual*precision*and*recall*, it is possible to consider only at a certain number of top recommendations*k*. This way, the metric does not care about low-ranked results. Depending on the chosen value of*k*, the corresponding metrics are denoted as*precision@k*(*&quot;precision at k&quot;*) and*recall@k*(*&quot;recall at k&quot;*) respectively. Their formulas are shown below.\n![precision@k and recall@k formulas](https://towardsdatascience.com/wp-content/uploads/2023/07/1PkmeKXkH6-Unyph3IJoiNQ.png)precision@k and recall@k formulas\nImagine top*k*results are shown to the user where each result can be relevant or not.*precision@k*measures the percentage of relevant results among top*k*results. At the same time,*recall@k*evaluates the ratio of relevant results among top*k*to the total number of relevant items in the whole dataset.\nTo better understand the calculation process of these metrics, let us refer to the example below.\n![precision@k and recall@k calculation example. Green documents represent relevant items while the red ones correspond to irrelevant ones.](https://towardsdatascience.com/wp-content/uploads/2023/07/13GupAUTjS1YFDWGsgZBZEA.png)precision@k and recall@k calculation example. Green documents represent relevant items while the red ones correspond to irrelevant ones.\nThere are 7 documents in the system (named from*A*to*G*). Based on its predictions, the algorithm chooses*k = 5*documents among them for the user. As we can notice, there are 3 relevant documents*(A, C, G)*among top*k = 5*which results in*precision@5*b...",
      "url": "https://towardsdatascience.com/comprehensive-guide-to-ranking-evaluation-metrics-7d10382c1025"
    }
  ]
}