{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c17d11",
   "metadata": {},
   "source": [
    "# Expanded Ensemble: MLP + Trees + LightGBM + XGBoost\n",
    "\n",
    "Based on strategy recommendations:\n",
    "1. Add LightGBM and XGBoost for more diversity\n",
    "2. Increase MLP bagging (5 models) and epochs (250)\n",
    "3. Use weighted ensemble (optimize weights)\n",
    "\n",
    "Target: Beat CV 0.0103 (current best) to improve LB from 0.0949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd772b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:48.386461Z",
     "iopub.status.busy": "2026-01-13T23:09:48.385904Z",
     "iopub.status.idle": "2026-01-13T23:09:49.766550Z",
     "shell.execute_reply": "2026-01-13T23:09:49.766076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from abc import ABC, abstractmethod\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf6c8af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:49.767949Z",
     "iopub.status.busy": "2026-01-13T23:09:49.767769Z",
     "iopub.status.idle": "2026-01-13T23:09:49.777263Z",
     "shell.execute_reply": "2026-01-13T23:09:49.776845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "# Data loading utilities\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "# Load feature lookup tables\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "ACS_PCA_DF = load_features('acs_pca_descriptors')\n",
    "print(f\"Spange: {SPANGE_DF.shape}, ACS PCA: {ACS_PCA_DF.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1846c248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:49.778213Z",
     "iopub.status.busy": "2026-01-13T23:09:49.778093Z",
     "iopub.status.idle": "2026-01-13T23:09:49.780970Z",
     "shell.execute_reply": "2026-01-13T23:09:49.780542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Base class\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train_model(self, X_train, y_train):\n",
    "        raise NotImplementedError\n",
    "    def predict(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b1712b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:49.781956Z",
     "iopub.status.busy": "2026-01-13T23:09:49.781845Z",
     "iopub.status.idle": "2026-01-13T23:09:49.795054Z",
     "shell.execute_reply": "2026-01-13T23:09:49.794626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP component defined (5 models, 250 epochs)\n"
     ]
    }
   ],
   "source": [
    "# ============ MLP COMPONENT (Enhanced) ============\n",
    "class KineticMixingFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.featurizer = SPANGE_DF\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2 + 3\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        \n",
    "        X_kinetic = torch.tensor(np.hstack([X_vals, inv_temp, log_time, interaction]))\n",
    "        \n",
    "        if self.mixed:\n",
    "            A = torch.tensor(self.featurizer.loc[X[\"SOLVENT A NAME\"]].values)\n",
    "            B = torch.tensor(self.featurizer.loc[X[\"SOLVENT B NAME\"]].values)\n",
    "            pct = torch.tensor(X[\"SolventB%\"].values.reshape(-1, 1))\n",
    "            if flip:\n",
    "                X_chem = B * (1 - (1-pct)) + A * (1-pct)\n",
    "            else:\n",
    "                X_chem = A * (1 - pct) + B * pct\n",
    "        else:\n",
    "            X_chem = torch.tensor(self.featurizer.loc[X[\"SOLVENT NAME\"]].values)\n",
    "            \n",
    "        return torch.cat([X_kinetic, X_chem], dim=1)\n",
    "\n",
    "class MLPInternal(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPInternal, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MLPWithArrhenius(nn.Module):\n",
    "    def __init__(self, data='single', n_models=5, epochs=250):  # Increased from 3/200\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        self.featurizer = KineticMixingFeaturizer(mixed=(data=='full'))\n",
    "        self.n_models = n_models\n",
    "        self.epochs = epochs\n",
    "        self.models = nn.ModuleList()\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "            y_all = y_vals\n",
    "            \n",
    "        input_dim = X_all.shape[1]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            model = MLPInternal(input_dim).to(device)\n",
    "            model.train()\n",
    "            self.models.append(model)\n",
    "            \n",
    "            dataset = TensorDataset(X_all, y_all)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "            criterion = nn.HuberLoss()\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                scheduler.step(epoch_loss / len(dataset))\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.models[0].parameters()).device\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_std = self.featurizer.featurize(X, flip=False).to(device)\n",
    "            X_flip = self.featurizer.featurize(X, flip=True).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    p1 = model(X_std)\n",
    "                    p2 = model(X_flip)\n",
    "                    pred_sum += (p1 + p2) * 0.5\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        else:\n",
    "            X_std = self.featurizer.featurize(X).to(device)\n",
    "            pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    model.eval()\n",
    "                    pred_sum += model(X_std)\n",
    "            avg_pred = pred_sum / self.n_models\n",
    "        return avg_pred.cpu()\n",
    "\n",
    "print(\"MLP component defined (5 models, 250 epochs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "199648fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:49.796184Z",
     "iopub.status.busy": "2026-01-13T23:09:49.796069Z",
     "iopub.status.idle": "2026-01-13T23:09:50.453021Z",
     "shell.execute_reply": "2026-01-13T23:09:50.452574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeFeaturizer defined\n"
     ]
    }
   ],
   "source": [
    "# ============ TREE COMPONENTS ============\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "class TreeFeaturizer:\n",
    "    \"\"\"Shared featurizer for tree-based models.\"\"\"\n",
    "    def __init__(self, data_type='single'):\n",
    "        self.data_type = data_type\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        \n",
    "    def create_features(self, X, flip=False):\n",
    "        time_m = X[\"Residence Time\"].values.reshape(-1, 1)\n",
    "        temp_c = X[\"Temperature\"].values.reshape(-1, 1)\n",
    "        \n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                spange_A = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "                spange_B = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "                acs_A = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "                acs_B = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "                pct_use = 1 - pct\n",
    "            else:\n",
    "                spange_A = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "                spange_B = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "                acs_A = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "                acs_B = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "                pct_use = pct\n",
    "            spange_feat = spange_A * (1 - pct_use) + spange_B * pct_use\n",
    "            acs_feat = acs_A * (1 - pct_use) + acs_B * pct_use\n",
    "        else:\n",
    "            spange_feat = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            acs_feat = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        features = np.hstack([time_m, temp_c, inv_temp, log_time, interaction, spange_feat, acs_feat])\n",
    "        return features\n",
    "\n",
    "print(\"TreeFeaturizer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc3ba4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:50.454221Z",
     "iopub.status.busy": "2026-01-13T23:09:50.454046Z",
     "iopub.status.idle": "2026-01-13T23:09:50.460225Z",
     "shell.execute_reply": "2026-01-13T23:09:50.459794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM component defined\n"
     ]
    }
   ],
   "source": [
    "# ============ LIGHTGBM COMPONENT ============\n",
    "class LightGBMModel(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        self.featurizer = TreeFeaturizer(data_type=data)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_feat = self.featurizer.create_features(X_train, flip=False)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_train, flip=True)\n",
    "            X_feat = np.vstack([X_feat, X_flip])\n",
    "            y_train_aug = pd.concat([y_train, y_train], ignore_index=True)\n",
    "        else:\n",
    "            y_train_aug = y_train\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        for target in TARGET_LABELS:\n",
    "            self.models[target] = lgb.LGBMRegressor(\n",
    "                n_estimators=500, learning_rate=0.03, max_depth=6,\n",
    "                num_leaves=31, random_state=42, verbose=-1, n_jobs=-1\n",
    "            )\n",
    "            self.models[target].fit(X_scaled, y_train_aug[target].values)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self.featurizer.create_features(X_test, flip=False)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = self.models[target].predict(X_scaled)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_test, flip=True)\n",
    "            X_flip_scaled = self.scaler.transform(X_flip)\n",
    "            for target in TARGET_LABELS:\n",
    "                preds[target] = (preds[target] + self.models[target].predict(X_flip_scaled)) / 2\n",
    "        \n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = np.clip(preds[target], 0, 1)\n",
    "        \n",
    "        return torch.tensor(np.column_stack([preds['Product 2'], preds['Product 3'], preds['SM']]))\n",
    "\n",
    "print(\"LightGBM component defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d27cf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:50.461281Z",
     "iopub.status.busy": "2026-01-13T23:09:50.461165Z",
     "iopub.status.idle": "2026-01-13T23:09:50.467204Z",
     "shell.execute_reply": "2026-01-13T23:09:50.466778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost component defined\n"
     ]
    }
   ],
   "source": [
    "# ============ XGBOOST COMPONENT ============\n",
    "class XGBoostModel(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        self.featurizer = TreeFeaturizer(data_type=data)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_feat = self.featurizer.create_features(X_train, flip=False)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_train, flip=True)\n",
    "            X_feat = np.vstack([X_feat, X_flip])\n",
    "            y_train_aug = pd.concat([y_train, y_train], ignore_index=True)\n",
    "        else:\n",
    "            y_train_aug = y_train\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        for target in TARGET_LABELS:\n",
    "            self.models[target] = xgb.XGBRegressor(\n",
    "                n_estimators=500, learning_rate=0.03, max_depth=6,\n",
    "                random_state=42, verbosity=0, n_jobs=-1\n",
    "            )\n",
    "            self.models[target].fit(X_scaled, y_train_aug[target].values)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self.featurizer.create_features(X_test, flip=False)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = self.models[target].predict(X_scaled)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_test, flip=True)\n",
    "            X_flip_scaled = self.scaler.transform(X_flip)\n",
    "            for target in TARGET_LABELS:\n",
    "                preds[target] = (preds[target] + self.models[target].predict(X_flip_scaled)) / 2\n",
    "        \n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = np.clip(preds[target], 0, 1)\n",
    "        \n",
    "        return torch.tensor(np.column_stack([preds['Product 2'], preds['Product 3'], preds['SM']]))\n",
    "\n",
    "print(\"XGBoost component defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44aa423a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:50.468192Z",
     "iopub.status.busy": "2026-01-13T23:09:50.468074Z",
     "iopub.status.idle": "2026-01-13T23:09:50.474570Z",
     "shell.execute_reply": "2026-01-13T23:09:50.474140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Trees component defined\n"
     ]
    }
   ],
   "source": [
    "# ============ SKLEARN TREES COMPONENT ============\n",
    "class SklearnTreesModel(BaseModel):\n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        self.featurizer = TreeFeaturizer(data_type=data)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        \n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_feat = self.featurizer.create_features(X_train, flip=False)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_train, flip=True)\n",
    "            X_feat = np.vstack([X_feat, X_flip])\n",
    "            y_train_aug = pd.concat([y_train, y_train], ignore_index=True)\n",
    "        else:\n",
    "            y_train_aug = y_train\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # SM: HistGradientBoosting (better for smooth targets)\n",
    "        self.models['SM'] = HistGradientBoostingRegressor(\n",
    "            max_depth=7, max_iter=700, learning_rate=0.04, random_state=42\n",
    "        )\n",
    "        self.models['SM'].fit(X_scaled, y_train_aug['SM'].values)\n",
    "        \n",
    "        # Products: ExtraTrees\n",
    "        self.models['Product 2'] = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "        self.models['Product 2'].fit(X_scaled, y_train_aug['Product 2'].values)\n",
    "        \n",
    "        self.models['Product 3'] = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "        self.models['Product 3'].fit(X_scaled, y_train_aug['Product 3'].values)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self.featurizer.create_features(X_test, flip=False)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        preds = {}\n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = self.models[target].predict(X_scaled)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.create_features(X_test, flip=True)\n",
    "            X_flip_scaled = self.scaler.transform(X_flip)\n",
    "            for target in TARGET_LABELS:\n",
    "                preds[target] = (preds[target] + self.models[target].predict(X_flip_scaled)) / 2\n",
    "        \n",
    "        for target in TARGET_LABELS:\n",
    "            preds[target] = np.clip(preds[target], 0, 1)\n",
    "        \n",
    "        return torch.tensor(np.column_stack([preds['Product 2'], preds['Product 3'], preds['SM']]))\n",
    "\n",
    "print(\"Sklearn Trees component defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddcf939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:50.475474Z",
     "iopub.status.busy": "2026-01-13T23:09:50.475360Z",
     "iopub.status.idle": "2026-01-13T23:09:50.480236Z",
     "shell.execute_reply": "2026-01-13T23:09:50.479759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExpandedEnsembleModel defined (4 models)\n"
     ]
    }
   ],
   "source": [
    "# ============ EXPANDED ENSEMBLE ============\n",
    "class ExpandedEnsembleModel(BaseModel):\n",
    "    \"\"\"4-model ensemble: MLP + LightGBM + XGBoost + SklearnTrees\"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', weights=None):\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        # Default weights: equal for all 4 models\n",
    "        self.weights = weights if weights else [0.25, 0.25, 0.25, 0.25]\n",
    "        \n",
    "        # Initialize components\n",
    "        self.mlp = MLPWithArrhenius(data=data, n_models=5, epochs=250)\n",
    "        self.lgbm = LightGBMModel(data=data)\n",
    "        self.xgb = XGBoostModel(data=data)\n",
    "        self.trees = SklearnTreesModel(data=data)\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"Train all 4 components.\"\"\"\n",
    "        self.mlp.train_model(X_train, y_train)\n",
    "        self.lgbm.train_model(X_train, y_train)\n",
    "        self.xgb.train_model(X_train, y_train)\n",
    "        self.trees.train_model(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Weighted average of all 4 models.\"\"\"\n",
    "        pred_mlp = self.mlp.predict(X_test)\n",
    "        pred_lgbm = self.lgbm.predict(X_test)\n",
    "        pred_xgb = self.xgb.predict(X_test)\n",
    "        pred_trees = self.trees.predict(X_test)\n",
    "        \n",
    "        # Weighted average\n",
    "        combined = (self.weights[0] * pred_mlp + \n",
    "                    self.weights[1] * pred_lgbm + \n",
    "                    self.weights[2] * pred_xgb + \n",
    "                    self.weights[3] * pred_trees)\n",
    "        \n",
    "        return torch.clamp(combined, 0, 1)\n",
    "\n",
    "print(\"ExpandedEnsembleModel defined (4 models)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36f3ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:09:50.481133Z",
     "iopub.status.busy": "2026-01-13T23:09:50.481020Z",
     "iopub.status.idle": "2026-01-13T23:10:44.978625Z",
     "shell.execute_reply": "2026-01-13T23:10:44.978141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing expanded ensemble...\n",
      "Single solvent: (656, 3), (656, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([37, 3])\n",
      "Sample predictions: tensor([[0.0110, 0.0146, 0.8716],\n",
      "        [0.0200, 0.0152, 0.8811],\n",
      "        [0.0446, 0.0439, 0.7923]])\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "print(\"Testing expanded ensemble...\")\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Single solvent: {X.shape}, {Y.shape}\")\n",
    "\n",
    "# Test on first fold\n",
    "split_gen = generate_leave_one_out_splits(X, Y)\n",
    "(train_X, train_Y), (test_X, test_Y) = next(split_gen)\n",
    "\n",
    "model = ExpandedEnsembleModel(data='single')\n",
    "model.train_model(train_X, train_Y)\n",
    "preds = model.predict(test_X)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Sample predictions: {preds[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed356ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:11:18.935513Z",
     "iopub.status.busy": "2026-01-13T23:11:18.934830Z",
     "iopub.status.idle": "2026-01-13T23:33:11.973794Z",
     "shell.execute_reply": "2026-01-13T23:33:11.973325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:55<21:08, 55.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [01:49<19:59, 54.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [02:41<18:39, 53.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [03:33<17:35, 52.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [04:27<16:55, 53.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [05:22<16:08, 53.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [06:16<15:17, 53.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [07:10<14:26, 54.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [08:05<13:34, 54.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [09:00<12:42, 54.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [09:55<11:50, 54.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [10:50<10:55, 54.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [11:44<10:00, 54.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [12:39<09:06, 54.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [13:34<08:12, 54.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [14:29<07:19, 54.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [15:27<06:30, 55.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [16:22<05:34, 55.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [17:17<04:36, 55.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [18:11<03:40, 55.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [19:06<02:44, 54.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [20:03<01:51, 55.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [20:57<00:55, 55.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [21:53<00:00, 55.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [21:53<00:00, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Solvent CV MSE: 0.010684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExpandedEnsembleModel(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    all_actuals.append(test_Y.values)\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Calculate CV score\n",
    "all_actuals_np = np.vstack(all_actuals)\n",
    "all_preds_np = np.array([[p['target_1'], p['target_2'], p['target_3']] for p in all_predictions])\n",
    "single_mse = np.mean((all_actuals_np - all_preds_np) ** 2)\n",
    "print(f\"\\nSingle Solvent CV MSE: {single_mse:.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions_full = []\n",
    "all_actuals_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = ExpandedEnsembleModel(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    all_actuals_full.append(test_Y.values)\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions_full.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions_full)\n",
    "\n",
    "# Calculate CV score\n",
    "all_actuals_full_np = np.vstack(all_actuals_full)\n",
    "all_preds_full_np = np.array([[p['target_1'], p['target_2'], p['target_3']] for p in all_predictions_full])\n",
    "full_mse = np.mean((all_actuals_full_np - all_preds_full_np) ** 2)\n",
    "print(f\"\\nFull Data CV MSE: {full_mse:.6f}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "# Calculate overall CV score\n",
    "total_samples = len(all_actuals_np) + len(all_actuals_full_np)\n",
    "overall_mse = (single_mse * len(all_actuals_np) + full_mse * len(all_actuals_full_np)) / total_samples\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Single Solvent MSE: {single_mse:.6f}\")\n",
    "print(f\"Full Data MSE: {full_mse:.6f}\")\n",
    "print(f\"Overall CV MSE: {overall_mse:.6f}\")\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
