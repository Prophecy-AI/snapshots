{
  "phase": "executor",
  "loop_count": 1,
  "last_submission": null,
  "competition_id": "catechol-benchmark-hackathon",
  "seed_prompt": "# Catechol Reaction Yield Prediction - Techniques Guide\n\n## CRITICAL: Submission Structure Requirements\n**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.\n- The last three cells MUST remain unchanged except for the model definition line\n- `model = MLPModel()` can be replaced with a new model definition\n- Both single solvent and full (mixed) solvent tasks must be handled\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target statistics, CV structure\n\n**Key Data Facts:**\n- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)\n- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)\n- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)\n- **IMPORTANT:** Targets do NOT sum to 1 (mean ~0.8) - there is mass loss in the reaction\n- Temperature range: 175-225\u00b0C, Residence Time: ~2-15 minutes\n\n## Feature Engineering (Physics-Informed)\n\n### 1. Arrhenius Kinetics Features (HIGHLY RECOMMENDED)\nChemical reactions follow Arrhenius kinetics. Transform features accordingly:\n- `inv_temp = 1000 / (Temperature + 273.15)` - inverse temperature in Kelvin\n- `log_time = log(Residence Time)` - logarithm of time\n- `interaction = inv_temp * log_time` - interaction term\n\nThis physics-informed approach significantly improves predictions.\n\n### 2. Solvent Featurization\nAvailable pre-computed features (use via lookup tables):\n- **spange_descriptors** (13 features): Most commonly used, includes dielectric constant, ET(30), alpha, beta, pi*, etc.\n- **acs_pca_descriptors** (5 features): PCA-reduced green chemistry descriptors\n- **drfps_catechol** (2048 features): Differential reaction fingerprints - high dimensional\n- **fragprints** (2133 features): Fragment + fingerprint concatenation - very high dimensional\n\n**For mixed solvents:** Use weighted average of solvent A and B features based on SolventB%:\n```\nX_feat = A_feat * (1 - SolventB%) + B_feat * SolventB%\n```\n\n### 3. Additional Feature Ideas\n- `Reaction_Energy = Temperature * Residence Time`\n- `B_Conc_Temp = SolventB% * Temperature`\n\n## Model Architectures\n\n### 1. MLP with BatchNorm (Baseline++)\nArchitecture that works well:\n- Input BatchNorm\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Output: 3 neurons with Sigmoid activation (yields are 0-1)\n- Loss: MSELoss or HuberLoss (more robust to outliers)\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Gradient clipping: max_norm=1.0\n- Epochs: 300\n\n### 2. LightGBM Ensemble\nAlternative approach using gradient boosting:\n- Train 3 separate LightGBM regressors (one per target)\n- learning_rate=0.03, max_depth=6\n- Early stopping with 100 rounds patience\n- Use internal validation split (~12%) for early stopping\n\n### 3. Gaussian Processes\nFor small datasets like this, GPs can provide:\n- Good uncertainty quantification\n- Strong performance with limited data (~600-1200 samples is ideal for GPs)\n- Consider Deep Kernel Learning (DKL) to combine NN feature learning with GP uncertainty\n- GPs excel at extrapolation to unseen solvents (the core challenge here)\n\n### 4. Multi-Output Regression Strategies\nSince we predict 3 correlated targets (SM, Product 2, Product 3):\n\n**Regressor Chains:** Feed prediction of one yield as input to the next\n- Can capture correlations between outputs\n- Order matters: try SM \u2192 Product 2 \u2192 Product 3 (or optimize order)\n\n**Joint Multi-Output Models:**\n- Single model predicting all 3 outputs simultaneously\n- Neural networks naturally handle this with 3-output layer\n- Tree-based: use MultiOutputRegressor or native multi-output support\n\n**Curds & Whey Post-Processing:**\n- Linear shrinkage method that decorrelates responses\n- Can improve predictions when outputs are correlated\n\n## Ensemble and Augmentation Strategies\n\n### 1. Model Bagging\nTrain multiple models (e.g., 5-7) and average predictions:\n- Reduces variance\n- More robust predictions\n\n### 2. Chemical Symmetry TTA (Test-Time Augmentation)\n**For mixed solvents only:** A mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\"\n- During inference, predict twice: once with (A, B) and once with (B, A) flipped\n- Average the two predictions\n- This respects physical symmetry and improves predictions\n\n### 3. Symmetric Data Augmentation (Training)\nFor mixed solvents, augment training data by including both:\n- Original: (A, B, SolventB%)\n- Flipped: (B, A, 1-SolventB%)\nThis doubles the effective training set size.\n\n## Post-Processing\n\n### Output Constraints\n- Clip predictions to [0, 1] range (yields cannot be negative or >100%)\n- **DO NOT** normalize to sum to 1 - the targets naturally don't sum to 1 due to mass loss\n\n## Validation Strategy\n\nThe competition uses a specific CV structure:\n- **Single solvent:** Leave-one-solvent-out (24 folds) - tests generalization to unseen solvents\n- **Full data:** Leave-one-ramp-out (13 folds) - tests generalization to unseen solvent pairs\n\n**Important:** Same hyperparameters must be used across all folds (no per-fold tuning unless there's explainable rationale).\n\n## Advanced Techniques to Consider\n\n### Transfer Learning / Pre-training\n- Pre-train on full dataset, fine-tune for single solvent task\n- Use reaction transformer models if available\n\n### Feature Selection\n- With small datasets, fewer features can improve generalization\n- Consider PCA on high-dimensional features (drfps, fragprints)\n- spange_descriptors (13 features) may be optimal for this dataset size\n\n### Uncertainty-Aware Models\n- Bayesian approaches can help with small data\n- MC Dropout for uncertainty estimation\n- Ensemble disagreement as uncertainty proxy\n\n## Recommended Approach (Priority Order)\n\n1. **Physics-informed features:** Add Arrhenius kinetics features (inv_temp, log_time, interaction)\n2. **Use spange_descriptors** as base solvent features (compact and effective)\n3. **MLP with BatchNorm** architecture with Sigmoid output\n4. **Apply symmetry TTA** for mixed solvent predictions\n5. **Bagging** multiple models (5-7) for more robust predictions\n6. **Use HuberLoss** instead of MSELoss for robustness to outliers\n7. **Consider Gaussian Processes** for better extrapolation to unseen solvents\n\n## Target Score\nBeat **0.017270** (lower is better). Current best public kernel achieves ~0.098, so significant improvement is needed. The gap suggests there may be room for:\n- Better feature engineering\n- More sophisticated models (GPs, transformers)\n- Better handling of the leave-one-out CV structure\n\n## Code Structure Reminder\n```python\n# Third-to-last cell: Single solvent task\nmodel = YourModel(data='single')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Second-to-last cell: Full data task  \nmodel = YourModel(data='full')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Last cell: Combine and save submission (DO NOT MODIFY)\n```\n\n## Key Insights from Top Kernels\n\n1. **Arrhenius kinetics kernel (0.098 score):**\n   - Physics-informed features (1/T, ln(t))\n   - Symmetry TTA for mixed solvents\n   - Bagging 7 MLP models\n   - BatchNorm + Dropout + Sigmoid\n   - HuberLoss\n\n2. **LightGBM approach:**\n   - Separate models per target\n   - Early stopping\n   - Good for interpretability\n\n3. **XGBoost approach:**\n   - Feature engineering: Reaction_Energy, B_Conc_Temp\n   - Post-processing: clip to [0,1]\n",
  "evaluator_feedback": "",
  "latest_experiment": {},
  "competitive_intel": "## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`",
  "target_score": 0.01727,
  "messages": [],
  "experiments": [],
  "submissions": [],
  "candidates": [],
  "findings": [],
  "metric_lower_is_better": true,
  "strategy_history": [
    {
      "strategy": "# Catechol Reaction Yield Prediction - Techniques Guide\n\n## CRITICAL: Submission Structure Requirements\n**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.\n- The last three cells MUST remain unchanged except for the model definition line\n- `model = MLPModel()` can be replaced with a new model definition\n- Both single solvent and full (mixed) solvent tasks must be handled\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target statistics, CV structure\n\n**Key Data Facts:**\n- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)\n- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)\n- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)\n- **IMPORTANT:** Targets do NOT sum to 1 (mean ~0.8) - there is mass loss in the reaction\n- Temperature range: 175-225\u00b0C, Residence Time: ~2-15 minutes\n\n## Feature Engineering (Physics-Informed)\n\n### 1. Arrhenius Kinetics Features (HIGHLY RECOMMENDED)\nChemical reactions follow Arrhenius kinetics. Transform features accordingly:\n- `inv_temp = 1000 / (Temperature + 273.15)` - inverse temperature in Kelvin\n- `log_time = log(Residence Time)` - logarithm of time\n- `interaction = inv_temp * log_time` - interaction term\n\nThis physics-informed approach significantly improves predictions.\n\n### 2. Solvent Featurization\nAvailable pre-computed features (use via lookup tables):\n- **spange_descriptors** (13 features): Most commonly used, includes dielectric constant, ET(30), alpha, beta, pi*, etc.\n- **acs_pca_descriptors** (5 features): PCA-reduced green chemistry descriptors\n- **drfps_catechol** (2048 features): Differential reaction fingerprints - high dimensional\n- **fragprints** (2133 features): Fragment + fingerprint concatenation - very high dimensional\n\n**For mixed solvents:** Use weighted average of solvent A and B features based on SolventB%:\n```\nX_feat = A_feat * (1 - SolventB%) + B_feat * SolventB%\n```\n\n### 3. Additional Feature Ideas\n- `Reaction_Energy = Temperature * Residence Time`\n- `B_Conc_Temp = SolventB% * Temperature`\n\n## Model Architectures\n\n### 1. MLP with BatchNorm (Baseline++)\nArchitecture that works well:\n- Input BatchNorm\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Output: 3 neurons with Sigmoid activation (yields are 0-1)\n- Loss: MSELoss or HuberLoss (more robust to outliers)\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Gradient clipping: max_norm=1.0\n- Epochs: 300\n\n### 2. LightGBM Ensemble\nAlternative approach using gradient boosting:\n- Train 3 separate LightGBM regressors (one per target)\n- learning_rate=0.03, max_depth=6\n- Early stopping with 100 rounds patience\n- Use internal validation split (~12%) for early stopping\n\n### 3. Gaussian Processes\nFor small datasets like this, GPs can provide:\n- Good uncertainty quantification\n- Strong performance with limited data (~600-1200 samples is ideal for GPs)\n- Consider Deep Kernel Learning (DKL) to combine NN feature learning with GP uncertainty\n- GPs excel at extrapolation to unseen solvents (the core challenge here)\n\n### 4. Multi-Output Regression Strategies\nSince we predict 3 correlated targets (SM, Product 2, Product 3):\n\n**Regressor Chains:** Feed prediction of one yield as input to the next\n- Can capture correlations between outputs\n- Order matters: try SM \u2192 Product 2 \u2192 Product 3 (or optimize order)\n\n**Joint Multi-Output Models:**\n- Single model predicting all 3 outputs simultaneously\n- Neural networks naturally handle this with 3-output layer\n- Tree-based: use MultiOutputRegressor or native multi-output support\n\n**Curds & Whey Post-Processing:**\n- Linear shrinkage method that decorrelates responses\n- Can improve predictions when outputs are correlated\n\n## Ensemble and Augmentation Strategies\n\n### 1. Model Bagging\nTrain multiple models (e.g., 5-7) and average predictions:\n- Reduces variance\n- More robust predictions\n\n### 2. Chemical Symmetry TTA (Test-Time Augmentation)\n**For mixed solvents only:** A mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\"\n- During inference, predict twice: once with (A, B) and once with (B, A) flipped\n- Average the two predictions\n- This respects physical symmetry and improves predictions\n\n### 3. Symmetric Data Augmentation (Training)\nFor mixed solvents, augment training data by including both:\n- Original: (A, B, SolventB%)\n- Flipped: (B, A, 1-SolventB%)\nThis doubles the effective training set size.\n\n## Post-Processing\n\n### Output Constraints\n- Clip predictions to [0, 1] range (yields cannot be negative or >100%)\n- **DO NOT** normalize to sum to 1 - the targets naturally don't sum to 1 due to mass loss\n\n## Validation Strategy\n\nThe competition uses a specific CV structure:\n- **Single solvent:** Leave-one-solvent-out (24 folds) - tests generalization to unseen solvents\n- **Full data:** Leave-one-ramp-out (13 folds) - tests generalization to unseen solvent pairs\n\n**Important:** Same hyperparameters must be used across all folds (no per-fold tuning unless there's explainable rationale).\n\n## Advanced Techniques to Consider\n\n### Transfer Learning / Pre-training\n- Pre-train on full dataset, fine-tune for single solvent task\n- Use reaction transformer models if available\n\n### Feature Selection\n- With small datasets, fewer features can improve generalization\n- Consider PCA on high-dimensional features (drfps, fragprints)\n- spange_descriptors (13 features) may be optimal for this dataset size\n\n### Uncertainty-Aware Models\n- Bayesian approaches can help with small data\n- MC Dropout for uncertainty estimation\n- Ensemble disagreement as uncertainty proxy\n\n## Recommended Approach (Priority Order)\n\n1. **Physics-informed features:** Add Arrhenius kinetics features (inv_temp, log_time, interaction)\n2. **Use spange_descriptors** as base solvent features (compact and effective)\n3. **MLP with BatchNorm** architecture with Sigmoid output\n4. **Apply symmetry TTA** for mixed solvent predictions\n5. **Bagging** multiple models (5-7) for more robust predictions\n6. **Use HuberLoss** instead of MSELoss for robustness to outliers\n7. **Consider Gaussian Processes** for better extrapolation to unseen solvents\n\n## Target Score\nBeat **0.017270** (lower is better). Current best public kernel achieves ~0.098, so significant improvement is needed. The gap suggests there may be room for:\n- Better feature engineering\n- More sophisticated models (GPs, transformers)\n- Better handling of the leave-one-out CV structure\n\n## Code Structure Reminder\n```python\n# Third-to-last cell: Single solvent task\nmodel = YourModel(data='single')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Second-to-last cell: Full data task  \nmodel = YourModel(data='full')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Last cell: Combine and save submission (DO NOT MODIFY)\n```\n\n## Key Insights from Top Kernels\n\n1. **Arrhenius kinetics kernel (0.098 score):**\n   - Physics-informed features (1/T, ln(t))\n   - Symmetry TTA for mixed solvents\n   - Bagging 7 MLP models\n   - BatchNorm + Dropout + Sigmoid\n   - HuberLoss\n\n2. **LightGBM approach:**\n   - Separate models per target\n   - Early stopping\n   - Good for interpretability\n\n3. **XGBoost approach:**\n   - Feature engineering: Reaction_Energy, B_Conc_Temp\n   - Post-processing: clip to [0,1]\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T21:03:31.536979"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius Kinetics approach: Add 1/Temperature (in Kelvin), ln(Time), and their interaction as features. This physics-informed feature engineering improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical Symmetry TTA: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped, then average. This respects physical symmetry of mixtures.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble of 7 MLP models with BatchNorm, Dropout(0.2), HuberLoss, and Sigmoid output activation. Uses ReduceLROnPlateau scheduler.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data augmentation for mixed solvents: Train on both (A,B) and (B,A) symmetric versions of the data to double training set size.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "LightGBM approach: Use 3 separate LightGBM regressors (one per target), with early stopping (100 rounds patience), learning_rate=0.03, max_depth=6, and regularization. Uses spange_descriptors features.",
      "source": "../research/kernels/tobimichigan_catechol-pipeline-with-lightgbm/catechol-pipeline-with-lightgbm.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature engineering ideas: Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip predictions to [0,1] and normalize rows to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Targets (SM, Product 2, Product 3) do NOT sum to 1 - mean sum is ~0.8 with range 0.03-1.12. This indicates mass loss in the reaction - don't force normalization to 1.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Single solvent CV: 24 folds (leave-one-solvent-out), ~27 samples per test fold. Full data CV: 13 folds (leave-one-ramp-out), ~94 samples per test fold.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: spange_descriptors (13 features), acs_pca (5 features), drfps (2048 features), fragprints (2133 features). Spange is most compact and commonly used.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Multi-output regression: For correlated targets like chemical yields, consider regressor chains (feed one prediction as input to next), joint multi-output models, or Curds & Whey post-processing for linear shrinkage.",
      "source": "Web search: multi-output regression techniques",
      "agent": "explorer"
    },
    {
      "finding": "Gaussian Processes excel for small datasets (~600-1200 samples) and provide uncertainty quantification. Deep Kernel Learning (DKL) combines NN feature learning with GP uncertainty for best of both worlds.",
      "source": "Web search: Gaussian Processes vs Neural Networks for chemistry",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with small datasets?",
      "finding": "Predicting reaction yields when only a few hundred or fewer examples are available requires models that can extract maximal information from limited data and that can express their confidence.  Recent work shows that **transfer\u2011learning\u2011based transformers** fine\u2011tuned on a small, augmented set of reactions are among the most reliable choices.  By representing a reaction as a text string (e.g., SMILES) and pre\u2011training on millions of generic reactions, the model learns a rich chemical language; a modest amount of task\u2011specific data (\u22482\u20135\u202f% of the original high\u2011throughput set) is then sufficient to outperform physics\u2011based descriptor models, especially when test\u2011time augmentation is used to generate uncertainty estimates that correlate with prediction error\u30102\u2020https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches\u3011.  The same strategy is highlighted in a broader review of low\u2011data limits, which recommends fine\u2011tun",
      "agent": "explorer"
    },
    {
      "query": "How do Gaussian Processes perform for chemical reaction yield prediction compared to neural networks?",
      "finding": "Gaussian\u2011process (GP) models excel when only a few hundred reaction datapoints are available: a GP regression trained on 100\u2013150 experimental rate constants predicted activation\u2011energy barriers with a mean absolute error of\u202f0.77\u202fkcal\u202fmol\u207b\u00b9 and supplied calibrated error bars for risk assessment\u202f([ScienceDirect\u202f2023](https://www.sciencedirect.com/science/article/pii/S1093326322002352)). This uncertainty quantification and exact Bayesian optimisation make GPs attractive for low\u2011data yield or barrier predictions, but the O(n\u00b3) scaling of exact inference limits their use on larger reaction libraries\u202f([Quora\u202f2012](https://www.quora.com/What-are-some-advantages-of-using-Gaussian-Process-Models-vs-Neural-Networks)).\n\nNeural\u2011network approaches\u2014especially long\u2011short\u2011term memory (LSTM) and graph neural networks (GNNs)\u2014have achieved comparable or better point\u2011prediction accuracy on richer datasets. For ten Buchwald\u2011Hartwig couplings, an LSTM model predicted the instantaneous yield with a mean abso",
      "agent": "explorer"
    },
    {
      "query": "What are the best techniques for multi-output regression when targets are correlated chemical yields?",
      "finding": "Multi\u2011output regression aims to predict several continuous targets at once, and when those targets are chemically linked (e.g., yields of related reactions) exploiting their correlation can markedly improve accuracy.  The literature groups the most effective approaches into **problem\u2011transformation** methods\u2014where a standard univariate learner is applied to a reshaped target set\u2014and **algorithm\u2011adaptation** methods that modify the learning algorithm to handle multiple outputs directly\u202f([PDF\u202fsurvey](https://oa.upm.es/40804/1/INVE_MEM_2015_204213.pdf)).\n\nFor correlated chemical yields, the top\u2011performing techniques are:\n\n* **Regressor\u2011chain and stacking variants** that feed the prediction of one yield as an input to the next, thereby transferring information along the chain\u202f([Multi\u2011target SVR via correlation regressor chains](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946)).  \n* **Multi\u2011output support\u2011vector regression**, especially Bayesian formulations that mod",
      "agent": "explorer"
    }
  ],
  "remaining_submissions": 5,
  "max_submissions": 5,
  "last_reset_date_utc": "2026-01-13",
  "lb_scores": [],
  "bronze_threshold": 0.02565,
  "bronze_submitted": false,
  "best_lb_score": null,
  "max_experiments_without_submission": 5,
  "last_submitted_experiment_idx": 0,
  "start_time": "2026-01-13T20:51:08.020797",
  "time_limit_minutes": 2100,
  "_saved_at": "2026-01-13T21:03:31.541092"
}