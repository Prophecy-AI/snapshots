# Catechol Reaction Yield Prediction - Techniques Guide

## CRITICAL: Submission Structure Requirements
**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.
- The last three cells MUST remain unchanged except for the model definition line
- `model = MLPModel()` can be replaced with a new model definition
- Both single solvent and full (mixed) solvent tasks must be handled

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target statistics, CV structure

**Key Data Facts:**
- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)
- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)
- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)
- **IMPORTANT:** Targets do NOT sum to 1 (mean ~0.8) - there is mass loss in the reaction
- Temperature range: 175-225°C, Residence Time: ~2-15 minutes

## Feature Engineering (Physics-Informed)

### 1. Arrhenius Kinetics Features (HIGHLY RECOMMENDED)
Chemical reactions follow Arrhenius kinetics. Transform features accordingly:
- `inv_temp = 1000 / (Temperature + 273.15)` - inverse temperature in Kelvin
- `log_time = log(Residence Time)` - logarithm of time
- `interaction = inv_temp * log_time` - interaction term

This physics-informed approach significantly improves predictions.

### 2. Solvent Featurization
Available pre-computed features (use via lookup tables):
- **spange_descriptors** (13 features): Most commonly used, includes dielectric constant, ET(30), alpha, beta, pi*, etc.
- **acs_pca_descriptors** (5 features): PCA-reduced green chemistry descriptors
- **drfps_catechol** (2048 features): Differential reaction fingerprints - high dimensional
- **fragprints** (2133 features): Fragment + fingerprint concatenation - very high dimensional

**For mixed solvents:** Use weighted average of solvent A and B features based on SolventB%:
```
X_feat = A_feat * (1 - SolventB%) + B_feat * SolventB%
```

### 3. Additional Feature Ideas
- `Reaction_Energy = Temperature * Residence Time`
- `B_Conc_Temp = SolventB% * Temperature`

## Model Architectures

### 1. MLP with BatchNorm (Baseline++)
Architecture that works well:
- Input BatchNorm
- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)
- Output: 3 neurons with Sigmoid activation (yields are 0-1)
- Loss: MSELoss or HuberLoss (more robust to outliers)
- Optimizer: Adam with lr=5e-4, weight_decay=1e-5
- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)
- Gradient clipping: max_norm=1.0
- Epochs: 300

### 2. LightGBM Ensemble
Alternative approach using gradient boosting:
- Train 3 separate LightGBM regressors (one per target)
- learning_rate=0.03, max_depth=6
- Early stopping with 100 rounds patience
- Use internal validation split (~12%) for early stopping

### 3. Gaussian Processes
For small datasets like this, GPs can provide:
- Good uncertainty quantification
- Strong performance with limited data (~600-1200 samples is ideal for GPs)
- Consider Deep Kernel Learning (DKL) to combine NN feature learning with GP uncertainty
- GPs excel at extrapolation to unseen solvents (the core challenge here)

### 4. Multi-Output Regression Strategies
Since we predict 3 correlated targets (SM, Product 2, Product 3):

**Regressor Chains:** Feed prediction of one yield as input to the next
- Can capture correlations between outputs
- Order matters: try SM → Product 2 → Product 3 (or optimize order)

**Joint Multi-Output Models:**
- Single model predicting all 3 outputs simultaneously
- Neural networks naturally handle this with 3-output layer
- Tree-based: use MultiOutputRegressor or native multi-output support

**Curds & Whey Post-Processing:**
- Linear shrinkage method that decorrelates responses
- Can improve predictions when outputs are correlated

## Ensemble and Augmentation Strategies

### 1. Model Bagging
Train multiple models (e.g., 5-7) and average predictions:
- Reduces variance
- More robust predictions

### 2. Chemical Symmetry TTA (Test-Time Augmentation)
**For mixed solvents only:** A mixture of "Solvent A + Solvent B" is physically identical to "Solvent B + Solvent A"
- During inference, predict twice: once with (A, B) and once with (B, A) flipped
- Average the two predictions
- This respects physical symmetry and improves predictions

### 3. Symmetric Data Augmentation (Training)
For mixed solvents, augment training data by including both:
- Original: (A, B, SolventB%)
- Flipped: (B, A, 1-SolventB%)
This doubles the effective training set size.

## Post-Processing

### Output Constraints
- Clip predictions to [0, 1] range (yields cannot be negative or >100%)
- **DO NOT** normalize to sum to 1 - the targets naturally don't sum to 1 due to mass loss

## Validation Strategy

The competition uses a specific CV structure:
- **Single solvent:** Leave-one-solvent-out (24 folds) - tests generalization to unseen solvents
- **Full data:** Leave-one-ramp-out (13 folds) - tests generalization to unseen solvent pairs

**Important:** Same hyperparameters must be used across all folds (no per-fold tuning unless there's explainable rationale).

## Advanced Techniques to Consider

### Transfer Learning / Pre-training
- Pre-train on full dataset, fine-tune for single solvent task
- Use reaction transformer models if available

### Feature Selection
- With small datasets, fewer features can improve generalization
- Consider PCA on high-dimensional features (drfps, fragprints)
- spange_descriptors (13 features) may be optimal for this dataset size

### Uncertainty-Aware Models
- Bayesian approaches can help with small data
- MC Dropout for uncertainty estimation
- Ensemble disagreement as uncertainty proxy

## Recommended Approach (Priority Order)

1. **Physics-informed features:** Add Arrhenius kinetics features (inv_temp, log_time, interaction)
2. **Use spange_descriptors** as base solvent features (compact and effective)
3. **MLP with BatchNorm** architecture with Sigmoid output
4. **Apply symmetry TTA** for mixed solvent predictions
5. **Bagging** multiple models (5-7) for more robust predictions
6. **Use HuberLoss** instead of MSELoss for robustness to outliers
7. **Consider Gaussian Processes** for better extrapolation to unseen solvents

## Target Score
Beat **0.017270** (lower is better). Current best public kernel achieves ~0.098, so significant improvement is needed. The gap suggests there may be room for:
- Better feature engineering
- More sophisticated models (GPs, transformers)
- Better handling of the leave-one-out CV structure

## Code Structure Reminder
```python
# Third-to-last cell: Single solvent task
model = YourModel(data='single')  # ONLY change this line
model.train_model(train_X, train_Y)
predictions = model.predict(test_X)

# Second-to-last cell: Full data task  
model = YourModel(data='full')  # ONLY change this line
model.train_model(train_X, train_Y)
predictions = model.predict(test_X)

# Last cell: Combine and save submission (DO NOT MODIFY)
```

## Key Insights from Top Kernels

1. **Arrhenius kinetics kernel (0.098 score):**
   - Physics-informed features (1/T, ln(t))
   - Symmetry TTA for mixed solvents
   - Bagging 7 MLP models
   - BatchNorm + Dropout + Sigmoid
   - HuberLoss

2. **LightGBM approach:**
   - Separate models per target
   - Early stopping
   - Good for interpretability

3. **XGBoost approach:**
   - Feature engineering: Reaction_Energy, B_Conc_Temp
   - Post-processing: clip to [0,1]
