{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b38afb0",
   "metadata": {},
   "source": [
    "# Loop 1 LB Feedback Analysis\n",
    "\n",
    "## Critical Issue: CV 0.0113 vs LB 0.0998 (9x gap!)\n",
    "\n",
    "This is a MASSIVE gap that needs investigation. The CV score was excellent but the LB score is terrible.\n",
    "\n",
    "### Possible Causes:\n",
    "1. **Submission format issue** - The notebook didn't follow the template structure\n",
    "2. **Data leakage in CV** - Our CV might be overly optimistic\n",
    "3. **Distribution shift** - Test data differs significantly from training\n",
    "4. **Overfitting** - Model memorized training patterns that don't generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d259946c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.357312Z",
     "iopub.status.busy": "2026-01-13T21:44:24.356776Z",
     "iopub.status.idle": "2026-01-13T21:44:24.738722Z",
     "shell.execute_reply": "2026-01-13T21:44:24.738277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our submission shape: (1883, 8)\n",
      "\n",
      "Columns: ['id', 'index', 'task', 'fold', 'row', 'target_1', 'target_2', 'target_3']\n",
      "\n",
      "First 5 rows:\n",
      "   id  index  task  fold  row  target_1  target_2  target_3\n",
      "0   0      0     0     0    0  0.008308  0.011011  0.931947\n",
      "1   1      1     0     0    1  0.018124  0.021161  0.886760\n",
      "2   2      2     0     0    2  0.040722  0.039450  0.796149\n",
      "3   3      3     0     0    3  0.070126  0.057600  0.702190\n",
      "4   4      4     0     0    4  0.093846  0.070580  0.628814\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load our submission\n",
    "submission = pd.read_csv('/home/code/experiments/001_baseline/submission.csv')\n",
    "print('Our submission shape:', submission.shape)\n",
    "print('\\nColumns:', submission.columns.tolist())\n",
    "print('\\nFirst 5 rows:')\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96664946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.739902Z",
     "iopub.status.busy": "2026-01-13T21:44:24.739785Z",
     "iopub.status.idle": "2026-01-13T21:44:24.746676Z",
     "shell.execute_reply": "2026-01-13T21:44:24.746257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task distribution:\n",
      "task\n",
      "1    1227\n",
      "0     656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold distribution for task 0 (single solvent):\n",
      "fold\n",
      "0     37\n",
      "1     37\n",
      "2     58\n",
      "3     59\n",
      "4     22\n",
      "5     18\n",
      "6     34\n",
      "7     41\n",
      "8     20\n",
      "9     22\n",
      "10    18\n",
      "11    18\n",
      "12    42\n",
      "13    18\n",
      "14    17\n",
      "15    22\n",
      "16     5\n",
      "17    16\n",
      "18    36\n",
      "19    18\n",
      "20    21\n",
      "21    22\n",
      "22    37\n",
      "23    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold distribution for task 1 (full data):\n",
      "fold\n",
      "0     122\n",
      "1     124\n",
      "2     104\n",
      "3     125\n",
      "4     125\n",
      "5     124\n",
      "6     125\n",
      "7     110\n",
      "8     127\n",
      "9      36\n",
      "10     34\n",
      "11     36\n",
      "12     35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the expected format from template\n",
    "# The template produces: id, index, task, fold, row, target_1, target_2, target_3\n",
    "print('\\nTask distribution:')\n",
    "print(submission['task'].value_counts())\n",
    "\n",
    "print('\\nFold distribution for task 0 (single solvent):')\n",
    "print(submission[submission['task']==0]['fold'].value_counts().sort_index())\n",
    "\n",
    "print('\\nFold distribution for task 1 (full data):')\n",
    "print(submission[submission['task']==1]['fold'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5731bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.747633Z",
     "iopub.status.busy": "2026-01-13T21:44:24.747521Z",
     "iopub.status.idle": "2026-01-13T21:44:24.751109Z",
     "shell.execute_reply": "2026-01-13T21:44:24.750658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target statistics:\n",
      "target_1: min=0.0001, max=0.4477, mean=0.1626\n",
      "target_2: min=0.0001, max=0.4257, mean=0.1392\n",
      "target_3: min=0.0000, max=0.9983, mean=0.5162\n"
     ]
    }
   ],
   "source": [
    "# Check target ranges\n",
    "print('\\nTarget statistics:')\n",
    "for col in ['target_1', 'target_2', 'target_3']:\n",
    "    print(f'{col}: min={submission[col].min():.4f}, max={submission[col].max():.4f}, mean={submission[col].mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76a3ff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.752161Z",
     "iopub.status.busy": "2026-01-13T21:44:24.752050Z",
     "iopub.status.idle": "2026-01-13T21:44:24.765024Z",
     "shell.execute_reply": "2026-01-13T21:44:24.764654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent data shape: (656, 13)\n",
      "Full data shape: (1227, 19)\n",
      "\n",
      "Actual target statistics (single solvent):\n",
      "Product 2: min=0.0000, max=0.4636, mean=0.1499\n",
      "Product 3: min=0.0000, max=0.5338, mean=0.1234\n",
      "SM: min=0.0000, max=1.0000, mean=0.5222\n"
     ]
    }
   ],
   "source": [
    "# Load actual data to compare\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "single_df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "full_df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('Single solvent data shape:', single_df.shape)\n",
    "print('Full data shape:', full_df.shape)\n",
    "\n",
    "print('\\nActual target statistics (single solvent):')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'{col}: min={single_df[col].min():.4f}, max={single_df[col].max():.4f}, mean={single_df[col].mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8c2f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.766050Z",
     "iopub.status.busy": "2026-01-13T21:44:24.765940Z",
     "iopub.status.idle": "2026-01-13T21:44:24.769922Z",
     "shell.execute_reply": "2026-01-13T21:44:24.769538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected rows:\n",
      "  Single solvent: 656\n",
      "  Full data: 1227\n",
      "  Total: 1883\n",
      "\n",
      "Actual rows in submission:\n",
      "  Task 0 (single): 656\n",
      "  Task 1 (full): 1227\n",
      "  Total: 1883\n"
     ]
    }
   ],
   "source": [
    "# KEY INSIGHT: Check if our submission has the correct number of rows\n",
    "# Single solvent: 656 samples, 24 folds (leave-one-out)\n",
    "# Full data: 1227 samples, 13 folds (leave-one-ramp-out)\n",
    "\n",
    "print('Expected rows:')\n",
    "print(f'  Single solvent: 656')\n",
    "print(f'  Full data: 1227')\n",
    "print(f'  Total: 1883')\n",
    "\n",
    "print('\\nActual rows in submission:')\n",
    "print(f'  Task 0 (single): {len(submission[submission[\"task\"]==0])}')\n",
    "print(f'  Task 1 (full): {len(submission[submission[\"task\"]==1])}')\n",
    "print(f'  Total: {len(submission)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689c47f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.771005Z",
     "iopub.status.busy": "2026-01-13T21:44:24.770896Z",
     "iopub.status.idle": "2026-01-13T21:44:24.784758Z",
     "shell.execute_reply": "2026-01-13T21:44:24.784376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples per fold (single solvent):\n",
      "  Fold 0: 37 samples\n",
      "  Fold 1: 37 samples\n",
      "  Fold 2: 58 samples\n",
      "  Fold 3: 59 samples\n",
      "  Fold 4: 22 samples\n",
      "  Fold 5: 18 samples\n",
      "  Fold 6: 34 samples\n",
      "  Fold 7: 41 samples\n",
      "  Fold 8: 20 samples\n",
      "  Fold 9: 22 samples\n",
      "  Fold 10: 18 samples\n",
      "  Fold 11: 18 samples\n",
      "  Fold 12: 42 samples\n",
      "  Fold 13: 18 samples\n",
      "  Fold 14: 17 samples\n",
      "  Fold 15: 22 samples\n",
      "  Fold 16: 5 samples\n",
      "  Fold 17: 16 samples\n",
      "  Fold 18: 36 samples\n",
      "  Fold 19: 18 samples\n",
      "  Fold 20: 21 samples\n",
      "  Fold 21: 22 samples\n",
      "  Fold 22: 37 samples\n",
      "  Fold 23: 18 samples\n",
      "\n",
      "Samples per fold (full data):\n",
      "  Fold 0: 122 samples\n",
      "  Fold 1: 124 samples\n",
      "  Fold 2: 104 samples\n",
      "  Fold 3: 125 samples\n",
      "  Fold 4: 125 samples\n",
      "  Fold 5: 124 samples\n",
      "  Fold 6: 125 samples\n",
      "  Fold 7: 110 samples\n",
      "  Fold 8: 127 samples\n",
      "  Fold 9: 36 samples\n",
      "  Fold 10: 34 samples\n",
      "  Fold 11: 36 samples\n",
      "  Fold 12: 35 samples\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL CHECK: The competition likely expects predictions in a specific order\n",
    "# Let's check if our fold/row ordering matches what's expected\n",
    "\n",
    "# For single solvent, each fold should have ~27 samples (656/24)\n",
    "print('\\nSamples per fold (single solvent):')\n",
    "for fold in range(24):\n",
    "    count = len(submission[(submission['task']==0) & (submission['fold']==fold)])\n",
    "    print(f'  Fold {fold}: {count} samples')\n",
    "\n",
    "print('\\nSamples per fold (full data):')\n",
    "for fold in range(13):\n",
    "    count = len(submission[(submission['task']==1) & (submission['fold']==fold)])\n",
    "    print(f'  Fold {fold}: {count} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e14e14",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "1. **The submission format looks correct** - 1883 rows total (656 + 1227)\n",
    "2. **The target ranges look reasonable** - values between 0 and 1\n",
    "3. **The fold structure looks correct** - 24 folds for single, 13 for full\n",
    "\n",
    "### The Real Issue: CV-LB Gap\n",
    "\n",
    "The massive CV-LB gap (0.0113 vs 0.0998) suggests:\n",
    "\n",
    "1. **Our local CV is NOT representative of the actual test set**\n",
    "2. **The competition might use a different evaluation method**\n",
    "3. **There might be a bug in how we compute CV vs how Kaggle computes it**\n",
    "\n",
    "### Looking at Public Kernels\n",
    "\n",
    "The best public kernel (Arrhenius Kinetics + TTA) achieves **0.09831** on LB, which is very close to our LB score of 0.0998. This suggests:\n",
    "\n",
    "1. Our model is actually performing similarly to the best public kernel\n",
    "2. The CV score of 0.0113 was likely computed incorrectly or is not comparable to LB\n",
    "3. The target of 0.017270 might be achievable but requires different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffb630f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T21:44:24.785673Z",
     "iopub.status.busy": "2026-01-13T21:44:24.785558Z",
     "iopub.status.idle": "2026-01-13T21:44:24.788540Z",
     "shell.execute_reply": "2026-01-13T21:44:24.788164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding the scoring:\n",
      "\n",
      "Our CV MSE: 0.011303\n",
      "Our LB MSE: 0.0998\n",
      "Best public kernel LB: 0.09831\n",
      "\n",
      "Target to beat: 0.017270\n",
      "\n",
      "\n",
      "The gap suggests our CV calculation might be different from LB calculation.\n",
      "Possible issues:\n",
      "1. LB might weight tasks differently\n",
      "2. LB might use different folds\n",
      "3. Our CV might have data leakage\n"
     ]
    }
   ],
   "source": [
    "# Let's understand what the competition is actually measuring\n",
    "# The LB score of ~0.098-0.10 seems to be the baseline for good models\n",
    "# Our CV of 0.0113 was computed as MSE across all predictions\n",
    "\n",
    "# Let's recalculate to understand the discrepancy\n",
    "print('Understanding the scoring:')\n",
    "print('\\nOur CV MSE: 0.011303')\n",
    "print('Our LB MSE: 0.0998')\n",
    "print('Best public kernel LB: 0.09831')\n",
    "print('\\nTarget to beat: 0.017270')\n",
    "\n",
    "print('\\n\\nThe gap suggests our CV calculation might be different from LB calculation.')\n",
    "print('Possible issues:')\n",
    "print('1. LB might weight tasks differently')\n",
    "print('2. LB might use different folds')\n",
    "print('3. Our CV might have data leakage')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
