{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-13T20:51:08.020797",
  "time_limit_minutes": 2100,
  "experiments": [
    {
      "id": "exp_000",
      "name": "Baseline: Arrhenius Kinetics + MLP with BatchNorm + TTA",
      "model_type": "MLP (PyTorch)",
      "score": 0.011303,
      "notes": "First baseline implementing the recommended approach from the strategy:\n- Physics-informed features: inv_temp (1000/T_K), log_time (ln(t)), interaction (inv_temp * log_time)\n- Spange descriptors (13 features) for solvent featurization\n- MLP architecture: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2), Sigmoid output\n- HuberLoss for robustness to outliers\n- Bagging 3 models per fold\n- Symmetry TTA for mixed solvents (predict both A,B and B,A, average)\n- Data augmentation for mixed solvents (train on both symmetries)\n- 200 epochs, Adam lr=5e-4, weight_decay=1e-5, ReduceLROnPlateau scheduler\n- Gradient clipping max_norm=1.0\n\nResults:\n- Single Solvent CV MSE: 0.010916\n- Full Data CV MSE: 0.011510\n- Overall CV MSE: 0.011303\n\nThis already beats the target of 0.017270! The Arrhenius kinetics features and symmetry TTA are key improvements.",
      "experiment_folder": "/home/code/experiments/001_baseline",
      "timestamp": "2026-01-13T21:39:01.377478"
    },
    {
      "id": "exp_001",
      "name": "Tree-Based Per-Target Ensemble with Arrhenius Features",
      "model_type": "HistGradientBoosting + ExtraTrees",
      "score": 0.010986,
      "notes": "Tree-based per-target ensemble following the strategy recommendation:\n\nModel Architecture:\n- SM target: HistGradientBoostingRegressor (max_depth=7, max_iter=700, lr=0.04)\n- Product 2 & 3: ExtraTreesRegressor (n_estimators=500, min_samples_leaf=2)\n\nFeatures (23 total):\n- Original numeric: Residence Time, Temperature (2)\n- Arrhenius kinetics: inv_temp, log_time, interaction (3)\n- Spange descriptors (13)\n- ACS PCA descriptors (5)\n\nKey techniques:\n- StandardScaler preprocessing\n- Data augmentation for mixed solvents (train on both A,B and B,A)\n- Symmetry TTA for mixed solvents at inference\n- Clip predictions to [0, 1]\n- Reproducibility seeds set\n\nResults:\n- Single Solvent CV MSE: 0.011227\n- Full Data CV MSE: 0.010857\n- Overall CV MSE: 0.010986\n\nImprovements over exp_001:\n- Slightly better CV score (0.010986 vs 0.011303)\n- MUCH faster training (~50 seconds vs ~30 minutes)\n- Follows template structure correctly\n- Reproducibility seeds added\n\nThis approach is simpler, faster, and achieves comparable or better results than the MLP baseline.",
      "experiment_folder": "/home/code/experiments/002_tree_ensemble",
      "timestamp": "2026-01-13T21:49:14.621162"
    }
  ],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.011303,
      "model_name": "Baseline: Arrhenius Kinetics + MLP with BatchNorm + TTA",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-13T21:39:01.378549"
    },
    {
      "file_path": "/home/code/submission_candidates/candidate_001.csv",
      "score": 0.010986,
      "model_name": "Tree-Based Per-Target Ensemble with Arrhenius Features",
      "experiment_id": "exp_001",
      "timestamp": "2026-01-13T21:49:14.623990"
    }
  ],
  "submissions": [
    {
      "candidate_id": "exp_000",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-13T21:41:08.049533+00:00",
      "model_name": "Baseline: Arrhenius Kinetics + MLP with BatchNorm + TTA",
      "cv_score": 0.011303,
      "lb_score": 0.0998,
      "error": null
    }
  ],
  "strategy_history": [
    {
      "strategy": "# Catechol Reaction Yield Prediction - Techniques Guide\n\n## CRITICAL: Submission Structure Requirements\n**MANDATORY:** The submission must follow the exact structure of the benchmark template notebook.\n- The last three cells MUST remain unchanged except for the model definition line\n- `model = MLPModel()` can be replaced with a new model definition\n- Both single solvent and full (mixed) solvent tasks must be handled\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: feature distributions, target statistics, CV structure\n\n**Key Data Facts:**\n- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)\n- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)\n- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)\n- **IMPORTANT:** Targets do NOT sum to 1 (mean ~0.8) - there is mass loss in the reaction\n- Temperature range: 175-225\u00b0C, Residence Time: ~2-15 minutes\n\n## Feature Engineering (Physics-Informed)\n\n### 1. Arrhenius Kinetics Features (HIGHLY RECOMMENDED)\nChemical reactions follow Arrhenius kinetics. Transform features accordingly:\n- `inv_temp = 1000 / (Temperature + 273.15)` - inverse temperature in Kelvin\n- `log_time = log(Residence Time)` - logarithm of time\n- `interaction = inv_temp * log_time` - interaction term\n\nThis physics-informed approach significantly improves predictions.\n\n### 2. Solvent Featurization\nAvailable pre-computed features (use via lookup tables):\n- **spange_descriptors** (13 features): Most commonly used, includes dielectric constant, ET(30), alpha, beta, pi*, etc.\n- **acs_pca_descriptors** (5 features): PCA-reduced green chemistry descriptors\n- **drfps_catechol** (2048 features): Differential reaction fingerprints - high dimensional\n- **fragprints** (2133 features): Fragment + fingerprint concatenation - very high dimensional\n\n**For mixed solvents:** Use weighted average of solvent A and B features based on SolventB%:\n```\nX_feat = A_feat * (1 - SolventB%) + B_feat * SolventB%\n```\n\n### 3. Additional Feature Ideas\n- `Reaction_Energy = Temperature * Residence Time`\n- `B_Conc_Temp = SolventB% * Temperature`\n\n## Model Architectures\n\n### 1. MLP with BatchNorm (Baseline++)\nArchitecture that works well:\n- Input BatchNorm\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Output: 3 neurons with Sigmoid activation (yields are 0-1)\n- Loss: MSELoss or HuberLoss (more robust to outliers)\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Gradient clipping: max_norm=1.0\n- Epochs: 300\n\n### 2. LightGBM Ensemble\nAlternative approach using gradient boosting:\n- Train 3 separate LightGBM regressors (one per target)\n- learning_rate=0.03, max_depth=6\n- Early stopping with 100 rounds patience\n- Use internal validation split (~12%) for early stopping\n\n### 3. Gaussian Processes\nFor small datasets like this, GPs can provide:\n- Good uncertainty quantification\n- Strong performance with limited data (~600-1200 samples is ideal for GPs)\n- Consider Deep Kernel Learning (DKL) to combine NN feature learning with GP uncertainty\n- GPs excel at extrapolation to unseen solvents (the core challenge here)\n\n### 4. Multi-Output Regression Strategies\nSince we predict 3 correlated targets (SM, Product 2, Product 3):\n\n**Regressor Chains:** Feed prediction of one yield as input to the next\n- Can capture correlations between outputs\n- Order matters: try SM \u2192 Product 2 \u2192 Product 3 (or optimize order)\n\n**Joint Multi-Output Models:**\n- Single model predicting all 3 outputs simultaneously\n- Neural networks naturally handle this with 3-output layer\n- Tree-based: use MultiOutputRegressor or native multi-output support\n\n**Curds & Whey Post-Processing:**\n- Linear shrinkage method that decorrelates responses\n- Can improve predictions when outputs are correlated\n\n## Ensemble and Augmentation Strategies\n\n### 1. Model Bagging\nTrain multiple models (e.g., 5-7) and average predictions:\n- Reduces variance\n- More robust predictions\n\n### 2. Chemical Symmetry TTA (Test-Time Augmentation)\n**For mixed solvents only:** A mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\"\n- During inference, predict twice: once with (A, B) and once with (B, A) flipped\n- Average the two predictions\n- This respects physical symmetry and improves predictions\n\n### 3. Symmetric Data Augmentation (Training)\nFor mixed solvents, augment training data by including both:\n- Original: (A, B, SolventB%)\n- Flipped: (B, A, 1-SolventB%)\nThis doubles the effective training set size.\n\n## Post-Processing\n\n### Output Constraints\n- Clip predictions to [0, 1] range (yields cannot be negative or >100%)\n- **DO NOT** normalize to sum to 1 - the targets naturally don't sum to 1 due to mass loss\n\n## Validation Strategy\n\nThe competition uses a specific CV structure:\n- **Single solvent:** Leave-one-solvent-out (24 folds) - tests generalization to unseen solvents\n- **Full data:** Leave-one-ramp-out (13 folds) - tests generalization to unseen solvent pairs\n\n**Important:** Same hyperparameters must be used across all folds (no per-fold tuning unless there's explainable rationale).\n\n## Advanced Techniques to Consider\n\n### Transfer Learning / Pre-training\n- Pre-train on full dataset, fine-tune for single solvent task\n- Use reaction transformer models if available\n\n### Feature Selection\n- With small datasets, fewer features can improve generalization\n- Consider PCA on high-dimensional features (drfps, fragprints)\n- spange_descriptors (13 features) may be optimal for this dataset size\n\n### Uncertainty-Aware Models\n- Bayesian approaches can help with small data\n- MC Dropout for uncertainty estimation\n- Ensemble disagreement as uncertainty proxy\n\n## Recommended Approach (Priority Order)\n\n1. **Physics-informed features:** Add Arrhenius kinetics features (inv_temp, log_time, interaction)\n2. **Use spange_descriptors** as base solvent features (compact and effective)\n3. **MLP with BatchNorm** architecture with Sigmoid output\n4. **Apply symmetry TTA** for mixed solvent predictions\n5. **Bagging** multiple models (5-7) for more robust predictions\n6. **Use HuberLoss** instead of MSELoss for robustness to outliers\n7. **Consider Gaussian Processes** for better extrapolation to unseen solvents\n\n## Target Score\nBeat **0.017270** (lower is better). Current best public kernel achieves ~0.098, so significant improvement is needed. The gap suggests there may be room for:\n- Better feature engineering\n- More sophisticated models (GPs, transformers)\n- Better handling of the leave-one-out CV structure\n\n## Code Structure Reminder\n```python\n# Third-to-last cell: Single solvent task\nmodel = YourModel(data='single')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Second-to-last cell: Full data task  \nmodel = YourModel(data='full')  # ONLY change this line\nmodel.train_model(train_X, train_Y)\npredictions = model.predict(test_X)\n\n# Last cell: Combine and save submission (DO NOT MODIFY)\n```\n\n## Key Insights from Top Kernels\n\n1. **Arrhenius kinetics kernel (0.098 score):**\n   - Physics-informed features (1/T, ln(t))\n   - Symmetry TTA for mixed solvents\n   - Bagging 7 MLP models\n   - BatchNorm + Dropout + Sigmoid\n   - HuberLoss\n\n2. **LightGBM approach:**\n   - Separate models per target\n   - Early stopping\n   - Good for interpretability\n\n3. **XGBoost approach:**\n   - Feature engineering: Reaction_Energy, B_Conc_Temp\n   - Post-processing: clip to [0,1]\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T21:03:31.536979"
    },
    {
      "strategy": "# Catechol Reaction Yield Prediction - Strategy Guide (Loop 2)\n\n## Current Status\n- Best CV score: 0.0113 from exp_000 (Arrhenius Kinetics + MLP + TTA)\n- Best LB score: 0.0998 (from exp_000)\n- CV-LB gap: -0.0885 (MASSIVE - 9x worse on LB!)\n- Target to beat: 0.017270\n- Submissions remaining: 4/5\n\n## Response to Evaluator\n\n**Technical verdict was CONCERNS.** The evaluator correctly identified submission structure issues and lack of reproducibility seeds.\n\n**CRITICAL INSIGHT:** Our LB score of 0.0998 is actually competitive with the best public kernel (0.098). The massive CV-LB gap suggests our local CV calculation differs from how Kaggle evaluates. The target of 0.017270 is 6x better than any public kernel, suggesting either:\n1. A different metric interpretation\n2. Significant room for improvement through novel approaches\n\n## MANDATORY: Submission Structure Requirements\n\n**CRITICAL - MUST FOLLOW EXACTLY:**\n- The submission must follow the exact template structure from https://www.kaggle.com/code/josepablofolch/catechol-benchmark-hackathon-template\n- Last 3 cells must match the template exactly\n- Only the `model = MLPModel()` line can be changed to `model = YourModel(data='single')` or `model = YourModel(data='full')`\n- Model class must have `train_model(X, Y)` and `predict(X)` methods\n- predict() must return a tensor/array of shape [N, 3] with columns [Product 2, Product 3, SM]\n\n## Data Understanding\n\n**Reference notebooks:**\n- `exploration/eda.ipynb` - Feature distributions, target statistics\n- `exploration/evolver_loop1_lb_feedback.ipynb` - CV-LB gap analysis\n\n**Key Data Facts:**\n- Single solvent: 656 samples, 24 solvents, leave-one-solvent-out CV (24 folds)\n- Full/mixed solvent: 1227 samples, 13 solvent pairs, leave-one-ramp-out CV (13 folds)\n- Targets: SM, Product 2, Product 3 (yields as fractions 0-1)\n- Targets do NOT sum to 1 (mean ~0.8) - mass loss in reaction\n- Temperature range: 175-225\u00b0C, Residence Time: ~2-15 minutes\n\n## Recommended Approaches (Priority Order)\n\n### 1. PRIORITY: Tree-Based Per-Target Ensemble\nBased on the \"catechol strategy to get 0.11161\" kernel (LB 0.111):\n- **Different models per target:**\n  - SM: HistGradientBoostingRegressor (max_depth=7, max_iter=700, lr=0.04)\n  - Product 2 & 3: ExtraTreesRegressor (n_estimators=900, min_samples_leaf=2)\n- **Feature ensemble:** Combine acs_pca_descriptors (5 features) and spange_descriptors (13 features)\n- **Weighted average:** 0.65 * model1 + 0.35 * model2\n- **Post-processing:** Clip predictions to [0, 1]\n\n### 2. Multi-Model Ensemble (from \"mixall\" kernel)\nEnsemble of MLP + XGBoost + RandomForest + LightGBM:\n- StandardScaler preprocessing\n- Weighted ensemble (can optimize via Optuna)\n- Each model type captures different patterns\n\n### 3. Physics-Informed Features (Keep Using)\nThe Arrhenius kinetics features are well-motivated:\n```python\ninv_temp = 1000 / (Temperature + 273.15)  # Inverse temperature\nlog_time = np.log(Residence_Time + 1e-6)  # Log of time\ninteraction = inv_temp * log_time          # Interaction term\n```\n\n### 4. Symmetry TTA for Mixed Solvents (Keep Using)\nFor mixed solvents, predict twice and average:\n- Prediction 1: Input as (A, B, SolventB%)\n- Prediction 2: Input as (B, A, 1-SolventB%)\n- Final = (Pred1 + Pred2) / 2\n\n## What NOT to Try\n- Complex deep learning architectures (small data, risk of overfitting)\n- High-dimensional features (drfps: 2048, fragprints: 2133) without PCA\n- Normalizing targets to sum to 1 (they naturally don't)\n- Chasing local CV improvements without LB validation\n\n## Model Interface Template\n\n```python\nimport torch\nimport numpy as np\nimport pandas as pd\n\nclass YourModel:\n    def __init__(self, data='single'):\n        self.data_type = data\n        # Initialize your model components here\n    \n    def train_model(self, X_train, y_train):\n        # X_train: DataFrame with columns:\n        #   Single: ['Residence Time', 'Temperature', 'SOLVENT NAME']\n        #   Full: ['Residence Time', 'Temperature', 'SOLVENT A NAME', 'SOLVENT B NAME', 'SolventB%']\n        # y_train: DataFrame with ['Product 2', 'Product 3', 'SM']\n        pass\n    \n    def predict(self, X_test):\n        # Returns tensor/array of shape [N, 3]\n        # Columns must be [Product 2, Product 3, SM] in that order\n        return torch.tensor(predictions)\n```\n\n## Validation Notes\n- **DO NOT trust local CV** - it doesn't match LB\n- Focus on LB score as the true measure of progress\n- The target of 0.017270 is ambitious - try fundamentally different approaches\n- Submit to verify progress on LB before extensive optimization\n\n## Key Insights from Public Kernels\n\n1. **Arrhenius kinetics kernel (LB 0.098):** Physics-informed features, symmetry TTA, bagging MLPs\n2. **Per-target ensemble (LB 0.111):** Different models per target, tree-based, feature ensemble\n3. **mixall kernel:** Ensemble of MLP + XGBoost + RF + LightGBM\n\n## Next Experiment Recommendation\n\n**Implement a tree-based per-target ensemble** following the \"catechol strategy\" approach:\n1. Use HistGradientBoostingRegressor for SM target\n2. Use ExtraTreesRegressor for Product 2 and Product 3\n3. Ensemble predictions from acs_pca and spange descriptors\n4. Add Arrhenius kinetics features\n5. Clip predictions to [0, 1]\n\nThis approach is simpler, less prone to overfitting, and has shown good LB performance (0.111).",
      "agent": "evolver",
      "timestamp": "2026-01-13T21:46:08.617902"
    }
  ],
  "feedback_history": [
    {
      "feedback": "## What I Understood\n\nThe junior researcher implemented a first baseline model combining several best practices from the competition kernels: physics-informed Arrhenius kinetics features (1/T, ln(t), interaction term), MLP with BatchNorm and Dropout, HuberLoss for robustness, symmetry TTA for mixed solvents, and bagging of 3 models. The hypothesis was that combining these techniques would create a strong baseline. The reported CV MSE of 0.011303 already beats the target of 0.017270.\n\n## Technical Execution Assessment\n\n**Validation**: The CV methodology is sound. Leave-one-solvent-out (24 folds) for single solvent and leave-one-ramp-out (13 folds) for full data matches the competition specification. The submission has the correct structure: 656 single solvent samples + 1227 full data samples = 1883 total rows.\n\n**Leakage Risk**: No evidence of leakage detected. The model is trained fresh for each fold, features are computed from training data only, and the same hyperparameters are used across all folds. The symmetry augmentation for mixed solvents is applied correctly (training on both A,B and B,A orderings, then TTA at inference).\n\n**Score Integrity**: Verified in notebook output:\n- Single Solvent CV MSE: 0.010916\n- Full Data CV MSE: 0.011510  \n- Overall CV MSE: 0.011303\nThe weighted average calculation is correct.\n\n**Code Quality**: \n- Seeds are not explicitly set for reproducibility - this could cause variance between runs\n- The notebook structure does NOT follow the required template format. The competition rules state the last three cells must match the template exactly, with only the model definition line changeable. The current notebook has a completely custom structure.\n\n**Verdict: CONCERNS**\n\nThe CV score is trustworthy, but the submission structure is non-compliant with competition rules. This is a critical issue that must be fixed before any Kaggle submission.\n\n## Strategic Assessment\n\n**Approach Fit**: Excellent. The approach leverages domain knowledge (Arrhenius kinetics for chemical reactions), exploits the physical symmetry of solvent mixtures, and uses appropriate regularization (BatchNorm, Dropout, HuberLoss) for the small dataset size (~600-1200 samples). The choice of spange_descriptors (13 features) over high-dimensional alternatives (drfps: 2048, fragprints: 2133) is sensible for this data size.\n\n**Effort Allocation**: Good prioritization. The researcher started with a strong baseline combining multiple proven techniques rather than iterating on marginal improvements. This is the right approach.\n\n**Assumptions**: \n1. Linear interpolation of solvent features for mixtures (A*(1-pct) + B*pct) - reasonable but could be improved\n2. Same architecture for single and mixed solvents - may not be optimal\n3. 200 epochs is sufficient - no early stopping or validation monitoring\n\n**Blind Spots**:\n1. **No per-fold variance reporting** - we don't know if some folds are much harder than others\n2. **No comparison to simpler baselines** - hard to know which components are contributing most\n3. **Gaussian Processes not explored** - the strategy notes suggest GPs could excel for extrapolation to unseen solvents\n4. **No feature importance analysis** - which Arrhenius features matter most?\n\n**Trajectory**: Very promising start. The CV score of 0.011303 already beats the target of 0.017270 by a significant margin (~35% improvement). However, this is a local CV score - we need to verify it holds on the actual leaderboard.\n\n## What's Working\n\n1. **Physics-informed features**: The Arrhenius kinetics features (inv_temp, log_time, interaction) are well-motivated by chemistry domain knowledge\n2. **Symmetry handling**: Both training augmentation and TTA for mixed solvents is a clever way to exploit physical invariance\n3. **Regularization choices**: BatchNorm, Dropout(0.2), HuberLoss, weight_decay are all appropriate for small data\n4. **Feature selection**: Using compact spange_descriptors (13 features) rather than high-dimensional alternatives\n\n## Key Concerns\n\n### 1. **CRITICAL: Submission Structure Non-Compliance**\n- **Observation**: The notebook does not follow the required template structure. The competition explicitly states: \"the submission must have the same last three cells as in the notebook template, with the only allowed change being the line where the model is defined.\"\n- **Why it matters**: A non-compliant submission will likely be disqualified or fail to score correctly on Kaggle.\n- **Suggestion**: Refactor the code to define a `SymmetricBaggedModel` class that inherits from `BaseModel` and can be instantiated with `model = SymmetricBaggedModel(data='single')` in the template's third-to-last cell. All custom logic (featurization, training, TTA) must be encapsulated in the model class.\n\n### 2. **No Reproducibility Seeds**\n- **Observation**: No random seeds are set for PyTorch, NumPy, or Python's random module.\n- **Why it matters**: Results may vary between runs, making it hard to compare experiments reliably.\n- **Suggestion**: Add `torch.manual_seed(42)`, `np.random.seed(42)`, and `torch.backends.cudnn.deterministic = True` at the start.\n\n### 3. **No Early Stopping or Validation Monitoring**\n- **Observation**: Training runs for a fixed 200 epochs without monitoring validation loss.\n- **Why it matters**: Could be overfitting or underfitting without knowing. Some folds may need more/fewer epochs.\n- **Suggestion**: Consider adding a small validation split (10%) for early stopping, or at least log training loss curves.\n\n### 4. **CV Score Not Yet Validated on Leaderboard**\n- **Observation**: The 0.011303 CV score is promising but hasn't been submitted to Kaggle.\n- **Why it matters**: CV-LB gaps can be significant, especially with leave-one-out CV on small data.\n- **Suggestion**: After fixing the submission structure, submit to verify the score holds.\n\n## Top Priority for Next Experiment\n\n**FIX THE SUBMISSION STRUCTURE IMMEDIATELY.** The current notebook cannot be submitted to Kaggle in its current form. Refactor the code to:\n\n1. Create a self-contained model class that encapsulates all logic (featurization, training, TTA)\n2. Ensure the class can be instantiated with `model = YourModel(data='single')` and `model = YourModel(data='full')`\n3. Use the exact last three cells from the template notebook\n4. Add reproducibility seeds\n\nOnce compliant, submit to Kaggle to verify the CV score translates to the leaderboard. The 0.011303 CV score is excellent and beats the target, but it means nothing if we can't submit it.\n\nSecondary priorities for improvement after fixing compliance:\n- Add per-fold variance analysis to identify hard cases\n- Try Gaussian Processes for potentially better extrapolation\n- Experiment with deeper/wider architectures or different learning rate schedules\n- Consider ensemble with LightGBM for diversity\n",
      "experiment_id": "exp_000",
      "agent": "evaluator",
      "timestamp": "2026-01-13T21:40:58.650419"
    }
  ],
  "data_findings": [
    {
      "finding": "Arrhenius Kinetics approach: Add 1/Temperature (in Kelvin), ln(Time), and their interaction as features. This physics-informed feature engineering improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical Symmetry TTA: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped, then average. This respects physical symmetry of mixtures.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Bagging ensemble of 7 MLP models with BatchNorm, Dropout(0.2), HuberLoss, and Sigmoid output activation. Uses ReduceLROnPlateau scheduler.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data augmentation for mixed solvents: Train on both (A,B) and (B,A) symmetric versions of the data to double training set size.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "LightGBM approach: Use 3 separate LightGBM regressors (one per target), with early stopping (100 rounds patience), learning_rate=0.03, max_depth=6, and regularization. Uses spange_descriptors features.",
      "source": "../research/kernels/tobimichigan_catechol-pipeline-with-lightgbm/catechol-pipeline-with-lightgbm.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature engineering ideas: Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip predictions to [0,1] and normalize rows to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Targets (SM, Product 2, Product 3) do NOT sum to 1 - mean sum is ~0.8 with range 0.03-1.12. This indicates mass loss in the reaction - don't force normalization to 1.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Single solvent CV: 24 folds (leave-one-solvent-out), ~27 samples per test fold. Full data CV: 13 folds (leave-one-ramp-out), ~94 samples per test fold.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature dimensions: spange_descriptors (13 features), acs_pca (5 features), drfps (2048 features), fragprints (2133 features). Spange is most compact and commonly used.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Multi-output regression: For correlated targets like chemical yields, consider regressor chains (feed one prediction as input to next), joint multi-output models, or Curds & Whey post-processing for linear shrinkage.",
      "source": "Web search: multi-output regression techniques",
      "agent": "explorer"
    },
    {
      "finding": "Gaussian Processes excel for small datasets (~600-1200 samples) and provide uncertainty quantification. Deep Kernel Learning (DKL) combines NN feature learning with GP uncertainty for best of both worlds.",
      "source": "Web search: Gaussian Processes vs Neural Networks for chemistry",
      "agent": "explorer"
    },
    {
      "finding": "CRITICAL CV-LB GAP: CV 0.0113 vs LB 0.0998 (9x gap). The best public kernel achieves LB 0.098, suggesting our LB score is actually competitive. The CV calculation may not match how Kaggle evaluates. The target of 0.017270 is BELOW the best public kernel score of 0.098, suggesting the target might be achievable through different approaches or the target represents a different metric.",
      "source": "exploration/evolver_loop1_lb_feedback.ipynb",
      "agent": "evolver"
    },
    {
      "finding": "Competition uses custom 'catechol_hackathon_metric' for evaluation. The submission format includes fold, row, target_1, target_2, target_3 columns. The CV procedure is: (a) leave-one-solvent-out for single solvent data (24 folds), (b) leave-one-ramp-out for mixture data (13 folds). The LB score of 0.0998 is close to the best public kernel (0.098), suggesting our model is competitive but our local CV calculation may differ from the official metric.",
      "source": "research/description.md",
      "agent": "evolver"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields with small datasets?",
      "finding": "Predicting reaction yields when only a few hundred or fewer examples are available requires models that can extract maximal information from limited data and that can express their confidence.  Recent work shows that **transfer\u2011learning\u2011based transformers** fine\u2011tuned on a small, augmented set of reactions are among the most reliable choices.  By representing a reaction as a text string (e.g., SMILES) and pre\u2011training on millions of generic reactions, the model learns a rich chemical language; a modest amount of task\u2011specific data (\u22482\u20135\u202f% of the original high\u2011throughput set) is then sufficient to outperform physics\u2011based descriptor models, especially when test\u2011time augmentation is used to generate uncertainty estimates that correlate with prediction error\u30102\u2020https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches\u3011.  The same strategy is highlighted in a broader review of low\u2011data limits, which recommends fine\u2011tun",
      "agent": "explorer"
    },
    {
      "query": "How do Gaussian Processes perform for chemical reaction yield prediction compared to neural networks?",
      "finding": "Gaussian\u2011process (GP) models excel when only a few hundred reaction datapoints are available: a GP regression trained on 100\u2013150 experimental rate constants predicted activation\u2011energy barriers with a mean absolute error of\u202f0.77\u202fkcal\u202fmol\u207b\u00b9 and supplied calibrated error bars for risk assessment\u202f([ScienceDirect\u202f2023](https://www.sciencedirect.com/science/article/pii/S1093326322002352)). This uncertainty quantification and exact Bayesian optimisation make GPs attractive for low\u2011data yield or barrier predictions, but the O(n\u00b3) scaling of exact inference limits their use on larger reaction libraries\u202f([Quora\u202f2012](https://www.quora.com/What-are-some-advantages-of-using-Gaussian-Process-Models-vs-Neural-Networks)).\n\nNeural\u2011network approaches\u2014especially long\u2011short\u2011term memory (LSTM) and graph neural networks (GNNs)\u2014have achieved comparable or better point\u2011prediction accuracy on richer datasets. For ten Buchwald\u2011Hartwig couplings, an LSTM model predicted the instantaneous yield with a mean abso",
      "agent": "explorer"
    },
    {
      "query": "What are the best techniques for multi-output regression when targets are correlated chemical yields?",
      "finding": "Multi\u2011output regression aims to predict several continuous targets at once, and when those targets are chemically linked (e.g., yields of related reactions) exploiting their correlation can markedly improve accuracy.  The literature groups the most effective approaches into **problem\u2011transformation** methods\u2014where a standard univariate learner is applied to a reshaped target set\u2014and **algorithm\u2011adaptation** methods that modify the learning algorithm to handle multiple outputs directly\u202f([PDF\u202fsurvey](https://oa.upm.es/40804/1/INVE_MEM_2015_204213.pdf)).\n\nFor correlated chemical yields, the top\u2011performing techniques are:\n\n* **Regressor\u2011chain and stacking variants** that feed the prediction of one yield as an input to the next, thereby transferring information along the chain\u202f([Multi\u2011target SVR via correlation regressor chains](https://www.sciencedirect.com/science/article/abs/pii/S0020025517307946)).  \n* **Multi\u2011output support\u2011vector regression**, especially Bayesian formulations that mod",
      "agent": "explorer"
    },
    {
      "query": "What is the evaluation metric for the Catechol Benchmark Hackathon Kaggle competition?",
      "finding": "The provided search results do not contain information about the evaluation metric used for the Catechol Benchmark Hackathon Kaggle competition. ([Kaggle\u202fdocs](https://www.kaggle.com/docs/competitions-setup), [Kaggle\u202fbenchmarks](https://www.kaggle.com/benchmarks))",
      "agent": "evolver"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 4
}