{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77bb185",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis\n",
    "\n",
    "## Key Questions:\n",
    "1. Can per-target heterogeneous models improve performance?\n",
    "2. What's the impact of combining multiple feature sets (acs_pca + spange)?\n",
    "3. What's the gap between our best CV (0.0805) and target (0.0173)?\n",
    "4. What approaches from public kernels haven't been tried?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf750fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:44:21.583116Z",
     "iopub.status.busy": "2026-01-14T01:44:21.582643Z",
     "iopub.status.idle": "2026-01-14T01:44:22.561739Z",
     "shell.execute_reply": "2026-01-14T01:44:22.561316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent: (656, 13)\n",
      "Full data: (1227, 19)\n",
      "Spange: (26, 13)\n",
      "ACS PCA: (24, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "DATA_PATH = '/home/data'\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "acs_pca = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "TARGET_LABELS = ['Product 2', 'Product 3', 'SM']\n",
    "print(f'Single solvent: {df_single.shape}')\n",
    "print(f'Full data: {df_full.shape}')\n",
    "print(f'Spange: {spange.shape}')\n",
    "print(f'ACS PCA: {acs_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07277ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:44:22.562820Z",
     "iopub.status.busy": "2026-01-14T01:44:22.562719Z",
     "iopub.status.idle": "2026-01-14T01:44:54.572561Z",
     "shell.execute_reply": "2026-01-14T01:44:54.572153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Per-Target Heterogeneous Models (dabansherwani approach) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Target Heterogeneous (Spange only) MAE: 0.0755 +/- 0.0350\n"
     ]
    }
   ],
   "source": [
    "# Test the dabansherwani approach: per-target heterogeneous models\n",
    "# SM -> HistGradientBoosting\n",
    "# Products -> ExtraTrees\n",
    "\n",
    "print('=== Testing Per-Target Heterogeneous Models (dabansherwani approach) ===')\n",
    "\n",
    "# Prepare features - Spange only first\n",
    "df_test = df_single.copy()\n",
    "for col in spange.columns:\n",
    "    df_test[f'spange_{col}'] = df_test['SOLVENT NAME'].map(spange[col])\n",
    "\n",
    "# Add process features\n",
    "df_test['inv_temp'] = 1000 / (df_test['Temperature'] + 273.15)\n",
    "df_test['log_time'] = np.log(df_test['Residence Time'] + 1e-6)\n",
    "df_test['interaction'] = df_test['inv_temp'] * df_test['log_time']\n",
    "\n",
    "process_cols = ['Residence Time', 'Temperature', 'inv_temp', 'log_time', 'interaction']\n",
    "spange_cols = [f'spange_{col}' for col in spange.columns]\n",
    "feature_cols = process_cols + spange_cols\n",
    "\n",
    "X = df_test[feature_cols].values\n",
    "y = df_test[TARGET_LABELS].values\n",
    "groups = df_test['SOLVENT NAME'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Per-target heterogeneous model\n",
    "per_target_errors = []\n",
    "for train_idx, test_idx in logo.split(X_scaled, y, groups):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    preds = np.zeros_like(y_test)\n",
    "    \n",
    "    # SM -> HistGradientBoosting\n",
    "    hgb = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "    hgb.fit(X_train, y_train[:, 2])  # SM is index 2\n",
    "    preds[:, 2] = hgb.predict(X_test)\n",
    "    \n",
    "    # Products -> ExtraTrees\n",
    "    for t in [0, 1]:  # Product 2, Product 3\n",
    "        etr = ExtraTreesRegressor(n_estimators=900, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "        etr.fit(X_train, y_train[:, t])\n",
    "        preds[:, t] = etr.predict(X_test)\n",
    "    \n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    per_target_errors.append(mae)\n",
    "\n",
    "print(f'Per-Target Heterogeneous (Spange only) MAE: {np.mean(per_target_errors):.4f} +/- {np.std(per_target_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6d215a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:44:54.573800Z",
     "iopub.status.busy": "2026-01-14T01:44:54.573697Z",
     "iopub.status.idle": "2026-01-14T01:45:57.289984Z",
     "shell.execute_reply": "2026-01-14T01:45:57.289488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Combined Features (acs_pca + spange) with Weighted Averaging ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Features (0.65 acs + 0.35 spange) MAE: 0.0670 +/- 0.0322\n"
     ]
    }
   ],
   "source": [
    "# Now test with combined features (acs_pca + spange) with weighted averaging\n",
    "print('\\n=== Testing Combined Features (acs_pca + spange) with Weighted Averaging ===')\n",
    "\n",
    "# Prepare acs_pca features\n",
    "df_test_acs = df_single.copy()\n",
    "for col in acs_pca.columns:\n",
    "    df_test_acs[f'acs_{col}'] = df_test_acs['SOLVENT NAME'].map(acs_pca[col])\n",
    "\n",
    "# Add process features\n",
    "df_test_acs['inv_temp'] = 1000 / (df_test_acs['Temperature'] + 273.15)\n",
    "df_test_acs['log_time'] = np.log(df_test_acs['Residence Time'] + 1e-6)\n",
    "df_test_acs['interaction'] = df_test_acs['inv_temp'] * df_test_acs['log_time']\n",
    "\n",
    "acs_cols = [f'acs_{col}' for col in acs_pca.columns]\n",
    "feature_cols_acs = process_cols + acs_cols\n",
    "\n",
    "X_acs = df_test_acs[feature_cols_acs].values\n",
    "scaler_acs = StandardScaler()\n",
    "X_acs_scaled = scaler_acs.fit_transform(X_acs)\n",
    "\n",
    "# Per-target with weighted averaging of two feature sets\n",
    "combined_errors = []\n",
    "for train_idx, test_idx in logo.split(X_scaled, y, groups):\n",
    "    X_train_sp, X_test_sp = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    X_train_acs, X_test_acs = X_acs_scaled[train_idx], X_acs_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    preds_sp = np.zeros_like(y_test)\n",
    "    preds_acs = np.zeros_like(y_test)\n",
    "    \n",
    "    # SM -> HistGradientBoosting on both feature sets\n",
    "    hgb_sp = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "    hgb_sp.fit(X_train_sp, y_train[:, 2])\n",
    "    preds_sp[:, 2] = hgb_sp.predict(X_test_sp)\n",
    "    \n",
    "    hgb_acs = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "    hgb_acs.fit(X_train_acs, y_train[:, 2])\n",
    "    preds_acs[:, 2] = hgb_acs.predict(X_test_acs)\n",
    "    \n",
    "    # Products -> ExtraTrees on both feature sets\n",
    "    for t in [0, 1]:\n",
    "        etr_sp = ExtraTreesRegressor(n_estimators=900, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "        etr_sp.fit(X_train_sp, y_train[:, t])\n",
    "        preds_sp[:, t] = etr_sp.predict(X_test_sp)\n",
    "        \n",
    "        etr_acs = ExtraTreesRegressor(n_estimators=900, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "        etr_acs.fit(X_train_acs, y_train[:, t])\n",
    "        preds_acs[:, t] = etr_acs.predict(X_test_acs)\n",
    "    \n",
    "    # Weighted average (0.65 acs_pca + 0.35 spange as per dabansherwani)\n",
    "    preds = 0.65 * preds_acs + 0.35 * preds_sp\n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    \n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    combined_errors.append(mae)\n",
    "\n",
    "print(f'Combined Features (0.65 acs + 0.35 spange) MAE: {np.mean(combined_errors):.4f} +/- {np.std(combined_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb49a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:45:57.291160Z",
     "iopub.status.busy": "2026-01-14T01:45:57.291041Z",
     "iopub.status.idle": "2026-01-14T01:45:57.294015Z",
     "shell.execute_reply": "2026-01-14T01:45:57.293664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison of Approaches ===\n",
      "Our best (exp_003 RF): 0.0748 (single solvent)\n",
      "Per-Target Heterogeneous (Spange): 0.0755\n",
      "Per-Target + Combined Features: 0.0670\n",
      "Target: 0.0173\n",
      "\n",
      "Gap to target: 0.0497 (3.9x)\n"
     ]
    }
   ],
   "source": [
    "# Compare all approaches\n",
    "print('\\n=== Comparison of Approaches ===')\n",
    "print(f'Our best (exp_003 RF): 0.0748 (single solvent)')\n",
    "print(f'Per-Target Heterogeneous (Spange): {np.mean(per_target_errors):.4f}')\n",
    "print(f'Per-Target + Combined Features: {np.mean(combined_errors):.4f}')\n",
    "print(f'Target: 0.0173')\n",
    "print(f'\\nGap to target: {np.mean(combined_errors) - 0.0173:.4f} ({(np.mean(combined_errors)/0.0173):.1f}x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beaf171c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:45:57.294946Z",
     "iopub.status.busy": "2026-01-14T01:45:57.294853Z",
     "iopub.status.idle": "2026-01-14T01:50:37.721614Z",
     "shell.execute_reply": "2026-01-14T01:50:37.721093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Different Weight Combinations ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.3, spange=0.7: MAE = 0.0709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.4, spange=0.6: MAE = 0.0697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.5, spange=0.5: MAE = 0.0685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.6, spange=0.4: MAE = 0.0676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.7, spange=0.3: MAE = 0.0668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight acs=0.8, spange=0.2: MAE = 0.0662\n",
      "\n",
      "Best weight: acs=0.8, spange=0.2 with MAE=0.0662\n"
     ]
    }
   ],
   "source": [
    "# Test different weight combinations\n",
    "print('\\n=== Testing Different Weight Combinations ===')\n",
    "\n",
    "best_weight = None\n",
    "best_mae = float('inf')\n",
    "\n",
    "for w_acs in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    w_sp = 1 - w_acs\n",
    "    \n",
    "    weight_errors = []\n",
    "    for train_idx, test_idx in logo.split(X_scaled, y, groups):\n",
    "        X_train_sp, X_test_sp = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        X_train_acs, X_test_acs = X_acs_scaled[train_idx], X_acs_scaled[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        preds_sp = np.zeros_like(y_test)\n",
    "        preds_acs = np.zeros_like(y_test)\n",
    "        \n",
    "        # SM -> HistGradientBoosting\n",
    "        hgb_sp = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "        hgb_sp.fit(X_train_sp, y_train[:, 2])\n",
    "        preds_sp[:, 2] = hgb_sp.predict(X_test_sp)\n",
    "        \n",
    "        hgb_acs = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "        hgb_acs.fit(X_train_acs, y_train[:, 2])\n",
    "        preds_acs[:, 2] = hgb_acs.predict(X_test_acs)\n",
    "        \n",
    "        # Products -> ExtraTrees\n",
    "        for t in [0, 1]:\n",
    "            etr_sp = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "            etr_sp.fit(X_train_sp, y_train[:, t])\n",
    "            preds_sp[:, t] = etr_sp.predict(X_test_sp)\n",
    "            \n",
    "            etr_acs = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "            etr_acs.fit(X_train_acs, y_train[:, t])\n",
    "            preds_acs[:, t] = etr_acs.predict(X_test_acs)\n",
    "        \n",
    "        preds = w_acs * preds_acs + w_sp * preds_sp\n",
    "        preds = np.clip(preds, 0, 1)\n",
    "        \n",
    "        mae = np.mean(np.abs(preds - y_test))\n",
    "        weight_errors.append(mae)\n",
    "    \n",
    "    mean_mae = np.mean(weight_errors)\n",
    "    print(f'Weight acs={w_acs:.1f}, spange={w_sp:.1f}: MAE = {mean_mae:.4f}')\n",
    "    \n",
    "    if mean_mae < best_mae:\n",
    "        best_mae = mean_mae\n",
    "        best_weight = w_acs\n",
    "\n",
    "print(f'\\nBest weight: acs={best_weight:.1f}, spange={1-best_weight:.1f} with MAE={best_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f5bef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:50:37.722765Z",
     "iopub.status.busy": "2026-01-14T01:50:37.722641Z",
     "iopub.status.idle": "2026-01-14T01:50:41.903254Z",
     "shell.execute_reply": "2026-01-14T01:50:41.902834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Simpler Models with Combined Features ===\n",
      "Combined feature shape: (656, 23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF with Combined Features MAE: 0.0746 +/- 0.0351\n"
     ]
    }
   ],
   "source": [
    "# Test simpler models with combined features\n",
    "print('\\n=== Testing Simpler Models with Combined Features ===')\n",
    "\n",
    "# Concatenate both feature sets\n",
    "X_combined = np.hstack([X_scaled, X_acs_scaled[:, len(process_cols):]])  # Avoid duplicating process cols\n",
    "print(f'Combined feature shape: {X_combined.shape}')\n",
    "\n",
    "# Test RF with combined features\n",
    "rf_combined_errors = []\n",
    "for train_idx, test_idx in logo.split(X_combined, y, groups):\n",
    "    X_train, X_test = X_combined[train_idx], X_combined[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators=200, max_depth=8, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test)\n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    \n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    rf_combined_errors.append(mae)\n",
    "\n",
    "print(f'RF with Combined Features MAE: {np.mean(rf_combined_errors):.4f} +/- {np.std(rf_combined_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96f514e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:50:41.904342Z",
     "iopub.status.busy": "2026-01-14T01:50:41.904227Z",
     "iopub.status.idle": "2026-01-14T01:51:21.696854Z",
     "shell.execute_reply": "2026-01-14T01:51:21.696440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing ExtraTrees with Different Regularization ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=5, min_samples_leaf=2: MAE = 0.0722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=5, min_samples_leaf=5: MAE = 0.0727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=5, min_samples_leaf=10: MAE = 0.0740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=8, min_samples_leaf=2: MAE = 0.0693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=8, min_samples_leaf=5: MAE = 0.0728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=8, min_samples_leaf=10: MAE = 0.0743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=10, min_samples_leaf=2: MAE = 0.0687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=10, min_samples_leaf=5: MAE = 0.0708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=10, min_samples_leaf=10: MAE = 0.0748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=15, min_samples_leaf=2: MAE = 0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=15, min_samples_leaf=5: MAE = 0.0712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=15, min_samples_leaf=10: MAE = 0.0741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=None, min_samples_leaf=2: MAE = 0.0696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=None, min_samples_leaf=5: MAE = 0.0711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR max_depth=None, min_samples_leaf=10: MAE = 0.0741\n"
     ]
    }
   ],
   "source": [
    "# Test ExtraTrees with different regularization\n",
    "print('\\n=== Testing ExtraTrees with Different Regularization ===')\n",
    "\n",
    "for max_depth in [5, 8, 10, 15, None]:\n",
    "    for min_samples_leaf in [2, 5, 10]:\n",
    "        etr_errors = []\n",
    "        for train_idx, test_idx in logo.split(X_scaled, y, groups):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            etr = ExtraTreesRegressor(n_estimators=200, max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=42, n_jobs=-1)\n",
    "            etr.fit(X_train, y_train)\n",
    "            preds = etr.predict(X_test)\n",
    "            preds = np.clip(preds, 0, 1)\n",
    "            \n",
    "            mae = np.mean(np.abs(preds - y_test))\n",
    "            etr_errors.append(mae)\n",
    "        \n",
    "        print(f'ETR max_depth={max_depth}, min_samples_leaf={min_samples_leaf}: MAE = {np.mean(etr_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403cf63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:51:34.896415Z",
     "iopub.status.busy": "2026-01-14T01:51:34.895859Z",
     "iopub.status.idle": "2026-01-14T01:51:34.899306Z",
     "shell.execute_reply": "2026-01-14T01:51:34.898916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY ===\n",
      "\n",
      "Key findings:\n",
      "1. Per-target heterogeneous models (HGB for SM, ETR for Products) show promise\n",
      "2. Combining acs_pca + spange features may help\n",
      "3. The gap to target (0.0173) is still ~4-5x\n",
      "\n",
      "Recommendations for next experiment:\n",
      "1. Implement per-target heterogeneous model with combined features\n",
      "2. Try different weight combinations for feature set averaging\n",
      "3. Consider more aggressive regularization for tree models\n",
      "4. The target may require fundamentally different approaches (transformers, GPs)\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print('\\n=== SUMMARY ===')\n",
    "print('\\nKey findings:')\n",
    "print('1. Per-target heterogeneous models (HGB for SM, ETR for Products) show promise')\n",
    "print('2. Combining acs_pca + spange features may help')\n",
    "print('3. The gap to target (0.0173) is still ~4-5x')\n",
    "print('\\nRecommendations for next experiment:')\n",
    "print('1. Implement per-target heterogeneous model with combined features')\n",
    "print('2. Try different weight combinations for feature set averaging')\n",
    "print('3. Consider more aggressive regularization for tree models')\n",
    "print('4. The target may require fundamentally different approaches (transformers, GPs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f95b517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:51:53.631700Z",
     "iopub.status.busy": "2026-01-14T01:51:53.631222Z",
     "iopub.status.idle": "2026-01-14T01:52:09.489684Z",
     "shell.execute_reply": "2026-01-14T01:52:09.489173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing on Full Data (Mixed Solvents) ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Target Heterogeneous (Full Data) MAE: 0.0919 +/- 0.0245\n"
     ]
    }
   ],
   "source": [
    "# Test on full data (mixed solvents) as well\n",
    "print('\\n=== Testing on Full Data (Mixed Solvents) ===')\n",
    "\n",
    "# Prepare features for full data\n",
    "df_full_test = df_full.copy()\n",
    "\n",
    "# Add spange features for mixed solvents\n",
    "for col in spange.columns:\n",
    "    df_full_test[f'spange_A_{col}'] = df_full_test['SOLVENT A NAME'].map(spange[col])\n",
    "    df_full_test[f'spange_B_{col}'] = df_full_test['SOLVENT B NAME'].map(spange[col])\n",
    "\n",
    "# Create mixed features\n",
    "for col in spange.columns:\n",
    "    df_full_test[f'spange_mix_{col}'] = (\n",
    "        df_full_test[f'spange_A_{col}'] * (1 - df_full_test['SolventB%']/100) +\n",
    "        df_full_test[f'spange_B_{col}'] * (df_full_test['SolventB%']/100)\n",
    "    )\n",
    "\n",
    "# Add process features\n",
    "df_full_test['inv_temp'] = 1000 / (df_full_test['Temperature'] + 273.15)\n",
    "df_full_test['log_time'] = np.log(df_full_test['Residence Time'] + 1e-6)\n",
    "df_full_test['interaction'] = df_full_test['inv_temp'] * df_full_test['log_time']\n",
    "\n",
    "process_cols_full = ['Residence Time', 'Temperature', 'SolventB%', 'inv_temp', 'log_time', 'interaction']\n",
    "spange_mix_cols = [f'spange_mix_{col}' for col in spange.columns]\n",
    "feature_cols_full = process_cols_full + spange_mix_cols\n",
    "\n",
    "X_full = df_full_test[feature_cols_full].values\n",
    "y_full = df_full_test[TARGET_LABELS].values\n",
    "\n",
    "# Create groups for leave-one-ramp-out\n",
    "df_full_test['ramp'] = df_full_test['SOLVENT A NAME'] + '_' + df_full_test['SOLVENT B NAME']\n",
    "groups_full = df_full_test['ramp'].values\n",
    "\n",
    "scaler_full = StandardScaler()\n",
    "X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "\n",
    "# Test per-target heterogeneous on full data\n",
    "logo_full = LeaveOneGroupOut()\n",
    "full_errors = []\n",
    "for train_idx, test_idx in logo_full.split(X_full_scaled, y_full, groups_full):\n",
    "    X_train, X_test = X_full_scaled[train_idx], X_full_scaled[test_idx]\n",
    "    y_train, y_test = y_full[train_idx], y_full[test_idx]\n",
    "    \n",
    "    preds = np.zeros_like(y_test)\n",
    "    \n",
    "    # SM -> HistGradientBoosting\n",
    "    hgb = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\n",
    "    hgb.fit(X_train, y_train[:, 2])\n",
    "    preds[:, 2] = hgb.predict(X_test)\n",
    "    \n",
    "    # Products -> ExtraTrees\n",
    "    for t in [0, 1]:\n",
    "        etr = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, max_depth=10, random_state=42, n_jobs=-1)\n",
    "        etr.fit(X_train, y_train[:, t])\n",
    "        preds[:, t] = etr.predict(X_test)\n",
    "    \n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    mae = np.mean(np.abs(preds - y_test))\n",
    "    full_errors.append(mae)\n",
    "\n",
    "print(f'Per-Target Heterogeneous (Full Data) MAE: {np.mean(full_errors):.4f} +/- {np.std(full_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a317f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:52:29.732198Z",
     "iopub.status.busy": "2026-01-14T01:52:29.731763Z",
     "iopub.status.idle": "2026-01-14T01:52:29.734638Z",
     "shell.execute_reply": "2026-01-14T01:52:29.734303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test with TTA for mixed solvents (flip A and B)\\nprint('\\\\n=== Testing with TTA for Mixed Solvents ===')\\n\\n# Test per-target heterogeneous with TTA\\nfull_tta_errors = []\\nfor train_idx, test_idx in logo_full.split(X_full_scaled, y_full, groups_full):\\n    X_train, X_test = X_full_scaled[train_idx], X_full_scaled[test_idx]\\n    y_train, y_test = y_full[train_idx], y_full[test_idx]\\n    \\n    # Also create flipped training data\\n    df_train = df_full_test.iloc[train_idx].copy()\\n    df_test = df_full_test.iloc[test_idx].copy()\\n    \\n    # Create flipped features for training\\n    df_train_flip = df_train.copy()\\n    for col in spange.columns:\\n        df_train_flip[f'spange_mix_{col}'] = (\\n            df_train[f'spange_B_{col}'] * (1 - df_train['SolventB%']/100) +\\n            df_train[f'spange_A_{col}'] * (df_train['SolventB%']/100)\\n        )\\n    \\n    X_train_flip = df_train_flip[feature_cols_full].values\\n    X_train_flip_scaled = scaler_full.transform(X_train_flip)\\n    \\n    # Augment training data\\n    X_train_aug = np.vstack([X_train, X_train_flip_scaled])\\n    y_train_aug = np.vstack([y_train, y_train])\\n    \\n    preds = np.zeros_like(y_test)\\n    \\n    # SM -> HistGradientBoosting\\n    hgb = HistGradientBoostingRegressor(max_depth=7, max_iter=700, learning_rate=0.04, random_state=42)\\n    hgb.fit(X_train_aug, y_train_aug[:, 2])\\n    preds[:, 2] = hgb.predict(X_test)\\n    \\n    # Products -> ExtraTrees\\n    for t in [0, 1]:\\n        etr = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, max_depth=10, random_state=42, n_jobs=-1)\\n        etr.fit(X_train_aug, y_train_aug[:, t])\\n        preds[:, t] = etr.predict(X_test)\\n    \\n    # TTA: also predict with flipped test features\\n    df_test_flip = df_test.copy()\\n    for col in spange.columns:\\n        df_test_flip[f'spange_mix_{col}'] = (\\n            df_test[f'spange_B_{col}'] * (1 - df_test['SolventB%']/100) +\\n            df_test[f'spange_A_{col}'] * (df_test['SolventB%']/100)\\n        )\\n    X_test_flip = df_test_flip[feature_cols_full].values\\n    X_test_flip_scaled = scaler_full.transform(X_test_flip)\\n    \\n    preds_flip = np.zeros_like(y_test)\\n    preds_flip[:, 2] = hgb.predict(X_test_flip_scaled)\\n    for t in [0, 1]:\\n        etr = ExtraTreesRegressor(n_estimators=500, min_samples_leaf=2, max_depth=10, random_state=42, n_jobs=-1)\\n        etr.fit(X_train_aug, y_train_aug[:, t])\\n        preds_flip[:, t] = etr.predict(X_test_flip_scaled)\\n    \\n    # Average predictions\\n    preds = (preds + preds_flip) / 2\\n    preds = np.clip(preds, 0, 1)\\n    \\n    mae = np.mean(np.abs(preds - y_test))\\n    full_tta_errors.append(mae)\\n\\nprint(f'Per-Target Heterogeneous with TTA (Full Data) MAE: {np.mean(full_tta_errors):.4f} +/- {np.std(full_tta_errors):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec0febb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T01:52:55.158310Z",
     "iopub.status.busy": "2026-01-14T01:52:55.157719Z",
     "iopub.status.idle": "2026-01-14T01:52:55.160305Z",
     "shell.execute_reply": "2026-01-14T01:52:55.159979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final summary with combined score estimate\\nprint('\\\\n=== FINAL SUMMARY ===')\\nprint('\\\\nSingle Solvent Results:')\\nprint(f'  Current best (exp_003 RF): 0.0748')\\nprint(f'  Per-Target Heterogeneous + Combined Features: 0.0662')\\nprint(f'  Improvement: {(0.0748 - 0.0662)/0.0748 * 100:.1f}%')\\n\\nprint('\\\\nFull Data Results:')\\nprint(f'  Current best (exp_003 RF): 0.0836')\\nprint(f'  Per-Target Heterogeneous: 0.0919 (worse without TTA)')\\n\\nprint('\\\\nRecommendation:')\\nprint('  Implement per-target heterogeneous model with:')\\nprint('  - Combined features (0.8 acs_pca + 0.2 spange)')\\nprint('  - HGB for SM, ETR for Products')\\nprint('  - TTA for mixed solvents')\\nprint('  - Expected combined score: ~0.075 (vs current 0.0805)')\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
