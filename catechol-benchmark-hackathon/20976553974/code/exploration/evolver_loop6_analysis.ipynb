{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e38419",
   "metadata": {},
   "source": [
    "# Loop 6 Analysis: Understanding the CV-LB Gap and Finding Better Approaches\n",
    "\n",
    "## Key Problem\n",
    "- Best CV: 0.0623 (exp_004)\n",
    "- Best LB: 0.0956 (53% worse!)\n",
    "- Target: 0.01727 (5.5x better than LB)\n",
    "\n",
    "## Hypotheses to Test\n",
    "1. Our CV is too optimistic - need more aggressive validation\n",
    "2. Higher-dimensional features (DRFP, fragprints) might generalize better\n",
    "3. Intermediate regularization (not as extreme as Ridge) might be optimal\n",
    "4. Ensemble of diverse models might reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be682f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:27:56.326506Z",
     "iopub.status.busy": "2026-01-14T02:27:56.325999Z",
     "iopub.status.idle": "2026-01-14T02:27:56.377131Z",
     "shell.execute_reply": "2026-01-14T02:27:56.376771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13)\n",
      "ACS_PCA: (24, 5)\n",
      "DRFP: (24, 2048)\n",
      "Fragprints: (24, 2133)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "# Load data\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "# Load features\n",
    "spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "acs_pca = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "drfp = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "fragprints = pd.read_csv(f'{DATA_PATH}/fragprints_lookup.csv', index_col=0)\n",
    "\n",
    "print(f\"Spange: {spange.shape}\")\n",
    "print(f\"ACS_PCA: {acs_pca.shape}\")\n",
    "print(f\"DRFP: {drfp.shape}\")\n",
    "print(f\"Fragprints: {fragprints.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa135ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:27:56.378120Z",
     "iopub.status.busy": "2026-01-14T02:27:56.378020Z",
     "iopub.status.idle": "2026-01-14T02:27:58.840938Z",
     "shell.execute_reply": "2026-01-14T02:27:58.840380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing CV strategies with ExtraTrees (max_depth=10)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leave-One-Out CV: 0.0687 +/- 0.0371\n",
      "GroupKFold (5):   0.0730 +/- 0.0117\n",
      "\n",
      "GroupKFold is more pessimistic by 0.0043\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Compare Leave-One-Out vs GroupKFold (5-fold)\n",
    "# GroupKFold should give more realistic CV estimates\n",
    "\n",
    "def build_features_single(X, feature_df):\n",
    "    \"\"\"Build features for single solvent data.\"\"\"\n",
    "    rt = X['Residence Time'].values.reshape(-1, 1)\n",
    "    temp = X['Temperature'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Arrhenius features\n",
    "    temp_k = temp + 273.15\n",
    "    inv_temp = 1000.0 / temp_k\n",
    "    log_time = np.log(rt + 1e-6)\n",
    "    interaction = inv_temp * log_time\n",
    "    \n",
    "    process_feats = np.hstack([rt, temp, inv_temp, log_time, interaction])\n",
    "    solvent_feats = feature_df.loc[X['SOLVENT NAME']].values\n",
    "    \n",
    "    return np.hstack([process_feats, solvent_feats])\n",
    "\n",
    "def leave_one_out_cv(X, Y, feature_df, model_class, model_params):\n",
    "    \"\"\"Leave-one-solvent-out CV.\"\"\"\n",
    "    errors = []\n",
    "    for solvent in sorted(X['SOLVENT NAME'].unique()):\n",
    "        mask = X['SOLVENT NAME'] != solvent\n",
    "        train_X, train_Y = X[mask], Y[mask]\n",
    "        test_X, test_Y = X[~mask], Y[~mask]\n",
    "        \n",
    "        X_train_feat = build_features_single(train_X, feature_df)\n",
    "        X_test_feat = build_features_single(test_X, feature_df)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "        X_test_scaled = scaler.transform(X_test_feat)\n",
    "        \n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, train_Y.values)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        preds = np.clip(preds, 0, 1)\n",
    "        \n",
    "        mae = np.mean(np.abs(preds - test_Y.values))\n",
    "        errors.append(mae)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "def group_kfold_cv(X, Y, feature_df, model_class, model_params, n_splits=5):\n",
    "    \"\"\"GroupKFold CV with solvents as groups.\"\"\"\n",
    "    groups = X['SOLVENT NAME']\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    errors = []\n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        train_X, train_Y = X.iloc[train_idx], Y.iloc[train_idx]\n",
    "        test_X, test_Y = X.iloc[test_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        X_train_feat = build_features_single(train_X, feature_df)\n",
    "        X_test_feat = build_features_single(test_X, feature_df)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "        X_test_scaled = scaler.transform(X_test_feat)\n",
    "        \n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, train_Y.values)\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        preds = np.clip(preds, 0, 1)\n",
    "        \n",
    "        mae = np.mean(np.abs(preds - test_Y.values))\n",
    "        errors.append(mae)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "print(\"Comparing CV strategies with ExtraTrees (max_depth=10)...\")\n",
    "X_single = df_single[['Residence Time', 'Temperature', 'SOLVENT NAME']]\n",
    "Y_single = df_single[TARGET_LABELS]\n",
    "\n",
    "etr_params = {'n_estimators': 100, 'max_depth': 10, 'min_samples_leaf': 2, 'random_state': 42}\n",
    "\n",
    "loo_mean, loo_std = leave_one_out_cv(X_single, Y_single, spange, ExtraTreesRegressor, etr_params)\n",
    "gkf_mean, gkf_std = group_kfold_cv(X_single, Y_single, spange, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "\n",
    "print(f\"\\nLeave-One-Out CV: {loo_mean:.4f} +/- {loo_std:.4f}\")\n",
    "print(f\"GroupKFold (5):   {gkf_mean:.4f} +/- {gkf_std:.4f}\")\n",
    "print(f\"\\nGroupKFold is {'more pessimistic' if gkf_mean > loo_mean else 'more optimistic'} by {abs(gkf_mean - loo_mean):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7572100d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:27:58.842021Z",
     "iopub.status.busy": "2026-01-14T02:27:58.841911Z",
     "iopub.status.idle": "2026-01-14T02:28:01.948374Z",
     "shell.execute_reply": "2026-01-14T02:28:01.947809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing regularization levels with GroupKFold...\n",
      "Ridge(alpha=1.0): 0.1014 +/- 0.0345\n",
      "Ridge(alpha=10.0): 0.0899 +/- 0.0260\n",
      "Ridge(alpha=100.0): 0.0839 +/- 0.0185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR(depth=3): 0.0789 +/- 0.0126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR(depth=5): 0.0724 +/- 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR(depth=7): 0.0713 +/- 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETR(depth=10): 0.0730 +/- 0.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF(depth=5): 0.0800 +/- 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF(depth=7): 0.0772 +/- 0.0118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF(depth=10): 0.0775 +/- 0.0121\n",
      "\n",
      "Best models by GroupKFold CV:\n",
      "             model     mean      std\n",
      "      ETR(depth=7) 0.071322 0.012340\n",
      "      ETR(depth=5) 0.072402 0.010932\n",
      "     ETR(depth=10) 0.072981 0.011675\n",
      "       RF(depth=7) 0.077213 0.011797\n",
      "      RF(depth=10) 0.077486 0.012073\n",
      "      ETR(depth=3) 0.078923 0.012571\n",
      "       RF(depth=5) 0.079954 0.009604\n",
      "Ridge(alpha=100.0) 0.083874 0.018506\n",
      " Ridge(alpha=10.0) 0.089902 0.025995\n",
      "  Ridge(alpha=1.0) 0.101379 0.034456\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Compare different regularization levels\n",
    "# Find the sweet spot between underfitting (Ridge) and overfitting (deep trees)\n",
    "\n",
    "print(\"\\nComparing regularization levels with GroupKFold...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Ridge (strong regularization)\n",
    "for alpha in [1.0, 10.0, 100.0]:\n",
    "    mean, std = group_kfold_cv(X_single, Y_single, spange, Ridge, {'alpha': alpha}, n_splits=5)\n",
    "    results.append({'model': f'Ridge(alpha={alpha})', 'mean': mean, 'std': std})\n",
    "    print(f\"Ridge(alpha={alpha}): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# ExtraTrees with varying depth\n",
    "for depth in [3, 5, 7, 10]:\n",
    "    params = {'n_estimators': 100, 'max_depth': depth, 'min_samples_leaf': 2, 'random_state': 42}\n",
    "    mean, std = group_kfold_cv(X_single, Y_single, spange, ExtraTreesRegressor, params, n_splits=5)\n",
    "    results.append({'model': f'ETR(depth={depth})', 'mean': mean, 'std': std})\n",
    "    print(f\"ETR(depth={depth}): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# RandomForest with varying depth\n",
    "for depth in [5, 7, 10]:\n",
    "    params = {'n_estimators': 100, 'max_depth': depth, 'min_samples_leaf': 2, 'random_state': 42}\n",
    "    mean, std = group_kfold_cv(X_single, Y_single, spange, RandomForestRegressor, params, n_splits=5)\n",
    "    results.append({'model': f'RF(depth={depth})', 'mean': mean, 'std': std})\n",
    "    print(f\"RF(depth={depth}): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('mean')\n",
    "print(\"\\nBest models by GroupKFold CV:\")\n",
    "print(results_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de10f9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:28:01.949631Z",
     "iopub.status.busy": "2026-01-14T02:28:01.949527Z",
     "iopub.status.idle": "2026-01-14T02:28:05.466018Z",
     "shell.execute_reply": "2026-01-14T02:28:05.465646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing feature sets with ETR(depth=7)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange (13-dim): 0.0713 +/- 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACS_PCA (5-dim): 0.0708 +/- 0.0156\n",
      "\n",
      "Testing DRFP with PCA reduction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP-PCA(10): 0.1078 +/- 0.0169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP-PCA(15): 0.1089 +/- 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP-PCA(20): 0.1067 +/- 0.0250\n",
      "\n",
      "Testing Fragprints with PCA reduction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragprints-PCA(10): 0.0957 +/- 0.0266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragprints-PCA(15): 0.1011 +/- 0.0253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragprints-PCA(20): 0.0979 +/- 0.0264\n",
      "\n",
      "Best feature sets:\n",
      "          features     mean      std\n",
      "   ACS_PCA (5-dim) 0.070768 0.015573\n",
      "   Spange (13-dim) 0.071322 0.012340\n",
      "Fragprints-PCA(10) 0.095664 0.026636\n",
      "Fragprints-PCA(20) 0.097934 0.026364\n",
      "Fragprints-PCA(15) 0.101082 0.025321\n",
      "      DRFP-PCA(20) 0.106668 0.025040\n",
      "      DRFP-PCA(10) 0.107816 0.016935\n",
      "      DRFP-PCA(15) 0.108875 0.022584\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Compare different feature sets\n",
    "# Higher-dimensional features might capture chemistry better\n",
    "\n",
    "print(\"\\nComparing feature sets with ETR(depth=7)...\")\n",
    "\n",
    "etr_params = {'n_estimators': 100, 'max_depth': 7, 'min_samples_leaf': 2, 'random_state': 42}\n",
    "\n",
    "feature_results = []\n",
    "\n",
    "# Spange (13-dim)\n",
    "mean, std = group_kfold_cv(X_single, Y_single, spange, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "feature_results.append({'features': 'Spange (13-dim)', 'mean': mean, 'std': std})\n",
    "print(f\"Spange (13-dim): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# ACS_PCA (5-dim)\n",
    "mean, std = group_kfold_cv(X_single, Y_single, acs_pca, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "feature_results.append({'features': 'ACS_PCA (5-dim)', 'mean': mean, 'std': std})\n",
    "print(f\"ACS_PCA (5-dim): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# DRFP with PCA reduction (max 24 components since we have 24 solvents)\n",
    "print(\"\\nTesting DRFP with PCA reduction...\")\n",
    "for n_components in [10, 15, 20]:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    drfp_reduced = pd.DataFrame(\n",
    "        pca.fit_transform(drfp.values),\n",
    "        index=drfp.index,\n",
    "        columns=[f'drfp_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    mean, std = group_kfold_cv(X_single, Y_single, drfp_reduced, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "    feature_results.append({'features': f'DRFP-PCA({n_components})', 'mean': mean, 'std': std})\n",
    "    print(f\"DRFP-PCA({n_components}): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# Fragprints with PCA reduction\n",
    "print(\"\\nTesting Fragprints with PCA reduction...\")\n",
    "for n_components in [10, 15, 20]:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    frag_reduced = pd.DataFrame(\n",
    "        pca.fit_transform(fragprints.values),\n",
    "        index=fragprints.index,\n",
    "        columns=[f'frag_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    mean, std = group_kfold_cv(X_single, Y_single, frag_reduced, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "    feature_results.append({'features': f'Fragprints-PCA({n_components})', 'mean': mean, 'std': std})\n",
    "    print(f\"Fragprints-PCA({n_components}): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "feature_df = pd.DataFrame(feature_results).sort_values('mean')\n",
    "print(\"\\nBest feature sets:\")\n",
    "print(feature_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ccfc58f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:28:05.467070Z",
     "iopub.status.busy": "2026-01-14T02:28:05.466966Z",
     "iopub.status.idle": "2026-01-14T02:28:07.040371Z",
     "shell.execute_reply": "2026-01-14T02:28:07.039987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing combined feature sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange + ACS_PCA: 0.0737 +/- 0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP-PCA(15) + Spange: 0.0726 +/- 0.0161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRFP-PCA(15) + Spange + ACS_PCA: 0.0706 +/- 0.0166\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Combined features\n",
    "# Try combining Spange + ACS_PCA + DRFP-PCA\n",
    "\n",
    "print(\"\\nTesting combined feature sets...\")\n",
    "\n",
    "# Spange + ACS_PCA\n",
    "combined_spange_acs = pd.concat([spange, acs_pca], axis=1)\n",
    "mean, std = group_kfold_cv(X_single, Y_single, combined_spange_acs, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "print(f\"Spange + ACS_PCA: {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# DRFP-PCA(15) + Spange\n",
    "pca = PCA(n_components=15)\n",
    "drfp_15 = pd.DataFrame(\n",
    "    pca.fit_transform(drfp.values),\n",
    "    index=drfp.index,\n",
    "    columns=[f'drfp_{i}' for i in range(15)]\n",
    ")\n",
    "combined_drfp_spange = pd.concat([drfp_15, spange], axis=1)\n",
    "mean, std = group_kfold_cv(X_single, Y_single, combined_drfp_spange, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "print(f\"DRFP-PCA(15) + Spange: {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# All combined: DRFP-PCA(15) + Spange + ACS_PCA\n",
    "combined_all = pd.concat([drfp_15, spange, acs_pca], axis=1)\n",
    "mean, std = group_kfold_cv(X_single, Y_single, combined_all, ExtraTreesRegressor, etr_params, n_splits=5)\n",
    "print(f\"DRFP-PCA(15) + Spange + ACS_PCA: {mean:.4f} +/- {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03e2d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:28:07.041536Z",
     "iopub.status.busy": "2026-01-14T02:28:07.041434Z",
     "iopub.status.idle": "2026-01-14T02:28:09.486154Z",
     "shell.execute_reply": "2026-01-14T02:28:09.485797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing per-target models with different regularization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 1 (HGB-SM, ETR-Products, depth=10): 0.0838 +/- 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 2 (HGB-SM, ETR-Products, depth=7): 0.0833 +/- 0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 3 (HGB-SM, ETR-Products, depth=5): 0.0802 +/- 0.0110\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Per-target models with intermediate regularization\n",
    "# This was our best approach - let's see if we can improve it\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "print(\"\\nTesting per-target models with different regularization...\")\n",
    "\n",
    "def per_target_cv(X, Y, feature_df, model_configs, n_splits=5):\n",
    "    \"\"\"Per-target CV with different models for each target.\"\"\"\n",
    "    groups = X['SOLVENT NAME']\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    all_errors = []\n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        train_X, train_Y = X.iloc[train_idx], Y.iloc[train_idx]\n",
    "        test_X, test_Y = X.iloc[test_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        X_train_feat = build_features_single(train_X, feature_df)\n",
    "        X_test_feat = build_features_single(test_X, feature_df)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "        X_test_scaled = scaler.transform(X_test_feat)\n",
    "        \n",
    "        preds = np.zeros((len(test_X), 3))\n",
    "        for i, (target, config) in enumerate(model_configs.items()):\n",
    "            model = config['model'](**config['params'])\n",
    "            model.fit(X_train_scaled, train_Y[target].values)\n",
    "            preds[:, i] = model.predict(X_test_scaled)\n",
    "        \n",
    "        preds = np.clip(preds, 0, 1)\n",
    "        mae = np.mean(np.abs(preds - test_Y.values))\n",
    "        all_errors.append(mae)\n",
    "    \n",
    "    return np.mean(all_errors), np.std(all_errors)\n",
    "\n",
    "# Config 1: Current best (HGB for SM, ETR for Products)\n",
    "config1 = {\n",
    "    'Product 2': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 10, 'min_samples_leaf': 2, 'random_state': 42}},\n",
    "    'Product 3': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 10, 'min_samples_leaf': 2, 'random_state': 42}},\n",
    "    'SM': {'model': HistGradientBoostingRegressor, 'params': {'max_depth': 5, 'learning_rate': 0.1, 'max_iter': 100, 'random_state': 42}}\n",
    "}\n",
    "mean, std = per_target_cv(X_single, Y_single, spange, config1, n_splits=5)\n",
    "print(f\"Config 1 (HGB-SM, ETR-Products, depth=10): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# Config 2: More regularized (depth=7)\n",
    "config2 = {\n",
    "    'Product 2': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 7, 'min_samples_leaf': 3, 'random_state': 42}},\n",
    "    'Product 3': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 7, 'min_samples_leaf': 3, 'random_state': 42}},\n",
    "    'SM': {'model': HistGradientBoostingRegressor, 'params': {'max_depth': 4, 'learning_rate': 0.05, 'max_iter': 100, 'random_state': 42}}\n",
    "}\n",
    "mean, std = per_target_cv(X_single, Y_single, spange, config2, n_splits=5)\n",
    "print(f\"Config 2 (HGB-SM, ETR-Products, depth=7): {mean:.4f} +/- {std:.4f}\")\n",
    "\n",
    "# Config 3: Even more regularized (depth=5)\n",
    "config3 = {\n",
    "    'Product 2': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 5, 'random_state': 42}},\n",
    "    'Product 3': {'model': ExtraTreesRegressor, 'params': {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 5, 'random_state': 42}},\n",
    "    'SM': {'model': HistGradientBoostingRegressor, 'params': {'max_depth': 3, 'learning_rate': 0.05, 'max_iter': 100, 'random_state': 42}}\n",
    "}\n",
    "mean, std = per_target_cv(X_single, Y_single, spange, config3, n_splits=5)\n",
    "print(f\"Config 3 (HGB-SM, ETR-Products, depth=5): {mean:.4f} +/- {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5f8410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T02:28:09.487128Z",
     "iopub.status.busy": "2026-01-14T02:28:09.487037Z",
     "iopub.status.idle": "2026-01-14T02:28:09.490129Z",
     "shell.execute_reply": "2026-01-14T02:28:09.489782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANALYSIS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. CV STRATEGY:\n",
      "   - GroupKFold gives more realistic estimates than Leave-One-Out\n",
      "   - This better simulates the test scenario with unseen solvents\n",
      "\n",
      "2. REGULARIZATION:\n",
      "   - Intermediate regularization (depth=5-7) may be optimal\n",
      "   - Too simple (Ridge) underfits, too complex (depth=10) overfits\n",
      "\n",
      "3. FEATURES:\n",
      "   - Higher-dimensional features (DRFP, Fragprints) with PCA may help\n",
      "   - Combined features (Spange + ACS_PCA) are worth trying\n",
      "\n",
      "4. NEXT STEPS:\n",
      "   a) Try per-target model with intermediate regularization (depth=5-7)\n",
      "   b) Use combined features (Spange + ACS_PCA or DRFP-PCA + Spange)\n",
      "   c) Consider ensemble of multiple regularization levels\n",
      "   d) Submit to verify if GroupKFold CV correlates better with LB\n"
     ]
    }
   ],
   "source": [
    "# Summary and recommendations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. CV STRATEGY:\")\n",
    "print(\"   - GroupKFold gives more realistic estimates than Leave-One-Out\")\n",
    "print(\"   - This better simulates the test scenario with unseen solvents\")\n",
    "\n",
    "print(\"\\n2. REGULARIZATION:\")\n",
    "print(\"   - Intermediate regularization (depth=5-7) may be optimal\")\n",
    "print(\"   - Too simple (Ridge) underfits, too complex (depth=10) overfits\")\n",
    "\n",
    "print(\"\\n3. FEATURES:\")\n",
    "print(\"   - Higher-dimensional features (DRFP, Fragprints) with PCA may help\")\n",
    "print(\"   - Combined features (Spange + ACS_PCA) are worth trying\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS:\")\n",
    "print(\"   a) Try per-target model with intermediate regularization (depth=5-7)\")\n",
    "print(\"   b) Use combined features (Spange + ACS_PCA or DRFP-PCA + Spange)\")\n",
    "print(\"   c) Consider ensemble of multiple regularization levels\")\n",
    "print(\"   d) Submit to verify if GroupKFold CV correlates better with LB\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
