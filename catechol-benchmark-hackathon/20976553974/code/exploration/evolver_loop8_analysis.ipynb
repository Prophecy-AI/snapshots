{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84e8038",
   "metadata": {},
   "source": [
    "# Loop 8 Analysis: Strategic Assessment\n",
    "\n",
    "## Current Situation\n",
    "- Best CV: 0.0623 (exp_004, Per-Target HGB+ETR NO TTA)\n",
    "- Best LB: 0.0956 (exp_004, 53% CV-LB gap)\n",
    "- Target: 0.01727 (3.6x gap from best CV)\n",
    "- Submissions: 1/5 used, 4 remaining\n",
    "\n",
    "## Key Questions\n",
    "1. Why is there a 53% CV-LB gap?\n",
    "2. What approaches haven't been tried?\n",
    "3. What do top kernels do differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0448bebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:49.729200Z",
     "iopub.status.busy": "2026-01-14T03:07:49.728673Z",
     "iopub.status.idle": "2026-01-14T03:07:50.290065Z",
     "shell.execute_reply": "2026-01-14T03:07:50.289710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Summary:\n",
      "   name                     model       cv     lb\n",
      "exp_000 Ensemble (MLP+XGB+LGB+RF) 0.081393    NaN\n",
      "exp_001           Ensemble + Poly 0.081044    NaN\n",
      "exp_002            RF Regularized 0.080530    NaN\n",
      "exp_003       PerTarget (HGB+ETR) 0.081260    NaN\n",
      "exp_004          PerTarget NO TTA 0.062265 0.0956\n",
      "exp_005          Ridge (alpha=10) 0.089640    NaN\n",
      "exp_006       PerTarget depth=5/7 0.068848    NaN\n",
      "exp_007               GP (Matern) 0.072118    NaN\n",
      "\n",
      "Best CV: 0.0623 (exp_004)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load experiment results\n",
    "experiments = [\n",
    "    {'name': 'exp_000', 'model': 'Ensemble (MLP+XGB+LGB+RF)', 'cv': 0.081393},\n",
    "    {'name': 'exp_001', 'model': 'Ensemble + Poly', 'cv': 0.081044},\n",
    "    {'name': 'exp_002', 'model': 'RF Regularized', 'cv': 0.08053},\n",
    "    {'name': 'exp_003', 'model': 'PerTarget (HGB+ETR)', 'cv': 0.08126},\n",
    "    {'name': 'exp_004', 'model': 'PerTarget NO TTA', 'cv': 0.062265, 'lb': 0.0956},\n",
    "    {'name': 'exp_005', 'model': 'Ridge (alpha=10)', 'cv': 0.08964},\n",
    "    {'name': 'exp_006', 'model': 'PerTarget depth=5/7', 'cv': 0.068848},\n",
    "    {'name': 'exp_007', 'model': 'GP (Matern)', 'cv': 0.072118},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments)\n",
    "print(\"Experiment Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nBest CV: {df['cv'].min():.4f} ({df.loc[df['cv'].idxmin(), 'name']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636a4639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.291145Z",
     "iopub.status.busy": "2026-01-14T03:07:50.291009Z",
     "iopub.status.idle": "2026-01-14T03:07:50.294153Z",
     "shell.execute_reply": "2026-01-14T03:07:50.293831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CV-LB GAP ANALYSIS\n",
      "============================================================\n",
      "Best CV: 0.0623\n",
      "Best LB: 0.0956\n",
      "CV-LB Gap: 53.5%\n",
      "\n",
      "This means our CV is OVERLY OPTIMISTIC by 53.5%\n",
      "\n",
      "Possible reasons:\n",
      "1. Leave-one-solvent-out CV doesn't capture true OOD difficulty\n",
      "2. Test set has more chemically unique solvents\n",
      "3. Model is overfitting to training solvents\n"
     ]
    }
   ],
   "source": [
    "# Analyze CV-LB gap\n",
    "print(\"=\" * 60)\n",
    "print(\"CV-LB GAP ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_cv = 0.0623\n",
    "best_lb = 0.0956\n",
    "gap = (best_lb - best_cv) / best_cv * 100\n",
    "\n",
    "print(f\"Best CV: {best_cv:.4f}\")\n",
    "print(f\"Best LB: {best_lb:.4f}\")\n",
    "print(f\"CV-LB Gap: {gap:.1f}%\")\n",
    "print(f\"\\nThis means our CV is OVERLY OPTIMISTIC by {gap:.1f}%\")\n",
    "print(\"\\nPossible reasons:\")\n",
    "print(\"1. Leave-one-solvent-out CV doesn't capture true OOD difficulty\")\n",
    "print(\"2. Test set has more chemically unique solvents\")\n",
    "print(\"3. Model is overfitting to training solvents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e395ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.295052Z",
     "iopub.status.busy": "2026-01-14T03:07:50.294961Z",
     "iopub.status.idle": "2026-01-14T03:07:50.298377Z",
     "shell.execute_reply": "2026-01-14T03:07:50.298083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "APPROACHES NOT YET TRIED\n",
      "============================================================\n",
      "\n",
      "Tried:\n",
      "  ✓ Ensemble (MLP+XGB+LGB+RF)\n",
      "  ✓ Per-target models (HGB+ETR)\n",
      "  ✓ TTA (hurt performance)\n",
      "  ✓ Strong regularization (Ridge)\n",
      "  ✓ Intermediate regularization (depth=5/7)\n",
      "  ✓ Gaussian Process (Matern kernel)\n",
      "  ✓ Combined features (DRFP-PCA + Spange + ACS_PCA)\n",
      "\n",
      "NOT Tried:\n",
      "  ✗ Ensemble of DIVERSE model families (GP + Tree + MLP)\n",
      "  ✗ Optuna hyperparameter optimization\n",
      "  ✗ Different ensemble weights (learned vs fixed)\n",
      "  ✗ Neural network with BatchNorm + Dropout + LR scheduler\n",
      "  ✗ Stacking (meta-learner on base model predictions)\n",
      "  ✗ Blending (weighted average of diverse models)\n"
     ]
    }
   ],
   "source": [
    "# What approaches haven't been tried?\n",
    "print(\"=\" * 60)\n",
    "print(\"APPROACHES NOT YET TRIED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "approaches_tried = [\n",
    "    \"Ensemble (MLP+XGB+LGB+RF)\",\n",
    "    \"Per-target models (HGB+ETR)\",\n",
    "    \"TTA (hurt performance)\",\n",
    "    \"Strong regularization (Ridge)\",\n",
    "    \"Intermediate regularization (depth=5/7)\",\n",
    "    \"Gaussian Process (Matern kernel)\",\n",
    "    \"Combined features (DRFP-PCA + Spange + ACS_PCA)\",\n",
    "]\n",
    "\n",
    "approaches_not_tried = [\n",
    "    \"Ensemble of DIVERSE model families (GP + Tree + MLP)\",\n",
    "    \"Optuna hyperparameter optimization\",\n",
    "    \"Different ensemble weights (learned vs fixed)\",\n",
    "    \"Neural network with BatchNorm + Dropout + LR scheduler\",\n",
    "    \"Stacking (meta-learner on base model predictions)\",\n",
    "    \"Blending (weighted average of diverse models)\",\n",
    "]\n",
    "\n",
    "print(\"\\nTried:\")\n",
    "for a in approaches_tried:\n",
    "    print(f\"  ✓ {a}\")\n",
    "\n",
    "print(\"\\nNOT Tried:\")\n",
    "for a in approaches_not_tried:\n",
    "    print(f\"  ✗ {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ee5816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.299302Z",
     "iopub.status.busy": "2026-01-14T03:07:50.299203Z",
     "iopub.status.idle": "2026-01-14T03:07:50.302164Z",
     "shell.execute_reply": "2026-01-14T03:07:50.301851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INSIGHTS FROM TOP KERNELS\n",
      "============================================================\n",
      "\n",
      "1. lishellliang's kernel (good CV/LB):\n",
      "   - Ensemble: MLP + XGBoost + RF + LightGBM\n",
      "   - Weighted averaging with learned weights\n",
      "   - GroupKFold (5-fold) instead of Leave-One-Out\n",
      "   - Optuna for hyperparameter tuning\n",
      "   - Uses Spange descriptors\n",
      "\n",
      "2. omarafik's kernel (System Malfunction V1):\n",
      "   - Simple MLP with BatchNorm + Dropout\n",
      "   - ReduceLROnPlateau scheduler\n",
      "   - Uses Spange descriptors\n",
      "   - 300 epochs training\n",
      "\n",
      "3. Key differences from our approach:\n",
      "   - We use per-target models (HGB for SM, ETR for Products)\n",
      "   - They use single ensemble for all targets\n",
      "   - They use learned ensemble weights\n",
      "   - They use more sophisticated MLP training\n"
     ]
    }
   ],
   "source": [
    "# Key insight from top kernels\n",
    "print(\"=\" * 60)\n",
    "print(\"INSIGHTS FROM TOP KERNELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. lishellliang's kernel (good CV/LB):\")\n",
    "print(\"   - Ensemble: MLP + XGBoost + RF + LightGBM\")\n",
    "print(\"   - Weighted averaging with learned weights\")\n",
    "print(\"   - GroupKFold (5-fold) instead of Leave-One-Out\")\n",
    "print(\"   - Optuna for hyperparameter tuning\")\n",
    "print(\"   - Uses Spange descriptors\")\n",
    "\n",
    "print(\"\\n2. omarafik's kernel (System Malfunction V1):\")\n",
    "print(\"   - Simple MLP with BatchNorm + Dropout\")\n",
    "print(\"   - ReduceLROnPlateau scheduler\")\n",
    "print(\"   - Uses Spange descriptors\")\n",
    "print(\"   - 300 epochs training\")\n",
    "\n",
    "print(\"\\n3. Key differences from our approach:\")\n",
    "print(\"   - We use per-target models (HGB for SM, ETR for Products)\")\n",
    "print(\"   - They use single ensemble for all targets\")\n",
    "print(\"   - They use learned ensemble weights\")\n",
    "print(\"   - They use more sophisticated MLP training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8b0752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.302999Z",
     "iopub.status.busy": "2026-01-14T03:07:50.302914Z",
     "iopub.status.idle": "2026-01-14T03:07:50.305695Z",
     "shell.execute_reply": "2026-01-14T03:07:50.305390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATEGIC RECOMMENDATION\n",
      "============================================================\n",
      "\n",
      "1. SUBMIT exp_006 (intermediate regularization, CV 0.0689)\n",
      "   - Tests hypothesis that regularization reduces CV-LB gap\n",
      "   - If LB improves: continue with regularization\n",
      "   - If LB doesn't improve: pivot to ensemble diversity\n",
      "\n",
      "2. TRY: Ensemble of diverse model families\n",
      "   - Combine GP + ETR + MLP predictions\n",
      "   - Weighted average: 0.3*GP + 0.4*ETR + 0.3*MLP\n",
      "   - May reduce variance and improve generalization\n",
      "\n",
      "3. TRY: Stacking with meta-learner\n",
      "   - Train base models (GP, ETR, MLP) on folds\n",
      "   - Train meta-learner (Ridge) on base predictions\n",
      "   - May capture complementary information\n",
      "\n",
      "4. TRY: Optuna hyperparameter optimization\n",
      "   - Systematic search for optimal hyperparameters\n",
      "   - May find better configurations than manual tuning\n"
     ]
    }
   ],
   "source": [
    "# Strategic recommendation\n",
    "print(\"=\" * 60)\n",
    "print(\"STRATEGIC RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. SUBMIT exp_006 (intermediate regularization, CV 0.0689)\")\n",
    "print(\"   - Tests hypothesis that regularization reduces CV-LB gap\")\n",
    "print(\"   - If LB improves: continue with regularization\")\n",
    "print(\"   - If LB doesn't improve: pivot to ensemble diversity\")\n",
    "\n",
    "print(\"\\n2. TRY: Ensemble of diverse model families\")\n",
    "print(\"   - Combine GP + ETR + MLP predictions\")\n",
    "print(\"   - Weighted average: 0.3*GP + 0.4*ETR + 0.3*MLP\")\n",
    "print(\"   - May reduce variance and improve generalization\")\n",
    "\n",
    "print(\"\\n3. TRY: Stacking with meta-learner\")\n",
    "print(\"   - Train base models (GP, ETR, MLP) on folds\")\n",
    "print(\"   - Train meta-learner (Ridge) on base predictions\")\n",
    "print(\"   - May capture complementary information\")\n",
    "\n",
    "print(\"\\n4. TRY: Optuna hyperparameter optimization\")\n",
    "print(\"   - Systematic search for optimal hyperparameters\")\n",
    "print(\"   - May find better configurations than manual tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c199b3c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.306505Z",
     "iopub.status.busy": "2026-01-14T03:07:50.306420Z",
     "iopub.status.idle": "2026-01-14T03:07:50.309417Z",
     "shell.execute_reply": "2026-01-14T03:07:50.309088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GAP TO TARGET ANALYSIS\n",
      "============================================================\n",
      "Target: 0.01727\n",
      "Best CV: 0.06230 (260.7% above target)\n",
      "Best LB: 0.09560 (453.6% above target)\n",
      "\n",
      "To reach target:\n",
      "  - Need to reduce CV by 72.3%\n",
      "  - Need to reduce LB by 81.9%\n",
      "\n",
      "This is a MASSIVE gap. Possible explanations:\n",
      "1. Target is based on different evaluation (e.g., different metric)\n",
      "2. Target is based on private test set with different distribution\n",
      "3. There's domain knowledge or approach we're missing\n",
      "4. The winning solution uses fundamentally different approach\n"
     ]
    }
   ],
   "source": [
    "# Calculate expected improvement needed\n",
    "print(\"=\" * 60)\n",
    "print(\"GAP TO TARGET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "target = 0.01727\n",
    "best_cv = 0.0623\n",
    "best_lb = 0.0956\n",
    "\n",
    "print(f\"Target: {target:.5f}\")\n",
    "print(f\"Best CV: {best_cv:.5f} ({(best_cv/target - 1)*100:.1f}% above target)\")\n",
    "print(f\"Best LB: {best_lb:.5f} ({(best_lb/target - 1)*100:.1f}% above target)\")\n",
    "\n",
    "print(\"\\nTo reach target:\")\n",
    "print(f\"  - Need to reduce CV by {(1 - target/best_cv)*100:.1f}%\")\n",
    "print(f\"  - Need to reduce LB by {(1 - target/best_lb)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nThis is a MASSIVE gap. Possible explanations:\")\n",
    "print(\"1. Target is based on different evaluation (e.g., different metric)\")\n",
    "print(\"2. Target is based on private test set with different distribution\")\n",
    "print(\"3. There's domain knowledge or approach we're missing\")\n",
    "print(\"4. The winning solution uses fundamentally different approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee46cf40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:07:50.310330Z",
     "iopub.status.busy": "2026-01-14T03:07:50.310226Z",
     "iopub.status.idle": "2026-01-14T03:07:50.313263Z",
     "shell.execute_reply": "2026-01-14T03:07:50.312952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRIORITY ACTIONS FOR NEXT LOOP\n",
      "============================================================\n",
      "\n",
      "1. SUBMIT exp_006 to verify CV-LB gap hypothesis\n",
      "   - CV 0.0689 vs exp_004 CV 0.0623\n",
      "   - If LB improves proportionally, regularization helps\n",
      "\n",
      "2. TRY: Diverse ensemble (GP + ETR + MLP)\n",
      "   - Combine fundamentally different model families\n",
      "   - May capture different aspects of the data\n",
      "\n",
      "3. TRY: Stacking with meta-learner\n",
      "   - Use base model predictions as features\n",
      "   - Train simple meta-learner (Ridge/Linear)\n",
      "\n",
      "4. RESEARCH: What do top LB solutions do?\n",
      "   - Check if there's domain knowledge we're missing\n",
      "   - Look for physics-informed approaches\n"
     ]
    }
   ],
   "source": [
    "# Priority actions\n",
    "print(\"=\" * 60)\n",
    "print(\"PRIORITY ACTIONS FOR NEXT LOOP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. SUBMIT exp_006 to verify CV-LB gap hypothesis\")\n",
    "print(\"   - CV 0.0689 vs exp_004 CV 0.0623\")\n",
    "print(\"   - If LB improves proportionally, regularization helps\")\n",
    "\n",
    "print(\"\\n2. TRY: Diverse ensemble (GP + ETR + MLP)\")\n",
    "print(\"   - Combine fundamentally different model families\")\n",
    "print(\"   - May capture different aspects of the data\")\n",
    "\n",
    "print(\"\\n3. TRY: Stacking with meta-learner\")\n",
    "print(\"   - Use base model predictions as features\")\n",
    "print(\"   - Train simple meta-learner (Ridge/Linear)\")\n",
    "\n",
    "print(\"\\n4. RESEARCH: What do top LB solutions do?\")\n",
    "print(\"   - Check if there's domain knowledge we're missing\")\n",
    "print(\"   - Look for physics-informed approaches\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
