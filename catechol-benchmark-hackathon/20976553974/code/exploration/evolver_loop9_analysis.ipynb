{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1be9c83",
   "metadata": {},
   "source": [
    "# Loop 9 Analysis: Understanding the CV-LB Gap and Top Kernel Approaches\n",
    "\n",
    "## Key Questions:\n",
    "1. Why did the diverse ensemble (exp_009) have worse CV than per-target (exp_004)?\n",
    "2. What does the top kernel (lishellliang) do differently?\n",
    "3. What is the optimal strategy going forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86c2585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.055259Z",
     "iopub.status.busy": "2026-01-14T03:18:50.054758Z",
     "iopub.status.idle": "2026-01-14T03:18:50.589124Z",
     "shell.execute_reply": "2026-01-14T03:18:50.588756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Summary:\n",
      " exp_id             name  cv_score  lb_score\n",
      "exp_000     001_baseline  0.081393       NaN\n",
      "exp_001     002_template  0.081044       NaN\n",
      "exp_002           003_rf  0.080530       NaN\n",
      "exp_003   004_per_target  0.081260       NaN\n",
      "exp_004       005_no_tta  0.062265    0.0956\n",
      "exp_005        006_ridge  0.089640       NaN\n",
      "exp_006 007_intermediate  0.068848    0.0991\n",
      "exp_007           008_gp  0.072118       NaN\n",
      "exp_008      009_diverse  0.067268       NaN\n",
      "\n",
      "Best CV: 0.0623 (005_no_tta)\n",
      "Best LB: 0.0956 (005_no_tta)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load experiment results\n",
    "experiments = [\n",
    "    ('exp_000', '001_baseline', 0.081393, None),\n",
    "    ('exp_001', '002_template', 0.081044, None),\n",
    "    ('exp_002', '003_rf', 0.08053, None),\n",
    "    ('exp_003', '004_per_target', 0.08126, None),\n",
    "    ('exp_004', '005_no_tta', 0.062265, 0.0956),  # Submitted\n",
    "    ('exp_005', '006_ridge', 0.08964, None),\n",
    "    ('exp_006', '007_intermediate', 0.068848, 0.0991),  # Submitted\n",
    "    ('exp_007', '008_gp', 0.072118, None),\n",
    "    ('exp_008', '009_diverse', 0.067268, None),\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(experiments, columns=['exp_id', 'name', 'cv_score', 'lb_score'])\n",
    "print(\"Experiment Summary:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nBest CV: {df['cv_score'].min():.4f} ({df.loc[df['cv_score'].idxmin(), 'name']})\")\n",
    "print(f\"Best LB: {df['lb_score'].min():.4f} ({df.loc[df['lb_score'].idxmin(), 'name']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b04a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.590428Z",
     "iopub.status.busy": "2026-01-14T03:18:50.590064Z",
     "iopub.status.idle": "2026-01-14T03:18:50.593949Z",
     "shell.execute_reply": "2026-01-14T03:18:50.593612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitted Experiments:\n",
      "005_no_tta: CV=0.0623 -> LB=0.0956 (Gap: +53.5%)\n",
      "007_intermediate: CV=0.0688 -> LB=0.0991 (Gap: +43.9%)\n",
      "\n",
      "============================================================\n",
      "CRITICAL INSIGHT:\n",
      "exp_006 (more regularization) had WORSE LB than exp_004\n",
      "This DISPROVES the overfitting hypothesis!\n",
      "We need BETTER features/models, not simpler ones.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze CV-LB gap\n",
    "submitted = df[df['lb_score'].notna()]\n",
    "print(\"\\nSubmitted Experiments:\")\n",
    "for _, row in submitted.iterrows():\n",
    "    gap = (row['lb_score'] - row['cv_score']) / row['cv_score'] * 100\n",
    "    print(f\"{row['name']}: CV={row['cv_score']:.4f} -> LB={row['lb_score']:.4f} (Gap: +{gap:.1f}%)\")\n",
    "\n",
    "# Key insight: exp_006 (more regularization) had WORSE LB than exp_004\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CRITICAL INSIGHT:\")\n",
    "print(\"exp_006 (more regularization) had WORSE LB than exp_004\")\n",
    "print(\"This DISPROVES the overfitting hypothesis!\")\n",
    "print(\"We need BETTER features/models, not simpler ones.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ff32e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.594787Z",
     "iopub.status.busy": "2026-01-14T03:18:50.594696Z",
     "iopub.status.idle": "2026-01-14T03:18:50.598085Z",
     "shell.execute_reply": "2026-01-14T03:18:50.597760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Kernel (lishellliang) Key Differences:\n",
      "============================================================\n",
      "1. Uses GroupKFold (5-fold) instead of Leave-One-Out\n",
      "   - This gives more realistic CV estimates\n",
      "   - Each fold has ~20% of solvents held out\n",
      "   - More training data per fold (80% vs ~96% for LOO)\n",
      "\n",
      "2. Uses MLP + XGBoost + RF + LightGBM ensemble\n",
      "   - Weighted averaging with learned weights (Optuna)\n",
      "   - MLP captures non-linear patterns\n",
      "   - GBDT models capture different patterns\n",
      "\n",
      "3. Uses Spange descriptors only (not combined features)\n",
      "   - Simpler feature set may generalize better\n",
      "\n",
      "4. Uses Optuna for hyperparameter tuning\n",
      "   - Learns optimal weights for ensemble\n",
      "   - Tunes MLP architecture, GBDT depths, etc.\n"
     ]
    }
   ],
   "source": [
    "# Analyze top kernel approach (lishellliang)\n",
    "print(\"\\nTop Kernel (lishellliang) Key Differences:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Uses GroupKFold (5-fold) instead of Leave-One-Out\")\n",
    "print(\"   - This gives more realistic CV estimates\")\n",
    "print(\"   - Each fold has ~20% of solvents held out\")\n",
    "print(\"   - More training data per fold (80% vs ~96% for LOO)\")\n",
    "print(\"\")\n",
    "print(\"2. Uses MLP + XGBoost + RF + LightGBM ensemble\")\n",
    "print(\"   - Weighted averaging with learned weights (Optuna)\")\n",
    "print(\"   - MLP captures non-linear patterns\")\n",
    "print(\"   - GBDT models capture different patterns\")\n",
    "print(\"\")\n",
    "print(\"3. Uses Spange descriptors only (not combined features)\")\n",
    "print(\"   - Simpler feature set may generalize better\")\n",
    "print(\"\")\n",
    "print(\"4. Uses Optuna for hyperparameter tuning\")\n",
    "print(\"   - Learns optimal weights for ensemble\")\n",
    "print(\"   - Tunes MLP architecture, GBDT depths, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a3d77d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.599160Z",
     "iopub.status.busy": "2026-01-14T03:18:50.598948Z",
     "iopub.status.idle": "2026-01-14T03:18:50.601510Z",
     "shell.execute_reply": "2026-01-14T03:18:50.601196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Approach vs Top Kernel:\n",
      "============================================================\n",
      "| Aspect              | Our Approach           | Top Kernel             |\n",
      "|---------------------|------------------------|------------------------|\n",
      "| CV Scheme           | Leave-One-Out (24/13)  | GroupKFold (5-fold)    |\n",
      "| Base Models         | HGB+ETR (per-target)   | MLP+XGB+RF+LGB         |\n",
      "| Ensemble Weights    | Fixed [0.4,0.2,0.2,0.2]| Learned (Optuna)       |\n",
      "| Features            | Spange+ACS_PCA+DRFP    | Spange only            |\n",
      "| Best CV             | 0.0623                 | Unknown                |\n",
      "| Best LB             | 0.0956                 | ~0.08-0.09 (estimated) |\n"
     ]
    }
   ],
   "source": [
    "# Compare our approach vs top kernel\n",
    "print(\"\\nOur Approach vs Top Kernel:\")\n",
    "print(\"=\"*60)\n",
    "print(\"| Aspect              | Our Approach           | Top Kernel             |\")\n",
    "print(\"|---------------------|------------------------|------------------------|\")\n",
    "print(\"| CV Scheme           | Leave-One-Out (24/13)  | GroupKFold (5-fold)    |\")\n",
    "print(\"| Base Models         | HGB+ETR (per-target)   | MLP+XGB+RF+LGB         |\")\n",
    "print(\"| Ensemble Weights    | Fixed [0.4,0.2,0.2,0.2]| Learned (Optuna)       |\")\n",
    "print(\"| Features            | Spange+ACS_PCA+DRFP    | Spange only            |\")\n",
    "print(\"| Best CV             | 0.0623                 | Unknown                |\")\n",
    "print(\"| Best LB             | 0.0956                 | ~0.08-0.09 (estimated) |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12510259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.602379Z",
     "iopub.status.busy": "2026-01-14T03:18:50.602293Z",
     "iopub.status.idle": "2026-01-14T03:18:50.605034Z",
     "shell.execute_reply": "2026-01-14T03:18:50.604720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Next Steps:\n",
      "============================================================\n",
      "\n",
      "PRIORITY 1: Include MLP in ensemble\n",
      "- Top kernel uses MLP as a key component\n",
      "- MLP can capture non-linear patterns that trees miss\n",
      "- Our current ensemble doesn't have MLP\n",
      "\n",
      "PRIORITY 2: Learn ensemble weights with Optuna\n",
      "- Fixed weights [0.4,0.2,0.2,0.2] may be suboptimal\n",
      "- Optuna can find optimal weights for each model\n",
      "\n",
      "PRIORITY 3: Try GroupKFold validation\n",
      "- May give more realistic CV estimates\n",
      "- Better correlation with LB\n",
      "\n",
      "PRIORITY 4: Simplify features\n",
      "- Top kernel uses Spange only\n",
      "- Combined features may cause overfitting\n"
     ]
    }
   ],
   "source": [
    "# What should we try next?\n",
    "print(\"\\nRecommended Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"PRIORITY 1: Include MLP in ensemble\")\n",
    "print(\"- Top kernel uses MLP as a key component\")\n",
    "print(\"- MLP can capture non-linear patterns that trees miss\")\n",
    "print(\"- Our current ensemble doesn't have MLP\")\n",
    "print(\"\")\n",
    "print(\"PRIORITY 2: Learn ensemble weights with Optuna\")\n",
    "print(\"- Fixed weights [0.4,0.2,0.2,0.2] may be suboptimal\")\n",
    "print(\"- Optuna can find optimal weights for each model\")\n",
    "print(\"\")\n",
    "print(\"PRIORITY 3: Try GroupKFold validation\")\n",
    "print(\"- May give more realistic CV estimates\")\n",
    "print(\"- Better correlation with LB\")\n",
    "print(\"\")\n",
    "print(\"PRIORITY 4: Simplify features\")\n",
    "print(\"- Top kernel uses Spange only\")\n",
    "print(\"- Combined features may cause overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b61577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.605837Z",
     "iopub.status.busy": "2026-01-14T03:18:50.605751Z",
     "iopub.status.idle": "2026-01-14T03:18:50.608832Z",
     "shell.execute_reply": "2026-01-14T03:18:50.608521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gap to Target Analysis:\n",
      "============================================================\n",
      "Target: 0.01727\n",
      "Best CV: 0.0623 (3.6x target)\n",
      "Best LB: 0.0956 (5.5x target)\n",
      "\n",
      "To reach target from best LB:\n",
      "  Need 81.9% improvement\n",
      "  That's 5.5x better than current\n",
      "\n",
      "This is a MASSIVE gap. The target suggests:\n",
      "1. There's domain knowledge we're missing\n",
      "2. There's a fundamentally different approach\n",
      "3. The test set may have different characteristics\n"
     ]
    }
   ],
   "source": [
    "# Gap analysis\n",
    "print(\"\\nGap to Target Analysis:\")\n",
    "print(\"=\"*60)\n",
    "target = 0.01727\n",
    "best_cv = 0.0623\n",
    "best_lb = 0.0956\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Best CV: {best_cv} ({best_cv/target:.1f}x target)\")\n",
    "print(f\"Best LB: {best_lb} ({best_lb/target:.1f}x target)\")\n",
    "print(f\"\")\n",
    "print(f\"To reach target from best LB:\")\n",
    "print(f\"  Need {(best_lb - target)/best_lb * 100:.1f}% improvement\")\n",
    "print(f\"  That's {best_lb/target:.1f}x better than current\")\n",
    "print(f\"\")\n",
    "print(\"This is a MASSIVE gap. The target suggests:\")\n",
    "print(\"1. There's domain knowledge we're missing\")\n",
    "print(\"2. There's a fundamentally different approach\")\n",
    "print(\"3. The test set may have different characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef12e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:18:50.609650Z",
     "iopub.status.busy": "2026-01-14T03:18:50.609563Z",
     "iopub.status.idle": "2026-01-14T03:18:50.612402Z",
     "shell.execute_reply": "2026-01-14T03:18:50.612081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION FOR NEXT EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "Experiment 010: MLP + GBDT Ensemble with Learned Weights\n",
      "\n",
      "Key changes from exp_009:\n",
      "1. ADD MLP to ensemble (like top kernel)\n",
      "2. Use Optuna to learn optimal weights\n",
      "3. Try Spange-only features (simpler may generalize better)\n",
      "4. Keep Leave-One-Out CV for now (more rigorous)\n",
      "\n",
      "Expected outcome:\n",
      "- CV may be similar or slightly worse\n",
      "- LB may improve due to MLP capturing different patterns\n",
      "- Learned weights should optimize ensemble performance\n"
     ]
    }
   ],
   "source": [
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RECOMMENDATION FOR NEXT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"Experiment 010: MLP + GBDT Ensemble with Learned Weights\")\n",
    "print(\"\")\n",
    "print(\"Key changes from exp_009:\")\n",
    "print(\"1. ADD MLP to ensemble (like top kernel)\")\n",
    "print(\"2. Use Optuna to learn optimal weights\")\n",
    "print(\"3. Try Spange-only features (simpler may generalize better)\")\n",
    "print(\"4. Keep Leave-One-Out CV for now (more rigorous)\")\n",
    "print(\"\")\n",
    "print(\"Expected outcome:\")\n",
    "print(\"- CV may be similar or slightly worse\")\n",
    "print(\"- LB may improve due to MLP capturing different patterns\")\n",
    "print(\"- Learned weights should optimize ensemble performance\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
