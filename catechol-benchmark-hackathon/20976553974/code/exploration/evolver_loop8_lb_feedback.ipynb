{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636dd3fc",
   "metadata": {},
   "source": [
    "# Loop 8 LB Feedback Analysis\n",
    "\n",
    "## Submission Results\n",
    "- **exp_004** (best CV): CV 0.0623 → LB 0.0956 (gap: +53%)\n",
    "- **exp_006** (intermediate regularization): CV 0.0689 → LB 0.0991 (gap: +44%)\n",
    "\n",
    "## Key Observations\n",
    "1. Both submissions have large CV-LB gaps (44-53%)\n",
    "2. Intermediate regularization (exp_006) has WORSE LB despite better regularization\n",
    "3. The gap suggests test set has fundamentally different solvents\n",
    "\n",
    "## Questions to Answer\n",
    "1. What's the pattern in CV-LB gap?\n",
    "2. What approaches from top kernels should we try?\n",
    "3. How can we reduce the gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d87e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:14.888423Z",
     "iopub.status.busy": "2026-01-14T03:10:14.888009Z",
     "iopub.status.idle": "2026-01-14T03:10:15.386944Z",
     "shell.execute_reply": "2026-01-14T03:10:15.386589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CV-LB Gap Analysis ===\n",
      "                      name     cv     lb    gap   gap_pct\n",
      "         exp_004 (best CV) 0.0623 0.0956 0.0333 53.451043\n",
      "exp_006 (intermediate reg) 0.0689 0.0991 0.0302 43.831640\n",
      "\n",
      "Average gap: 0.0318 (48.6%)\n",
      "\n",
      "Target: 0.01727\n",
      "Best LB: 0.0956\n",
      "Gap to target: 453.6%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CV-LB Gap Analysis\n",
    "submissions = [\n",
    "    {'name': 'exp_004 (best CV)', 'cv': 0.0623, 'lb': 0.0956},\n",
    "    {'name': 'exp_006 (intermediate reg)', 'cv': 0.0689, 'lb': 0.0991},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "df['gap'] = df['lb'] - df['cv']\n",
    "df['gap_pct'] = (df['lb'] - df['cv']) / df['cv'] * 100\n",
    "\n",
    "print(\"=== CV-LB Gap Analysis ===\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nAverage gap: {df['gap'].mean():.4f} ({df['gap_pct'].mean():.1f}%)\")\n",
    "print(f\"\\nTarget: 0.01727\")\n",
    "print(f\"Best LB: {df['lb'].min():.4f}\")\n",
    "print(f\"Gap to target: {(df['lb'].min() - 0.01727) / 0.01727 * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be245a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:15.388012Z",
     "iopub.status.busy": "2026-01-14T03:10:15.387876Z",
     "iopub.status.idle": "2026-01-14T03:10:15.390634Z",
     "shell.execute_reply": "2026-01-14T03:10:15.390333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Critical Insight ===\n",
      "exp_004 (less regularized): CV 0.0623 → LB 0.0956\n",
      "exp_006 (more regularized): CV 0.0689 → LB 0.0991\n",
      "\n",
      "More regularization made LB WORSE!\n",
      "This suggests:\n",
      "1. The problem is NOT traditional overfitting\n",
      "2. We need BETTER features that generalize to new solvents\n",
      "3. Simpler models lose signal without reducing the gap\n"
     ]
    }
   ],
   "source": [
    "# Key insight: More regularization (exp_006) gave WORSE LB!\n",
    "# This suggests the problem is NOT overfitting in the traditional sense\n",
    "# The test set likely has chemically unique solvents that require BETTER features, not simpler models\n",
    "\n",
    "print(\"=== Critical Insight ===\")\n",
    "print(\"exp_004 (less regularized): CV 0.0623 → LB 0.0956\")\n",
    "print(\"exp_006 (more regularized): CV 0.0689 → LB 0.0991\")\n",
    "print(\"\\nMore regularization made LB WORSE!\")\n",
    "print(\"This suggests:\")\n",
    "print(\"1. The problem is NOT traditional overfitting\")\n",
    "print(\"2. We need BETTER features that generalize to new solvents\")\n",
    "print(\"3. Simpler models lose signal without reducing the gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327a4fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:15.391548Z",
     "iopub.status.busy": "2026-01-14T03:10:15.391453Z",
     "iopub.status.idle": "2026-01-14T03:10:15.395561Z",
     "shell.execute_reply": "2026-01-14T03:10:15.395215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top Kernel Approaches ===\n",
      "\n",
      "1. lishellliang kernel (good CV/LB):\n",
      "   - Ensemble: MLP + XGBoost + RF + LightGBM\n",
      "   - Weighted averaging with learned weights\n",
      "   - GroupKFold (5-fold) for validation\n",
      "   - Optuna hyperparameter tuning\n",
      "   - Spange descriptors\n",
      "\n",
      "2. Key differences from our approach:\n",
      "   - We use per-target models, they use single ensemble for all targets\n",
      "   - We use Leave-One-Out, they use GroupKFold\n",
      "   - We haven't tried ensemble of diverse model families\n"
     ]
    }
   ],
   "source": [
    "# What the top kernels do differently:\n",
    "# 1. lishellliang kernel: MLP + XGBoost + RF + LightGBM ensemble with learned weights\n",
    "# 2. Uses GroupKFold (5-fold) instead of Leave-One-Out\n",
    "# 3. Uses Optuna for hyperparameter tuning\n",
    "# 4. Uses Spange descriptors\n",
    "\n",
    "print(\"=== Top Kernel Approaches ===\")\n",
    "print(\"\\n1. lishellliang kernel (good CV/LB):\")\n",
    "print(\"   - Ensemble: MLP + XGBoost + RF + LightGBM\")\n",
    "print(\"   - Weighted averaging with learned weights\")\n",
    "print(\"   - GroupKFold (5-fold) for validation\")\n",
    "print(\"   - Optuna hyperparameter tuning\")\n",
    "print(\"   - Spange descriptors\")\n",
    "print(\"\\n2. Key differences from our approach:\")\n",
    "print(\"   - We use per-target models, they use single ensemble for all targets\")\n",
    "print(\"   - We use Leave-One-Out, they use GroupKFold\")\n",
    "print(\"   - We haven't tried ensemble of diverse model families\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c7589b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:15.396572Z",
     "iopub.status.busy": "2026-01-14T03:10:15.396260Z",
     "iopub.status.idle": "2026-01-14T03:10:15.399433Z",
     "shell.execute_reply": "2026-01-14T03:10:15.399063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Recommended Strategy ===\n",
      "\n",
      "1. ENSEMBLE OF DIVERSE MODELS (HIGH PRIORITY)\n",
      "   - Combine MLP + XGBoost + RandomForest + LightGBM\n",
      "   - Use weighted averaging (learn weights via Optuna or grid search)\n",
      "   - This is what top kernels do!\n",
      "\n",
      "2. FEATURE ENGINEERING\n",
      "   - Focus on features that capture chemical similarity\n",
      "   - Spange descriptors work well\n",
      "   - Consider adding interaction features\n",
      "\n",
      "3. VALIDATION\n",
      "   - GroupKFold may give more realistic CV estimates\n",
      "   - But our Leave-One-Out is more conservative\n",
      "\n",
      "4. DO NOT TRY\n",
      "   - More regularization (made LB worse)\n",
      "   - Simpler models (lose signal without reducing gap)\n"
     ]
    }
   ],
   "source": [
    "# Strategy for next experiments:\n",
    "# 1. Try ensemble of diverse models (MLP + XGB + RF + LGB)\n",
    "# 2. Use learned weights for ensemble\n",
    "# 3. Focus on feature engineering that generalizes\n",
    "\n",
    "print(\"=== Recommended Strategy ===\")\n",
    "print(\"\\n1. ENSEMBLE OF DIVERSE MODELS (HIGH PRIORITY)\")\n",
    "print(\"   - Combine MLP + XGBoost + RandomForest + LightGBM\")\n",
    "print(\"   - Use weighted averaging (learn weights via Optuna or grid search)\")\n",
    "print(\"   - This is what top kernels do!\")\n",
    "print(\"\\n2. FEATURE ENGINEERING\")\n",
    "print(\"   - Focus on features that capture chemical similarity\")\n",
    "print(\"   - Spange descriptors work well\")\n",
    "print(\"   - Consider adding interaction features\")\n",
    "print(\"\\n3. VALIDATION\")\n",
    "print(\"   - GroupKFold may give more realistic CV estimates\")\n",
    "print(\"   - But our Leave-One-Out is more conservative\")\n",
    "print(\"\\n4. DO NOT TRY\")\n",
    "print(\"   - More regularization (made LB worse)\")\n",
    "print(\"   - Simpler models (lose signal without reducing gap)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d43d8ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:15.400671Z",
     "iopub.status.busy": "2026-01-14T03:10:15.400574Z",
     "iopub.status.idle": "2026-01-14T03:10:15.406344Z",
     "shell.execute_reply": "2026-01-14T03:10:15.405955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Experiments (sorted by CV) ===\n",
      "                  name     cv                      model\n",
      "            005_no_tta 0.0623 PerTarget (HGB+ETR) NO TTA\n",
      "  007_intermediate_reg 0.0689        PerTarget depth=5/7\n",
      "  008_gaussian_process 0.0721                GP (Matern)\n",
      "         003_simple_rf 0.0805               RandomForest\n",
      "002_template_compliant 0.0810             MLP+XGB+LGB+RF\n",
      "        004_per_target 0.0813        PerTarget (HGB+ETR)\n",
      " 001_baseline_ensemble 0.0814             MLP+XGB+LGB+RF\n",
      "             006_ridge 0.0896                      Ridge\n",
      "\n",
      "Best CV: 0.0623 (005_no_tta)\n"
     ]
    }
   ],
   "source": [
    "# Experiments we have:\n",
    "experiments = [\n",
    "    {'name': '001_baseline_ensemble', 'cv': 0.0814, 'model': 'MLP+XGB+LGB+RF'},\n",
    "    {'name': '002_template_compliant', 'cv': 0.0810, 'model': 'MLP+XGB+LGB+RF'},\n",
    "    {'name': '003_simple_rf', 'cv': 0.0805, 'model': 'RandomForest'},\n",
    "    {'name': '004_per_target', 'cv': 0.0813, 'model': 'PerTarget (HGB+ETR)'},\n",
    "    {'name': '005_no_tta', 'cv': 0.0623, 'model': 'PerTarget (HGB+ETR) NO TTA'},\n",
    "    {'name': '006_ridge', 'cv': 0.0896, 'model': 'Ridge'},\n",
    "    {'name': '007_intermediate_reg', 'cv': 0.0689, 'model': 'PerTarget depth=5/7'},\n",
    "    {'name': '008_gaussian_process', 'cv': 0.0721, 'model': 'GP (Matern)'},\n",
    "]\n",
    "\n",
    "df_exp = pd.DataFrame(experiments)\n",
    "df_exp = df_exp.sort_values('cv')\n",
    "print(\"=== All Experiments (sorted by CV) ===\")\n",
    "print(df_exp.to_string(index=False))\n",
    "print(f\"\\nBest CV: {df_exp['cv'].min():.4f} ({df_exp[df_exp['cv'] == df_exp['cv'].min()]['name'].values[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4302ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:10:15.407304Z",
     "iopub.status.busy": "2026-01-14T03:10:15.407185Z",
     "iopub.status.idle": "2026-01-14T03:10:15.410351Z",
     "shell.execute_reply": "2026-01-14T03:10:15.409969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== APPROACHES NOT YET TRIED ===\n",
      "\n",
      "1. ENSEMBLE OF DIVERSE MODEL FAMILIES\n",
      "   - Combine GP + ETR + HGB + MLP\n",
      "   - Each model family captures different patterns\n",
      "   - Weighted averaging can reduce variance\n",
      "\n",
      "2. NEURAL NETWORK WITH BETTER ARCHITECTURE\n",
      "   - Deeper MLP with residual connections\n",
      "   - Attention mechanism for solvent features\n",
      "\n",
      "3. STACKING/BLENDING\n",
      "   - Use predictions from multiple models as features\n",
      "   - Train meta-learner on stacked predictions\n",
      "\n",
      "4. FEATURE SELECTION/ENGINEERING\n",
      "   - PCA on combined features\n",
      "   - Feature importance analysis\n",
      "   - Interaction features between process and solvent\n"
     ]
    }
   ],
   "source": [
    "# What we haven't tried:\n",
    "print(\"=== APPROACHES NOT YET TRIED ===\")\n",
    "print(\"\\n1. ENSEMBLE OF DIVERSE MODEL FAMILIES\")\n",
    "print(\"   - Combine GP + ETR + HGB + MLP\")\n",
    "print(\"   - Each model family captures different patterns\")\n",
    "print(\"   - Weighted averaging can reduce variance\")\n",
    "print(\"\\n2. NEURAL NETWORK WITH BETTER ARCHITECTURE\")\n",
    "print(\"   - Deeper MLP with residual connections\")\n",
    "print(\"   - Attention mechanism for solvent features\")\n",
    "print(\"\\n3. STACKING/BLENDING\")\n",
    "print(\"   - Use predictions from multiple models as features\")\n",
    "print(\"   - Train meta-learner on stacked predictions\")\n",
    "print(\"\\n4. FEATURE SELECTION/ENGINEERING\")\n",
    "print(\"   - PCA on combined features\")\n",
    "print(\"   - Feature importance analysis\")\n",
    "print(\"   - Interaction features between process and solvent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f96fd8",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Insight\n",
    "More regularization made LB WORSE (0.0956 → 0.0991). This means:\n",
    "1. The problem is NOT traditional overfitting\n",
    "2. We need BETTER features, not simpler models\n",
    "3. The test set has chemically unique solvents\n",
    "\n",
    "### Recommended Next Steps\n",
    "1. **Ensemble of diverse models** - Combine GP + ETR + HGB + MLP with learned weights\n",
    "2. **Feature engineering** - Focus on features that capture chemical similarity\n",
    "3. **Stacking** - Use predictions from multiple models as features\n",
    "\n",
    "### What NOT to Try\n",
    "- More regularization (made LB worse)\n",
    "- Simpler models (lose signal without reducing gap)\n",
    "- GP alone (CV worse than tree-based)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
