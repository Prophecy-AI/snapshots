{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe1b3e8",
   "metadata": {},
   "source": [
    "# Experiment 001: Baseline with Arrhenius Kinetics + Ensemble + TTA\n",
    "\n",
    "This baseline combines:\n",
    "1. Arrhenius kinetics features (1/T, ln(t), interaction)\n",
    "2. Chemical symmetry TTA for mixed solvents\n",
    "3. Ensemble of MLP + XGBoost + LightGBM + RF\n",
    "4. Bagging with multiple seeds (5 models)\n",
    "5. Spange descriptors for solvent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb40c2c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:49.007221Z",
     "iopub.status.busy": "2026-01-13T23:50:49.006703Z",
     "iopub.status.idle": "2026-01-13T23:50:50.922435Z",
     "shell.execute_reply": "2026-01-13T23:50:50.922027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from abc import ABC\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths for local execution\n",
    "DATA_PATH = '/home/data'\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31455c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:50.923594Z",
     "iopub.status.busy": "2026-01-13T23:50:50.923436Z",
     "iopub.status.idle": "2026-01-13T23:50:50.930272Z",
     "shell.execute_reply": "2026-01-13T23:50:50.929909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange descriptors shape: (26, 13)\n",
      "Solvents: ['Cyclohexane', 'Ethyl Acetate', 'Acetic Acid', '2-Methyltetrahydrofuran [2-MeTHF]', '1,1,1,3,3,3-Hexafluoropropan-2-ol', 'IPA [Propan-2-ol]', 'Ethanol', 'Methanol', 'Ethylene Glycol [1,2-Ethanediol]', 'Acetonitrile', 'Water', 'Diethyl Ether [Ether]', 'MTBE [tert-Butylmethylether]', 'Dimethyl Carbonate', 'tert-Butanol [2-Methylpropan-2-ol]', 'DMA [N,N-Dimethylacetamide]', '2,2,2-Trifluoroethanol', 'Dihydrolevoglucosenone (Cyrene)', 'Decanol', 'Butanone [MEK]', 'Ethyl Lactate', 'Methyl Propionate', 'THF [Tetrahydrofuran]', 'Water.Acetonitrile', 'Acetonitrile.Acetic Acid', 'Water.2,2,2-Trifluoroethanol']\n"
     ]
    }
   ],
   "source": [
    "# --- UTILITY FUNCTIONS ---\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "def load_features(name=\"spange_descriptors\"):\n",
    "    return pd.read_csv(f'{DATA_PATH}/{name}_lookup.csv', index_col=0)\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "# Load spange descriptors\n",
    "SPANGE_DF = load_features('spange_descriptors')\n",
    "print(f\"Spange descriptors shape: {SPANGE_DF.shape}\")\n",
    "print(f\"Solvents: {list(SPANGE_DF.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cd7617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:50.931136Z",
     "iopub.status.busy": "2026-01-13T23:50:50.931040Z",
     "iopub.status.idle": "2026-01-13T23:50:50.941451Z",
     "shell.execute_reply": "2026-01-13T23:50:50.941062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent feature shape: torch.Size([5, 18])\n",
      "Feature dim: 18\n"
     ]
    }
   ],
   "source": [
    "# --- KINETIC FEATURIZER WITH ARRHENIUS FEATURES ---\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def featurize(self, X, flip=False): raise NotImplementedError\n",
    "\n",
    "class KineticMixingFeaturizer(SmilesFeaturizer):\n",
    "    \"\"\"Featurizer with Arrhenius kinetics features and chemical symmetry support.\"\"\"\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.featurizer = SPANGE_DF\n",
    "        # Base features: 2 numeric + 3 kinetic + 13 spange = 18\n",
    "        # For mixed: add SolventB% = 19\n",
    "        self.feats_dim = self.featurizer.shape[1] + 2 + 3 + (1 if mixed else 0)\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        \n",
    "        # --- ARRHENIUS KINETIC FEATURES ---\n",
    "        temp_c = X_vals[:, 1:2]  # Temperature in Celsius\n",
    "        time_m = X_vals[:, 0:1]  # Residence time in minutes\n",
    "        \n",
    "        temp_k = temp_c + 273.15  # Convert to Kelvin\n",
    "        inv_temp = 1000.0 / temp_k  # Inverse temperature (Arrhenius)\n",
    "        log_time = np.log(time_m + 1e-6)  # Log of time\n",
    "        interaction = inv_temp * log_time  # Kinetic interaction term\n",
    "        \n",
    "        kinetic_features = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        X_kinetic = torch.tensor(kinetic_features)\n",
    "        \n",
    "        # --- CHEMICAL FEATURES ---\n",
    "        if self.mixed:\n",
    "            A = torch.tensor(self.featurizer.loc[X[\"SOLVENT A NAME\"]].values)\n",
    "            B = torch.tensor(self.featurizer.loc[X[\"SOLVENT B NAME\"]].values)\n",
    "            pct = torch.tensor(X[\"SolventB%\"].values.reshape(-1, 1))\n",
    "            \n",
    "            if flip:\n",
    "                # SYMMETRY FLIP: Swap A and B\n",
    "                X_chem = B * (1 - pct) + A * pct\n",
    "            else:\n",
    "                X_chem = A * (1 - pct) + B * pct\n",
    "            \n",
    "            # Add SolventB% as feature\n",
    "            X_out = torch.cat([X_kinetic, pct, X_chem], dim=1)\n",
    "        else:\n",
    "            X_chem = torch.tensor(self.featurizer.loc[X[\"SOLVENT NAME\"]].values)\n",
    "            X_out = torch.cat([X_kinetic, X_chem], dim=1)\n",
    "            \n",
    "        return X_out\n",
    "\n",
    "# Test featurizer\n",
    "X_test, Y_test = load_data(\"single_solvent\")\n",
    "feat = KineticMixingFeaturizer(mixed=False)\n",
    "X_feat = feat.featurize(X_test.head(5))\n",
    "print(f\"Single solvent feature shape: {X_feat.shape}\")\n",
    "print(f\"Feature dim: {feat.feats_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b172e2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:54.196398Z",
     "iopub.status.busy": "2026-01-13T23:50:54.196161Z",
     "iopub.status.idle": "2026-01-13T23:50:54.199727Z",
     "shell.execute_reply": "2026-01-13T23:50:54.199390Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- MLP ARCHITECTURE ---\n",
    "class MLPInternal(nn.Module):\n",
    "    \"\"\"MLP with BatchNorm, ReLU, Dropout, and Sigmoid output.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPInternal, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()  # Bounded output [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ee8560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:54.200579Z",
     "iopub.status.busy": "2026-01-13T23:50:54.200477Z",
     "iopub.status.idle": "2026-01-13T23:50:54.211763Z",
     "shell.execute_reply": "2026-01-13T23:50:54.211430Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- ENSEMBLE MODEL WITH BAGGING AND TTA ---\n",
    "class EnsembleModel(nn.Module):\n",
    "    \"\"\"Ensemble of MLP + XGBoost + LightGBM + RandomForest with TTA for mixed solvents.\"\"\"\n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data_type = data\n",
    "        self.featurizer = KineticMixingFeaturizer(mixed=(data=='full'))\n",
    "        \n",
    "        # Bagging: 5 MLP models\n",
    "        self.n_mlp_models = 5\n",
    "        self.mlp_models = nn.ModuleList()\n",
    "        \n",
    "        # Gradient boosting models\n",
    "        self.xgb_model = None\n",
    "        self.lgb_model = None\n",
    "        self.rf_model = None\n",
    "        \n",
    "        # Scaler for GBDT models\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Ensemble weights: [MLP, XGB, LGB, RF]\n",
    "        self.weights = [0.35, 0.25, 0.25, 0.15]\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        # 1. Prepare features\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            # Data augmentation with flipped features\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all = X_std\n",
    "            y_all = y_vals\n",
    "        \n",
    "        X_np = X_all.numpy()\n",
    "        y_np = y_all.numpy()\n",
    "        \n",
    "        # Fit scaler\n",
    "        X_scaled = self.scaler.fit_transform(X_np)\n",
    "        \n",
    "        input_dim = X_scaled.shape[1]\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # --- Train MLP models (bagging) ---\n",
    "        for i in range(self.n_mlp_models):\n",
    "            torch.manual_seed(42 + i)\n",
    "            np.random.seed(42 + i)\n",
    "            \n",
    "            model = MLPInternal(input_dim).to(device)\n",
    "            model.train()\n",
    "            self.mlp_models.append(model)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "            y_tensor = torch.tensor(y_np, dtype=torch.double).to(device)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "            criterion = nn.HuberLoss()\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=20\n",
    "            )\n",
    "            \n",
    "            for epoch in range(200):\n",
    "                epoch_loss = 0.0\n",
    "                for inputs, targets in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                scheduler.step(epoch_loss / len(dataset))\n",
    "        \n",
    "        # --- Train XGBoost ---\n",
    "        self.xgb_model = MultiOutputRegressor(\n",
    "            xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "        )\n",
    "        self.xgb_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # --- Train LightGBM ---\n",
    "        self.lgb_model = MultiOutputRegressor(\n",
    "            lgb.LGBMRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=31,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbosity=-1\n",
    "            )\n",
    "        )\n",
    "        self.lgb_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # --- Train RandomForest ---\n",
    "        self.rf_model = MultiOutputRegressor(\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        )\n",
    "        self.rf_model.fit(X_scaled, y_np)\n",
    "\n",
    "    def predict(self, X):\n",
    "        device = next(self.mlp_models[0].parameters()).device\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            # --- TEST TIME AUGMENTATION (TTA) ---\n",
    "            X_std = self.featurizer.featurize(X, flip=False)\n",
    "            X_flip = self.featurizer.featurize(X, flip=True)\n",
    "            \n",
    "            X_std_scaled = self.scaler.transform(X_std.numpy())\n",
    "            X_flip_scaled = self.scaler.transform(X_flip.numpy())\n",
    "            \n",
    "            # MLP predictions with TTA\n",
    "            mlp_pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.mlp_models:\n",
    "                    model.eval()\n",
    "                    X_std_t = torch.tensor(X_std_scaled, dtype=torch.double).to(device)\n",
    "                    X_flip_t = torch.tensor(X_flip_scaled, dtype=torch.double).to(device)\n",
    "                    p1 = model(X_std_t)\n",
    "                    p2 = model(X_flip_t)\n",
    "                    mlp_pred_sum += (p1 + p2) * 0.5\n",
    "            mlp_preds = (mlp_pred_sum / self.n_mlp_models).cpu().numpy()\n",
    "            \n",
    "            # GBDT predictions with TTA\n",
    "            xgb_preds = (self.xgb_model.predict(X_std_scaled) + self.xgb_model.predict(X_flip_scaled)) / 2\n",
    "            lgb_preds = (self.lgb_model.predict(X_std_scaled) + self.lgb_model.predict(X_flip_scaled)) / 2\n",
    "            rf_preds = (self.rf_model.predict(X_std_scaled) + self.rf_model.predict(X_flip_scaled)) / 2\n",
    "        else:\n",
    "            X_std = self.featurizer.featurize(X)\n",
    "            X_std_scaled = self.scaler.transform(X_std.numpy())\n",
    "            \n",
    "            # MLP predictions\n",
    "            mlp_pred_sum = torch.zeros((len(X), 3)).to(device)\n",
    "            with torch.no_grad():\n",
    "                for model in self.mlp_models:\n",
    "                    model.eval()\n",
    "                    X_t = torch.tensor(X_std_scaled, dtype=torch.double).to(device)\n",
    "                    mlp_pred_sum += model(X_t)\n",
    "            mlp_preds = (mlp_pred_sum / self.n_mlp_models).cpu().numpy()\n",
    "            \n",
    "            # GBDT predictions\n",
    "            xgb_preds = self.xgb_model.predict(X_std_scaled)\n",
    "            lgb_preds = self.lgb_model.predict(X_std_scaled)\n",
    "            rf_preds = self.rf_model.predict(X_std_scaled)\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_preds = (\n",
    "            self.weights[0] * mlp_preds +\n",
    "            self.weights[1] * xgb_preds +\n",
    "            self.weights[2] * lgb_preds +\n",
    "            self.weights[3] * rf_preds\n",
    "        )\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        final_preds = np.clip(final_preds, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efb9659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T23:50:57.554475Z",
     "iopub.status.busy": "2026-01-13T23:50:57.554064Z",
     "iopub.status.idle": "2026-01-14T00:05:37.248257Z",
     "shell.execute_reply": "2026-01-14T00:05:37.247829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TASK 0: Single Solvent (Leave-One-Out) ===\n",
      "Data shape: X=(656, 3), Y=(656, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 1/24 [00:36<13:57, 36.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 2/24 [01:12<13:17, 36.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 3/24 [01:47<12:31, 35.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 4/24 [02:22<11:43, 35.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 5/24 [02:57<11:13, 35.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 6/24 [03:34<10:46, 35.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 7/24 [04:11<10:14, 36.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 8/24 [04:47<09:37, 36.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 9/24 [05:22<08:58, 35.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 10/24 [05:58<08:22, 35.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 11/24 [06:34<07:44, 35.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 12/24 [07:10<07:09, 35.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 13/24 [07:46<06:34, 35.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 14/24 [08:22<05:58, 35.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 15/24 [08:59<05:26, 36.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 16/24 [09:36<04:53, 36.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 17/24 [10:18<04:27, 38.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 18/24 [11:00<03:55, 39.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 19/24 [11:38<03:14, 38.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 20/24 [12:16<02:33, 38.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 21/24 [12:52<01:53, 37.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 22/24 [13:27<01:14, 37.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 23/24 [14:03<00:36, 36.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [14:39<00:00, 36.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 24/24 [14:39<00:00, 36.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Solvent CV MAE: 0.068386 +/- 0.036173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CROSS-VALIDATION FOR SINGLE SOLVENT TASK ---\n",
    "print(\"\\n=== TASK 0: Single Solvent (Leave-One-Out) ===\")\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "print(f\"Data shape: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "all_errors = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=24):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold error\n",
    "    fold_error = np.mean(np.abs(predictions_np - test_Y.values))\n",
    "    all_errors.append(fold_error)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nSingle Solvent CV MAE: {np.mean(all_errors):.6f} +/- {np.std(all_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83485c7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T00:06:03.310668Z",
     "iopub.status.busy": "2026-01-14T00:06:03.310031Z",
     "iopub.status.idle": "2026-01-14T00:33:30.699179Z",
     "shell.execute_reply": "2026-01-14T00:33:30.698769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TASK 1: Full Data (Leave-One-Ramp-Out) ===\n",
      "Data shape: X=(1227, 5), Y=(1227, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/13 [02:03<24:45, 123.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 2/13 [04:07<22:37, 123.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 3/13 [06:09<20:30, 123.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [08:12<18:25, 122.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 5/13 [10:18<16:32, 124.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 6/13 [12:22<14:29, 124.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [14:24<12:19, 123.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 8/13 [16:28<10:18, 123.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 9/13 [18:31<08:13, 123.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 10/13 [20:44<06:18, 126.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [22:56<04:16, 128.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 12/13 [25:08<02:09, 129.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [27:27<00:00, 132.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 13/13 [27:27<00:00, 126.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Data CV MAE: 0.088347 +/- 0.030664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CROSS-VALIDATION FOR FULL DATA TASK ---\n",
    "print(\"\\n=== TASK 1: Full Data (Leave-One-Ramp-Out) ===\")\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "print(f\"Data shape: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "all_errors_full = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator), total=13):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "    \n",
    "    model = EnsembleModel(data='full')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    \n",
    "    predictions = model.predict(test_X)\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    # Calculate fold error\n",
    "    fold_error = np.mean(np.abs(predictions_np - test_Y.values))\n",
    "    all_errors_full.append(fold_error)\n",
    "    \n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "print(f\"\\nFull Data CV MAE: {np.mean(all_errors_full):.6f} +/- {np.std(all_errors_full):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a88114c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T00:33:45.187748Z",
     "iopub.status.busy": "2026-01-14T00:33:45.187177Z",
     "iopub.status.idle": "2026-01-14T00:33:45.206200Z",
     "shell.execute_reply": "2026-01-14T00:33:45.205767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS ===\n",
      "Single Solvent CV MAE: 0.068386\n",
      "Full Data CV MAE: 0.088347\n",
      "Combined CV MAE (weighted): 0.081393\n",
      "\n",
      "Submission saved to /home/submission/submission.csv\n",
      "Submission shape: (1883, 7)\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE SUBMISSION ---\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "\n",
    "# Save to submission folder\n",
    "import os\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission.to_csv('/home/submission/submission.csv', index=True)\n",
    "\n",
    "# Also save locally\n",
    "submission.to_csv('/home/code/experiments/001_baseline/submission.csv', index=True)\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Single Solvent CV MAE: {np.mean(all_errors):.6f}\")\n",
    "print(f\"Full Data CV MAE: {np.mean(all_errors_full):.6f}\")\n",
    "\n",
    "# Combined score (weighted average based on data sizes)\n",
    "total_single = len(submission_single_solvent)\n",
    "total_full = len(submission_full_data)\n",
    "total = total_single + total_full\n",
    "combined_mae = (np.mean(all_errors) * total_single + np.mean(all_errors_full) * total_full) / total\n",
    "print(f\"Combined CV MAE (weighted): {combined_mae:.6f}\")\n",
    "print(f\"\\nSubmission saved to /home/submission/submission.csv\")\n",
    "print(f\"Submission shape: {submission.shape}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
