{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a423566",
   "metadata": {},
   "source": [
    "# Experiment 015: Per-Target + MLP Hybrid with COMBINED Features\n",
    "\n",
    "**Key improvements over exp_014:**\n",
    "1. COMBINED features (0.8*ACS_PCA + 0.2*Spange) like exp_004 - NOT Spange-only\n",
    "2. DEEPER models (depth=None) - NOT shallow like exp_014\n",
    "3. Add MLP component for non-linear patterns\n",
    "4. Optuna for ensemble WEIGHTS (not just hyperparameters)\n",
    "\n",
    "**Architecture:**\n",
    "- Per-target: HGB for SM, ETR for Products (from exp_004)\n",
    "- MLP: [128, 64, 32] with BatchNorm, ReLU, Dropout\n",
    "- Ensemble: Optuna-optimized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6284424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:19:59.672626Z",
     "iopub.status.busy": "2026-01-14T04:19:59.672111Z",
     "iopub.status.idle": "2026-01-14T04:20:01.342954Z",
     "shell.execute_reply": "2026-01-14T04:20:01.342553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from abc import ABC\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "torch.set_default_dtype(torch.double)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376d83b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:01.344277Z",
     "iopub.status.busy": "2026-01-14T04:20:01.344107Z",
     "iopub.status.idle": "2026-01-14T04:20:01.357829Z",
     "shell.execute_reply": "2026-01-14T04:20:01.357157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 14)\n",
      "ACS_PCA: (24, 6)\n",
      "\n",
      "Spange features: 13\n",
      "ACS_PCA features: 5\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD FEATURES ---\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "# Load COMBINED features (0.8*ACS_PCA + 0.2*Spange) like exp_004\n",
    "Spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv')\n",
    "ACS_PCA = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv')\n",
    "\n",
    "print(f\"Spange: {Spange.shape}\")\n",
    "print(f\"ACS_PCA: {ACS_PCA.shape}\")\n",
    "\n",
    "# Create lookup dictionaries\n",
    "Spange_dict = {row['SOLVENT NAME']: row.drop('SOLVENT NAME').values.astype(float) for _, row in Spange.iterrows()}\n",
    "ACS_PCA_dict = {row['SOLVENT NAME']: row.drop('SOLVENT NAME').values.astype(float) for _, row in ACS_PCA.iterrows()}\n",
    "\n",
    "print(f\"\\nSpange features: {len(list(Spange_dict.values())[0])}\")\n",
    "print(f\"ACS_PCA features: {len(list(ACS_PCA_dict.values())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c62ded6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:01.358801Z",
     "iopub.status.busy": "2026-01-14T04:20:01.358684Z",
     "iopub.status.idle": "2026-01-14T04:20:01.373948Z",
     "shell.execute_reply": "2026-01-14T04:20:01.373565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOO utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# --- LOO UTILITY FUNCTIONS (REQUIRED FOR SUBMISSION) ---\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Leave-One-Solvent-Out for single solvent data (24 folds).\"\"\"\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Leave-One-Ramp-Out for full data (13 folds).\"\"\"\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & \n",
    "                 (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print(\"LOO utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67d1a1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:01.374830Z",
     "iopub.status.busy": "2026-01-14T04:20:01.374720Z",
     "iopub.status.idle": "2026-01-14T04:20:01.377439Z",
     "shell.execute_reply": "2026-01-14T04:20:01.377096Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- BASE CLASSES ---\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self): raise NotImplementedError\n",
    "    def featurize(self, X): raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self): pass\n",
    "    def train_model(self, X_train, y_train): raise NotImplementedError\n",
    "    def predict(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae746624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:01.378358Z",
     "iopub.status.busy": "2026-01-14T04:20:01.378249Z",
     "iopub.status.idle": "2026-01-14T04:20:01.394019Z",
     "shell.execute_reply": "2026-01-14T04:20:01.393683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature dim (single): 20\n",
      "Combined feature dim (full): 21\n"
     ]
    }
   ],
   "source": [
    "# --- COMBINED FEATURE EXTRACTION (0.8*ACS_PCA + 0.2*Spange) ---\n",
    "SPANGE_WEIGHT = 0.2\n",
    "ACS_WEIGHT = 0.8\n",
    "\n",
    "def get_combined_features_single(X):\n",
    "    \"\"\"Extract COMBINED features for single solvent data.\"\"\"\n",
    "    features = []\n",
    "    for _, row in X.iterrows():\n",
    "        solvent = row['SOLVENT NAME']\n",
    "        spange = Spange_dict.get(solvent, np.zeros(13))\n",
    "        acs_pca = ACS_PCA_dict.get(solvent, np.zeros(15))\n",
    "        \n",
    "        # Combine: 0.8*ACS_PCA + 0.2*Spange (like exp_004)\n",
    "        combined = np.concatenate([\n",
    "            [row['Residence Time'], row['Temperature']],\n",
    "            ACS_WEIGHT * acs_pca,\n",
    "            SPANGE_WEIGHT * spange\n",
    "        ])\n",
    "        features.append(combined)\n",
    "    return np.array(features)\n",
    "\n",
    "def get_combined_features_full(X):\n",
    "    \"\"\"Extract COMBINED features for full (mixed solvent) data.\"\"\"\n",
    "    features = []\n",
    "    for _, row in X.iterrows():\n",
    "        solvent_a = row['SOLVENT A NAME']\n",
    "        solvent_b = row['SOLVENT B NAME']\n",
    "        pct_b = row['SolventB%'] / 100.0\n",
    "        \n",
    "        spange_a = Spange_dict.get(solvent_a, np.zeros(13))\n",
    "        spange_b = Spange_dict.get(solvent_b, np.zeros(13))\n",
    "        acs_a = ACS_PCA_dict.get(solvent_a, np.zeros(15))\n",
    "        acs_b = ACS_PCA_dict.get(solvent_b, np.zeros(15))\n",
    "        \n",
    "        # Linear interpolation for mixed solvents\n",
    "        spange_mix = (1 - pct_b) * spange_a + pct_b * spange_b\n",
    "        acs_mix = (1 - pct_b) * acs_a + pct_b * acs_b\n",
    "        \n",
    "        # Combine: 0.8*ACS_PCA + 0.2*Spange\n",
    "        combined = np.concatenate([\n",
    "            [row['Residence Time'], row['Temperature'], pct_b],\n",
    "            ACS_WEIGHT * acs_mix,\n",
    "            SPANGE_WEIGHT * spange_mix\n",
    "        ])\n",
    "        features.append(combined)\n",
    "    return np.array(features)\n",
    "\n",
    "# Test feature extraction\n",
    "X_test, _ = load_data(\"single_solvent\")\n",
    "test_feat = get_combined_features_single(X_test.head(5))\n",
    "print(f\"Combined feature dim (single): {test_feat.shape[1]}\")\n",
    "\n",
    "X_test_full, _ = load_data(\"full\")\n",
    "test_feat_full = get_combined_features_full(X_test_full.head(5))\n",
    "print(f\"Combined feature dim (full): {test_feat_full.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12e5712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:01.394877Z",
     "iopub.status.busy": "2026-01-14T04:20:01.394785Z",
     "iopub.status.idle": "2026-01-14T04:20:01.398665Z",
     "shell.execute_reply": "2026-01-14T04:20:01.398291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP defined\n"
     ]
    }
   ],
   "source": [
    "# --- MLP ARCHITECTURE ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"MLP with BatchNorm + ReLU + Dropout.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm1d(input_dim))\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())  # Output in [0, 1]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.size(0) == 1 and self.training:\n",
    "            self.eval()\n",
    "            out = self.network(x)\n",
    "            self.train()\n",
    "            return out\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"SimpleMLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744eab1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:07.350350Z",
     "iopub.status.busy": "2026-01-14T04:20:07.349844Z",
     "iopub.status.idle": "2026-01-14T04:20:07.356983Z",
     "shell.execute_reply": "2026-01-14T04:20:07.356606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerTargetMLPHybrid defined\n"
     ]
    }
   ],
   "source": [
    "# --- PER-TARGET + MLP HYBRID MODEL ---\n",
    "class PerTargetMLPHybrid(BaseModel):\n",
    "    \"\"\"Hybrid model combining Per-Target GBDT + MLP.\n",
    "    \n",
    "    Architecture:\n",
    "    - HGB for SM (target 2) - captures gradient patterns (DEEP, not shallow)\n",
    "    - ETR for Products (targets 0, 1) - robust to outliers (DEEP, not shallow)\n",
    "    - MLP for all targets - captures non-linear patterns\n",
    "    - Ensemble: weighted average with Optuna-optimized weights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single', weights=None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.weights = weights or {'mlp': 0.3, 'hgb': 0.35, 'etr': 0.35}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp = None\n",
    "        self.hgb = None\n",
    "        self.etr = None\n",
    "    \n",
    "    def _get_features(self, X):\n",
    "        if self.data == 'single':\n",
    "            return get_combined_features_single(X)\n",
    "        else:\n",
    "            return get_combined_features_full(X)\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_feat = self._get_features(X_train)\n",
    "        y_np = y_train.values\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # 1. Train MLP\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        self.mlp = SimpleMLP(input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.2).to(device)\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "        y_tensor = torch.tensor(y_np, dtype=torch.double).to(device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp.train()\n",
    "        for epoch in range(100):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp(batch_X)\n",
    "                loss = criterion(pred, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # 2. Train HGB for SM (target 2) - DEEP, not shallow\n",
    "        self.hgb = HistGradientBoostingRegressor(\n",
    "            max_depth=None,  # DEEP - no limit (unlike exp_014's depth=3)\n",
    "            learning_rate=0.1,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.hgb.fit(X_scaled, y_np[:, 2])\n",
    "        \n",
    "        # 3. Train ETR for Products (targets 0, 1) - DEEP, not shallow\n",
    "        self.etr = ExtraTreesRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=None,  # DEEP - no limit (unlike exp_014's depth=6)\n",
    "            min_samples_split=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.etr.fit(X_scaled, y_np[:, :2])\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self._get_features(X_test)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # MLP prediction\n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "            mlp_pred = self.mlp(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # HGB prediction (SM only)\n",
    "        hgb_pred_sm = self.hgb.predict(X_scaled).reshape(-1, 1)\n",
    "        \n",
    "        # ETR prediction (Products only)\n",
    "        etr_pred_products = self.etr.predict(X_scaled)\n",
    "        \n",
    "        # Combine per-target predictions\n",
    "        # For Products (0, 1): use ETR\n",
    "        # For SM (2): use HGB\n",
    "        gbdt_pred = np.column_stack([etr_pred_products, hgb_pred_sm])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        w_mlp = self.weights['mlp']\n",
    "        w_gbdt = 1 - w_mlp  # Remaining weight for GBDT\n",
    "        \n",
    "        final_pred = w_mlp * mlp_pred + w_gbdt * gbdt_pred\n",
    "        final_pred = np.clip(final_pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_pred)\n",
    "\n",
    "print(\"PerTargetMLPHybrid defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05532255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:20:12.533910Z",
     "iopub.status.busy": "2026-01-14T04:20:12.533416Z",
     "iopub.status.idle": "2026-01-14T04:24:48.909578Z",
     "shell.execute_reply": "2026-01-14T04:24:48.909176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Optuna optimization for ensemble weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eead82dd90946b49a8330954eea9e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MLP weight: 0.5012\n",
      "Best GroupKFold CV: 0.070156\n"
     ]
    }
   ],
   "source": [
    "# --- OPTUNA OPTIMIZATION FOR ENSEMBLE WEIGHTS ---\n",
    "print(\"Running Optuna optimization for ensemble weights...\")\n",
    "\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_feat_single = get_combined_features_single(X_single)\n",
    "y_single = Y_single.values\n",
    "groups_single = X_single[\"SOLVENT NAME\"].values\n",
    "\n",
    "def objective_weights(trial):\n",
    "    # Ensemble weight to optimize\n",
    "    mlp_weight = trial.suggest_float('mlp_weight', 0.1, 0.6)\n",
    "    \n",
    "    # Use GroupKFold for internal CV (faster)\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    errors = []\n",
    "    \n",
    "    for train_idx, val_idx in gkf.split(X_feat_single, y_single, groups=groups_single):\n",
    "        X_train_df = X_single.iloc[train_idx]\n",
    "        X_val_df = X_single.iloc[val_idx]\n",
    "        y_train = Y_single.iloc[train_idx]\n",
    "        y_val = Y_single.iloc[val_idx].values\n",
    "        \n",
    "        # Train model with trial weights\n",
    "        weights = {'mlp': mlp_weight, 'hgb': (1-mlp_weight)/2, 'etr': (1-mlp_weight)/2}\n",
    "        model = PerTargetMLPHybrid(data='single', weights=weights)\n",
    "        model.train_model(X_train_df, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        preds = model.predict(X_val_df).numpy()\n",
    "        mae = np.mean(np.abs(preds - y_val))\n",
    "        errors.append(mae)\n",
    "    \n",
    "    return np.mean(errors)\n",
    "\n",
    "# Run Optuna (fewer trials since each is expensive)\n",
    "study_weights = optuna.create_study(direction='minimize')\n",
    "study_weights.optimize(objective_weights, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest MLP weight: {study_weights.best_params['mlp_weight']:.4f}\")\n",
    "print(f\"Best GroupKFold CV: {study_weights.best_value:.6f}\")\n",
    "\n",
    "best_mlp_weight = study_weights.best_params['mlp_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f726ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QUICK VALIDATION TEST ---\n",
    "print(\"Quick test of PerTargetMLPHybrid with LOO...\")\n",
    "\n",
    "best_weights = {'mlp': best_mlp_weight, 'hgb': (1-best_mlp_weight)/2, 'etr': (1-best_mlp_weight)/2}\n",
    "print(f\"Using weights: {best_weights}\")\n",
    "\n",
    "errors = []\n",
    "for i, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X_single, Y_single)):\n",
    "    if i >= 3: break\n",
    "    solvent = test_X['SOLVENT NAME'].iloc[0]\n",
    "    model = PerTargetMLPHybrid(data='single', weights=best_weights)\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mae = np.mean(np.abs(preds - test_Y.values))\n",
    "    errors.append(mae)\n",
    "    print(f\"Fold {i} ({solvent}): MAE = {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nQuick test MAE (3 folds): {np.mean(errors):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetMLPHybrid(data='single', weights=best_weights) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = PerTargetMLPHybrid(data='full', weights=best_weights) # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
