{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7284dd32",
   "metadata": {},
   "source": [
    "# Experiment 013: LOO Ensemble (Fixed Fold Structure)\n",
    "\n",
    "**CRITICAL FIX**: Revert to LOO validation (24 folds for task 0, 13 folds for task 1).\n",
    "\n",
    "exp_012 FAILED because GroupKFold changed the fold structure to 5 folds.\n",
    "The evaluation metric expects the ORIGINAL LOO fold structure.\n",
    "\n",
    "**Key changes:**\n",
    "1. Use ORIGINAL LOO utility functions (NOT GroupKFold)\n",
    "2. Keep MLP + GBDT ensemble architecture\n",
    "3. Template-compliant submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c98b51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:07.757919Z",
     "iopub.status.busy": "2026-01-14T03:57:07.757428Z",
     "iopub.status.idle": "2026-01-14T03:57:09.509832Z",
     "shell.execute_reply": "2026-01-14T03:57:09.509447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from abc import ABC\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "torch.set_default_dtype(torch.double)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff67f3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.511114Z",
     "iopub.status.busy": "2026-01-14T03:57:09.510958Z",
     "iopub.status.idle": "2026-01-14T03:57:09.520063Z",
     "shell.execute_reply": "2026-01-14T03:57:09.519732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 14)\n"
     ]
    }
   ],
   "source": [
    "# --- UTILITY FUNCTIONS (ORIGINAL LOO - NOT GROUPKFOLD) ---\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "# Load Spange descriptors\n",
    "Spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv')\n",
    "print(f\"Spange: {Spange.shape}\")\n",
    "Spange_dict = {row['SOLVENT NAME']: row.drop('SOLVENT NAME').values for _, row in Spange.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5326d8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.520869Z",
     "iopub.status.busy": "2026-01-14T03:57:09.520776Z",
     "iopub.status.idle": "2026-01-14T03:57:09.550743Z",
     "shell.execute_reply": "2026-01-14T03:57:09.550415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single solvent folds: 24\n",
      "Full data folds: 13\n"
     ]
    }
   ],
   "source": [
    "# --- ORIGINAL LOO UTILITY FUNCTIONS (REQUIRED FOR SUBMISSION) ---\n",
    "# DO NOT USE GROUPKFOLD - it breaks the submission format!\n",
    "# Task 0 expects 24 folds (one per solvent)\n",
    "# Task 1 expects 13 folds (one per solvent ramp)\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"Leave-One-Solvent-Out for single solvent data (24 folds).\"\"\"\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"Leave-One-Ramp-Out for full data (13 folds).\"\"\"\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & \n",
    "                 (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "# Verify fold counts\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "print(f\"Single solvent folds: {len(list(generate_leave_one_out_splits(X_single, Y_single)))}\")\n",
    "print(f\"Full data folds: {len(list(generate_leave_one_ramp_out_splits(X_full, Y_full)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5157070e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.552190Z",
     "iopub.status.busy": "2026-01-14T03:57:09.551894Z",
     "iopub.status.idle": "2026-01-14T03:57:09.554493Z",
     "shell.execute_reply": "2026-01-14T03:57:09.554183Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- BASE CLASSES ---\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self): raise NotImplementedError\n",
    "    def featurize(self, X): raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self): pass\n",
    "    def train_model(self, X_train, y_train): raise NotImplementedError\n",
    "    def predict(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018461b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.555417Z",
     "iopub.status.busy": "2026-01-14T03:57:09.555329Z",
     "iopub.status.idle": "2026-01-14T03:57:09.559056Z",
     "shell.execute_reply": "2026-01-14T03:57:09.558751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKernelMLP defined\n"
     ]
    }
   ],
   "source": [
    "# --- MLP ARCHITECTURE (TOP KERNEL - NO SIGMOID) ---\n",
    "class TopKernelMLP(nn.Module):\n",
    "    \"\"\"MLP with BatchNorm + ReLU + Dropout, LINEAR output (no Sigmoid).\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.BatchNorm1d(input_dim))\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle batch_size=1 for BatchNorm\n",
    "        if x.size(0) == 1 and self.training:\n",
    "            self.eval()\n",
    "            out = self.network(x)\n",
    "            self.train()\n",
    "            return out\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"TopKernelMLP defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad43a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.559880Z",
     "iopub.status.busy": "2026-01-14T03:57:09.559788Z",
     "iopub.status.idle": "2026-01-14T03:57:09.568443Z",
     "shell.execute_reply": "2026-01-14T03:57:09.568104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKernelEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# --- MLP + GBDT ENSEMBLE MODEL ---\n",
    "class TopKernelEnsemble(BaseModel):\n",
    "    \"\"\"Ensemble of MLP + XGBoost + RandomForest + LightGBM.\n",
    "    \n",
    "    Architecture:\n",
    "    - MLP: [128, 64, 32], NO Sigmoid, 100 epochs, lr=1e-3, dropout=0.1\n",
    "    - XGBoost: n_estimators=300, max_depth=6\n",
    "    - RandomForest: n_estimators=300, max_depth=15\n",
    "    - LightGBM: n_estimators=300\n",
    "    - Weights: [0.4, 0.2, 0.2, 0.2]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp = None\n",
    "        self.xgb_models = []\n",
    "        self.rf_model = None\n",
    "        self.lgb_models = []\n",
    "        self.weights = [0.4, 0.2, 0.2, 0.2]\n",
    "    \n",
    "    def _get_features(self, X):\n",
    "        features = []\n",
    "        if self.data == 'single':\n",
    "            for _, row in X.iterrows():\n",
    "                solvent = row['SOLVENT NAME']\n",
    "                spange = Spange_dict.get(solvent, np.zeros(13))\n",
    "                feat = np.concatenate([[row['Residence Time'], row['Temperature']], spange])\n",
    "                features.append(feat)\n",
    "        else:\n",
    "            for _, row in X.iterrows():\n",
    "                solvent_a = row['SOLVENT A NAME']\n",
    "                solvent_b = row['SOLVENT B NAME']\n",
    "                pct_b = row['SolventB%'] / 100.0\n",
    "                spange_a = Spange_dict.get(solvent_a, np.zeros(13))\n",
    "                spange_b = Spange_dict.get(solvent_b, np.zeros(13))\n",
    "                spange_mix = (1 - pct_b) * spange_a + pct_b * spange_b\n",
    "                feat = np.concatenate([[row['Residence Time'], row['Temperature'], pct_b], spange_mix])\n",
    "                features.append(feat)\n",
    "        return np.array(features)\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_feat = self._get_features(X_train)\n",
    "        y_np = y_train.values\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # Train MLP\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        self.mlp = TopKernelMLP(input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.1).to(device)\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "        y_tensor = torch.tensor(y_np, dtype=torch.double).to(device)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp.train()\n",
    "        for epoch in range(100):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp(batch_X)\n",
    "                loss = criterion(pred, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Train XGBoost\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = xgb.XGBRegressor(n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "                                     subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0)\n",
    "            model.fit(X_scaled, y_np[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "        \n",
    "        # Train RandomForest\n",
    "        self.rf_model = MultiOutputRegressor(\n",
    "            RandomForestRegressor(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1))\n",
    "        self.rf_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # Train LightGBM\n",
    "        self.lgb_models = []\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.05, num_leaves=31,\n",
    "                                      max_depth=-1, random_state=42, verbosity=-1)\n",
    "            model.fit(X_scaled, y_np[:, i])\n",
    "            self.lgb_models.append(model)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self._get_features(X_test)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "            mlp_pred = self.mlp(X_tensor).cpu().numpy()\n",
    "        \n",
    "        xgb_pred = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        rf_pred = self.rf_model.predict(X_scaled)\n",
    "        lgb_pred = np.column_stack([m.predict(X_scaled) for m in self.lgb_models])\n",
    "        \n",
    "        final_pred = (self.weights[0] * mlp_pred + self.weights[1] * xgb_pred +\n",
    "                      self.weights[2] * rf_pred + self.weights[3] * lgb_pred)\n",
    "        final_pred = np.clip(final_pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_pred)\n",
    "\n",
    "print(\"TopKernelEnsemble defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e71f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:09.569456Z",
     "iopub.status.busy": "2026-01-14T03:57:09.569364Z",
     "iopub.status.idle": "2026-01-14T03:57:22.552727Z",
     "shell.execute_reply": "2026-01-14T03:57:22.552320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick test of TopKernelEnsemble with LOO...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 (1,1,1,3,3,3-Hexafluoropropan-2-ol): MAE = 0.1856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 (2,2,2-Trifluoroethanol): MAE = 0.1221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 (2-Methyltetrahydrofuran [2-MeTHF]): MAE = 0.0305\n",
      "\n",
      "Quick test MAE (3 folds): 0.1127\n"
     ]
    }
   ],
   "source": [
    "# --- QUICK VALIDATION TEST (BEFORE TEMPLATE CELLS) ---\n",
    "print(\"Quick test of TopKernelEnsemble with LOO...\")\n",
    "X_test, Y_test = load_data(\"single_solvent\")\n",
    "\n",
    "errors = []\n",
    "for i, ((train_X, train_Y), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X_test, Y_test)):\n",
    "    if i >= 3: break\n",
    "    solvent = test_X['SOLVENT NAME'].iloc[0]\n",
    "    model = TopKernelEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mae = np.mean(np.abs(preds - test_Y.values))\n",
    "    errors.append(mae)\n",
    "    print(f\"Fold {i} ({solvent}): MAE = {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nQuick test MAE (3 folds): {np.mean(errors):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4d107f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:57:27.549896Z",
     "iopub.status.busy": "2026-01-14T03:57:27.549280Z",
     "iopub.status.idle": "2026-01-14T03:59:08.718870Z",
     "shell.execute_reply": "2026-01-14T03:59:08.718490Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:04,  4.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:08,  4.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:12,  4.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:16,  4.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:20,  4.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:24,  4.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:29,  4.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:33,  4.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [00:37,  4.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [00:41,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [00:45,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [00:50,  4.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [00:54,  4.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [00:58,  4.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [01:02,  4.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [01:07,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [01:11,  4.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [01:15,  4.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [01:20,  4.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [01:24,  4.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [01:28,  4.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [01:32,  4.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [01:36,  4.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [01:41,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [01:41,  4.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = TopKernelEnsemble(data='single') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92535b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:59:08.719890Z",
     "iopub.status.busy": "2026-01-14T03:59:08.719793Z",
     "iopub.status.idle": "2026-01-14T04:00:38.542710Z",
     "shell.execute_reply": "2026-01-14T04:00:38.542303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:06,  6.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:13,  6.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:20,  6.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:26,  6.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:33,  6.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [00:40,  6.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [00:47,  6.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [00:54,  6.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [01:00,  6.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [01:08,  6.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [01:15,  6.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [01:22,  7.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:29,  7.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [01:29,  6.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = TopKernelEnsemble(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec31970d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T04:00:38.543673Z",
     "iopub.status.busy": "2026-01-14T04:00:38.543576Z",
     "iopub.status.idle": "2026-01-14T04:00:38.552855Z",
     "shell.execute_reply": "2026-01-14T04:00:38.552540Z"
    }
   },
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
