{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da86c9b5",
   "metadata": {},
   "source": [
    "# Experiment 011: GroupKFold + Top Kernel Architecture\n",
    "\n",
    "**CRITICAL CHANGE**: Overwrite utility functions with GroupKFold (5-fold) instead of Leave-One-Out.\n",
    "\n",
    "This is the SINGLE MOST IMPORTANT change identified by the evaluator. The top kernel (lishellliang) uses GroupKFold, which gives more realistic CV estimates.\n",
    "\n",
    "**Key implementation:**\n",
    "1. GroupKFold (5-fold) validation\n",
    "2. MLP: [128, 64, 32], NO Sigmoid, 100 epochs, lr=1e-3, dropout=0.1\n",
    "3. GBDT: n_estimators=300, max_depth=15 for RF\n",
    "4. Ensemble weights: [0.4, 0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89408a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:37.355129Z",
     "iopub.status.busy": "2026-01-14T03:38:37.354593Z",
     "iopub.status.idle": "2026-01-14T03:38:39.107321Z",
     "shell.execute_reply": "2026-01-14T03:38:39.106854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from abc import ABC\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/home/data'\n",
    "torch.set_default_dtype(torch.double)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b444e73b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:39.108458Z",
     "iopub.status.busy": "2026-01-14T03:38:39.108297Z",
     "iopub.status.idle": "2026-01-14T03:38:39.117653Z",
     "shell.execute_reply": "2026-01-14T03:38:39.117307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 14)\n",
      "Columns: ['SOLVENT NAME', 'dielectric constant', 'ET(30)', 'alpha', 'beta', 'pi*', 'SA', 'SB', 'SP', 'SdP', 'N', 'n', 'f(n)', 'delta']\n"
     ]
    }
   ],
   "source": [
    "# --- UTILITY FUNCTIONS (ORIGINAL) ---\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "TARGET_LABELS = [\"Product 2\", \"Product 3\", \"SM\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    assert name in [\"full\", \"single_solvent\"]\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[[\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]]\n",
    "    Y = df[TARGET_LABELS]\n",
    "    return X, Y\n",
    "\n",
    "# Load Spange descriptors (correct filename)\n",
    "Spange = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv')\n",
    "print(f\"Spange: {Spange.shape}\")\n",
    "print(f\"Columns: {Spange.columns.tolist()}\")\n",
    "Spange_dict = {row['SOLVENT NAME']: row.drop('SOLVENT NAME').values for _, row in Spange.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c415da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:39.118455Z",
     "iopub.status.busy": "2026-01-14T03:38:39.118361Z",
     "iopub.status.idle": "2026-01-14T03:38:39.121947Z",
     "shell.execute_reply": "2026-01-14T03:38:39.121634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupKFold utility functions defined (5-fold instead of LOO)\n"
     ]
    }
   ],
   "source": [
    "# --- CRITICAL: OVERWRITE UTILITY FUNCTIONS WITH GROUPKFOLD ---\n",
    "# This is what the top kernel (lishellliang) does!\n",
    "# GroupKFold gives more realistic CV estimates than Leave-One-Out\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    \"\"\"GroupKFold (5-fold) instead of Leave-One-Out for single solvent data.\"\"\"\n",
    "    groups = X[\"SOLVENT NAME\"]\n",
    "    n_splits = min(5, len(groups.unique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield ((X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx]))\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    \"\"\"GroupKFold (5-fold) instead of Leave-One-Out for full data.\"\"\"\n",
    "    groups = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n",
    "    n_splits = min(5, len(groups.unique()))\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    for train_idx, test_idx in gkf.split(X, Y, groups):\n",
    "        yield ((X.iloc[train_idx], Y.iloc[train_idx]), (X.iloc[test_idx], Y.iloc[test_idx]))\n",
    "\n",
    "print(\"GroupKFold utility functions defined (5-fold instead of LOO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5258ad07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:39.123120Z",
     "iopub.status.busy": "2026-01-14T03:38:39.122845Z",
     "iopub.status.idle": "2026-01-14T03:38:39.125347Z",
     "shell.execute_reply": "2026-01-14T03:38:39.125019Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- BASE CLASSES ---\n",
    "class SmilesFeaturizer(ABC):\n",
    "    def __init__(self): raise NotImplementedError\n",
    "    def featurize(self, X): raise NotImplementedError\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self): pass\n",
    "    def train_model(self, X_train, y_train): raise NotImplementedError\n",
    "    def predict(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41dbf8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:39.126117Z",
     "iopub.status.busy": "2026-01-14T03:38:39.126027Z",
     "iopub.status.idle": "2026-01-14T03:38:39.129807Z",
     "shell.execute_reply": "2026-01-14T03:38:39.129484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKernelMLP defined (NO Sigmoid output, handles batch_size=1)\n"
     ]
    }
   ],
   "source": [
    "# --- MLP ARCHITECTURE (TOP KERNEL - NO SIGMOID) ---\n",
    "class TopKernelMLP(nn.Module):\n",
    "    \"\"\"MLP with BatchNorm + ReLU + Dropout, LINEAR output (no Sigmoid).\n",
    "    \n",
    "    This matches the top kernel architecture exactly.\n",
    "    Uses eval mode for BatchNorm during inference to handle batch_size=1.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input BatchNorm\n",
    "        layers.append(nn.BatchNorm1d(input_dim))\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        # Output layer - NO SIGMOID (linear output)\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle batch_size=1 by using eval mode for BatchNorm\n",
    "        if x.size(0) == 1 and self.training:\n",
    "            self.eval()\n",
    "            out = self.network(x)\n",
    "            self.train()\n",
    "            return out\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"TopKernelMLP defined (NO Sigmoid output, handles batch_size=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480ad1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:39.130658Z",
     "iopub.status.busy": "2026-01-14T03:38:39.130564Z",
     "iopub.status.idle": "2026-01-14T03:38:39.139644Z",
     "shell.execute_reply": "2026-01-14T03:38:39.139310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKernelEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# --- MLP + GBDT ENSEMBLE MODEL (TOP KERNEL ARCHITECTURE) ---\n",
    "class TopKernelEnsemble(BaseModel):\n",
    "    \"\"\"Ensemble of MLP + XGBoost + RandomForest + LightGBM.\n",
    "    \n",
    "    Matches top kernel (lishellliang) architecture exactly:\n",
    "    - MLP: [128, 64, 32], NO Sigmoid, 100 epochs, lr=1e-3, dropout=0.1\n",
    "    - XGBoost: n_estimators=300, max_depth=6, learning_rate=0.05\n",
    "    - RandomForest: n_estimators=300, max_depth=15\n",
    "    - LightGBM: n_estimators=300, learning_rate=0.05\n",
    "    - Weights: [0.4, 0.2, 0.2, 0.2] for MLP, XGB, RF, LGB\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data='single'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.mlp = None\n",
    "        self.xgb_models = []\n",
    "        self.rf_model = None\n",
    "        self.lgb_models = []\n",
    "        \n",
    "        # Top kernel weights\n",
    "        self.weights = [0.4, 0.2, 0.2, 0.2]  # MLP, XGB, RF, LGB\n",
    "    \n",
    "    def _get_features(self, X):\n",
    "        \"\"\"Extract Spange features only (like top kernel).\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        if self.data == 'single':\n",
    "            for _, row in X.iterrows():\n",
    "                solvent = row['SOLVENT NAME']\n",
    "                spange = Spange_dict.get(solvent, np.zeros(12))\n",
    "                feat = np.concatenate([\n",
    "                    [row['Residence Time'], row['Temperature']],\n",
    "                    spange\n",
    "                ])\n",
    "                features.append(feat)\n",
    "        else:\n",
    "            for _, row in X.iterrows():\n",
    "                solvent_a = row['SOLVENT A NAME']\n",
    "                solvent_b = row['SOLVENT B NAME']\n",
    "                pct_b = row['SolventB%'] / 100.0\n",
    "                \n",
    "                spange_a = Spange_dict.get(solvent_a, np.zeros(12))\n",
    "                spange_b = Spange_dict.get(solvent_b, np.zeros(12))\n",
    "                \n",
    "                # Linear interpolation of Spange features\n",
    "                spange_mix = (1 - pct_b) * spange_a + pct_b * spange_b\n",
    "                \n",
    "                feat = np.concatenate([\n",
    "                    [row['Residence Time'], row['Temperature'], pct_b],\n",
    "                    spange_mix\n",
    "                ])\n",
    "                features.append(feat)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        # Get features\n",
    "        X_feat = self._get_features(X_train)\n",
    "        y_np = y_train.values\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X_feat)\n",
    "        \n",
    "        # 1. Train MLP (100 epochs, lr=1e-3, dropout=0.1)\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        self.mlp = TopKernelMLP(input_dim, hidden_dims=[128, 64, 32], output_dim=3, dropout=0.1).to(device)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "        y_tensor = torch.tensor(y_np, dtype=torch.double).to(device)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.mlp.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        self.mlp.train()\n",
    "        for epoch in range(100):  # 100 epochs like top kernel\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.mlp(batch_X)\n",
    "                loss = criterion(pred, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # 2. Train XGBoost (per target, n_estimators=300)\n",
    "        self.xgb_models = []\n",
    "        for i in range(3):\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbosity=0\n",
    "            )\n",
    "            model.fit(X_scaled, y_np[:, i])\n",
    "            self.xgb_models.append(model)\n",
    "        \n",
    "        # 3. Train RandomForest (n_estimators=300, max_depth=15)\n",
    "        self.rf_model = MultiOutputRegressor(\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=15,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        )\n",
    "        self.rf_model.fit(X_scaled, y_np)\n",
    "        \n",
    "        # 4. Train LightGBM (per target, n_estimators=300)\n",
    "        self.lgb_models = []\n",
    "        for i in range(3):\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=31,\n",
    "                max_depth=-1,\n",
    "                random_state=42,\n",
    "                verbosity=-1\n",
    "            )\n",
    "            model.fit(X_scaled, y_np[:, i])\n",
    "            self.lgb_models.append(model)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_feat = self._get_features(X_test)\n",
    "        X_scaled = self.scaler.transform(X_feat)\n",
    "        \n",
    "        # MLP prediction\n",
    "        self.mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.double).to(device)\n",
    "            mlp_pred = self.mlp(X_tensor).cpu().numpy()\n",
    "        \n",
    "        # XGBoost prediction\n",
    "        xgb_pred = np.column_stack([m.predict(X_scaled) for m in self.xgb_models])\n",
    "        \n",
    "        # RandomForest prediction\n",
    "        rf_pred = self.rf_model.predict(X_scaled)\n",
    "        \n",
    "        # LightGBM prediction\n",
    "        lgb_pred = np.column_stack([m.predict(X_scaled) for m in self.lgb_models])\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        final_pred = (\n",
    "            self.weights[0] * mlp_pred +\n",
    "            self.weights[1] * xgb_pred +\n",
    "            self.weights[2] * rf_pred +\n",
    "            self.weights[3] * lgb_pred\n",
    "        )\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        final_pred = np.clip(final_pred, 0, 1)\n",
    "        \n",
    "        return torch.tensor(final_pred)\n",
    "\n",
    "print(\"TopKernelEnsemble defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d009e0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:37:47.962227Z",
     "iopub.status.busy": "2026-01-14T03:37:47.961753Z",
     "iopub.status.idle": "2026-01-14T03:37:56.118166Z",
     "shell.execute_reply": "2026-01-14T03:37:56.117743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TopKernelEnsemble with GroupKFold...\n",
      "\n",
      "Fold 0: Train=531, Test=125\n",
      "  Test solvents: ['IPA [Propan-2-ol]' 'Acetonitrile' 'Diethyl Ether [Ether]']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE = 0.0667\n",
      "\n",
      "Fold 1: Train=526, Test=130\n",
      "  Test solvents: ['2-Methyltetrahydrofuran [2-MeTHF]' 'Cyclohexane' 'Decanol']...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE = 0.0638\n",
      "\n",
      "Quick test MAE (2 folds): 0.0653\n",
      "Note: GroupKFold gives ~20% test data per fold (vs 4% for LOO)\n"
     ]
    }
   ],
   "source": [
    "# --- QUICK VALIDATION TEST ---\n",
    "print(\"Testing TopKernelEnsemble with GroupKFold...\")\n",
    "X_test, Y_test = load_data(\"single_solvent\")\n",
    "\n",
    "# Quick test on first 2 folds\n",
    "errors = []\n",
    "split_gen = generate_leave_one_out_splits(X_test, Y_test)\n",
    "for i, ((train_X, train_Y), (test_X, test_Y)) in enumerate(split_gen):\n",
    "    if i >= 2: break\n",
    "    print(f\"\\nFold {i}: Train={len(train_X)}, Test={len(test_X)}\")\n",
    "    print(f\"  Test solvents: {test_X['SOLVENT NAME'].unique()[:3]}...\")\n",
    "    model = TopKernelEnsemble(data='single')\n",
    "    model.train_model(train_X, train_Y)\n",
    "    preds = model.predict(test_X).numpy()\n",
    "    mae = np.mean(np.abs(preds - test_Y.values))\n",
    "    errors.append(mae)\n",
    "    print(f\"  MAE = {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nQuick test MAE (2 folds): {np.mean(errors):.4f}\")\n",
    "print(f\"Note: GroupKFold gives ~20% test data per fold (vs 4% for LOO)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2959bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:38:41.431356Z",
     "iopub.status.busy": "2026-01-14T03:38:41.430903Z",
     "iopub.status.idle": "2026-01-14T03:39:00.415100Z",
     "shell.execute_reply": "2026-01-14T03:39:00.414651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:04,  4.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:07,  3.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:11,  3.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:15,  3.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:18,  3.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:18,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: MAE = 0.0668\n",
      "Fold 1: MAE = 0.0612\n",
      "Fold 2: MAE = 0.0552\n",
      "Fold 3: MAE = 0.0970\n",
      "Fold 4: MAE = 0.0864\n",
      "\n",
      "Single Solvent CV MAE: 0.073326 +/- 0.015822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = TopKernelEnsemble(data='single') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions.numpy(), columns=[\"target_1\", \"target_2\", \"target_3\"])\n",
    "    predictions_df[\"fold\"] = fold_idx\n",
    "    predictions_df[\"task\"] = 0\n",
    "    all_predictions.append(predictions_df)\n",
    "\n",
    "submission_single_solvent = pd.concat(all_predictions)\n",
    "submission_single_solvent = submission_single_solvent.reset_index(drop=True)\n",
    "\n",
    "# Calculate CV MAE\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "single_errors = []\n",
    "for fold_idx, ((_, _), (test_X, test_Y)) in enumerate(generate_leave_one_out_splits(X_single, Y_single)):\n",
    "    fold_preds = submission_single_solvent[submission_single_solvent['fold'] == fold_idx]\n",
    "    pred_vals = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    mae = np.mean(np.abs(pred_vals - test_Y.values))\n",
    "    single_errors.append(mae)\n",
    "    print(f\"Fold {fold_idx}: MAE = {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nSingle Solvent CV MAE: {np.mean(single_errors):.6f} +/- {np.std(single_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35265f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:39:00.416225Z",
     "iopub.status.busy": "2026-01-14T03:39:00.416097Z",
     "iopub.status.idle": "2026-01-14T03:39:30.483467Z",
     "shell.execute_reply": "2026-01-14T03:39:30.482903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:06,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:12,  6.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:18,  6.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:24,  5.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:30,  6.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:30,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: MAE = 0.0631\n",
      "Fold 1: MAE = 0.0882\n",
      "Fold 2: MAE = 0.0901\n",
      "Fold 3: MAE = 0.1151\n",
      "Fold 4: MAE = 0.0931\n",
      "\n",
      "Full Data CV MAE: 0.089923 +/- 0.016541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = TopKernelEnsemble(data = 'full') # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "    predictions = model.predict(test_X)\n",
    "\n",
    "    predictions_df = pd.DataFrame(predictions.numpy(), columns=[\"target_1\", \"target_2\", \"target_3\"])\n",
    "    predictions_df[\"fold\"] = fold_idx\n",
    "    predictions_df[\"task\"] = 1\n",
    "    all_predictions.append(predictions_df)\n",
    "\n",
    "submission_full_data = pd.concat(all_predictions)\n",
    "submission_full_data = submission_full_data.reset_index(drop=True)\n",
    "\n",
    "# Calculate CV MAE\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "full_errors = []\n",
    "for fold_idx, ((_, _), (test_X, test_Y)) in enumerate(generate_leave_one_ramp_out_splits(X_full, Y_full)):\n",
    "    fold_preds = submission_full_data[submission_full_data['fold'] == fold_idx]\n",
    "    pred_vals = fold_preds[['target_1', 'target_2', 'target_3']].values\n",
    "    mae = np.mean(np.abs(pred_vals - test_Y.values))\n",
    "    full_errors.append(mae)\n",
    "    print(f\"Fold {fold_idx}: MAE = {mae:.4f}\")\n",
    "\n",
    "print(f\"\\nFull Data CV MAE: {np.mean(full_errors):.6f} +/- {np.std(full_errors):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19dede90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T03:39:30.484630Z",
     "iopub.status.busy": "2026-01-14T03:39:30.484521Z",
     "iopub.status.idle": "2026-01-14T03:39:30.494542Z",
     "shell.execute_reply": "2026-01-14T03:39:30.494182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS (GroupKFold 5-fold) ===\n",
      "Single Solvent CV MAE: 0.073326 +/- 0.015822\n",
      "Full Data CV MAE: 0.089923 +/- 0.016541\n",
      "Combined CV MAE: 0.084141\n",
      "\n",
      "=== COMPARISON ===\n",
      "Best LOO CV (exp_004): 0.0623\n",
      "Best LB (exp_004): 0.0956\n",
      "This experiment (GroupKFold): 0.084141\n",
      "\n",
      "Note: GroupKFold CV should be MORE REALISTIC (closer to LB)\n",
      "Expected CV-LB gap: ~10-20% (vs 50% for LOO)\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "# Final summary\n",
    "total_single = len(submission_single_solvent)\n",
    "total_full = len(submission_full_data)\n",
    "total = total_single + total_full\n",
    "combined_cv = (np.mean(single_errors) * total_single + np.mean(full_errors) * total_full) / total\n",
    "\n",
    "print(f\"\\n=== FINAL RESULTS (GroupKFold 5-fold) ===\")\n",
    "print(f\"Single Solvent CV MAE: {np.mean(single_errors):.6f} +/- {np.std(single_errors):.6f}\")\n",
    "print(f\"Full Data CV MAE: {np.mean(full_errors):.6f} +/- {np.std(full_errors):.6f}\")\n",
    "print(f\"Combined CV MAE: {combined_cv:.6f}\")\n",
    "print(f\"\\n=== COMPARISON ===\")\n",
    "print(f\"Best LOO CV (exp_004): 0.0623\")\n",
    "print(f\"Best LB (exp_004): 0.0956\")\n",
    "print(f\"This experiment (GroupKFold): {combined_cv:.6f}\")\n",
    "print(f\"\\nNote: GroupKFold CV should be MORE REALISTIC (closer to LB)\")\n",
    "print(f\"Expected CV-LB gap: ~10-20% (vs 50% for LOO)\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
