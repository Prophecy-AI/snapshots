{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:03.073449Z","iopub.execute_input":"2025-10-08T09:47:03.073896Z","iopub.status.idle":"2025-10-08T09:47:05.569575Z","shell.execute_reply.started":"2025-10-08T09:47:03.073854Z","shell.execute_reply":"2025-10-08T09:47:05.568468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES, load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:05.570876Z","iopub.execute_input":"2025-10-08T09:47:05.57127Z","iopub.status.idle":"2025-10-08T09:47:05.582531Z","shell.execute_reply.started":"2025-10-08T09:47:05.571247Z","shell.execute_reply":"2025-10-08T09:47:05.581246Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the cells below we create the base classes of the two main objects you must write for the competition. \n\nThe first thing to write is a SmilesFeaturizer, which will take the solvent molecules and create a machine-learning ready featurization of the molecule. Finding better ways of featurizing solvents is one of the goals of the hackathon, however, you can also skip this step and use the pre-computed featurizations given in the utils file. Further down, you can see a SmilesFeaturizer that loads all the precomputed representations. A **featurizer** object simply consists of:\n- An initialization function\n- A featurize function that takes \n\nThe second one being a **model** which has:\n- An initialization function, where the model internally defines which featurizer to use\n- A \"train_model\" which lets the model train on data given by X_train, y_train as pandas data-frames. \n- A \"predict\" which takes a data frame of test inputs and makes a prediction","metadata":{}},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        raise NotImplementedError\n\n    def featurize(X, Y):\n        raise NotImplementedError\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n\n    def train_model(self, X_train, y_train):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:05.583701Z","iopub.execute_input":"2025-10-08T09:47:05.584103Z","iopub.status.idle":"2025-10-08T09:47:05.607211Z","shell.execute_reply.started":"2025-10-08T09:47:05.584068Z","shell.execute_reply":"2025-10-08T09:47:05.605678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the next cell we define two featurizers, which allow you to use the pre-computed featurizations from the original benchmark paper. These are:\n\n- drfps\n- fragprints\n- acs_pca_descriptors\n- spange_descriptors\n\nYou can refer to the paper for more details on them. We also include the simple SMILES string featurization which can be chained into more complicated representations.\n\nThe first featurizer simply uses the features directly. The second one is expanded to featurize *mixed* solvents too, which is done by taking a weighted average of the two single-solvent features.\n\nWe also show how to write code for a simple multi-layer perceptron on the data.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntorch.set_default_dtype(torch.double) \n\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    def __init__(self, features = 'spange_descriptors'):\n        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n        self.features = features\n\n        #load the featurizer\n        self.featurizer = load_features(self.features)\n\n        #load the dimension, which depends on the featurizer + the 2 numeric dimensions\n        self.feats_dim = self.featurizer.shape[1] + 2\n        \n\n    def featurize(self, X):\n        # obtain the numeric inputs i.e. residence time and temperature\n        X_numeric = X[INPUT_LABELS_NUMERIC]\n        # obtain the relevant molecule information\n        X_smiles = X[INPUT_LABELS_SINGLE_FEATURES]\n        \n        X_smiles_feat = self.featurizer.loc[X[\"SOLVENT NAME\"]]\n\n        X_numeric_tensor = torch.tensor(X_numeric.values)\n        X_smiles_feat_tensor = torch.tensor(X_smiles_feat.values)\n\n        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n\n        return X_out\n        \n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    def __init__(self, features = 'spange_descriptors'):\n        assert features in ['drfps_catechol', 'fragprints', 'smiles', 'acs_pca_descriptors', 'spange_descriptors']\n        self.features = features\n\n        #load the featurizer\n        self.featurizer = load_features(self.features)\n\n        #load the dimension, which depends on the featurizer + the 2 numeric dimensions\n        self.feats_dim = self.featurizer.shape[1] + 2\n\n    def featurize(self, X):\n        # obtain the numeric inputs i.e. residence time and temperature\n        X_numeric = X[INPUT_LABELS_NUMERIC]\n        # obtain the relevant molecule information\n        X_smiles = X[INPUT_LABELS_FULL_FEATURES]\n\n        X_smiles_A_feat = self.featurizer.loc[X[\"SOLVENT A NAME\"]]\n        X_smiles_B_feat = self.featurizer.loc[X[\"SOLVENT B NAME\"]]\n\n        X_numeric_tensor = torch.tensor(X_numeric.values)\n        # take the weighted average accoriding to the SolventB% in the mixture\n        X_smiles_feat_tensor = X_smiles_A_feat.values * (1 - X[\"SolventB%\"].values.reshape(-1, 1)) + X_smiles_B_feat.values * X[\"SolventB%\"].values.reshape(-1, 1)\n        X_smiles_feat_tensor = torch.tensor(X_smiles_feat_tensor)\n\n        X_out = torch.cat((X_numeric_tensor, X_smiles_feat_tensor), dim=1)\n\n        return X_out\n        \n\nclass MLPModel(nn.Module, BaseModel):\n    def __init__(self, features = 'spange_descriptors', hidden_dims = [64, 64], output_dim = 3, dropout=0.0, data = 'single'):\n        super(MLPModel, self).__init__()\n        layers = []\n\n        if data == 'single':\n            self.smiles_featurizer = PrecomputedFeaturizer(features = features)\n        else:\n            self.smiles_featurizer = PrecomputedFeaturizerMixed(features = features)\n        \n        input_dim = self.smiles_featurizer.feats_dim\n        prev_dim = input_dim\n\n        for h_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, h_dim))\n            layers.append(nn.ReLU())\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            prev_dim = h_dim\n\n        layers.append(nn.Linear(prev_dim, output_dim))\n        self.model = nn.Sequential(*layers)\n\n    def train_model(\n        self,\n        train_X,\n        train_Y,\n        criterion=nn.MSELoss,\n        optimizer=torch.optim.Adam,\n        num_epochs=100,\n        batch_size=1048,\n        device=\"cpu\",\n        verbose=True,\n        lr = 1e-3,\n    ):\n        \"\"\"\n        Trains the MLP model.\n        Args:\n            train_X: pandas array of training inputs\n            train_Y: pandas array of training outputs\n            criterion: Loss function.\n            optimizer: Optimizer.\n            num_epochs (int): Number of epochs.\n            batch_size (int): Batch size.\n            device (torch.device): Device to train on.\n            verbose (bool): Whether to print progress.\n            lr (float): learning rate to use\n        \"\"\"\n        self.train()\n        \n        train_X_tensor = self.smiles_featurizer.featurize(train_X)\n        train_Y_tensor = torch.tensor(train_Y.values)\n        \n        train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = False)\n        \n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to(device)\n\n        criterion = criterion()\n        optimizer = optimizer(self.parameters(), lr = lr)\n\n        for epoch in range(1, num_epochs + 1):\n            self.train()\n            running_loss = 0.0\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                optimizer.zero_grad()\n                outputs = self(inputs)\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n\n            epoch_loss = running_loss / len(train_loader.dataset)\n\n            if (verbose) and (epoch % 10 == 1):\n                msg = f\"Epoch {epoch}/{num_epochs} | Train Loss: {epoch_loss:.4f}\"\n\n                # if val_loader is not None:\n                #     val_loss = self.evaluate(val_loader, criterion, device)\n                #     msg += f\" | Val Loss: {val_loss:.4f}\"\n\n                # print(msg)\n\n    def predict(self, X):\n        X = self.smiles_featurizer.featurize(X)\n        return self.forward(X)\n\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:05.609376Z","iopub.execute_input":"2025-10-08T09:47:05.609714Z","iopub.status.idle":"2025-10-08T09:47:10.997383Z","shell.execute_reply.started":"2025-10-08T09:47:05.609679Z","shell.execute_reply":"2025-10-08T09:47:10.996258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From this point onward the cross-validation procedure is calculated. **For a submission to be valid the next three cells must be the final three of your submission, and you can only modify the lines where the models are defined.**","metadata":{}},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nimport tqdm\n\nX, Y = load_data(\"single_solvent\")\n\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = MLPModel() # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_single_solvent = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:10.998483Z","iopub.execute_input":"2025-10-08T09:47:10.999148Z","iopub.status.idle":"2025-10-08T09:47:39.598673Z","shell.execute_reply.started":"2025-10-08T09:47:10.999109Z","shell.execute_reply":"2025-10-08T09:47:39.597575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nX, Y = load_data(\"full\")\n\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = MLPModel(data = 'full') # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_full_data = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:47:39.600167Z","iopub.execute_input":"2025-10-08T09:47:39.600511Z","iopub.status.idle":"2025-10-08T09:48:01.670153Z","shell.execute_reply.started":"2025-10-08T09:47:39.600486Z","shell.execute_reply":"2025-10-08T09:48:01.669172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T09:48:01.671066Z","iopub.execute_input":"2025-10-08T09:48:01.671412Z","iopub.status.idle":"2025-10-08T09:48:01.700618Z","shell.execute_reply.started":"2025-10-08T09:48:01.671381Z","shell.execute_reply":"2025-10-08T09:48:01.699646Z"}},"outputs":[],"execution_count":null}]}