{
  "query": "How to implement Gaussian Process regression with Tanimoto kernel for molecular fingerprints in scikit-learn?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Implementing Gaussian\u2011Process regression with a Tanimoto kernel for molecular fingerprints in scikit\u2011learn** can be done in a few straightforward steps. The approach combines scikit\u2011learn\u2019s `GaussianProcessRegressor` ([scikit\u2011learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)) with the Tanimoto\u2011compatible kernel provided by the\u202f`sklearn_tanimoto_gp` package ([GitHub\u202fAustinT](https://github.com/austint/sklearn_tanimoto_gp)) and a fingerprint generator such as\u202fscikit\u2011fingerprints ([GitHub\u202fscikit\u2011fingerprints](https://github.com/scikit-fingerprints/scikit-fingerprints)).\n\n1. **Install the required libraries**  \n   ```bash\n   pip install scikit-learn scikit-fingerprints\n   pip install git+https://github.com/AustinT/sklearn_tanimoto_gp.git\n   ```  \n   (The Tanimoto kernel package is installed directly from its repository)\u202f([source\u202f2]).\n\n2. **Generate non\u2011negative molecular fingerprints** (e.g., Morgan/ECFP bits)  \n   ```python\n   from skfp import MorganFingerprint   # scikit\u2011fingerprints\n   fp = MorganFingerprint(radius=2, nBits=1024)\n   X_train = fp.transform(smiles_train)   # shape (n_samples, n_features)\n   X_test  = fp.transform(smiles_test)\n   ```  \n   (scikit\u2011fingerprints produces scikit\u2011learn\u2011compatible arrays)\u202f([source\u202f3]).\n\n3. **Import the Tanimoto kernel**  \n   ```python\n   from sklearn_tanimoto_gp import Tanimoto   # or MinMaxTanimoto\n   ```\n\n4. **Create the Gaussian\u2011Process regressor**  \n   ```python\n   from sklearn.gaussian_process import GaussianProcessRegressor\n   gp = GaussianProcessRegressor(\n           kernel=Tanimoto(),      # Tanimoto similarity kernel\n           alpha=1e-5,             # small nugget for numerical stability\n           normalize_y=True)      # optional, scales targets\n   ```  \n   (`GaussianProcessRegressor` accepts any scikit\u2011learn\u2011compatible kernel)\u202f([source\u202f1]\u202f&\u202f[source\u202f2]).\n\n5. **Fit the model**  \n   ```python\n   gp.fit(X_train, y_train)\n   ```\n\n6. **Predict on new molecules (and obtain uncertainties if desired)**  \n   ```python\n   y_pred, sigma = gp.predict(X_test, return_std=True)\n   ```  \n   (The `predict` method returns mean predictions and standard deviations)\u202f([source\u202f2]).\n\n7. **(Optional) Tune hyper\u2011parameters** \u2013 you can enable optimizer restarts (`n_restarts_optimizer`) or adjust `alpha`/`normalize_y` as described in the `GaussianProcessRegressor` documentation\u202f([source\u202f1]).\n\nFollowing these steps gives you a fully functional Gaussian\u2011Process regression model that uses the Tanimoto similarity of molecular fingerprints as its covariance function, all within the standard scikit\u2011learn API.",
      "url": ""
    },
    {
      "title": "GaussianProcessRegressor #",
      "text": "GaussianProcessRegressor &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../../_static/scikit-learn-logo-without-subtitle.svg)](../../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# GaussianProcessRegressor[#](#gaussianprocessregressor)\n*class*sklearn.gaussian\\_process.GaussianProcessRegressor(*kernel=None*,*\\**,*alpha=1e-10*,*optimizer='fmin\\_l\\_bfgs\\_b'*,*n\\_restarts\\_optimizer=0*,*normalize\\_y=False*,*copy\\_X\\_train=True*,*n\\_targets=None*,*random\\_state=None*)[[source]](https://github.com/scikit-learn/scikit-learn/blob/d3898d9d5/sklearn/gaussian_process/_gpr.py#L32)[#](#sklearn.gaussian_process.GaussianProcessRegressor)\nGaussian process regression (GPR).\nThe implementation is based on Algorithm 2.1 of[[RW2006]](#rf75674b0f418-rw2006).\nIn addition to standard scikit-learn estimator API,[`GaussianProcessRegressor`](#sklearn.gaussian_process.GaussianProcessRegressor):\n* allows prediction without prior fitting (based on the GP prior)\n* provides an additional method`sample\\_y(X)`, which evaluates samples\ndrawn from the GPR (prior or posterior) at given inputs\n* exposes a method`log\\_marginal\\_likelihood(theta)`, which can be used\nexternally for other ways of selecting hyperparameters, e.g., via\nMarkov chain Monte Carlo.\nTo learn the difference between a point-estimate approach vs. a more\nBayesian modelling approach, refer to the example entitled[Comparison of kernel ridge and Gaussian process regression](../../auto_examples/gaussian_process/plot_compare_gpr_krr.html#sphx-glr-auto-examples-gaussian-process-plot-compare-gpr-krr-py).\nRead more in the[User Guide](../gaussian_process.html#gaussian-process).\nAdded in version 0.18.\nParameters:**kernel**kernel instance, default=None\nThe kernel specifying the covariance function of the GP. If None is\npassed, the kernel`ConstantKernel(1.0,constant\\_value\\_bounds=&quot;&quot;fixed&quot;&quot;)\\*RBF(1.0,length\\_scale\\_bounds=&quot;&quot;fixed&quot;&quot;)`is used as default. Note that\nthe kernel hyperparameters are optimized during fitting unless the\nbounds are marked as \u201cfixed\u201d.\n**alpha**float or ndarray of shape (n\\_samples,), default=1e-10\nValue added to the diagonal of the kernel matrix during fitting.\nThis can prevent a potential numerical issue during fitting, by\nensuring that the calculated values form a positive definite matrix.\nIt can also be interpreted as the variance of additional Gaussian\nmeasurement noise on the training observations. Note that this is\ndifferent from using a`WhiteKernel`. If an array is passed, it must\nhave the same number of entries as the data used for fitting and is\nused as datapoint-dependent noise level. Allowing to specify the\nnoise level directly as a parameter is mainly for convenience and\nfor consistency with[`Ridge`](sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge).\nFor an example illustrating how the alpha parameter controls\nthe noise variance in Gaussian Process Regression, see[Gaussian Processes regression: basic introductory example](../../auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py).\n**optimizer**\u201cfmin\\_l\\_bfgs\\_b\u201d, callable or None, default=\u201dfmin\\_l\\_bfgs\\_b\u201d\nCan either be one of the internally supported optimizers for optimizing\nthe kernel\u2019s parameters, specified by a string, or an externally\ndefined optimizer passed as a callable. If a callable is passed, it\nmust have the signature:\n```\ndefoptimizer(obj\\_func,initial\\_theta,bounds):# \\* &#39;&#39;obj\\_func&#39;&#39;: the objective function to be minimized, which# takes the hyperparameters theta as a parameter and an# optional flag eval\\_gradient, which determines if the# gradient is returned additionally to the function value# \\* &#39;&#39;initial\\_theta&#39;&#39;: the initial value for theta, which can be# used by local optimizers# \\* &#39;&#39;bounds&#39;&#39;: the bounds on the values of theta....# Returned are the best found hyperparameters theta and# the corresponding value of the target function.returntheta\\_opt,func\\_min\n```\nPer default, the L-BFGS-B algorithm from`scipy.optimize.minimize`is used. If None is passed, the kernel\u2019s parameters are kept fixed.\nAvailable internal optimizers are:`{'fmin\\_l\\_bfgs\\_b'}`.\n**n\\_restarts\\_optimizer**int, default=0\nThe number of restarts of the optimizer for finding the kernel\u2019s\nparameters which maximize the log-marginal likelihood. The first run\nof the optimizer is performed from the kernel\u2019s initial parameters,\nthe remaining ones (if any) from thetas sampled log-uniform randomly\nfrom the space of allowed theta-values. If greater than 0, all bounds\nmust be finite. Note that`n\\_restarts\\_optimizer==0`implies that one\nrun is performed.\n**normalize\\_y**bool, default=False\nWhether or not to normalize the target values`y`by removing the mean\nand scaling to unit-variance. This is recommended for cases where\nzero-mean, unit-variance priors are used. Note that, in this\nimplementation, the normalisation is reversed before the GP predictions\nare reported.\nChanged in version 0.23.\n**copy\\_X\\_train**bool, default=True\nIf True, a persistent copy of the training data is stored in the\nobject. Otherwise, just a reference to the training data is stored,\nwhich might cause predictions to change if the data is modified\nexternally.\n**n\\_targets**int, default=None\nThe number of dimensions of the target values. Used to decide the number\nof outputs when sampling from the prior distributions (i.e. calling[`sample\\_y`](#sklearn.gaussian_process.GaussianProcessRegressor.sample_y)before[`fit`](#sklearn.gaussian_process.GaussianProcessRegressor.fit)). This parameter is ignored once[`fit`](#sklearn.gaussian_process.GaussianProcessRegressor.fit)has been called.\nAdded in version 1.3.\n**random\\_state**int, RandomState instance or None, default=None\nDetermines random number generation used to initialize the centers.\nPass an int for reproducible results across multiple function calls.\nSee[Glossary](../../glossary.html#term-random_state).\nAttributes:**X\\_train\\_**array-like of shape (n\\_samples, n\\_features) or list of object\nFeature vectors or other representations of training data (also\nrequired for prediction).\n**y\\_train\\_**array-like of shape (n\\_samples,) or (n\\_samples, n\\_targets)\nTarget values in training data (also required for prediction).\n**kernel\\_**kernel instance\nThe kernel used for prediction. The structure of the kernel is the\nsame as the one passed as parameter but with optimized hyperparameters.\n**L\\_**array-like of shape (n\\_samples, n\\_samples)\nLower-triangular Cholesky decomposition of the kernel in`X\\_train\\_`.\n**alpha\\_**array-like of shape (n\\_samples,)\nDual coefficients of training data points in kernel space.\n**log\\_marginal\\_likelihood\\_value\\_**float\nThe log-marginal-likelihood of`self.kernel\\_.theta`.\n**n\\_features\\_in\\_**int\nNumber of features seen during[fit](../../glossary.html#term-fit).\nAdded in version 0.24.\n**feature\\_names\\_in\\_**ndarray of shape (`n\\_features\\_in\\_`,)\nNames of features seen during[fit](../../glossary.html#term-fit). Defined only when`X`has feature names that are all strings.\nAdded in version 1.0.\nSee also\n[`GaussianProcessClassifier`](sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)\nGaussian process classification (GPC) based on Laplace approximation.\nReferences\n[[RW2006](#id1)]\n[Carl E. Rasmussen and Christopher K.I. Williams,\n\u201cGaussian Processes for Machine Learning\u201d,\nMIT Press 2006](https://www.gaussianprocess.org/gpml/chapters/RW.pdf)\nExamples\n```\n&gt;&gt;&gt;fromsklearn.datasetsimportmake\\_friedman2&gt;&gt;&gt;fromsklearn.gaussian\\_processimportGaussianProcessRegressor&gt;&gt;&gt;fromsklearn.gaussian\\_process....",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html"
    },
    {
      "title": "GitHub - AustinT/sklearn_tanimoto_gp: Tanimoto kernels for scikit-learn",
      "text": "<div><div><article><p></p><h2>Sklearn Tanimoto GP Kernels</h2><a href=\"#sklearn-tanimoto-gp-kernels\"></a><p></p>\n<p>This package provides Tanimoto similarity kernels compatible with scikit-learn's Gaussian Process module (<code>sklearn.gaussian_process.kernels.Kernel</code>).</p>\n<p>Intended usage:</p>\n<div><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>\n<span>from</span> <span>sklearn</span>.<span>gaussian_process</span> <span>import</span> <span>GaussianProcessRegressor</span>\n<span>from</span> <span>sklearn_tanimoto_gp</span> <span>import</span> <span>Tanimoto</span>\n<span># Example Data</span>\n<span>X_train</span> <span>=</span> <span>np</span>.<span>random</span>.<span>rand</span>(<span>10</span>, <span>5</span>) <span>*</span> <span>10</span> <span># Example non-negative data</span>\n<span>y_train</span> <span>=</span> <span>np</span>.<span>sum</span>(<span>X_train</span>[:, :<span>2</span>], <span>axis</span><span>=</span><span>1</span>) <span>+</span> <span>np</span>.<span>random</span>.<span>randn</span>(<span>10</span>) <span>*</span> <span>0.1</span>\n<span>X_test</span> <span>=</span> <span>np</span>.<span>random</span>.<span>rand</span>(<span>5</span>, <span>5</span>) <span>*</span> <span>10</span>\n<span># Using MinMaxTanimoto (or Tanimoto)</span>\n<span>gp</span> <span>=</span> <span>GaussianProcessRegressor</span>(<span>kernel</span><span>=</span><span>Tanimoto</span>(), <span>alpha</span><span>=</span><span>1e-5</span>, <span>normalize_y</span><span>=</span><span>True</span>)\n<span>gp</span>.<span>fit</span>(<span>X_train</span>, <span>y_train</span>)\n<span>y_pred</span>, <span>sigma</span> <span>=</span> <span>gp</span>.<span>predict</span>(<span>X_test</span>, <span>return_std</span><span>=</span><span>True</span>)</pre></div>\n<p></p><h2>Installation</h2><a href=\"#installation\"></a><p></p>\n<div><pre>pip install git+https://github.com/AustinT/sklearn_tanimoto_gp.git</pre></div>\n<p></p><h2>Development</h2><a href=\"#development\"></a><p></p>\n<p>PRs are welcome. Please use pre-commit and run tests.</p>\n<p>NOTE: LLMs were used in the initial development of this package.\nI have checked the outputs but cannot guarantee that there are no\nmistakes.</p>\n</article></div></div>",
      "url": "https://github.com/austint/sklearn_tanimoto_gp"
    },
    {
      "title": "GitHub - scikit-fingerprints/scikit-fingerprints: Scikit-learn compatible library for molecular fingerprints",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[scikit-fingerprints](https://github.com/scikit-fingerprints)/ **[scikit-fingerprints](https://github.com/scikit-fingerprints/scikit-fingerprints)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fscikit-fingerprints%2Fscikit-fingerprints) You must be signed in to change notification settings\n- [Fork\\\n19](https://github.com/login?return_to=%2Fscikit-fingerprints%2Fscikit-fingerprints)\n- [Star\\\n261](https://github.com/login?return_to=%2Fscikit-fingerprints%2Fscikit-fingerprints)\n\n\nScikit-learn compatible library for molecular fingerprints and chemoinformatics\n\n[scikit-fingerprints.readthedocs.io/latest/](https://scikit-fingerprints.readthedocs.io/latest/)\n\n### License\n\n[MIT license](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/LICENSE.md)\n\n[261\\\nstars](https://github.com/scikit-fingerprints/scikit-fingerprints/stargazers) [19\\\nforks](https://github.com/scikit-fingerprints/scikit-fingerprints/forks) [Branches](https://github.com/scikit-fingerprints/scikit-fingerprints/branches) [Tags](https://github.com/scikit-fingerprints/scikit-fingerprints/tags) [Activity](https://github.com/scikit-fingerprints/scikit-fingerprints/activity)\n\n[Star](https://github.com/login?return_to=%2Fscikit-fingerprints%2Fscikit-fingerprints)\n\n[Notifications](https://github.com/login?return_to=%2Fscikit-fingerprints%2Fscikit-fingerprints) You must be signed in to change notification settings\n\n# scikit-fingerprints/scikit-fingerprints\n\nmaster\n\n[Branches](https://github.com/scikit-fingerprints/scikit-fingerprints/branches) [Tags](https://github.com/scikit-fingerprints/scikit-fingerprints/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[570 Commits](https://github.com/scikit-fingerprints/scikit-fingerprints/commits/master/) |\n| [.github](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/.github) | [.github](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/.github) |\n| [benchmarking](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/benchmarking) | [benchmarking](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/benchmarking) |\n| [docs](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/docs) | [docs](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/docs) |\n| [examples](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/examples) | [examples](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/examples) |\n| [skfp](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/skfp) | [skfp](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/skfp) |\n| [tests](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/tests) | [tests](https://github.com/scikit-fingerprints/scikit-fingerprints/tree/master/tests) |\n| [.coveragerc](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.coveragerc) | [.coveragerc](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.coveragerc) |\n| [.gitignore](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.gitignore) | [.gitignore](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.gitignore) |\n| [.pre-commit-config.yaml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.pre-commit-config.yaml) | [.pre-commit-config.yaml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.pre-commit-config.yaml) |\n| [.readthedocs.yaml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.readthedocs.yaml) | [.readthedocs.yaml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/.readthedocs.yaml) |\n| [CODE\\_OF\\_CONDUCT.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/CODE_OF_CONDUCT.md) | [CODE\\_OF\\_CONDUCT.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/CODE_OF_CONDUCT.md) |\n| [CONTRIBUTING.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/CONTRIBUTING.md) | [CONTRIBUTING.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/CONTRIBUTING.md) |\n| [LICENSE.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/LICENSE.md) | [LICENSE.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/LICENSE.md) |\n| [Makefile](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/Makefile) | [Makefile](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/Makefile) |\n| [README.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/README.md) | [README.md](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/README.md) |\n| [mypy.ini](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/mypy.ini) | [mypy.ini](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/mypy.ini) |\n| [pyproject.toml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/pyproject.toml) | [pyproject.toml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/pyproject.toml) |\n| [ruff.toml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/ruff.toml) | [ruff.toml](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/ruff.toml) |\n| [uv.lock](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/uv.lock) | [uv.lock](https://github.com/scikit-fingerprints/scikit-fingerprints/blob/master/uv.lock) |\n| View all files |\n\n## Repository files navigation\n\n# scikit-fingerprints\n\n[scikit-fingerprints](https://scikit-fingerprints.readthedocs.io/latest/) is a Python library for efficient\ncomputation of molecular fingerprints.\n\n## Table of Contents\n\n- [Description](https://github.com/github.com#description)\n- [Supported platforms](https://github.com/github.com#supported-platforms)\n- [Installation](https://github.com/github.com#installation)\n- [Quickstart](https://github.com/github.com#quickstart)\n- [Project overview](https://github.com/github.com#project-overview)\n- [Contributing](https://github.com/github.com#contributing)\n- [License](https://github.com/github.com#license)\n\n## Description\n\nMolecular fingerprints are crucial in various scientific fields, including drug discovery, materials science, and\nchemical analysis. However, existing Python libraries for computing molecular fingerprints often lack performance,\nuser-friendliness, and support for modern programming standards. This project aims to address these shortcomings by\ncreating an efficient and accessible Python library for molecular fingerprint computation.\n\nSee [the documentation and API reference](https://scikit-fingerprints.readthedocs.io/latest/) for details.\n\nMain features:\n\n- scikit-learn compatible\n- feature-rich, with >30 fingerprints\n- parallelization\n- sparse matrix support\n- commercial-friendly MIT license\n\n## Supported platforms\n\n| `python3.10` | `python3.11` | `python3.12` | `python3.13` |\n| --- | --- | --- | --- |\n| **Linux** | \u2705 | \u2705 | \u2705 | \u2705 |\n| **Windows** | \u2705 | \u2705 | \u2705 | \u2705 |\n| **macOS** | \u2705 | \u2705 | \u2705 | \u2705 |\n\nPython 3.9 was supported up to scikit-fingerprints 1.13.0.\n\n## Installation\n\nYou can install the library using pip:\n\n```\npip install scikit-fingerprints\n```\n\nIf you need bleeding-edge features and don't mind potentially unstable or undocumented functionalities,\nyou can also install directly from GitHub:\n\n```\npip install git+https://github.com/scikit-fingerprints/scikit-fingerprints...",
      "url": "https://github.com/scikit-fingerprints/scikit-fingerprints"
    },
    {
      "title": "GitHub - Ryan-Rhys/FlowMO: Library for training Gaussian Processes on Molecules",
      "text": "[Skip to content](https://github.com/Ryan-Rhys/FlowMO#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/Ryan-Rhys/FlowMO) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/Ryan-Rhys/FlowMO) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/Ryan-Rhys/FlowMO) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[Ryan-Rhys](https://github.com/Ryan-Rhys)/ **[FlowMO](https://github.com/Ryan-Rhys/FlowMO)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FRyan-Rhys%2FFlowMO) You must be signed in to change notification settings\n- [Fork\\\n15](https://github.com/login?return_to=%2FRyan-Rhys%2FFlowMO)\n- [Star\\\n36](https://github.com/login?return_to=%2FRyan-Rhys%2FFlowMO)\n\n\nLibrary for training Gaussian Processes on Molecules\n\n### License\n\n[MIT license](https://github.com/Ryan-Rhys/FlowMO/blob/master/LICENSE)\n\n[36\\\nstars](https://github.com/Ryan-Rhys/FlowMO/stargazers) [15\\\nforks](https://github.com/Ryan-Rhys/FlowMO/forks) [Branches](https://github.com/Ryan-Rhys/FlowMO/branches) [Tags](https://github.com/Ryan-Rhys/FlowMO/tags) [Activity](https://github.com/Ryan-Rhys/FlowMO/activity)\n\n[Star](https://github.com/login?return_to=%2FRyan-Rhys%2FFlowMO)\n\n[Notifications](https://github.com/login?return_to=%2FRyan-Rhys%2FFlowMO) You must be signed in to change notification settings\n\n# Ryan-Rhys/FlowMO\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmaster\n\n[Branches](https://github.com/Ryan-Rhys/FlowMO/branches) [Tags](https://github.com/Ryan-Rhys/FlowMO/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[68 Commits](https://github.com/Ryan-Rhys/FlowMO/commits/master/) |\n| [GP](https://github.com/Ryan-Rhys/FlowMO/tree/master/GP) | [GP](https://github.com/Ryan-Rhys/FlowMO/tree/master/GP) |  |  |\n| [Theano-master](https://github.com/Ryan-Rhys/FlowMO/tree/master/Theano-master) | [Theano-master](https://github.com/Ryan-Rhys/FlowMO/tree/master/Theano-master) |  |  |\n| [baselines](https://github.com/Ryan-Rhys/FlowMO/tree/master/baselines) | [baselines](https://github.com/Ryan-Rhys/FlowMO/tree/master/baselines) |  |  |\n| [datasets](https://github.com/Ryan-Rhys/FlowMO/tree/master/datasets) | [datasets](https://github.com/Ryan-Rhys/FlowMO/tree/master/datasets) |  |  |\n| [examples](https://github.com/Ryan-Rhys/FlowMO/tree/master/examples) | [examples](https://github.com/Ryan-Rhys/FlowMO/tree/master/examples) |  |  |\n| [property\\_prediction](https://github.com/Ryan-Rhys/FlowMO/tree/master/property_prediction) | [property\\_prediction](https://github.com/Ryan-Rhys/FlowMO/tree/master/property_prediction) |  |  |\n| [smiles\\_enumeration](https://github.com/Ryan-Rhys/FlowMO/tree/master/smiles_enumeration) | [smiles\\_enumeration](https://github.com/Ryan-Rhys/FlowMO/tree/master/smiles_enumeration) |  |  |\n| [.gitignore](https://github.com/Ryan-Rhys/FlowMO/blob/master/.gitignore) | [.gitignore](https://github.com/Ryan-Rhys/FlowMO/blob/master/.gitignore) |  |  |\n| [FreeSolv.png](https://github.com/Ryan-Rhys/FlowMO/blob/master/FreeSolv.png) | [FreeSolv.png](https://github.com/Ryan-Rhys/FlowMO/blob/master/FreeSolv.png) |  |  |\n| [LICENSE](https://github.com/Ryan-Rhys/FlowMO/blob/master/LICENSE) | [LICENSE](https://github.com/Ryan-Rhys/FlowMO/blob/master/LICENSE) |  |  |\n| [README.md](https://github.com/Ryan-Rhys/FlowMO/blob/master/README.md) | [README.md](https://github.com/Ryan-Rhys/FlowMO/blob/master/README.md) |  |  |\n| [SMILES.jpeg](https://github.com/Ryan-Rhys/FlowMO/blob/master/SMILES.jpeg) | [SMILES.jpeg](https://github.com/Ryan-Rhys/FlowMO/blob/master/SMILES.jpeg) |  |  |\n| [fingerpin.jpeg](https://github.com/Ryan-Rhys/FlowMO/blob/master/fingerpin.jpeg) | [fingerpin.jpeg](https://github.com/Ryan-Rhys/FlowMO/blob/master/fingerpin.jpeg) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# FlowMO\n\n[![License](https://camo.githubusercontent.com/7edda6b40df66cdf6d87ee014ce8a73af8830d12f325162978d3b72836ea332d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667)](https://github.com/Ryan-Rhys/FlowMO/blob/master/LICENSE)\n\nLibrary for training Gaussian Processes on Molecules\n\n## Install\n\nWe recommend using a conda environment.\n\n```\nconda create -n gp_molecule python==3.7\n\nconda install -c conda-forge rdkit\nconda install matplotlib pytest scikit-learn pandas pytorch\npip install git+https://github.com/GPflow/GPflow.git@develop#egg=gpflow\npip3 install jupyter\n\ncd Theano-master\npython setup.py install\n\n```\n\n## Examples\n\nSee the examples folder for a bare minimum required to fit Tanimoto and string kernel GPs on training molecules and\nevaluate on a heldout test set.\n\n## Representations\n\nThe library currently supports SMILES and ECFP6 Fingerprints (pictured below)\nas well as RDKit fragment features\n\n[![](https://github.com/Ryan-Rhys/FlowMO/raw/master/fingerpin.jpeg)](https://github.com/Ryan-Rhys/FlowMO/blob/master/fingerpin.jpeg)\n\n[![](https://github.com/Ryan-Rhys/FlowMO/raw/master/SMILES.jpeg)](https://github.com/Ryan-Rhys/FlowMO/blob/master/SMILES.jpeg)\n\n## Uncertainty Calibration\n\nAn illustration of the uncertainty calibration of models: string kernel GP (SSK GP),\nTanimoto GP (TK GP), Black Box Alpha Divergence Minimisation Bayesian Neural Network (BNN),\nAttentive Neural Process (ANP) on the Photoswitch Dataset.\n\n[![](https://github.com/Ryan-Rhys/FlowMO/raw/master/FreeSolv.png)](https://github.com/Ryan-Rhys/FlowMO/blob/master/FreeSolv.png)\n\n## Citing\n\nIf you find FlowMO useful for your research we would greatly appreciate if you would consider citing the following article!\n\n```\n@misc{moss2020gaussian,\n      title={Gaussian Process Molecule Property Prediction with FlowMO},\n      author={Henry B. Moss and Ryan-Rhys Griffiths},\n      year={2020},\n      eprint={2010.01118},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n\n```\n\n## About\n\nLibrary for training Gaussian Processes on Molecules\n\n### Resources\n\n[Readme](https://github.com/Ryan-Rhys/FlowMO#readme-ov-file)\n\n### License\n\n[MIT license](https://github.com/Ryan-Rhys/FlowMO#MIT-1-ov-file)\n\n[Activity](https://github.com/Ryan-Rhys/FlowMO/activity)\n\n### Stars\n\n[**36**\\\nstars](https://github.com/Ryan-Rhys/FlowMO/stargazers)\n\n### Watchers\n\n[**2**\\\nwatching](https://github.com/Ryan-Rhys/FlowMO/watchers)\n\n### Forks\n\n[**15**\\\nforks](https://github.com/Ryan-Rhys/FlowMO/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FRyan-Rhys%2FFlowMO&report=Ryan-Rhys+%28user%29)\n\n## [Releases](https://github.com/Ryan-Rhys/FlowMO/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/Ryan-Rhys/packages?repo_name=FlowMO)\n\nNo packages published\n\n## Languages\n\n- [Python89.5%](https://github.com/Ryan-Rhys/FlowMO/search?l=python)\n- [Cuda2.6%](https://github.com/Ryan-Rhys/FlowMO/search?l=cuda)\n- [C++2.3%](https://github.com/Ryan-Rhys/FlowMO/search?l=c%2B%2B)\n- [Jupyter Notebook2.1%](https://github.com/Ryan-Rhys/FlowMO/search?l=jupyter-notebook)\n- [C1.7%](https://github.com/Ryan-Rhys/FlowMO/search?l=c)\n- [TeX1.1%](https://github.com/Ryan-Rhys/FlowMO/search?l=tex)\n- Other0.7%\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/Ryan-Rhys/FlowMO"
    },
    {
      "title": "Predicting molecular properties with GPflow - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbd25bf4c08eb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Predicting molecular properties with GPflow\n\n[![TDS Editors](https://miro.medium.com/v2/resize:fill:88:88/1*W8dhinLQHGYmwipTuH0k3A.png)](https://towardsdatascience.medium.com/?source=post_page-----bd25bf4c08eb--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[TDS Editors](https://towardsdatascience.medium.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----bd25bf4c08eb---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n\u00b7\n\nSent as a [Newsletter](https://towardsdatascience.com/newsletter?source=post_page-----bd25bf4c08eb--------------------------------)\n\n\u00b7\n\n2 min read\n\n\u00b7\n\nJul 23, 2020\n\n--\n\nListen\n\nShare\n\nPhoto by Rapha\u00ebl Biscaldi on Unsplash\n\n## [Gaussian Process Regression on Molecules in GPflow](https://towardsdatascience.com/gaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130)\n\nBy [Ryan-Rhys Griffiths](https://medium.com/u/ef605a16eb6b?source=post_page-----bd25bf4c08eb--------------------------------) \u2014 5 min read\n\nThis post demonstrates how to train a Gaussian Process (GP) to predict molecular properties using the GPflow library by creating a custom-defined Tanimoto kernel to operate on Morgan fingerprints. Please visit my GitHub repo for the Jupyter notebook!\n\nPhoto by Valentin B. Kremer on Unsplash\n\n## [Lessons from a Deep Learning Master](https://towardsdatascience.com/lessons-from-a-deep-learning-master-1e38404dd2d5)\n\nBy [Rama Ramakrishnan](https://medium.com/u/28748480e8bd?source=post_page-----bd25bf4c08eb--------------------------------) \u2014 11 min read\n\nYoshua Bengio is a Deep Learning legend and won the Turing Award in 2018, along with Geoff Hinton and Yann LeCun.\n\nIn this short post, I want to highlight for you some clever things that Yoshua and his collaborators did to win a Machine Learning competition from a field of 381 competing teams. Perhaps these ideas will be useful for your own work.\n\nPhoto by Brooke Cagle on Unsplash\n\n## [10 signs a startup has awesome work culture](https://towardsdatascience.com/10-signs-a-startup-has-awesome-work-culture-90d7aa25e3a5)\n\nBy [Kate Marie Lewis](https://medium.com/u/feb55b31bbd5?source=post_page-----bd25bf4c08eb--------------------------------) \u2014 15 min read\n\nData scientists are in high demand. So when looking for a company to work for, you may be able to afford to be a bit picky. Even if you are looking for your first data science gig, I would caution you not to blindly jump at the first offer you receive. It is so important to do your homework and make sure that the company is a good fit for you.\n\n_Our daily picks will be back on Monday! If you want to receive our_ [_weekly digest_](https://towardsdatascience.com/receive-our-newsletters-681049ffa0cf) _on Fridays, it\u2019s easy! Follow_ [_our publication_](http://towardsdatascience.com/) _and then go to your settings and turn on \u201creceive letters.\u201d You can learn more about how to get the most out of Towards Data Science_ [_here_](https://towardsdatascience.com/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345) _._\n\n[The Daily Pick](https://medium.com/tag/the-daily-pick?source=post_page-----bd25bf4c08eb---------------the_daily_pick-----------------)\n\n[![TDS Editors](https://miro.medium.com/v2/resize:fill:144:144/1*W8dhinLQHGYmwipTuH0k3A.png)](https://towardsdatascience.medium.com/?source=post_page-----bd25bf4c08eb--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:64:64/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----bd25bf4c08eb---------------------follow_profile-----------)\n\n[**Written by TDS Editors**](https://towardsdatascience.medium.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[67K Followers](https://towardsdatascience.medium.com/followers?source=post_page-----bd25bf4c08eb--------------------------------)\n\n\u00b7Editor for\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\nBuilding a vibrant data science and machine learning community. Share your insights and projects with our global audience: [bit.ly/write-for-tds](http://bit.ly/write-for-tds)\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7e12c71dfa81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredicting-molecular-properties-with-gpflow-bd25bf4c08eb&user=TDS+Editors&userId=7e12c71dfa81&source=post_page-7e12c71dfa81----bd25bf4c08eb---------------------follow_profile-----------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Press](mailto:pressinquiries@medium.com?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Blog](https://blog.medium.com/?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bd25bf4c08eb--------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----bd25bf4c08eb-...",
      "url": "https://towardsdatascience.com/predicting-molecular-properties-with-gpflow-bd25bf4c08eb?gi=df6dc19f7764"
    },
    {
      "title": "Gaussian Process Regression on Molecules in GPflow - Towards Data Science",
      "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fee6fedab2130&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)\n\n[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Gaussian Process Regression on Molecules in GPflow\n\n[![Ryan-Rhys Griffiths](https://miro.medium.com/v2/resize:fill:88:88/1*EFaP4oiWCfDVVzCv4adeRQ.png)](https://medium.com/@ryangriff123?source=post_page-----ee6fedab2130--------------------------------)[![Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page-----ee6fedab2130--------------------------------)\n\n[Ryan-Rhys Griffiths](https://medium.com/@ryangriff123?source=post_page-----ee6fedab2130--------------------------------)\n\n\u00b7\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fef605a16eb6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130&user=Ryan-Rhys+Griffiths&userId=ef605a16eb6b&source=post_page-ef605a16eb6b----ee6fedab2130---------------------post_header-----------)\n\nPublished in\n\n[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ee6fedab2130--------------------------------)\n\n\u00b7\n\n5 min read\n\n\u00b7\n\nJul 9, 2020\n\n--\n\nListen\n\nShare\n\nThis post demonstrates how to train a Gaussian Process (GP) to predict molecular properties using the [GPflow library](https://gpflow.readthedocs.io/en/master/) by creating a custom-defined Tanimoto kernel to operate on Morgan fingerprints. Please visit my [GitHub repo](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/examples/gp_regression_on_molecules.ipynb) for the Jupyter notebook!\n\nIn this example, we\u2019ll be trying to predict the experimentally-determined electronic transition wavelengths of molecular photoswitches, a class of molecule that undergoes a reversible transformation between its E and Z isomers upon irradiation by light.\n\nImage from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper\n\nFirst things first, dependencies!\n\n```\nconda create -n photoswitch python=3.7\n\nconda install -c conda-forge rdkit\n\nconda install scikit-learn pandas\n\npip install git+https://github.com/GPflow/GPflow.git@develop#egg=gpflow\n```\n\nWe\u2019ll start by importing all of the machine learning and chemistry libraries we\u2019re going to use.\n\nFor our molecular representation, we\u2019re going to be working with the widely-used Morgan fingerprints. Under this representation, molecules are represented as bit vectors. As such, standard Gaussian Process kernels such as the squared exponential kernel or the Matern kernel will be less than ideal given that they\u2019re designed with continuous spaces in mind. We may however, define a custom \u201cTanimoto\u201d or \u201cJaccard\u201d kernel to act as our similarity metric between bit vectors.\n\nCodewise, it is relatively straightforward to define a custom [Tanimoto kernel in GPflow](https://gpflow.readthedocs.io/en/master/notebooks/tailor/kernel_design.html.)\n\nThe definition below differs slightly from the equation given above because we are computing the Tanimoto similarity over matrix inputs.\n\nNext, we read in our photoswitch molecules, represented as SMILES strings. The property we\u2019re predicting is the electronic transition wavelength of the E isomer for each molecule. In general for any dataset, all the GP requires is a list of molecule SMILES and a numpy array of property values.\n\nWe convert our molecules to 2048-bit Morgan fingerprints using a bond radius of 3. We denote the molecules as X and the property values by y. I guess this might make it easier to keep track of what the inputs and outputs are!\n\nWe define a utility function to standardise our output values. This is typically good practice before fitting a GP. Note that although the code also supports standardisation of the inputs we will choose NOT to make use of this later on. The reason being that standardising a bit vector, as opposed to a real-valued vector, doesn\u2019t seem to make a whole lot of sense (at least to me!).\n\nWe define an 80/20 train/test split.\n\nNext we fit the GP. We can inspect the learned kernel hyperparameters although these might not be so informative in the case of bit vectors!\n\nNow we can output the train and test root-mean-square error (RMSE), mean absolute error (MAE) and R\u00b2 value.\n\n```\nTrain RMSE (Standardised): 0.036\nTrain RMSE: 2.422 nm\n\nTest R^2: 0.916\nTest RMSE: 17.997 nm\nTest MAE: 11.333 nm\n```\n\nNot bad! In our [paper](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899), we compared the predictions of the GP-Tanimoto model against those made by a cohort of human photoswitch chemists achieving lower test error in the case of all 5 molecules comprising the test set:\n\nMAEs are computed on a per molecule basis across all human participants. Image from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper\n\nOne defining characteristics of GPs is their ability to produce calibrated uncertainty estimates. In molecule discovery campaigns these uncertainty estimates can be helpful in capturing the model\u2019s confidence; if the test molecule is very different from the molecules seen in training the model can tell you that it\u2019s guessing!\n\nIn programmatic terms, the test set uncertainties can be accessed by inspecting the variable y\\_var. One can obtain a ranked confidence list for the predictions by e.g.\n\nWhich outputs a list in which the molecules (represented by their test set index) are ranked by their prediction confidence The following [script](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/property_prediction/predict_with_GPR.py) offers further details\n\nGraphically, it is possible to generate confidence-error curves in order to check that the uncertainties obtained by the GP are actually correlated with test set error.\n\nImage from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper.\n\nIn the plot above the x-axis, confidence percentile, may be obtained simply by ranking each model prediction of the test set in terms of the predictive variance at the location of that test input. As an example, molecules that lie in the 80th confidence percenti...",
      "url": "https://towardsdatascience.com/gaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130?gi=a575b4c0daf6"
    },
    {
      "title": "Gaussian Process Regression on Molecules in GPflow",
      "text": "[Machine Learning](https://towardsdatascience.com/category/artificial-intelligence/machine-learning/)\n\n# Gaussian Process Regression on Molecules in GPflow\n\nThis post demonstrates how it's possible to train a Gaussian Process (GP) to predict molecular properties using the GPflow library by\u2026\n\n[Ryan-Rhys Griffiths](https://towardsdatascience.com/author/ryangriff123/)\n\nJul 9, 2020\n\n5 min read\n\nShare\n\nThis post demonstrates how to train a Gaussian Process (GP) to predict molecular properties using the [GPflow library](https://gpflow.readthedocs.io/en/master/) by creating a custom-defined Tanimoto kernel to operate on Morgan fingerprints. Please visit my [GitHub repo](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/examples/gp_regression_on_molecules.ipynb) for the Jupyter notebook!\n\nIn this example, we\u2019ll be trying to predict the experimentally-determined electronic transition wavelengths of molecular photoswitches, a class of molecule that undergoes a reversible transformation between its E and Z isomers upon irradiation by light.\n\n![Image from the Photoswitch Dataset paper](https://towardsdatascience.com/wp-content/uploads/2020/07/1VxbSasrc5fhEOMFVVek6qQ.png)Image from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper\n\nFirst things first, dependencies!\n\n```\nconda create -n photoswitch python=3.7\n\nconda install -c conda-forge rdkit\n\nconda install scikit-learn pandas\n\npip install git+https://github.com/GPflow/GPflow.git@develop#egg=gpflow\n```\n\nWe\u2019ll start by importing all of the machine learning and chemistry libraries we\u2019re going to use.\n\nFor our molecular representation, we\u2019re going to be working with the widely-used Morgan fingerprints. Under this representation, molecules are represented as bit vectors. As such, standard [Gaussian Process](https://towardsdatascience.com/tag/gaussian-process/) kernels such as the squared exponential kernel or the Matern kernel will be less than ideal given that they\u2019re designed with continuous spaces in mind. We may however, define a custom \"Tanimoto\" or \"Jaccard\" kernel to act as our similarity metric between bit vectors.\n\n![](https://towardsdatascience.com/wp-content/uploads/2020/07/1oPTSUSEQqimPMldPv7OwEg.png)\n\nCodewise, it is relatively straightforward to define a custom [Tanimoto kernel in GPflow](https://gpflow.readthedocs.io/en/master/notebooks/tailor/kernel_design.html.)\n\nThe definition below differs slightly from the equation given above because we are computing the Tanimoto similarity over matrix inputs.\n\nNext, we read in our photoswitch molecules, represented as SMILES strings. The property we\u2019re predicting is the electronic transition wavelength of the E isomer for each molecule. In general for any dataset, all the GP requires is a list of molecule SMILES and a numpy array of property values.\n\nWe convert our molecules to 2048-bit Morgan fingerprints using a bond radius of 3. We denote the molecules as X and the property values by y. I guess this might make it easier to keep track of what the inputs and outputs are!\n\nWe define a utility function to standardise our output values. This is typically good practice before fitting a GP. Note that although the code also supports standardisation of the inputs we will choose NOT to make use of this later on. The reason being that standardising a bit vector, as opposed to a real-valued vector, doesn\u2019t seem to make a whole lot of sense (at least to me!).\n\nWe define an 80/20 train/test split.\n\nNext we fit the GP. We can inspect the learned kernel hyperparameters although these might not be so informative in the case of bit vectors!\n\nNow we can output the train and test root-mean-square error (RMSE), mean absolute error (MAE) and R\u00b2 value.\n\n```\nTrain RMSE (Standardised): 0.036\nTrain RMSE: 2.422 nm\n\nTest R^2: 0.916\nTest RMSE: 17.997 nm\nTest MAE: 11.333 nm\n```\n\nNot bad! In our [paper](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899), we compared the predictions of the GP-Tanimoto model against those made by a cohort of human photoswitch chemists achieving lower test error in the case of all 5 molecules comprising the test set:\n\n![MAEs are computed on a per molecule basis across all human participants. Image from the Photoswitch Dataset paper](https://towardsdatascience.com/wp-content/uploads/2020/07/1wwacUI7rea7xu12yP5DIOw.png)MAEs are computed on a per molecule basis across all human participants. Image from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper\n\nOne defining characteristics of GPs is their ability to produce calibrated uncertainty estimates. In molecule discovery campaigns these uncertainty estimates can be helpful in capturing the model\u2019s confidence; if the test molecule is very different from the molecules seen in training the model can tell you that it\u2019s guessing!\n\nIn programmatic terms, the test set uncertainties can be accessed by inspecting the variable y\\_var. One can obtain a ranked confidence list for the predictions by e.g.\n\nWhich outputs a list in which the molecules (represented by their test set index) are ranked by their prediction confidence The following [script](https://github.com/Ryan-Rhys/The-Photoswitch-Dataset/blob/master/property_prediction/predict_with_GPR.py) offers further details\n\nGraphically, it is possible to generate confidence-error curves in order to check that the uncertainties obtained by the GP are actually correlated with test set error.\n\n![Image from the Photoswitch Dataset paper.](https://towardsdatascience.com/wp-content/uploads/2020/07/1rLN6Ys4u9-hmyfcTYwmBoQ.png)Image from the [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper.\n\nIn the plot above the x-axis, confidence percentile, may be obtained simply by ranking each model prediction of the test set in terms of the predictive variance at the location of that test input. As an example, molecules that lie in the 80th confidence percentile will be the 20% of test set molecules with the lowest model uncertainty. We then measure the prediction error at each confidence percentile across 200 random train/test splits to see whether the model\u2019s confidence is correlated with the prediction error. We observe that the GP-Tanimoto model\u2019s uncertainty estimates are positively correlated with prediction error.\n\nThe practical takeaway from this plot is a proof of concept that model uncertainty can be incorporated into the decision process for selecting which photoswitch molecules to physically synthesise in the lab; if the predicted wavelength value of an \"unsynthesised\" molecule is desirable and the confidence in this prediction is high you might want to make it!\n\nAnother nice benefit of GPs; it is possible to interpret the chemistry which different molecular representations capture by inspecting the learned kernel. See our [paper](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) for more details!\n\n![Image from The Photoswitch Dataset paper.](https://towardsdatascience.com/wp-content/uploads/2020/07/1SD3uYxka96kDe2YpJREo4Q.png)Image from The [Photoswitch Dataset](https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899) paper.\n\nFigure credits to the artistically indefatigable [Raymond Thawani](https://twitter.com/RaymondThawani). Please reach out on [Twitter](https://twitter.com/Ryan__Rhys) for comments/feedback/discussion! If you find the data or implementation...",
      "url": "https://towardsdatascience.com/gaussian-process-regression-on-molecules-in-gpflow-ee6fedab2130"
    },
    {
      "title": "Multitask GP Regression on Molecules",
      "text": "<div><article>\n<section>\n<p>An example notebook for multitask GP regression on a molecular dataset. We use a multioutput GP model, the intrinsic coregionalisation model (ICM) [1] on the Photoswitch Dataset [2] \u2014 using a Tanimoto kernel applied to fragprint representations [2]. The paper and code for the dataset is available here:</p>\n<p>Paper: <a href=\"https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h\">https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h</a></p>\n<p>Code: <a href=\"https://github.com/Ryan-Rhys/The-Photoswitch-Dataset\">https://github.com/Ryan-Rhys/The-Photoswitch-Dataset</a></p>\n<section>\n<h2>Multitask Learning with Gaussian Processes<a href=\"#Multitask-Learning-with-Gaussian-Processes\">#</a></h2>\n<p>Multitask learning is concerned with using a shared representation to learn several tasks; the idea being that predictive performance on a given task may benefit from the training signals of related tasks. Multioutput Gaussian processes (MOGPs) is the term given to models that perform multitask learning in the Gaussian process framework.</p>\n<p>Formally, we seek to carry out Bayesian inference over a stochastic function <span>\\(f: \\mathbb{R}^D \\to \\mathbb{R}^P\\)</span> where <span>\\(P\\)</span> is the number of tasks and we have access to observations <span>\\(\\{(\\mathbf{x_{11}}, y_{11}), \\dotsc , (\\mathbf{x_{1N}}, y_{1N}), \\dotsc , (\\mathbf{x_{P1}}, y_{P1}), \\dotsc , (\\mathbf{x_{PN}}, y_{PN})\\}\\)</span>. For each input, we may only have labels for a subset of the tasks.</p>\n<p>To build a MOGP we compute a kernel <span>\\(k(\\mathbf{x}, \\mathbf{x'}) \\cdot B[i, j]\\)</span> where <span>\\(B\\)</span> is a positive semidefinite <span>\\(P \\times P\\)</span> matrix , where the <span>\\((i, j)\\text{th}\\)</span> entry of the matrix <span>\\(B\\)</span> multiplies the covariance of the <span>\\(i\\)</span>-th function at <span>\\(\\mathbf{x}\\)</span> and the <span>\\(j\\)</span>-th function at <span>\\(\\mathbf{x'}\\)</span>. <span>\\(B\\)</span> is often referred to as an index kernel because it indexes the tasks.</p>\n<p>Inference proceeds in analogous fashion to vanilla Gaussian processes by substituting the new expression for the kernel into the equations for the predictive mean and variance.</p>\n<p>Positive semi-definiteness of <span>\\(B\\)</span> is guaranteed by parametrising the Cholesky decomposition <span>\\(LL^{\\top}\\)</span> where <span>\\(L\\)</span>, the Cholesky factor, is a lower triangular matrix and the parameters may be learned alongside the kernel hyperparameters by optimising the marginal likelihood.</p>\n<p>An example of what correlated tasks for continuous input spaces might look like is provided below. Data taken from the GPflow tutorial (<a href=\"https://gpflow.readthedocs.io/en/v1.5.1-docs/notebooks/advanced/coregionalisation.html\">https://gpflow.readthedocs.io/en/v1.5.1-docs/notebooks/advanced/coregionalisation.html</a>).</p>\n<p></p>\n<p>We define our model. See</p>\n<p><a href=\"https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html\">https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html</a></p>\n<p>for a tutorial for the use of the base multioutput GP on non-molecular data!</p>\n<div><pre><span></span><span># We define our MOGP model using the Tanimoto kernel</span>\n<span>from</span> <a href=\"https://leojklarner.github.io/gauche/modules/kernels.html#module-gauche.kernels.fingerprint_kernels.tanimoto_kernel\"><span>gauche.kernels.fingerprint_kernels.tanimoto_kernel</span></a> <span>import</span> <a href=\"https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.tanimoto_kernel.TanimotoKernel\"><span>TanimotoKernel</span></a>\n<span>num_tasks</span> <span>=</span> <span>4</span> <span># number of tasks i.e. labels</span>\n<span>rank</span> <span>=</span> <span>1</span> <span># increasing the rank hyperparameter allows the model to learn more expressive</span>\n <span># correlations between objectives at the expense of increasing the number of</span>\n <span># model hyperparameters and potentially overfitting.</span>\n<span>class</span> <span>MultitaskGPModel</span><span>(</span><span>gpytorch</span><span>.</span><span>models</span><span>.</span><span>ExactGP</span><span>):</span>\n <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>train_x</span><span>,</span> <span>train_y</span><span>,</span> <span>likelihood</span><span>):</span>\n <a href=\"https://docs.python.org/3/library/functions.html#super\"><span>super</span></a><span>(</span><span>MultitaskGPModel</span><span>,</span> <span>self</span><span>)</span><span>.</span><span>__init__</span><span>(</span><span>train_x</span><span>,</span> <span>train_y</span><span>,</span> <span>likelihood</span><span>)</span>\n <span>self</span><span>.</span><span>mean_module</span> <span>=</span> <span>gpytorch</span><span>.</span><span>means</span><span>.</span><span>ConstantMean</span><span>()</span>\n <span>self</span><span>.</span><span>covar_module</span> <span>=</span> <a href=\"https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.tanimoto_kernel.TanimotoKernel\"><span>TanimotoKernel</span></a><span>()</span>\n <span># We learn an IndexKernel for 4 tasks</span>\n <span># (so we'll actually learn 4x4=16 tasks with correlations)</span>\n <span>self</span><span>.</span><span>task_covar_module</span> <span>=</span> <span>gpytorch</span><span>.</span><span>kernels</span><span>.</span><span>IndexKernel</span><span>(</span><span>num_tasks</span><span>=</span><span>4</span><span>,</span> <span>rank</span><span>=</span><span>1</span><span>)</span>\n <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>i</span><span>):</span>\n <span>mean_x</span> <span>=</span> <span>self</span><span>.</span><span>mean_module</span><span>(</span><span>x</span><span>)</span>\n <span># Get input-input covariance</span>\n <span>covar_x</span> <span>=</span> <span>self</span><span>.</span><span>covar_module</span><span>(</span><span>x</span><span>)</span>\n <span># Get task-task covariance</span>\n <span>covar_i</span> <span>=</span> <span>self</span><span>.</span><span>task_covar_module</span><span>(</span><span>i</span><span>)</span>\n <span># Multiply the two together to get the covariance we want</span>\n <span>covar</span> <span>=</span> <span>covar_x</span><span>.</span><span>mul</span><span>(</span><span>covar_i</span><span>)</span>\n <span>return</span> <span>gpytorch</span><span>.</span><span>distributions</span><span>.</span><span>MultivariateNormal</span><span>(</span><span>mean_x</span><span>,</span> <span>covar</span><span>)</span>\n<br/></pre></div>\n<p>We define our experiment parameters. In this case we are reproducing the results of the multioutput GP prediction task from <a href=\"https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h\">https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h</a> using 20 random splits in the ratio 80/20.</p>\n<div><pre><span></span><span># Regression experiment parameters, number of random splits and train/test split size</span>\n<span>n_trials</span> <span>=</span> <span>20</span>\n<span>test_set_size</span> <span>=</span> <span>0.2</span>\n</pre></div>\n<p>Load the Photoswitch Dataset via the MolPropLoader class.</p>\n<div><pre><span></span><span># Load the Photoswitch dataset</span>\n<a href=\"https://leojklarner.github.io/gauche/modules/dataloader.html#gauche.dataloader.molprop_loader.MolPropLoader\"><span>loader</span></a> <span>=</span> <a href=\"https://leojklarner.github.io/gauche/modules/dataloader.html#gauche.dataloader.molprop_loader.MolPropLoader\"><span>MolPropLoader</span></a><span>()</span>\n<span># Define a utility function for dataloading</span>\n<span>def</span> <span>load_task_data</span><span>(</span><span>task</span><span>,</span>\n <span>loader</span><span>=</span><a href=\"https://leojklarner.github.io/...",
      "url": "https://leojklarner.github.io/gauche/notebooks/multitask_gp_regression_on_molecules.html"
    }
  ]
}