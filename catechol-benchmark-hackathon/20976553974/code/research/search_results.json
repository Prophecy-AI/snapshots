{
  "query": "What are the best neural network architectures for predicting chemical reaction yields with small datasets under 2000 samples?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting reaction yields when only a few\u2011thousand examples are available benefits most from **transfer\u2011learning\u2011based architectures** that can leverage knowledge learned on large, publicly\u2011available reaction corpora.  The most successful approaches reported in recent literature are:\n\n* **Pre\u2011trained graph neural networks (GNNs)** \u2013 By first training a GNN on a massive molecular database using a pseudo\u2011label task (principal\u2011component scores of molecular descriptors) and then fine\u2011tuning it on the limited yield set, performance remains robust even with <\u202f2000 reactions\u202f([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC10905905)).  The graph\u2011based representation captures atom\u2011wise connectivity and electronic environments, which are especially informative for yield\u2011driving features.\n\n* **Transformer\u2011based reaction models** \u2013 Text\u2011oriented encoders such as the **RXN\u2011Transformer** and the open\u2011source **ReactionT5** model treat the reaction SMILES as a language sequence and are pre\u2011trained on millions of USPTO reactions.  Fine\u2011tuning these models on a small yield dataset yields state\u2011of\u2011the\u2011art regression accuracy and naturally provides uncertainty estimates via test\u2011time augmentation\u202f([rxn4chemistry.github.io](https://rxn4chemistry.github.io/rxn_yields)),\u202f([research.ibm.com](https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches)).  Because the transformer already encodes rich reaction context (reagents, conditions, catalysts), it requires far fewer labeled examples to converge.\n\n* **Hybrid local\u2011to\u2011global representation learning** \u2013 Recent work introduces a \u201clog\u2011RRIM\u201d architecture that first learns local atom\u2011level embeddings and then aggregates them globally with interaction modules, achieving high accuracy on benchmark Suzuki and Buchwald\u2011Hartwig datasets even when only 2.5\u20115\u202f% of the full reaction space is used for training\u202f([arxiv.org/abs/2411.03320](https://arxiv.org/abs/2411.03320)).  This design is well\u2011suited to small\u2011sample regimes because the local encoder can be pre\u2011trained separately and the global interaction layer adapts quickly to the limited yield labels.\n\nIn practice, the best strategy is to **choose a pre\u2011trained GNN or transformer**, freeze most of the backbone, and fine\u2011tune the final regression head on the \u2264\u202f2000\u2011sample yield set, optionally augmenting the data (e.g., by generating stereochemical or reagent permutations) to further improve robustness and to obtain calibrated uncertainty estimates.",
      "url": ""
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Graph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.</p>\n<section><h3>Supplementary Information</h3>\n<p>The online version contains supplementary material available at 10.1186/s13321-024-00818-z.</p></section><section><p><strong>Keywords:</strong> Chemical reaction yield prediction, Graph neural network, Pre-training, Deep learning</p></section></section><section><h2>Introduction</h2>\n<p>A chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner [<a href=\"#CR1\">1</a>\u2013<a href=\"#CR8\">8</a>].</p>\n<p>Recently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. [<a href=\"#CR6\">6</a>, <a href=\"#CR7\">7</a>] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. [<a href=\"#CR8\">8</a>] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs [<a href=\"#CR9\">9</a>, <a href=\"#CR10\">10</a>].</p>\n<p>Despite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.</p>\n<p>To alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches [<a href=\"#CR11\">11</a>, <a href=\"#CR12\">12</a>]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart [<a href=\"#CR13\">13</a>\u2013<a href=\"#CR18\">18</a>]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented [<a href=\"#CR19\">19</a>, <a href=\"#CR20\">20</a>]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them [<a href=\"#CR21\">21</a>\u2013<a href=\"#CR25\">25</a>]. Existing methods have attempted to define appropriate pre-text tasks in various ways to effectively learn molecular representations. The process of acquiring pseudo-labels can be costly and time-consuming depending on how the pre-text task is defined. Since both approaches have their own advantages and drawbacks, it is important to choose the most suitable pre-training method that best aligns with the objective of a specific downstream task that needs to be addressed.</p>\n<p>In this study, we propose a novel pre-training method, <strong>MolDescPred</strong>, to improve the performance in predicting chemical reaction yields. <strong>MolDescPred</strong> is based on the pre-text task approach to pre-train a GNN. Given a molecular database containing a substantial number of molecules, we calculate the molecular descriptors for the molecules and reduce their dimensionality by applying principal component analysis (PCA). Each molecule is then pseudo-labeled with a vector of its principal component scores. The GNN is then pre-trained to predict the pseudo-label of its input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then is fine-tuned with a training dataset composed of chemical reactions and their corresponding yields. Through experiments on benchmark datasets, we demonstrate the effectiveness of the proposed method compared to existing methods, especially when the training dataset is insufficient.</p></section><section><h2>Method</h2>\n<section><h3>Problem definition</h3>\n<p>For chemical reaction yield prediction, we aim to build an accurate prediction model <em>f</em> which takes a chemical reaction <span></span> as the input to predict the yield <em>y</em> by learning from the training dataset <span></span>. Given a query chemical reaction <span></span>, the prediction model <em>f</em> can be used to make a prediction for the yield <span></span> as:</p>\n<p>It should be noted that additional information, such as the operating conditions for chemical reactions, can be utilized as extra input for the model <em>f</em>. If we denote this additional information by <span></span>, the problem can be formulated as learning the model <em>f</em> from the dataset <span></span>. The input and output of the model <em>f</em> can be described as:</p>\n<p>The data representation used for the prediction model <em>f</em> is as follows. In a chemical reaction <span></span>, <span></span> and <span></span> denote the sets of reactants and products, respectively. The set <span></span> contains <em>m</em> reactant molecules represented as molecular graphs, where <em>m</em> can vary for each reaction. The set <span></span> contains a single molecular graph representing a product molecule. Each molecular graph <span></span> represents the topology of a molecule. Here, <span></span> and <span></span> are the sets of nodes and edges associated with heavy atoms and their chemical bonds within the molecule. Hydrogen atoms are implicitly handled as node features of their neighboring heavy atoms. ...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10905905"
    },
    {
      "title": "Predicting Chemical Reaction Yields | RXN yield prediction",
      "text": "Predicting Chemical Reaction Yields | RXN yield prediction\n* * [rxn\\_yields](#)\n* [Overview](https://rxn4chemistry.github.io/rxn_yields//)\n* [Data](https://rxn4chemistry.github.io/rxn_yields/data)\n* [Training Tutorial](https://rxn4chemistry.github.io/rxn_yields/model_training)\n* [Evaluation Buchwald Hartwig](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_buchwald_hartwig_yields_prediction)\n* [Evaluation Suzuki Miyaura](https://rxn4chemistry.github.io/rxn_yields/results_evaluation_of_suzuki_miyaura_yields_prediction)\n* [USPTO Exploration](https://rxn4chemistry.github.io/rxn_yields/uspto_data_exploration)\n# Predicting Chemical Reaction Yields\nPredicting the yield of a chemical reaction from a reaction SMILES using Transformers\nArtificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, successfully became part of the organic chemists\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, reaction yields models have been less investigated, despite the enormous potential of accurately predicting them. Reaction yields models, describing the percentage of the reactants that is converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the dataset applicability in reaction yields predictions.\nThis repository complements our studies on[predicting chemical reaction yields](https://iopscience.iop.org/article/10.1088/2632-2153/abc81d)(published in Machine Learning: Science and Technology) and[data augmentation and uncertainty estimation for yield predictions](https://doi.org/10.26434/chemrxiv.13286741)(presented at the Machine Learning for Molecules Workshop at NeurIPS 2020).\n## Install[](#Install)\nAs the library is based on the chemoinformatics toolkit[RDKit](http://www.rdkit.org)it is best installed using the[Anaconda](https://docs.conda.io/en/latest/miniconda.html)package manager. Once you have conda, you can simply run:\n```\n`conda create -n yields python=3.6 -y\nconda activate yields\nconda install -c rdkit rdkit=2020.03.3.0 -y\nconda install -c tmap tmap -y`\n```\n```\n`git clone https://github.com/rxn4chemistry/rxn\\_yields.git\ncd rxn\\_yields\npip install -e .`\n```\n**NOTE:**\nIf you are fine-tuning your own models. Make sure that the pretrained model (from which you start training) is loaded from a folder with the same structure as for our[rxnfp models](https://github.com/rxn4chemistry/rxnfp/tree/master/rxnfp/models/transformers/bert_pretrained).\n## Approach - predicting yields from reaction SMILES[](#Approach---predicting-yields-from-reaction-SMILES)\nTransformer models have recently revolutionised Natural Language Processing and were also successfully applied to task in chemistry, using a text-based representation of molecules and chemical reactions called Simplified molecular-input line-entry system (SMILES).\nSequence-2-Sequence transformers as in[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)were used for:\n* Chemical Reaction Prediction\n* [Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction](https://pubs.acs.org/doi/full/10.1021/acscentsci.9b00576)\n* [Carbohydrate Transformer: Predicting Regio- and Stereoselective Reactions Using Transfer Learning](http://dx.doi.org/10.26434/chemrxiv.11935635)\n* Multi-step retrosynthesis\n* [Predicting retrosynthetic pathways using a combined linguistic model and hyper-graph exploration strategy](http://dx.doi.org/10.1039/c9sc05704h)\n* [Unassisted Noise-Reduction of Chemical Reactions Data Sets](https://chemrxiv.org/articles/Unassisted_Noise-Reduction_of_Chemical_Reactions_Data_Sets/12395120/1)\nEncoder Transformers like[BERT](https://openreview.net/forum?id=SkZmKmWOWH)and[ALBERT](https://openreview.net/forum?id=H1eA7AEtvS)for:\n* Reaction fingerprints and classification\n* [Mapping the Space of Chemical Reactions using Attention-Based Neural Networks](https://chemrxiv.org/articles/Data-Driven_Chemical_Reaction_Classification_with_Attention-Based_Neural_Networks/9897365)\n* Atom rearrangements during chemical reactions\n* [Unsupervised Attention-Guided Atom-Mapping](https://chemrxiv.org/articles/Unsupervised_Attention-Guided_Atom-Mapping/12298559)\nThose studies show that Transformer models are able to learn organic chemistry and chemical reactions from SMILES.\nHere we asked the question, how well a**BERT**model would perform when applied to a**yield prediction**task:\n![](https://rxn4chemistry.github.io/rxn_yields/images/pipeline.jpg)\n**Figure:**Pipeline and task description.\nTo do so, we started with the reaction fingerprint models from the[rxnfp](https://rxn4chemistry.github.io/rxnfp/)library and added a fine-tuning regression head through[SimpleTransformers.ai](https://simpletransformers.ai). As we don't need to change the hyperparameters of the base model, we only tune the learning rate for the training and the dropout probability.\nWe explored two high-throughput experiment (HTE) data sets and then also the yields data found in the USPTO data base.\n## Buchwald-Hartwig HTE data set[](#Buchwald-Hartwig-HTE-data-set)\n### Canonical reaction representation[](#Canonical-reaction-representation)\nOne of the best studied reaction yield is the one that was published by Ahneman et al. in[Predicting reaction performance in C\u2013N cross-coupling using machine learning](https://science.sciencemag.org/content/360/6385/186.full), where the authors have used DFT-computed descriptors as inputs to different machine learning descriptors. There best model was a random forest model. More recently,[one-hot encodings](https://science.sciencemag.org/content/362/6416/eaat8603)and[multi-fingerprint features (MFF)](https://www.sciencedirect.com/science/article/pii/S2451929420300851)as input representations were investigated. Here, we show competitive results starting simply from a text-based reaction SMILES input to our models.\n![](https://rxn4chemistry.github.io/rxn_yields/images/buchwald_hartwig.jpg)\n**Figure:**a) Summary of the results on the Buchwald\u2013Hartwig data set. b) Example regression plot for the first random-split.\n### Augmentated reaction representations[](#Augmentated-reaction-representations)\nWe were able to further improve the results on this data set using data augmentation on reaction SMILES (molecule order permuations and SMILES randomisations). This extension will be presented at the NeurIPS 2020[Machine Learning for Molecules Workshop](https://nips.cc/Conferences/2020/ScheduleMultitrack?event=16136).\n![](https://rxn4chemistry.github.io/rxn_yields/images/rxn_randomizations.png)\n**Figure:**The two different data augmentation techniques investigated in the NeurIPS workshop paper.\n#### Results[](#Results)\n![](https://rxn4chemistry.github.io/rxn_yields/images/results_augm.png)\n**Figure:**a) Results on the 70/30 random splits, averaged over 10 splits. b) Comparison of DFT descriptors + RF, canonical SMILES and data augmented randomized SMILES on reduced training sets. c) Out-of-sample test sets\nOn random splits 70/30 in a), the data augmented Yield-BERT models perform better than ...",
      "url": "https://rxn4chemistry.github.io/rxn_yields"
    },
    {
      "title": "Quantitative Biology > Biomolecules",
      "text": "[2411.03320] log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[q-bio](https://arxiv.org/list/q-bio/recent)&gt;arXiv:2411.03320\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Quantitative Biology \\> Biomolecules\n**arXiv:2411.03320**(q-bio)\n[Submitted on 20 Oct 2024 ([v1](https://arxiv.org/abs/2411.03320v1)), last revised 9 Mar 2025 (this version, v4)]\n# Title:log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling\nAuthors:[Xiao Hu](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Hu,+X),[Ziqi Chen](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Chen,+Z),[Bo Peng](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Peng,+B),[Daniel Adu-Ampratwum](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Adu-Ampratwum,+D),[Xia Ning](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Ning,+X)\nView a PDF of the paper titled log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling, by Xiao Hu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2411.03320)[HTML (experimental)](https://arxiv.org/html/2411.03320v4)> > Abstract:\n> Accurate prediction of chemical reaction yields is crucial for optimizing organic synthesis, potentially reducing time and resources spent on experimentation. With the rise of artificial intelligence (AI), there is growing interest in leveraging AI-based methods to accelerate yield predictions without conducting in vitro experiments. We present log-RRIM, an innovative graph transformer-based framework designed for predicting chemical reaction yields. A key feature of log-RRIM is its integration of a cross-attention mechanism that focuses on the interplay between reagents and reaction centers. This design reflects a fundamental principle in chemical reactions: the crucial role of reagents in influencing bond-breaking and formation processes, which ultimately affect reaction yields. log-RRIM also implements a local-to-global reaction representation learning strategy. This approach initially captures detailed molecule-level information and then models and aggregates intermolecular interactions. Through this hierarchical process, log-RRIM effectively captures how different molecular fragments contribute to and influence the overall reaction yield, regardless of their size variations. log-RRIM shows superior performance in our experiments, especially for medium to high-yielding reactions, proving its reliability as a predictor. The framework&#39;s sophisticated modeling of reactant-reagent interactions and precise capture of molecular fragment contributions make it a valuable tool for reaction planning and optimization in chemical synthesis. The data and codes of log-RRIM are accessible through [> this https URL\n](https://github.com/ninglab/Yield_log_RRIM)> . Comments:|45 pages, 8 figures|\nSubjects:|Biomolecules (q-bio.BM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2411.03320](https://arxiv.org/abs/2411.03320)[q-bio.BM]|\n|(or[arXiv:2411.03320v4](https://arxiv.org/abs/2411.03320v4)[q-bio.BM]for this version)|\n|[https://doi.org/10.48550/arXiv.2411.03320](https://doi.org/10.48550/arXiv.2411.03320)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Xiao Hu [[view email](https://arxiv.org/show-email/fe86e812/2411.03320)]\n**[[v1]](https://arxiv.org/abs/2411.03320v1)**Sun, 20 Oct 2024 18:35:56 UTC (712 KB)\n**[[v2]](https://arxiv.org/abs/2411.03320v2)**Fri, 8 Nov 2024 17:50:33 UTC (712 KB)\n**[[v3]](https://arxiv.org/abs/2411.03320v3)**Tue, 19 Nov 2024 16:49:12 UTC (713 KB)\n**[v4]**Sun, 9 Mar 2025 03:43:34 UTC (769 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling, by Xiao Hu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2411.03320)\n* [HTML (experimental)](https://arxiv.org/html/2411.03320v4)\n* [TeX Source](https://arxiv.org/src/2411.03320)\n* [Other Formats](https://arxiv.org/format/2411.03320)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nq-bio.BM\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2411.03320&amp;function=prev&amp;context=q-bio.BM) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2411.03320&amp;function=next&amp;context=q-bio.BM)\n[new](https://arxiv.org/list/q-bio.BM/new)|[recent](https://arxiv.org/list/q-bio.BM/recent)|[2024-11](https://arxiv.org/list/q-bio.BM/2024-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2411.03320?context=cs)\n[cs.AI](https://arxiv.org/abs/2411.03320?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2411.03320?context=cs.LG)\n[q-bio](https://arxiv.org/abs/2411.03320?context=q-bio)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.03320)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.03320)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.03320)\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2411.03320&amp;description=log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2411.03320&amp;title=log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is Sc...",
      "url": "https://arxiv.org/abs/2411.03320"
    },
    {
      "title": "Low-data regime yield predictions with uncertainty estimation using deep learning approaches for ACS Fall 2021",
      "text": "Authors Low-data regime yield predictions with uncertainty estimation using deep learning approaches for ACS Fall 2021 https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches\nLow-data regime yield predictions with uncertainty estimation using deep learning approaches for ACS Fall 2021\nAuthors\n2021-08-22T00:00:00Z\n## Abstract\nArtificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists\u2019 daily laboratory work, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could help chemists navigate reaction space, optimize reactions, and accelerate the design of more effective routes. Here, we investigate high-throughput experimentation data sets and show how data augmentation on chemical reactions can improve yield predictions\u2019 accuracy, even when only small training sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In our work, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the augmented training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Related\nPaper\n### [Will OLED displays challenge liquid crystal displays in notebook computer applications?](https://research.ibm.com/publications/will-oled-displays-challenge-liquid-crystal-displays-in-notebook-computer-applications)\nRonald Troutman\nSynthetic Metals\nPaper\n### [Effect of oxygen on the electrical transport in RuO2](https://research.ibm.com/publications/effect-of-oxygen-on-the-electrical-transport-in-ruolessinfgreater2lessinfgreater)\nL. Krusin-Elbaum\nThin Solid Films\nPaper\n### [Kinetic model for the chemical vapor deposition of tungsten in the silane reduction process](https://research.ibm.com/publications/kinetic-model-for-the-chemical-vapor-deposition-of-tungsten-in-the-silane-reduction-process)\nJulian J. Hsieh\nJournal of Vacuum Science and Technology A: Vacuum, Surfaces and Films\nTalk\n### [On the automation of de-novo molecular design and chemical synthesis planning: A case study on SARS-CoV-2](https://research.ibm.com/publications/on-the-automation-of-de-novo-molecular-design-and-chemical-synthesis-planning-a-case-study-on-sars-cov-2)\nJannis Born, Matteo Manica, et al.\nACS Fall 2021\n[View all publications](https://research.ibm.com/publications)",
      "url": "https://research.ibm.com/publications/low-data-regime-yield-predictions-with-uncertainty-estimation-using-deep-learning-approaches"
    },
    {
      "title": "Interpretation of chemical reaction yields with graph neural additive ...",
      "text": "<div><div>\n<h2>We apologize for the inconvenience...</h2>\n<p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p>\n<p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>\n<p>\n<a href=\"https://ioppublishing.org/contacts/\">https://ioppublishing.org/contacts/</a>\n</p>\n</div></div>",
      "url": "https://iopscience.iop.org/article/10.1088/2632-2153/addfaa"
    },
    {
      "title": "An active representation learning method for reaction yield ... - Nature",
      "text": "<div><div>\n \n <div><h2>Introduction</h2><div><p>The optimization of chemical reaction is a fundamental task that has numerous important applications in synthetic chemistry<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR1\">1</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR2\">2</a></sup>. For example, we often need to adopt some new structures to investigate the classic reactions, such as Buchwald\u2013Hartwig coupling, for discovering novel molecules with new functions<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR3\">3</a></sup>. But the real performance caused by those new structures are usually unreported and also not easy to predict. In traditional optimization process, chemists usually need to consider a \u201creaction space\u201d which contains a set of reaction combinations with several critical conditions, such as different catalysts, ligands, additives, solvents and other components (Fig.\u00a0<a href=\"https://www.nature.com/articles/s42004-025-01434-0#Fig1\">1A</a>). In the well-known Suzuki coupling dataset provided by Pfizer\u2019s group<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR4\">4</a></sup>, for example, the size of the reaction space is 5760\u2009=\u200915\u2009\u00d7\u200912\u2009\u00d7\u20098\u2009\u00d7\u20094. As an important optimization objective, reaction yield has been widely studied for evaluating experimental performance<sup><a href=\"#ref-CR5\">5</a>,<a href=\"#ref-CR6\">6</a>,<a href=\"#ref-CR7\">7</a>,<a href=\"#ref-CR8\">8</a>,<a href=\"#ref-CR9\">9</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR10\">10</a></sup>, since it can reflect the quality of a reaction and reveal the underlying principles in chemistry. Therefore, when studying a new reaction system, it is urgently needed to understand the patterns of reaction yield and explore high-yield reaction combinations. However, this process often needs to take a large amount of experimental time, and moreover, the performance heavily relies on the expertise of the experimenter. As a consequence, some potentially viable reaction conditions are very likely to be overlooked. For example, Buchwald\u2019s group recommended only a limited range of conditions on Buchwald\u2013Hartwig coupling in their early research<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR11\">11</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR12\">12</a></sup>, but they further discovered a series of important new combinations in the following study<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR13\">13</a></sup>.</p><div><figure><figcaption><b>Fig. 1: Reaction yield prediction framework for small-scale data.</b></figcaption><div><div><a href=\"https://www.nature.com/articles/s42004-025-01434-0/figures/1\"></a></div><p><b>A</b> Traditional yield optimization. <b>B</b> The overall framework of our model with RS-coreset. Our yield prediction result can be achieved via an iterative procedure, where each iteration includes 3 steps. Step 1 (\u201c<i>yield evaluation</i>\u201d), evaluate the yields of selected reactions by chemists. Step 2 (\u201c<i>representation learning</i>\u201d), update the representation learning model with the newly added experimental data. Step 3 (\u201c<i>data selection</i>\u201d), select the most informative reactions in the reaction space guided by our coreset method. After several iterations, the representation for reaction space becomes stable and then we can build the final yield prediction model upon it.</p></div><p><a href=\"https://www.nature.com/articles/s42004-025-01434-0/figures/1\"><span>Full size image</span></a></p></figure></div><p>The emerging high-throughput experimentation (HTE) technology, which is able to run a large number of reactions in parallel<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR14\">14</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR15\">15</a></sup>, has attracted a lot of attention for accelerating the traditional reaction optimization process<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR16\">16</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR17\">17</a></sup>. HTE can significantly promote the experimental efficiency and reduce the workload for exploring a new reaction system. The favorable parallelization of HTE can also generate a sufficiently large amount of experimental data to support reaction optimization, and thus greatly decreases the chance of missing high-yield reaction combinations.</p><p>To effectively utilize the experimental data generated from HTE, several different machine learning (ML) based methodologies have been proposed. Machine learning is an active research subarea of Artificial Intelligence<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR18\">18</a></sup>. In general, the goal of machine learning is to learn some effective model from observed data, where the model can be applied to solve various tasks, e.g., classification and regression. In particular, machine learning techniques have been successfully applied to different fields of scientific research<sup><a href=\"#ref-CR19\">19</a>,<a href=\"#ref-CR20\">20</a>,<a href=\"#ref-CR21\">21</a>,<a href=\"#ref-CR22\">22</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR23\">23</a></sup>. For example, we can exploit machine learning-assisted methods to address a variety of chemical tasks such as molecular design<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR24\">24</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR25\">25</a></sup>, reaction prediction<sup><a href=\"#ref-CR26\">26</a>,<a href=\"#ref-CR27\">27</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR28\">28</a></sup>, retrosynthetic analyses<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR29\">29</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR30\">30</a></sup>, reaction condition optimization<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR31\">31</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR32\">32</a></sup>, selectivity prediction<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR33\">33</a>,<a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR34\">34</a></sup>, etc. In the area of yield prediction, Doyle\u2019s group provided an open-source chemical reaction optimization tool based on the Bayesian method, and they validated their approach through the experiments involving Palladium-catalysed C-N coupling and C\u2013H functionalization system, Mitsunobu reaction and deoxyfluorination reaction of alcohols<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR6\">6</a></sup>. Recently, Denmark\u2019s group designed a machine learning tool to predict substrate-adaptive conditions for Palladium-catalyzed C\u2013N couplings, where the tool can be used to optimize reaction combinations within a large reaction space of 450,000 possible reactions<sup><a href=\"https://www.nature.com/articles/s42004-025-01434-0#ref-CR17\">17</a></sup>.</p><p>Most of the current studies on reaction yield prediction rely on large amounts of data provided by HTE equipment. HTE, while powerful, its high cost makes it unaffordable for most research laboratories in the world. Consequently, the limited data and budget unfortunately hinder many chemists from leveraging ML methods to predict reaction yield or guide reaction screening. Therefore, our major task in this paper is to design a method that can provide guidance for yield prediction and optimization with small-scale experimental data. Through this method, we aim to interpret the entire reaction space and then achieve an overview of the yields across the space, which could help us to discover more potential reaction pathways that might otherwise be overlooked.</p><p>Our idea is to design some effective sampling strategy to approximate the reaction space. Moreo...",
      "url": "https://www.nature.com/articles/s42004-025-01434-0"
    },
    {
      "title": "Machine learning for yield prediction for chemical reactions using in ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling\"><span><span></span></span></a></p><p><a href=\"https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling/vol/118/suppl/C\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><h2>Abstract</h2><div><p>Machine learning models were developed to predict product formation from time-series reaction data for ten Buchwald-Hartwig coupling reactions. The data was provided by DeepMatter and was collected in their DigitalGlassware cloud platform. The reaction probe has 12 sensors to measure properties of interest, including temperature, pressure, and colour. Colour was a good predictor of product formation for this reaction and machine learning models were able to learn which of the properties were important. Predictions for the current product formation (in terms of % yield) had a mean absolute error of 1.2%. For predicting 30, 60 and 120\u00a0min ahead the error rose to 3.4, 4.1 and 4.6%, respectively. The work here presents an example into the insight that can be obtained from applying machine learning methods to sensor data in synthetic chemistry.</p></div></div><ul><li></li><li></li></ul><div><h2>Keywords</h2><p><span>Buchwald-Hartwig cross-coupling</span></p><p><span>Long short-term memory neural network</span></p><p><span>Reaction monitoring</span></p><p><span>Time series data</span></p></div><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2022 The Author(s). Published by Elsevier Inc.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S1093326322002352"
    },
    {
      "title": "[PDF] Prediction of Chemical Reaction Yields using Deep Learning",
      "text": "Prediction of Chemical Reaction Yields using Deep\nLearning\nPhilippe Schwaller\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nE-mail: phs@zurich.ibm.com\nAlain C. Vaucher, Teodoro Laino\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nJean-Louis Reymond\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nAbstract. Artificial intelligence is driving one of the most important revolutions\nin organic chemistry. Multiple platforms, including tools for reaction prediction and\nsynthesis planning based on machine learning, successfully became part of the organic\nchemists\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike\nreaction prediction and retrosynthetic models, the prediction of reaction yields has\nreceived less attention in spite of the enormous potential of accurately predicting\nreaction conversion rates. Reaction yields models, describing the percentage of\nthe reactants converted to the desired products, could guide chemists and help\nthem select high-yielding reactions and score synthesis routes, reducing the number\nof attempts. So far, yield predictions have been predominantly performed for\nhigh-throughput experiments using a categorical (one-hot) encoding of reactants,\nconcatenated molecular fingerprints, or computed chemical descriptors. Here, we\nextend the application of natural language processing architectures to predict reaction\nproperties given a text-based representation of the reaction, using an encoder\ntransformer model combined with a regression layer. We demonstrate outstanding\nprediction performance on two high-throughput experiment reactions sets. An analysis\nof the yields reported in the open-source USPTO data set shows that their distribution\ndi\u21b5ers depending on the mass scale, limiting the dataset applicability in reaction yields\npredictions.\n1. Introduction\nChemical reactions in organic chemistry are described by writing the structural\nformula of reactants and products separated by an arrow, representing the chemical\ntransformation by specifying how the atoms rearrange between one or several reactant\nPrediction of Chemical Reaction Yields using Deep Learning 2\nmolecules and one or several product molecules [1]. Economic, logistic, and energetic\nconsiderations drive chemists to prefer chemical transformations capable of converting\nall reactant molecules into products with the highest yield possible. However, side\u0002reactions, degradation of reactants, reagents or products in the course of the reaction,\nequilibrium processes with incomplete conversion to a product, or simply by product\nisolation and purification undermine the quantitative conversion of reactants into\nproducts, rarely reaching optimal performance.\nReaction yields are usually reported as a percentage of the theoretical chemical\nconversion, i.e., the percentage of the reactant molecules successfully converted to the\ndesired product compared to the theoretical value. It is not uncommon for chemists\nto synthesise a molecule in a dozen or more reaction steps. Hence, low-yield reactions\nmay have a disastrous e\u21b5ect on the overall route yield because of the individual steps\u2019\nmultiplicative e\u21b5ect. Therefore, it is not surprising that designing new reactions with\nyields higher than existing ones attracts much e\u21b5ort in organic chemistry research.\nIn practice, specific chemical reaction classes are characterised by lower or higher\nyields, with the actual value depending on the reaction conditions (temperature,\nconcentrations, etc.) and on the specific substrates.\nEstimating the reaction yield can be a game-changing asset for synthesis planning.\nIt provides chemists with the ability to evaluate the overall yield of complex reaction\npaths, addressing possible shortcomings well ahead of investing hours and materials in\nwet-lab experiments. Computational models predicting reaction yields could support\nsynthetic chemists in choosing an appropriate synthesis route among many predicted\nby data-driven algorithms. Moreover, reaction yields prediction models could also be\nemployed as scoring functions in computer-assisted retrosynthesis route planning tools\n[2, 3, 4, 5], to complement forward prediction models [6, 4] and in-scope filters [2].\nMost of the existing e\u21b5orts in constructing models for the prediction of reactivity or\nof reaction yields focused on a particular reaction class: oxidative dehydrogenations of\nethylbenzene with tin oxide catalysts [7], reactions of vanadium selenites [8], Buchwald\u2013\nHartwig aminations [9, 10, 11], and Suzuki\u2013Miyaura cross-coupling reactions [12, 13, 14].\nTo the best of our knowledge, there was only one attempt to design a general-purpose\nprediction model for reactivity and yields, without applicability constraints to a specific\nreaction class [15]. In this work, the authors design a model predicting whether the\nreaction yield is above or below a threshold value and conclude that the models and\ndescriptors they consider cannot deliver satisfactory results.\nHere, we build on our legacy of treating organic chemistry as a language to introduce\na new model that predicts reaction yields starting from reaction SMILES [16]. More\nspecifically, we fine-tune the rxnfp models by Schwaller et al. [17] based on a BERT\u0002encoder [18] by extending it with a regression layer to predict reaction yields. BERT\nencoders belong to the transformer model family, which has revolutionised natural\nlanguage processing [19, 18]. These models take sequences of tokens as input to compute\ncontextualised representations of all the input tokens, and can be applied to reactions\nrepresented in the SMILES [20] format. In this work, we demonstrate for the first\nPrediction of Chemical Reaction Yields using Deep Learning 3\ntime, that these natural language architectures are very useful not only when working\nwith language tokens, but also to provide descriptors of high quality to predict reaction\nproperties such as reaction yields.\nIt is possible to train our approach both on data specific to a given reaction class\nor on data representing di\u21b5erent reaction types. Thus, we initially trained the model on\ntwo high-throughput experimentation (HTE) data sets. Among the few HTE reaction\ndata sets published in recent years, we selected the data sets for palladium-catalysed\nBuchwald\u2013Hartwig reactions provided by Ahneman et al. [9] and for Suzuki\u2013Miyaura\ncoupling reactions provided by Perera et al. [21]. Finally, we trained our model on\npatent data available in the USPTO data set [22, 23].\nHTE and Patent data sets are very di\u21b5erent in terms of content and quality. HTE\ndata sets typically cover a very narrow region in the chemical reaction space, with\nchemical reaction data related to one or a few reaction templates applied to large\ncombinations of selected precursors (reactants, solvents, bases, catalysts, etc.). In\ncontrast, patent reactions cover a much wider reaction space. In terms of quality, HTE\ndata sets report reactions represented uniformly and with yields measured using the\nsame analytical equipment, thus providing a consistent and high quality collection of\nknowledge. In comparison, the yields from patents were measured by di\u21b5erent scientists\nusing di\u21b5erent equipments. Incomplete information in the original documents, such\nas unreported reagents or reaction conditions, and the extensive limitation in text\nmining technologies makes the entire set of patent reactions quite noisy and sparse.\nAn extensive analysis of the USPTO data set revealed that the experimental conditions\nand reaction parameters, such as scale of the reaction, concentrations, temperature,\npressure, or reaction duration, may have a significant e\u21b5ect on the measured reaction\nyields. The functional dependency of the yields from the reaction conditions poses\nadditional constraints, as the model presented in this w...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60c750f2ee301c70b1c7a973/original/prediction-of-chemical-reaction-yields-using-deep-learning.pdf"
    }
  ]
}