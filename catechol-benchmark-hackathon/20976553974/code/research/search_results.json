{
  "query": "What is the best approach for predicting reaction yields in the Catechol rearrangement benchmark using graph neural networks?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The Catechol rearrangement benchmark provides a time\u2011series, high\u2011throughput dataset of\u202f\u2248\u202f1\u202f200 solvent\u2011selection experiments, where each reaction is described by the molecular graphs of the reagents and a set of continuous process variables (solvent identity, temperature, flow rate, etc.)\u202f\u3010openreview.net\u3011. Because the data are sparse in the chemical space and contain many correlated continuous features, the most reliable strategy is to combine a graph\u2011neural\u2011network backbone with additional modality\u2011specific encoders and uncertainty\u2011aware training.\n\nA strong baseline is to start from a **pre\u2011trained GNN** that has already learned general chemical representations (e.g., from USPTO or large reaction corpora) and fine\u2011tune it on the Catechol data\u202f\u3010jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z\u3011. The graph encoder should be paired with a **multimodal module** that embeds the solvent and other process conditions (e.g., using physicochemical descriptors or one\u2011hot vectors) and concatenates them with the graph latent vector\u202f\u3010ojs.aaai.org/index.php/AAAI/article/view/28668\u3011. Training the combined model with a **probabilistic loss** (e.g., evidential or Bayesian regression) yields both point predictions and calibrated uncertainty estimates, which have been shown to improve yield accuracy on benchmark datasets\u202f\u3010jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-021-00579-z.pdf\u3011.  \n\nFinally, enriching the graph representation with **knowledge\u2011based steric/electronic features** (e.g., atomic\u2011level electronic fields) further boosts extrapolative performance and provides interpretability, a benefit demonstrated on similar yield\u2011prediction tasks\u202f\u3010nature.com/articles/s41467-023-39283-x\u3011. In practice, the best approach for the Catechol rearrangement benchmark therefore consists of a fine\u2011tuned, pre\u2011trained GNN augmented by multimodal process\u2011condition encoders and trained with uncertainty\u2011aware objectives, optionally enhanced with chemistry\u2011derived steric/electronic embeddings.",
      "url": ""
    },
    {
      "title": "GitHub - seokhokang/reaction_yield_nn: Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
      "text": "[Skip to content](https://github.com/seokhokang/reaction_yield_nn#start-of-content)\n\nYou signed in with another tab or window. [Reload](https://github.com/seokhokang/reaction_yield_nn) to refresh your session.You signed out in another tab or window. [Reload](https://github.com/seokhokang/reaction_yield_nn) to refresh your session.You switched accounts on another tab or window. [Reload](https://github.com/seokhokang/reaction_yield_nn) to refresh your session.Dismiss alert\n\n{{ message }}\n\n[seokhokang](https://github.com/seokhokang)/ **[reaction\\_yield\\_nn](https://github.com/seokhokang/reaction_yield_nn)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fseokhokang%2Freaction_yield_nn) You must be signed in to change notification settings\n- [Fork\\\n7](https://github.com/login?return_to=%2Fseokhokang%2Freaction_yield_nn)\n- [Star\\\n19](https://github.com/login?return_to=%2Fseokhokang%2Freaction_yield_nn)\n\n\nUncertainty-aware prediction of chemical reaction yields with graph neural networks\n\n### License\n\n[Apache-2.0 license](https://github.com/seokhokang/reaction_yield_nn/blob/main/LICENSE)\n\n[19\\\nstars](https://github.com/seokhokang/reaction_yield_nn/stargazers) [7\\\nforks](https://github.com/seokhokang/reaction_yield_nn/forks) [Branches](https://github.com/seokhokang/reaction_yield_nn/branches) [Tags](https://github.com/seokhokang/reaction_yield_nn/tags) [Activity](https://github.com/seokhokang/reaction_yield_nn/activity)\n\n[Star](https://github.com/login?return_to=%2Fseokhokang%2Freaction_yield_nn)\n\n[Notifications](https://github.com/login?return_to=%2Fseokhokang%2Freaction_yield_nn) You must be signed in to change notification settings\n\n# seokhokang/reaction\\_yield\\_nn\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\nmain\n\n[Branches](https://github.com/seokhokang/reaction_yield_nn/branches) [Tags](https://github.com/seokhokang/reaction_yield_nn/tags)\n\nGo to file\n\nCode\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[11 Commits](https://github.com/seokhokang/reaction_yield_nn/commits/main/) |\n| [data](https://github.com/seokhokang/reaction_yield_nn/tree/main/data) | [data](https://github.com/seokhokang/reaction_yield_nn/tree/main/data) |  |  |\n| [LICENSE](https://github.com/seokhokang/reaction_yield_nn/blob/main/LICENSE) | [LICENSE](https://github.com/seokhokang/reaction_yield_nn/blob/main/LICENSE) |  |  |\n| [README.md](https://github.com/seokhokang/reaction_yield_nn/blob/main/README.md) | [README.md](https://github.com/seokhokang/reaction_yield_nn/blob/main/README.md) |  |  |\n| [dataset.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/dataset.py) | [dataset.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/dataset.py) |  |  |\n| [model.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/model.py) | [model.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/model.py) |  |  |\n| [run\\_code.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/run_code.py) | [run\\_code.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/run_code.py) |  |  |\n| [util.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/util.py) | [util.py](https://github.com/seokhokang/reaction_yield_nn/blob/main/util.py) |  |  |\n| View all files |\n\n## Repository files navigation\n\n# reaction\\_yield\\_nn\n\nPyTorch implementation of the model described in the paper [Uncertainty-Aware Prediction of Chemical Reaction Yields with Graph Neural Networks](https://doi.org/10.1186/s13321-021-00579-z)\n\n## Components\n\n- **data/**\\\\* \\- dataset files used\n- **data/get\\_data.py** \\- script for dataset file generation\n- **model/**\\\\* \\- model files used\n- **run\\_code.py** \\- script for model training/evaluation\n- **dataset.py** \\- data structure & functions\n- **model.py** \\- model architecture & functions\n- **util.py**\n\n## Data\n\n- The datasets used in the paper can be downloaded from\n  - [https://github.com/rxn4chemistry/rxn\\_yields/](https://github.com/rxn4chemistry/rxn_yields/)\n\n## Dependencies\n\n- **Python**\n- **PyTorch**\n- **DGL**\n- **RDKit**\n\n## Citation\n\n```\n@Article{Kwon2022,\n  title={Uncertainty-aware prediction of chemical reaction yields with graph neural networks},\n  author={Kwon, Youngchun and Lee, Dongseon and Choi, Youn-Suk and Kang, Seokho},\n  journal={Journal of Cheminformatics},\n  volume={14},\n  pages={2},\n  year={2022},\n  doi={10.1186/s13321-021-00579-z}\n}\n\n```\n\n## About\n\nUncertainty-aware prediction of chemical reaction yields with graph neural networks\n\n### Resources\n\n[Readme](https://github.com/seokhokang/reaction_yield_nn#readme-ov-file)\n\n### License\n\n[Apache-2.0 license](https://github.com/seokhokang/reaction_yield_nn#Apache-2.0-1-ov-file)\n\n[Activity](https://github.com/seokhokang/reaction_yield_nn/activity)\n\n### Stars\n\n[**19**\\\nstars](https://github.com/seokhokang/reaction_yield_nn/stargazers)\n\n### Watchers\n\n[**1**\\\nwatching](https://github.com/seokhokang/reaction_yield_nn/watchers)\n\n### Forks\n\n[**7**\\\nforks](https://github.com/seokhokang/reaction_yield_nn/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fseokhokang%2Freaction_yield_nn&report=seokhokang+%28user%29)\n\n## [Releases](https://github.com/seokhokang/reaction_yield_nn/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/users/seokhokang/packages?repo_name=reaction_yield_nn)\n\nNo packages published\n\n## Languages\n\n- [Python100.0%](https://github.com/seokhokang/reaction_yield_nn/search?l=python)\n\nYou can\u2019t perform that action at this time.",
      "url": "https://github.com/seokhokang/reaction_yield_nn"
    },
    {
      "title": "Quantitative Biology > Biomolecules",
      "text": "[2411.03320] log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[q-bio](https://arxiv.org/list/q-bio/recent)&gt;arXiv:2411.03320\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Quantitative Biology \\> Biomolecules\n**arXiv:2411.03320**(q-bio)\n[Submitted on 20 Oct 2024 ([v1](https://arxiv.org/abs/2411.03320v1)), last revised 9 Mar 2025 (this version, v4)]\n# Title:log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling\nAuthors:[Xiao Hu](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Hu,+X),[Ziqi Chen](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Chen,+Z),[Bo Peng](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Peng,+B),[Daniel Adu-Ampratwum](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Adu-Ampratwum,+D),[Xia Ning](https://arxiv.org/search/q-bio?searchtype=author&amp;query=Ning,+X)\nView a PDF of the paper titled log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling, by Xiao Hu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2411.03320)[HTML (experimental)](https://arxiv.org/html/2411.03320v4)> > Abstract:\n> Accurate prediction of chemical reaction yields is crucial for optimizing organic synthesis, potentially reducing time and resources spent on experimentation. With the rise of artificial intelligence (AI), there is growing interest in leveraging AI-based methods to accelerate yield predictions without conducting in vitro experiments. We present log-RRIM, an innovative graph transformer-based framework designed for predicting chemical reaction yields. A key feature of log-RRIM is its integration of a cross-attention mechanism that focuses on the interplay between reagents and reaction centers. This design reflects a fundamental principle in chemical reactions: the crucial role of reagents in influencing bond-breaking and formation processes, which ultimately affect reaction yields. log-RRIM also implements a local-to-global reaction representation learning strategy. This approach initially captures detailed molecule-level information and then models and aggregates intermolecular interactions. Through this hierarchical process, log-RRIM effectively captures how different molecular fragments contribute to and influence the overall reaction yield, regardless of their size variations. log-RRIM shows superior performance in our experiments, especially for medium to high-yielding reactions, proving its reliability as a predictor. The framework&#39;s sophisticated modeling of reactant-reagent interactions and precise capture of molecular fragment contributions make it a valuable tool for reaction planning and optimization in chemical synthesis. The data and codes of log-RRIM are accessible through [> this https URL\n](https://github.com/ninglab/Yield_log_RRIM)> . Comments:|45 pages, 8 figures|\nSubjects:|Biomolecules (q-bio.BM); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2411.03320](https://arxiv.org/abs/2411.03320)[q-bio.BM]|\n|(or[arXiv:2411.03320v4](https://arxiv.org/abs/2411.03320v4)[q-bio.BM]for this version)|\n|[https://doi.org/10.48550/arXiv.2411.03320](https://doi.org/10.48550/arXiv.2411.03320)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Xiao Hu [[view email](https://arxiv.org/show-email/fe86e812/2411.03320)]\n**[[v1]](https://arxiv.org/abs/2411.03320v1)**Sun, 20 Oct 2024 18:35:56 UTC (712 KB)\n**[[v2]](https://arxiv.org/abs/2411.03320v2)**Fri, 8 Nov 2024 17:50:33 UTC (712 KB)\n**[[v3]](https://arxiv.org/abs/2411.03320v3)**Tue, 19 Nov 2024 16:49:12 UTC (713 KB)\n**[v4]**Sun, 9 Mar 2025 03:43:34 UTC (769 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling, by Xiao Hu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2411.03320)\n* [HTML (experimental)](https://arxiv.org/html/2411.03320v4)\n* [TeX Source](https://arxiv.org/src/2411.03320)\n* [Other Formats](https://arxiv.org/format/2411.03320)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nq-bio.BM\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2411.03320&amp;function=prev&amp;context=q-bio.BM) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2411.03320&amp;function=next&amp;context=q-bio.BM)\n[new](https://arxiv.org/list/q-bio.BM/new)|[recent](https://arxiv.org/list/q-bio.BM/recent)|[2024-11](https://arxiv.org/list/q-bio.BM/2024-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2411.03320?context=cs)\n[cs.AI](https://arxiv.org/abs/2411.03320?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2411.03320?context=cs.LG)\n[q-bio](https://arxiv.org/abs/2411.03320?context=q-bio)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.03320)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.03320)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.03320)\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2411.03320&amp;description=log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2411.03320&amp;title=log-RRIM: Yield Prediction via Local-to-global Reaction Representation Learning and Interaction Modeling>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is Sc...",
      "url": "https://arxiv.org/abs/2411.03320"
    },
    {
      "title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
      "text": "Kwon\u00a0et\u00a0al. Journal of Cheminformatics (2022) 14:2 \nhttps://doi.org/10.1186/s13321-021-00579-z\nRESEARCH ARTICLE\nUncertainty-aware prediction of\u00a0chemical \nreaction yields with\u00a0graph neural networks\nYoungchun Kwon1,2, Dongseon Lee1, Youn\u2011Suk Choi1* and Seokho Kang3*\nAbstract\nIn this paper, we present a data-driven method for the uncertainty-aware prediction of chemical reaction yields. The \nreactants and products in a chemical reaction are represented as a set of molecular graphs. The predictive distribution \nof the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance. \nUncertainty-aware learning and inference are applied to the model to make accurate predictions and to evaluate their \nuncertainty. We demonstrate the efectiveness of the proposed method on benchmark datasets with various settings. \nCompared to the existing methods, the proposed method improves the prediction and uncertainty quantifcation \nperformance in most settings.\nKeywords: Chemical reaction yield prediction, Uncertainty-aware prediction, Graph neural network, Deep learning\n\u00a9 The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativeco\nmmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nIn organic chemistry, the prediction of chemical reac\u0002tion yields is an important research topic in chemical \nsynthesis planning [1, 2]. Tis enables the estimation of \nthe overall yield of a complex synthetic pathway and the \ndetection of low-yield reactions that negatively afect \nthe overall yield. It also provides clues for designing new \nreactions that provide higher yields to save on the time \nand cost required for experimental syntheses.\nMachine learning has achieved remarkable success in \nthe data-driven prediction of chemical reaction yields \n[1, 3\u20137]. Te main concept is to construct a prediction \nmodel that predicts the yield of a chemical reaction by \nlearning from previously accumulated data compris\u0002ing a number of chemical reactions annotated with \ntheir experimentally measured yields. Te successful \napplication of a prediction model enables fast and ef\u0002cient estimation of chemical reaction yields without \nperforming experimental syntheses, which are costly and \ntime-consuming.\nEarly studies represented each chemical reaction as a \nfxed-size vector of handcrafted features, such as molecu\u0002lar fngerprints and chemical property descriptors, and \nconstructed an of-the-shelf prediction model on top of \nthe vector representation [3\u20135, 8]. Te limitation of this \napproach is that the choice of adequate features relies \non chemical knowledge and intuition, and some inher\u0002ent information to the original reaction may be lost \nin the representation. With advances in deep learning \n[9], recent studies have applied deep neural networks \nconstructed on a more informative representation of a \nchemical reaction. Schwaller et\u00a0al. [6, 10] used simplifed \nmolecular-input line-entry system (SMILES) to represent \na chemical reaction. To predict the reaction yield, they \nfne-tuned a bidirectional encoder representations from \ntransformers (BERT) model pre-trained using a reaction \nSMILES database [11] to predict the yield. Saebi et\u00a0al. [7] \nrepresented a chemical reaction as a set of graphs, on \nwhich a graph neural network was constructed to predict \nthe yield.\nIn this paper, we present an alternative method for pre\u0002dicting chemical reaction yields. As a prediction model, \nwe adapt a graph neural network that directly operates \nOpen Access\nJournal of Cheminformatics\n*Correspondence: ysuk.choi@samsung.com; s.kang@skku.edu\n1\n Samsung Advanced Institute of Technology, Samsung Electronics Co. \nLtd., 130 Samsung\u2011ro, Yeongtong\u2011gu, Suwon, Republic of Korea 3\n Department of Industrial Engineering, Sungkyunkwan University, 2066 \nSeobu\u2011ro, Jangan\u2011gu, Suwon, Republic of Korea\nFull list of author information is available at the end of the article\nKwon\u00a0et\u00a0al. Journal of Cheminformatics (2022) 14:2 Page 2 of 10\non the graph representation of a chemical reaction in \na permutation-invariant fashion. We use uncertainty\u0002aware learning and inference in the model to make accu\u0002rate predictions of yields and determine the confdence of \npredictions.\nMethods\nData representation\nWe suppose that a chemical reaction consists of a num\u0002ber of reactants and a single product. Tis chemical \nreaction is labeled with its reaction yield. Each instance \nis represented as (R,P, y), where R = {GR,1, ... , GR,m}\nand P = {GP} are the set of m reactants and the resulting \nproduct in the reaction, respectively, and y is the reac\u0002tion yield. Te number of reactants m can be diferent for \neach reaction.\nEach molecule in R and P is defned as an undirected \ngraph G = (V, E), where V and E represent the set of \nnodes and the set of edges, respectively. Te node fea\u0002ture vectors vj \u2208 V and edge feature vectors ej,k \u2208 E\nare associated with heavy atoms (e.g., C, N, O, and F) \nand their bonds (e.g., single, double, triple, and \naromatic), respectively. Hydrogen atoms are treated \nimplicitly. Te number of heavy atoms and bonds in each \nmolecule is the same as the number of node feature vec\u0002tors and edge feature vectors in the corresponding graph \nrepresentation, respectively. Figure\u00a01 illustrates an exam\u0002ple of the graph representation of a molecule.\nFor the j-th atom, vj = (vj,1, ... , vj,p) is a vec\u0002tor indicating the atom type, formal charge, degree, \nhybridization, number of hydrogens, valence, chiral\u0002ity, whether it accepts or donates electrons, whether it \nis aromatic, whether it is in a ring, and associated ring \nsizes. For the bond between the j-th and k-th atoms, \nej,k = (ej,k,1, ... , ej,k,q) is a vector indicating the bond \ntype, stereochemistry, whether it is in a ring, and whether \nit is conjugated.\nPrediction model\nTo predict the reaction yield y, we introduce a predictive \ndistribution for y conditioned on the set of reactants R\nand product P, denoted by p\u03b8 (y|R,P), which is modeled \nas a normal distribution as follows:\nwhere \u00b5 and \u03c32\n are the mean and variance of the distri\u0002bution, respectively. We parameterize the predictive dis\u0002tribution p\u03b8 using a neural network f that produces \u00b5 and \n\u03c32\n as a function of R and P with a set of parameters \u03b8:\nTo construct the neural network f, we adapt the architec\u0002ture presented by Saebi et al. [7] to process two sets of \nmolecular graphs with advanced neural network mod\u0002ules. Figure\u00a0 2 illustrates the architecture used in this \nstudy. Te architectural details of each component are \npresented next.\nA message passing neural network (MPNN) [12] is \nused as the GNN component of f to process each molec\u0002ular graph G in R and P. Te GNN is designed to take G\nas the input and return the graph representation vector r\nas the output:\nIn the GNN, we apply multiple message passing steps \nusing an edge network as a message function and a gated \nrecurrent unit (GRU) network as an update function to \ngenerate node representation vectors. We then apply a \nset2set model [13] as a readout function for g...",
      "url": "https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-021-00579-z.pdf"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph neural networks",
      "text": "Search all BMC articles\n\nSearch\n\nImproving chemical reaction yield prediction using pre-trained graph neural networks\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 01 March 2024\n\n# Improving chemical reaction yield prediction using pre-trained graph neural networks\n\n- [Jongmin Han](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Jongmin-Han-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1),\n- [Youngchun Kwon](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youngchun-Kwon-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Youn-Suk Choi](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youn_Suk-Choi-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2) &\n- \u2026\n- [Seokho Kang](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Seokho-Kang-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a025 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 5588 Accesses\n\n- 9 Citations\n\n- 2 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z/metrics)\n\n\n## Abstract\n\nGraph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.\n\n## Introduction\n\nA chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner \\[ [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR3), [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR4), [5](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR5), [6](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR6), [7](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\].\n\nRecently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR6), [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR7)\\] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR9), [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR10)\\].\n\nDespite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.\n\nTo alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR12)\\]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart \\[ [13](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR13), [14](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR14), [15](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR15), [16](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR16), [17](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR18)\\]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented \\[ [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR20)\\]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them \\[ [21](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR21), [22](https://jcheminf.biomedcentral.com/jcheminf.biome...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z"
    },
    {
      "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
      "text": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features\nJiayuan Chen1, Kehan Guo2, Zhen Liu3, Olexandr Isayev3, Xiangliang Zhang2*\n1The Ohio State University\n2 Department of Computer Science and Engineering, University of Notre Dame\n3Department of Chemistry, Carnegie Mellon University\nchen.12930@osu.edu, kguo2@nd.edu, liu5@andrew.cmu.edu, olexandr@olexandrisayev.com, xzhang33@nd.edu\nAbstract\nPredicting chemical reaction yields is pivotal for effcient\nchemical synthesis, an area that focuses on the creation of\nnovel compounds for diverse uses. Yield prediction demands\naccurate representations of reactions for forecasting practical\ntransformation rates. Yet, the uncertainty issues broadcasting\nin real-world situations prohibit current models to excel in\nthis task owing to the high sensitivity of yield activities and\nthe uncertainty in yield measurements. Existing models often\nutilize single-modal feature representations, such as molec\u0002ular fngerprints, SMILES sequences, or molecular graphs,\nwhich is not suffcient to capture the complex interactions\nand dynamic behavior of molecules in reactions. In this pa\u0002per, we present an advanced Uncertainty-Aware Multimodal\nmodel (UAM) to tackle these challenges. Our approach seam\u0002lessly integrates data sources from multiple modalities by\nencompassing sequence representations, molecular graphs,\nand expert-defned chemical reaction features for a com\u0002prehensive representation of reactions. Additionally, we ad\u0002dress both the model and data-based uncertainty, refning the\nmodel\u2019s predictive capability. Extensive experiments on three\ndatasets, including two high throughput experiment (HTE)\ndatasets and one chemist-constructed Amide coupling reac\u0002tion dataset, demonstrate that UAM outperforms the state\u0002of-the-art methods. The code and used datasets are available\nat https://github.com/jychen229/Multimodal-reaction-yield\u0002prediction.\nIntroduction\nComputer-Assisted Synthesis Prediction (CASP) has\nemerged as a key area of focus in the intersection of artif\u0002cial intelligence in scientifc domains. The goal of CASP\nrevolves around tackling a diverse array of chemical chal\u0002lenges, including the prediction of reaction products (Coley\net al. 2017) and the intricacies of retro-synthesis (Ishida\net al. 2019). Yield prediction, among the spectrum of CASP\ntasks, is particularly crucial. The target of yield prediction\nis to accurately estimate the practical conversion rates in\nchemical reactions, illustrating the transition from reactants\nto products. In this context, yield prediction lays the foun\u0002dation for reaction-related predictions, thereby supporting\nthe advancements in CASP (Ahneman et al. 2018).\n*The corresponding author.\nCopyright \u00a9 2024, Association for the Advancement of Artifcial\nIntelligence (www.aaai.org). All rights reserved.\nWhen conceptualized as a machine learning problem,\nyield prediction is essentially a regression task. The devel\u0002opment of an effective yield prediction model depends crit\u0002ically on obtaining high-quality representations of the re\u0002actants and products involved in chemical reactions. Early,\nmolecular fngerprints were employed to depict chemical\nstructures, yet their effcacy in handling complex structures\nwas limited. Deep learning-based methods can automati\u0002cally learn intricate patterns and features from data. For\ninstance, (Schwaller et al. 2020) employ BERT (Devlin\net al. 2018), a bidirectional transformer language model,\nfor learning the representation of molecules involved in\nchemical reactions based on their sequential SMILES ex\u0002pressions. This learned representation is then utilized in a\nsubsequent regression model to predict yields. Similarly,\n(Kwon et al. 2022) employ molecular graphs to represent\nmolecules within chemical reactions and utilize graph neural\nnetworks to learn useful features for yield prediction. These\ncurrent yield prediction models exhibit strong performance\non specially curated reaction datasets, such as the High\u0002Throughput (HTE) datasets (Ahneman et al. 2018; Perera\net al. 2018). However, when applied to real-world tasks, their\neffcacy diminishes signifcantly (Saebi et al. 2023). One pri\u0002mary reason for this decline is the pervasive issue of uncer\u0002tainty in real-world yield prediction datasets, manifesting in\ntwo major aspects.\nHigh sensitivity of yield. In chemical reactions, structural\nisomers\u2014compounds with identical molecular formulas but\ndifferent arrangements of atoms\u2014can signifcantly impact\nthe yield. Even minor structural variations within the reac\u0002tants themselves can lead to pronounced discrepancies in\nthe resulting yields. For example, the addition of a methoxy\ngroup that is far from the reaction center can lower the reac\u0002tion center by as much as 55% (Schierle et al. 2020). This\nhighlights how real-world reactions can be extremely sen\u0002sitive to slight variations in the reactants and products in\u0002volved. Existing models, as referenced by (Schwaller et al.\n2021), primarily utilize single-modal data such as graphs or\nsequences, and thus may not adequately capture the subtle\nstructural variations in molecules. These subtle yet critical\nvariations include minor differences in stereochemistry and\nthe presence of specifc functional groups, both of which can\nhave a signifcant impact on reaction pathways and yields.\nUncertainty in the yield measurement. The yield from\nThe Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)\n8274\nthe reaction process depends on many factors in the reaction\ncycle, including the properties of the molecules, the envi\u0002ronmental condition, and human operations. As a result, the\nsame reaction can exhibit signifcant yield variations. For ex\u0002ample, (Liu, Moroz, and Isayev 2023) pointed out that the\nyield standardized deviation can be as large as 23.7% when\nthe same reaction was reported by different research groups.\nAlthough (Kwon et al. 2022) considered yield prediction un\u0002certainty and introduced an uncertainty-related loss for train\u0002ing the prediction model, the inherent intricacies of data un\u0002certainty hinder a precise prediction.\nTo address the aforementioned challenges, we propose\nan advanced Uncertainty-Aware Multimodal model (UAM)\nfor yield prediction by taking into account multi-modal fea\u0002tures to combat the prediction uncertainty. Specifcally, we\nintroduce a multi-modal feature extractor that integrates\nsequence features, graph structural features, and human\u0002defned reaction condition features to acquire a more com\u0002prehensive representation of reactants and products. More\u0002over, aided by cross-modal contrastive learning, we facil\u0002itate modal fusion to capture the shared information and\ndistinctive features across modalities to alleviate discrep\u0002ancies induced by the high sensitivity of yield. Addition\u0002ally, we incorporate a Mixture-of-Experts (MoE) module to\nenhance model expressiveness without additional computa\u0002tional costs. This facilitates a dynamic equilibrium between\nthe model\u2019s sensitivity to variations and its ability to discern\nreaction types. Last, we introduce an uncertainty quantif\u0002cation module, which mitigates the inherent training uncer\u0002tainty of the model while focusing on quantifying the uncer\u0002tainty presented in the data itself, thereby enhancing predic\u0002tive accuracy.\nOur contributions in this work are summarized as follows:\n\u2022 We study the reaction yield prediction problem and pro\u0002posed a novel model called UAM to tackle the uncer\u0002tainty issue by fusing multi-modal molecular features;\n\u2022 We explore an innovative and effective way to utilize\ncross-modal contrastive learning and an additional MoE\nmodule is added to enhance the reaction representation;\n\u2022 Experimental results on three real-world datasets demon\u0002strate the effectiveness of UAM in comparison to the\nstate-of-the-art approaches.\nRelated Work\nMolecular Representation Learning\nMolecular representation learning is a crucial link between\nmachine learning and chemistry and is gaining rising aware\u0002ness in computational chemistry. Early tech...",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28668/29297"
    },
    {
      "title": "A graph-convolutional neural network model for the prediction of chemical reactivity",
      "text": "Connor W Coley \nDepartment of Chemical Engineering b Computer Science\nArtificial Intelligence Laboratory\n\n\nWengong Jin \nLuke Rogers \nDepartment of Chemical Engineering b Computer Science\nArtificial Intelligence Laboratory\n\n\nTimothy F Jamison \nDepartment of Chemistry\nMassachusetts Institute of Technology\n77 Massachusetts Avenue02139CambridgeMA\n\nTommi S Jaakkola \nWilliam H Green \nDepartment of Chemical Engineering b Computer Science\nArtificial Intelligence Laboratory\n\n\nRegina Barzilay regina@csail.mit.edu \nKlavs F Jensen kfjensen@mit.edu \nDepartment of Chemical Engineering b Computer Science\nArtificial Intelligence Laboratory\n\n\nSupporting Information A graph-convolutional neural network model for the prediction of chemical reactivity Electronic Supplementary Material (ESI) for\nPage 1\nAll code used for model training can be found at https://github.com/connorcoley/rexgen_direct. The full data set of USPTO reactions used in this study can be found at the same link. We have included a \"deployed\" model that uses the trained weights of the model analyzed in detail in the manuscript.\n\nS2 Methods\n\n\nS2.1 Notation\n\n\nSymbol\n\nMeaning u, v atoms N(v) Set of atoms adjacent to v \u03c4(\u00b7) ReLU activation function \u03c3 (\u00b7) Sigmoid function U,V,W matrices in WLN/WLDN\n\n\nS2.2 Weisfeiler-Lehman Network (WLN)\n\nWeisfeiler-Lehman Network 32 is a type of graph convolutional network derived from Weisfeiler-Lehman (WL) graph kernel 38 . The architecture is designed to embed the computations inherent in WL graph kernel to learn isomorphism invariant representation of atoms. The atom representation is computed by iteratively augmenting the representation of adjacent atoms. Specifically, each atom v is initialized with a feature vector f v indicating its atomic number, formal charge, degree of connectivity, explicit and implicit valence, and aromaticity. Each bond (u, v) is associated with a feature vector f uv indicating its bond order and ring status. In each iteration, we updated atom representations as follows:\nf l v = \u03c4 U 1 f l\u22121 v +U 2 \u2211 u\u2208N(v) \u03c4(V 1 f l\u22121 u +V 2 f uv ) (1 \u2264 l \u2264 L)\nwhere f l v is the atom representation at the lth iteration, initialized with f 0 v = f v atom features. U 1 ,U 2 ,V 1 ,V 2 are model parameters to be learned, shared across all L iterations. The final local atom representations are computed as\nc v = \u2211 u\u2208N(v) W 1 f L u W 2 f uv W 3 f L v\nWe refer the reader to 32 for more details about the mathematical intuition and justification of the WLN.\n\n\nS2.3 Attention Mechanism\n\nThe atom embedding c v only record local chemical environment, namely atoms and bonds accessible within L steps from atom v. Even if L were very large, c v could not encode any information about other reactant molecules, as information cannot be propagated between two reactant molecules that are disconnected. We argue that it is important to enable information to flow between distant or disconnected atoms. For example, the reaction center may be influenced by certain reagents that are disconnected from reactant molecules. In this case, it is necessary for atom representation c v to encode such distal chemical effects. Therefore, we propose to enhance the model in previous section with an attention mechanism 39 . Specifically, let \u03b1 vz be the attention score of atom v upon atom z. The \"global\" atom representationc v of atom v is calculated as the weighted sum of all reactant atoms where the weight comes from the attention module:\n\u03b1 vz = \u03c3 (u T \u03c4(P a c v + P a c z + P b b vz )) c v = \u2211 v \u03b1 vz c z\nThe attention score is computed based on \"local\" atom representations c v from WLN.\n\n\nS2.4 Reaction Center Prediction\n\nThe WLN is trained to predict reaction center, a set of changes in graph connectivity that describe the difference between reactant molecules and major products. Mathematically, a reaction center is a set {(u, v, b)}, where (u, v) is a pair of atoms whose connecting bond has changed to type b. We predict the likelihood of (u, v, b) being in reaction center by passing atom representations from WLN through another neural network:\ns u,v,b = \u03c3 u T b \u03c4(M acu + M acv + P a c u + P a c v + M b f uv )\nThe above neural network is jointly optimized with WLN to minimize the cross entropy loss:\n\u2212 \u2211 u,v,b;u =v y u,v,b log s u,v,b + (1 \u2212 y u,v,b ) log(1 \u2212 s u,v,b )\nwhere y u,v,b = 1 iff (u, v, b) is in the reaction center, and the above loss sweeps over every pair of atoms and bond types (including no bond).\n\n\nS2.5 Candidate Ranking via Weisfeiler-Lehman Difference Network (WLDN)\n\nAt the stage of candidate reaction evaluation, we have a list of candidate products {p 0 , p 1 , \u00b7 \u00b7 \u00b7 , p m } given a set of reactant molecules r. The goal is to learn a scoring function that ranks the true product p 0 to be the highest. The challenge in ranking candidate products is again representational. We must learn to represent (r, p) in a manner that can focus on the key difference between the reactants r and products p, while also incorporating the necessary chemical contexts surrounding the changes.\n\nThe architecture of WLDN is designed to highlight such differences. Specifically, it has two components. The first component is a Siamese WLN that learns atom representation of reactant r and candidate products p i . Let c\n(p i ) v\nbe the learned atom representation of atom v in candidate product molecule p i . We define difference vector d\n(p i ) v\npertaining to atom v as follows:\nd (p i ) v = c (p i ) v \u2212 c (r) v\nBecause the reactants and products are atom-mapped, we can use v to refer to the same atom in different molecules. The second component of WLDN is another WLN that operates on the difference graph between reactants and products. A difference graph D(r, p i ) is defined as a molecular graph which has the graph structure as p i , with atom v's feature vector replaced by d\n(p i ) v .\nOperating on the difference graph has several benefits. First, in D(r, p i ), atom v's feature vector deviates from zero only if it is close to the reaction center, thus focusing the processing on the reaction center and its immediate context. Second, D(r, p i ) explicates neighbor dependencies between difference vectors. The WLDN maps this graph-based representation into a fixed-length vector, by applying the second WLN on top of D(r, p i ):\nh (p i ,l) v = \u03c4 U 1 h (p i ,l\u22121) v +U 2 \u2211 u\u2208N(v) \u03c4 V 1 h (p i ,l\u22121) u +V 2 f uv (1 \u2264 l \u2264 L) g (p i ) v = \u2211 u\u2208N(v) W 1 h (p i ,L) u W 2 f uv W 3 h (p i ,L) v where h (p i ,0) v = d (p i ) v .\nNote that though with the same notation, matrices U * ,V * ,W * are distinct parameters from the WLN used in the reaction center prediction. We use the same character for notational convenience.\n\nLet RC(p i ) = {(u i , v i , b i )}, the set of bonds that changed from the reactant r to product p i . The final score of candidate p i is:\ns(p i ) = u T \u03c4 M \u2211 v\u2208p i g (p i ) v + \u2211 (u,v,b)\u2208RC(p i ) s u,v,b\nCompared to 29, we augment the WLDN with the quantitative scores s u,v,b for each bond change in reaction center prediction. This is beneficial as the candidate outcomes produced by combinations of more likely bond changes are themselves more likely to be the true outcome.\n\n\nS3 Additional Results\n\n\nS3.1 Number of bond changes per reaction\n\nThe combinatorics of the enumeration scales poorly with the number of bond changes allowed per reaction. As stated in the main text, the number of candidates per reaction is bounded by\n5 \u2211 n=1 K n\nWhere we have allowed up to 5 simultaneous bond changes and K = 16 (i.e., select up to 5 bond changes from the 16 most likely bond changes) in our later evaluations. The choice of 5 was motivated by an analysis of the number of bond changes in training and validation examples shown in Table S1. We sacrifice 0.1-0.2% loss in maximum possible predictive accuracy through this limitation, but significantly restrict the number of candidates that must be ranked. To allow predictions of the remaining 0.1-0.2%, it would be possible to have a dynamic upper limit of the number of simul...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2019/sc/c8sc04228d"
    },
    {
      "title": "Reaction performance prediction with an extrapolative and interpretable graph model based on chemical knowledge",
      "text": "Reaction performance prediction with an extrapolative and interpretable graph model based on chemical knowledge\n\n[Download PDF](https://www.nature.com/articles/s41467-023-39283-x.pdf)\n\n[Download PDF](https://www.nature.com/articles/s41467-023-39283-x.pdf)\n\n### Subjects\n\n- [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n- [Method development](https://www.nature.com/subjects/method-development)\n- [Reaction mechanisms](https://www.nature.com/subjects/reaction-mechanisms)\n\n## Abstract\n\nAccurate prediction of reactivity and selectivity provides the desired guideline for synthetic development. Due to the high-dimensional relationship between molecular structure and synthetic function, it is challenging to achieve the predictive modelling of synthetic transformation with the required extrapolative ability and chemical interpretability. To meet the gap between the rich domain knowledge of chemistry and the advanced molecular graph model, herein we report a knowledge-based graph model that embeds the digitalized steric and electronic information. In addition, a molecular interaction module is developed to enable the learning of the synergistic influence of reaction components. In this study, we demonstrate that this knowledge-based graph model achieves excellent predictions of reaction yield and stereoselectivity, whose extrapolative ability is corroborated by additional scaffold-based data splittings and experimental verifications with new catalysts. Because of the embedding of local environment, the model allows the atomic level of interpretation of the steric and electronic influence on the overall synthetic performance, which serves as a useful guide for the molecular engineering towards the target synthetic function. This model offers an extrapolative and interpretable approach for reaction performance prediction, pointing out the importance of chemical knowledge-constrained reaction modelling for synthetic purpose.\n\n### Similar content being viewed by others\n\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\n\nArticleOpen access04 May 2023\n\n### [A generalized-template-based graph neural network for accurate organic reactivity prediction](https://www.nature.com/articles/s42256-022-00526-z?fromPaywallRec=false)\n\nArticle15 September 2022\n\n### [A meta-learning approach for selectivity prediction in asymmetric catalysis](https://www.nature.com/articles/s41467-025-58854-8?fromPaywallRec=false)\n\nArticleOpen access16 April 2025\n\n## Introduction\n\nThe chemical comprehension and accurate prediction of reactivity and selectivity provide the foundation for the rational and efficient exploration of massive synthetic space[1](https://www.nature.com/articles/s41467-023-39283-x#ref-CR1), [2](https://www.nature.com/articles/s41467-023-39283-x#ref-CR2).\u00a0This establishment of the structure\u2013performance relationship (SPR) has been focused on the reaction mechanism study and elucidation of the determining transition state model[3](https://www.nature.com/articles/s41467-023-39283-x#ref-CR3). Using the transition state model, chemists can elucidate the origins of the observed reactivity/selectivity trend and make synthetic judgments based on chemical theory and empirical experience[4](https://www.nature.com/articles/s41467-023-39283-x#ref-CR4). This classic knowledge-driven strategy has reached remarkable success in synthetic chemistry and continues to provide strong support for the discovery of new catalysts, reagents, and reaction[5](https://www.nature.com/articles/s41467-023-39283-x#ref-CR5). Despite the advantage of offering qualitative guidance in the synthetic universe, it is challenging for the knowledge-driven strategy to handle the high-dimensional SPR without a clear mechanistic basis and analytic equation. The seemingly subtle change in catalyst, additive, or even solvent may result in significant perturbation of the overall synthetic performance[6](https://www.nature.com/articles/s41467-023-39283-x#ref-CR6), [7](https://www.nature.com/articles/s41467-023-39283-x#ref-CR7). This is why laborious and repetitive condition optimization is still inevitably required, limiting the efficiency of synthetic development[8](https://www.nature.com/articles/s41467-023-39283-x#ref-CR8).\n\nThe data-driven approach has recently emerged as a powerful strategy for SPR establishment[9](https://www.nature.com/articles/s41467-023-39283-x#ref-CR9), [10](https://www.nature.com/articles/s41467-023-39283-x#ref-CR10). By harnessing the interrelationship within the synthetic data, modern machine learning (ML) algorithms can create powerful models for synthetic prediction. Accurate predictions of reaction yield[11](https://www.nature.com/www.nature.com#ref-CR11), [12](https://www.nature.com/www.nature.com#ref-CR12), [13](https://www.nature.com/www.nature.com#ref-CR13), [14](https://www.nature.com/articles/s41467-023-39283-x#ref-CR14), kinetic rate[15](https://www.nature.com/articles/s41467-023-39283-x#ref-CR15), [16](https://www.nature.com/articles/s41467-023-39283-x#ref-CR16) and activation energy[17](https://www.nature.com/www.nature.com#ref-CR17), [18](https://www.nature.com/www.nature.com#ref-CR18), [19](https://www.nature.com/articles/s41467-023-39283-x#ref-CR19), chemo-[20](https://www.nature.com/articles/s41467-023-39283-x#ref-CR20), regio-[21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/www.nature.com#ref-CR22), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/articles/s41467-023-39283-x#ref-CR25), and stereoselectivity[26](https://www.nature.com/www.nature.com#ref-CR26), [27](https://www.nature.com/www.nature.com#ref-CR27), [28](https://www.nature.com/www.nature.com#ref-CR28), [29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/articles/s41467-023-39283-x#ref-CR32). have been achieved in a wide array of organic transformations, which validated the exciting concept of ML prediction of synthetic performances. However, the ML prediction and design of synthetic transformation are still far from mature. One of the major bottlenecks is the availability of the molecular encoding approach and the ML framework that are suitable for SPR prediction (Fig.\u00a0[1a](https://www.nature.com/articles/s41467-023-39283-x#Fig1)). Quantum chemical descriptors[27](https://www.nature.com/articles/s41467-023-39283-x#ref-CR27) are known for their solid physical basis and high descriptive ability, but their application typically requires a sophisticated understanding of the underlying reaction mechanism, and the descriptor generation can be time- and resource-consuming for large-scale screening. The string- and topological structure-based encodings (i.e., SMILES, molecular fingerprints, etc.)[33](https://www.nature.com/articles/s41467-023-39283-x#ref-CR33), [34](https://www.nature.com/articles/s41467-023-39283-x#ref-CR34) do not require expert knowledge of the studied transformation and can be efficiently generated, while it is difficult to trace the physical organic origins of the synthetic performance. In addition, the extrapolation problem presents additional challenges for SPR prediction[35](https://www.nature.com/articles/s41467-023-39283-x#ref-CR35), [36](https://www.nature.com/articles/s41467-023-39283-x#ref-CR36). Current synthetic models still lack sufficient guidance for developing new catalysts and transformations.\n\n**Fig. 1: Machine learning prediction of synthetic performance and molecular property.**\n\n**a** Representative strategy of synthetic prediction by concatenating the molecular encodings of reactant 1 (orange), reactant 2 (blue), additive (green), and product (yellow). **b** Previous work of quantum chemistry-augmente...",
      "url": "https://www.nature.com/articles/s41467-023-39283-x"
    },
    {
      "title": "",
      "text": "The Catechol Benchmark: Time-series Solvent\nSelection Data for Few-shot Machine Learning\nToby Boyne1\u2217, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College London, London, UK1\nDepartment of Chemistry, Imperial College London, London, UK2\nSOLVE Chemistry, London, UK3\nAbstract\n1 Machine learning has promised to change the landscape of laboratory chem\u00022 istry, with impressive results in molecular property prediction and reaction retro\u00023 synthesis. However, chemical datasets are often inaccessible to the machine\n4 learning community as they tend to require cleaning, thorough understanding of the\n5 chemistry, or are simply not available. In this paper, we introduce a novel dataset\n6 for yield prediction, providing the first-ever transient flow dataset for machine\n7 learning benchmarking, covering over 1200 process conditions. While previous\n8 datasets focus on discrete parameters, our experimental set-up allow us to sample\n9 a large number of continuous process conditions, generating new challenges for\n10 machine learning models. We focus on solvent selection, a task that is particularly\n11 difficult to model theoretically and therefore ripe for machine learning applica\u000212 tions. We showcase benchmarking for regression algorithms, transfer-learning\n13 approaches, feature engineering, and active learning, with important applications\n14 towards solvent replacement and sustainable manufacturing.\n15 1 Introduction\n16 Machine learning (ML) and artificial intelligence (AI) have showcased enormous potential in em\u000217 powering the world of the natural sciences: from famous examples such as AlphaFold for protein\n18 predictions [1], to fusion reactor control [2], disease detection [3], battery design [4], and material\n19 discovery [5], among many more. However, we seldom see the machine learning community bench\u000220 mark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world\n21 data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how\n22 expensive the data can be to produce, resulting in many datasets being locked behind closed doors by\n23 large companies.\n24 AIchemy (https://aichemy.ac.uk) is an interdisciplinary UK hub with the mission of transform\u000225 ing the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as\n26 addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE\n27 Chemistry (https://www.solvechemistry.com), we present a first important step into addressing\n28 the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data\n29 machine learning algorithms for chemistry.\n30 Solvent selection is one of the biggest challenges for chemical manufacturing, with solvents often\n31 being the main source of waste in the manufacturing process [6]. Increased regulation on solvents and\n32 a drive to making process manufacturing more sustainable led to an interest in the discovery of greener\n\u2217\nt.boyne23@imperial.ac.uk ;\n\u2020\njose@solvechemistry.com\nSubmitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.\nFigure 1: Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the\nreaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement\nproducts. We investigate the yield of the reaction for a range of different solvents. Product 1 was not\nobserved and reacted immediately to form Product 2 and later 3.\n33 solvents and for improved solvent replacement tools. However, most of the solvent replacement tools\n34 focus purely on learning unsupervised representations of solvents, with the hope that experimentalists\n35 can find solvents with similar properties to replace those with environmental concerns. A much\n36 stronger approach would consider the interaction of a variety of different solvents with a reaction of\n37 interest to directly predict reaction yields, in such a way that the best possible solvent can be selected\n38 according to a yield-sustainability trade-off.\n39 Machine learning approaches have been shown to be a powerful tool for the prediction of chemical\n40 reaction conditions. Success has been reported in retro-synthesis [7, 8], condition recommendations\n41 [9], product predictions [10, 11], among others. While yield prediction has proven to be more difficult\n42 due to large inconsistencies in procedure and data reporting [12], we have still seen promising yield\n43 prediction results for smaller and more carefully curated datasets [13\u201316]. However, these datasets\n44 lack the continuous reaction conditions, such as temperature and residence time, that are required to\n45 scale-up processes to practical manufacturing conditions.\n46 In this paper, we release the first machine-learning-ready transient flow dataset, a framework that\n47 allows for quick and efficient screening of continuous reaction conditions. We specifically provide\n48 yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure 1, with dense\n49 measurements across the residence time, temperature, and solvent space. We answer the call for\n50 more flow chemistry reaction data [17], further showcase how this type of kinetic data poses new\n51 challenges to current machine learning methods for chemistry, and identify potential solutions.\n52 1.1 Related works\n53 Reaction datasets are common in chemistry research, but their suitability for machine learning\n54 benchmarking tends to be poor. This can be a result of improper formatting or documentation,\n55 incomplete information about reaction conditions or the experimental set-up, or the lack of machine\n56 readability, leading to limited usage by the ML community. However, some effort has been made\n57 to address this, with the biggest example being the creation of the Open Reaction Database (ORD)\n58 [18], a repository containing over 2M different reactions, many of which come from US patent data\n59 (USPTO) [19]. However, the dataset falls short in some aspects, in particular with respect to machine\n60 learning readiness and data inconsistencies across reactions.\n61 ORDerly [12] allows for easy cleaning and preparation of ORD data, showing the promise of the\n62 dataset for forward and retro-synthetic prediction using transformers; however, it also shows that\n63 yield prediction cannot be done well due to data inconsistencies. Schwaller et al. [13] drew similar\n64 conclusions when using the USPTO dataset, stating that reaction conditions such as temperature,\n65 concentrations, and duration have a significant effect on yield. The assumption that every reaction in\n66 the dataset is optimized for reaction parameters proved too loose, resulting in inaccurate predictive\n67 models for yield, and highlighting the importance of creating datasets with full (including potentially\n68 sub-optimal) reaction conditions.\n69 More relevant to our work, Perera et al. [20] introduced a dataset of 5760 Suzuki-Miyaura cross\u000270 coupling reactions, Ahneman et al. [21] introduced a dataset of 3956 Buchwald\u2013Hartwig aminations,\n71 and Prieto Kullmer et al. [22] investigated screening additives for Ni-catalysed reactions, all for the\n72 purposes of yield prediction. The datasets have been used in the benchmarking of Gaussian processes\n73 and Bayesian neural networks [14], deep learning models [13], language-model-based embeddings\n2\n74 [16], data augmentation techniques [23], and Bayesian optimisation [15]. In each case, the datasets\n75 focus on discrete reaction variables, such as ligand, base, additives, or reactants at fixed temperatures\n76 and residence times. We are instead introducing a dataset rich in continuous reaction conditions (in\n77 our case temperature ...",
      "url": "https://openreview.net/pdf?id=6l8q74TabE"
    }
  ]
}