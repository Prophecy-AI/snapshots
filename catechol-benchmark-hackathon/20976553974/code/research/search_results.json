{
  "query": "What are the best techniques for predicting chemical properties when test molecules are completely different from training molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting molecular properties for out\u2011of\u2011distribution (OOD) compounds\u2014where the test set contains chemistries that were never seen during training\u2014requires models that can transfer knowledge from large, inexpensive datasets to the scarce target task and that are explicitly calibrated for uncertainty.  The most effective strategies reported in recent literature combine **graph\u2011neural\u2011network (GNN) pre\u2011training with transfer learning**, **task\u2011similarity\u2011driven source selection**, and **regularisation that respects chemical substructure rules**.  Large\u2011scale pre\u2011training on low\u2011fidelity or related properties (e.g., quantum energies, high\u2011throughput screening data) yields atom\u2011wise embeddings that can be fine\u2011tuned on a small high\u2011fidelity set, improving sparse\u2011task performance by up to eight\u2011fold while using far fewer expensive labels\u202f([nature.com](https://www.nature.com/articles/s41467-024-45566-8)).  Similar gains are obtained by re\u2011using a SchNet\u2011derived GNN representation for diverse targets such as pK\u2090, \u00b9\u00b3C\u2011NMR and solubility\u202f([chemrxiv.org](https://chemrxiv.org/engage/chemrxiv/article-details/64f167e379853bbd78d62c1e)).  \n\nA critical complement to raw transfer learning is **quantifying how transferable a source task is** before fine\u2011tuning.  The Principal Gradient\u2011based Measurement (PGM) builds a \u201ctransferability map\u201d that measures the distance between the principal optimization gradients of source and target datasets, allowing rapid, optimization\u2011free selection of the most related source and avoiding negative transfer\u202f([nature.com](https://www.nature.com/articles/s42004-024-01169-4)).  An independent approach, MoTSE (Molecular Tasks Similarity Estimator), estimates task similarity from learned representations and has been shown to guide transfer learning and improve prediction accuracy on low\u2011data benchmarks\u202f([pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/36274947)).  \n\nTo further boost OOD robustness, **chemical\u2011knowledge\u2011aware regularisation** can be added to the loss.  The MolRuleLoss framework injects substructure\u2011substitution\u2011rule constraints into standard regression models, yielding 2\u201333\u202f% lower RMSE on lipophilicity, solubility and free\u2011energy tasks and markedly reducing catastrophic failures on unseen chemistries\u202f([arxiv.org](https://arxiv.org/abs/2511.08314)).  Probabilistic models such as DIONYSUS, which are calibrated on low\u2011data regimes, provide reliable uncertainty estimates that help flag predictions on molecules far from the training distribution\u202f([rsc.org](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  Finally, the BOOM benchmark suite quantifies OOD performance across many models, guiding practitioners toward architectures that generalise best to novel chemical space\u202f([arxiv.org](https://arxiv.org/abs/2505.01912)).  \n\nTogether, these techniques\u2014large\u2011scale GNN pre\u2011training, similarity\u2011driven source selection, substructure\u2011aware loss regularisation, calibrated probabilistic outputs, and rigorous OOD benchmarking\u2014constitute the current best practice for reliable property prediction on completely new molecules.",
      "url": ""
    },
    {
      "title": "Fast and effective molecular property prediction with transferability map",
      "text": "Fast and effective molecular property prediction with transferability map\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n### Subjects\n\n- [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n- [Drug discovery and development](https://www.nature.com/subjects/drug-discovery-and-development)\n\n## Abstract\n\nEffective transfer learning for molecular property prediction has shown considerable strength in addressing insufficient labeled molecules. Many existing methods either disregard the quantitative relationship between source and target properties, risking negative transfer, or require intensive training on target tasks. To quantify transferability concerning task-relatedness, we propose Principal Gradient-based Measurement (PGM) for transferring molecular property prediction ability. First, we design an optimization-free scheme to calculate a principal gradient for approximating the direction of model optimization on a molecular property prediction dataset. We have analyzed the close connection between the principal gradient and model optimization through mathematical proof. PGM measures the transferability as the distance between the principal gradient obtained from the source dataset and that derived from the target dataset. Then, we perform PGM on various molecular property prediction datasets to build a quantitative transferability map for source dataset selection. Finally, we evaluate PGM on multiple combinations of transfer learning tasks across 12 benchmark molecular property prediction datasets and demonstrate that it can serve as fast and effective guidance to improve the performance of a target task. This work contributes to more efficient discovery of drugs, materials, and catalysts by offering a task-relatedness quantification prior to transfer learning and understanding the relationship between chemical properties.\n\n### Similar content being viewed by others\n\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\n\nArticleOpen access04 May 2023\n\n### [A knowledge-guided pre-training framework for improving molecular representation learning](https://www.nature.com/articles/s41467-023-43214-1?fromPaywallRec=false)\n\nArticleOpen access21 November 2023\n\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://www.nature.com/articles/s42004-024-01155-w?fromPaywallRec=false)\n\nArticleOpen access05 April 2024\n\n## Introduction\n\nMolecular property prediction, which involves identifying molecules with desired properties[1](https://www.nature.com/articles/s42004-024-01169-4#ref-CR1), [2](https://www.nature.com/articles/s42004-024-01169-4#ref-CR2), poses a critical challenge prevalent across various scientific fields. It holds particular significance in chemistry for designing drugs, catalysts, and materials. In recent years, artificial intelligence (AI) technologies have come mainstream in this area, and AI-guided chemical design can efficiently explore chemical space while improving performance based on experimental feedback, showing promise from laboratory research to real-world industry applications[3](https://www.nature.com/articles/s42004-024-01169-4#ref-CR3). However, it is common that the experimental data size is small as producing labeled data requires time-consuming and expensive experiments[4](https://www.nature.com/articles/s42004-024-01169-4#ref-CR4), [5](https://www.nature.com/articles/s42004-024-01169-4#ref-CR5). In contrast, transfer learning[6](https://www.nature.com/articles/s42004-024-01169-4#ref-CR6) has become a powerful paradigm for addressing data scarcity problem by exploiting the knowledge from related datasets across fields such as natural language processing[7](https://www.nature.com/articles/s42004-024-01169-4#ref-CR7), [8](https://www.nature.com/articles/s42004-024-01169-4#ref-CR8), computer vision[9](https://www.nature.com/articles/s42004-024-01169-4#ref-CR9), [10](https://www.nature.com/articles/s42004-024-01169-4#ref-CR10), and biomedcine[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12). In chemistry, transfer learning leverages pre-trained models on extensive or related datasets to facilitate efficient exploration of vast chemical space[13](https://www.nature.com/articles/s42004-024-01169-4#ref-CR13), [14](https://www.nature.com/articles/s42004-024-01169-4#ref-CR14) for various downstream tasks. It has been used to predict properties[15](https://www.nature.com/articles/s42004-024-01169-4#ref-CR15), [16](https://www.nature.com/articles/s42004-024-01169-4#ref-CR16), plan synthesis[17](https://www.nature.com/articles/s42004-024-01169-4#ref-CR17), [18](https://www.nature.com/articles/s42004-024-01169-4#ref-CR18), and explore the space of chemical reactions[19](https://www.nature.com/www.nature.com#ref-CR19), [20](https://www.nature.com/www.nature.com#ref-CR20), [21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/articles/s42004-024-01169-4#ref-CR22).\n\nTransfer learning can enhance molecular property prediction in limited data sets by borrowing knowledge from sufficient source data sets, thus improving both model accuracy and computation efficiency. Although several previous works have explored the power of transfer learning to enhance molecular property prediction[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/articles/s42004-024-01169-4#ref-CR25), challenges remain. One major challenge is negative transfer, which occurs when the performance after transfer learning is adversely affected due to minimal similarity between the source and target tasks[26](https://www.nature.com/articles/s42004-024-01169-4#ref-CR26), [27](https://www.nature.com/articles/s42004-024-01169-4#ref-CR27). For example, Hu et al.[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23) observed that pretrained GNN (at both node-level and graph-level) performed well but yielded negative transfer when pretrained at the level of either entire graphs or individual nodes. Additionally, some supervised pre-training tasks unrelated to the downstream task of interest can even degrade the downstream performance[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23), [28](https://www.nature.com/articles/s42004-024-01169-4#ref-CR28).\n\nNegative transfer primarily stems from suboptimal model and layer choices, as well as insufficient task relatedness, highlighting the need to evaluate transferability prior to applying transfer learning. In computer vision, some researchers have recently focused on selecting the best model from a pool of options by estimating the transferability of each model[29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/articles/s42004-024-01169-4#ref-CR32). In molecular property prediction, recent efforts involve investigating the relatedness of the source task to the target task. To maximize the performance on a target task and prevent negative transfer, existing methods mainly rely on a molecular distance metric to measure the similarity of molecules, such as Tanimoto coefficient (based on molecular fingerprint)[33](https://www.nature.com/articles/s42004-024-01169-4#ref-CR33), [34](https://www.nature.com/articles/s42004-024-01169-4#ref-CR34) and a chemical distance measure (based on fingerprint and subgraph)[35](https://www.nature.com/articles/s42004-024-01169-...",
      "url": "https://www.nature.com/articles/s42004-024-01169-4"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2505.01912] BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2505.01912\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2505.01912**(cs)\n[Submitted on 3 May 2025 ([v1](https://arxiv.org/abs/2505.01912v1)), last revised 19 Dec 2025 (this version, v2)]\n# Title:BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nAuthors:[Evan R. Antoniuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Antoniuk,+E+R),[Shehtab Zaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaman,+S),[Tal Ben-Nun](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Nun,+T),[Peggy Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[James Diffenderfer](https://arxiv.org/search/cs?searchtype=author&amp;query=Diffenderfer,+J),[Busra Sahin](https://arxiv.org/search/cs?searchtype=author&amp;query=Sahin,+B),[Obadiah Smolenski](https://arxiv.org/search/cs?searchtype=author&amp;query=Smolenski,+O),[Tim Hsu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsu,+T),[Anna M. Hiszpanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Hiszpanski,+A+M),[Kenneth Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+K),[Bhavya Kailkhura](https://arxiv.org/search/cs?searchtype=author&amp;query=Kailkhura,+B),[Brian Van Essen](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Essen,+B)\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n[View PDF](https://arxiv.org/pdf/2505.01912)[HTML (experimental)](https://arxiv.org/html/2505.01912v2)> > Abstract:\n> Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\\mathbf{BOOM}$, $\\mathbf{b}$enchmarks for $\\mathbf{o}$ut-$\\mathbf{o}$f-distribution $\\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at [> this https URL\n](https://github.com/FLASK-LLNL/BOOM)> Subjects:|Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2505.01912](https://arxiv.org/abs/2505.01912)[cs.LG]|\n|(or[arXiv:2505.01912v2](https://arxiv.org/abs/2505.01912v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2505.01912](https://doi.org/10.48550/arXiv.2505.01912)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Evan Antoniuk [[view email](https://arxiv.org/show-email/6abb5e36/2505.01912)]\n**[[v1]](https://arxiv.org/abs/2505.01912v1)**Sat, 3 May 2025 19:51:23 UTC (35,134 KB)\n**[v2]**Fri, 19 Dec 2025 23:00:10 UTC (16,070 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2505.01912)\n* [HTML (experimental)](https://arxiv.org/html/2505.01912v2)\n* [TeX Source](https://arxiv.org/src/2505.01912)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2505.01912&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2505.01912&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-05](https://arxiv.org/list/cs.LG/2025-05)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/2505.01912?context=cond-mat)\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2505.01912?context=cond-mat.mtrl-sci)\n[cs](https://arxiv.org/abs/2505.01912?context=cs)\n[cs.AI](https://arxiv.org/abs/2505.01912?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.01912)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.01912)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.01912)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2505.01912&amp;description=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2505.01912&amp;title=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*...",
      "url": "https://arxiv.org/abs/2505.01912"
    },
    {
      "title": "Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2511.08314** (cs)\n\n\\[Submitted on 11 Nov 2025\\]\n\n# Title:Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework\n\nAuthors: [Xiaoyu Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan,+X), [Lin Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo,+L), [Ruizhen Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+R), [Yang Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+Y), [Zhihao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+Z), [Boxue Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+B)\n\nView a PDF of the paper titled Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework, by Xiaoyu Fan and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.08314)\n\n> Abstract:Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for \"activity cliff\" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2511.08314](https://arxiv.org/abs/2511.08314) \\[cs.LG\\] |\n| (or [arXiv:2511.08314v1](https://arxiv.org/abs/2511.08314v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.08314](https://doi.org/10.48550/arXiv.2511.08314) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Boxue Tian \\[ [view email](https://arxiv.org/show-email/f7cab571/2511.08314)\\] **\\[v1\\]**\nTue, 11 Nov 2025 14:44:07 UTC (4,363 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework, by Xiaoyu Fan and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.08314)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.08314&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.08314&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-11](https://arxiv.org/list/cs.LG/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.08314?context=cs) [cs.AI](https://arxiv.org/abs/2511.08314?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.08314)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.08314)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.08314)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.08314) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2511.08314"
    },
    {
      "title": "Improving molecular property prediction through a task similarity enhanced transfer learning strategy - PubMed",
      "text": "<div><div>\n \n <main>\n \n \n \n \n \n \n \n<header>\n \n \n \n <div>\n \n \n<h2>\n \n \n \n \n \n \n Improving molecular property prediction through a task similarity enhanced transfer learning strategy\n \n \n</h2>\n \n \n<p><span>\n \n \n <span><span>Han Li</span><span>\u00a0et al.</span></span>\n \n \n </span>\n \n \n <span>\n iScience<span>.</span>\n </span>\n \n <span>\n <time>2022</time><span>.</span>\n </span>\n \n \n \n</p>\n \n \n \n </div>\n \n \n</header>\n \n \n \n <div>\n \n <h2>\n Abstract\n \n </h2>\n \n \n \n <p>\n \n Deeply understanding the properties (e.g., chemical or biological characteristics) of small molecules plays an essential role in drug development. A large number of molecular property datasets have been rapidly accumulated in recent years. However, most of these datasets contain only a limited amount of data, which hinders deep learning methods from making accurate predictions of the corresponding molecular properties. In this work, we propose a transfer learning strategy to alleviate such a data scarcity problem by exploiting the similarity between molecular property prediction tasks. We introduce an effective and interpretable computational framework, named MoTSE (Molecular Tasks Similarity Estimator), to provide an accurate estimation of task similarity. Comprehensive tests demonstrated that the task similarity derived from MoTSE can serve as useful guidance to improve the prediction performance of transfer learning on molecular properties. We also showed that MoTSE can capture the intrinsic relationships between molecular properties and provide meaningful interpretability for the derived similarity.\n </p>\n \n \n \n \n \n \n \n \n \n \n <p>\n \n <strong>\n Keywords:\n </strong>\n \n Artificial intelligence; Bioinformatics; Computational chemistry; Drugs.\n </p>\n \n \n </div>\n \n \n <p>\n \u00a9 2022 The Author(s).\n </p>\n <p>\n <a href=\"https://pubmed.ncbi.nlm.nih.gov/disclaimer/\">PubMed Disclaimer</a>\n</p>\n \n <div>\n <h2>\n Conflict of interest statement\n </h2>\n <p>J.Z. is a founder of the Silexon AI Technology Co. Ltd and has an equity interest.</p>\n </div>\n \n \n \n \n <div>\n <h2>\n Figures\n </h2>\n <div>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/5def2f8abab6/fx1.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Graphical abstract\n </strong>\n \n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/93388606cf30/gr1.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a01\n </strong>\n \n \n <p>An illustrative diagram of MoTSE (A) Given a task, MoTSE first pre-trains a GNN model using the corresponding dataset in a supervised manner. (B) By means of a probe dataset, MoTSE extracts the task-related knowledge from the pre-trained GNN and projects the task into a latent task space. The knowledge extraction is achieved by two methods: an attribution method extracting the task-related local knowledge by assigning importance scores to atoms in molecules; and a molecular representation similarity analysis (MRSA) method extracting the task-related global knowledge by pair-wisely measuring the similarity between molecular representations. (C) Finally, MoTSE calculates the similarity between tasks by measuring the distances between the corresponding vectors in the task space.</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/7e14488c9c22/gr2.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a02\n </strong>\n \n \n <p>Schematic illustration of different learning\u00a0strategies (A)\u00a0Training from scratch directly trains a model on the dataset of each target task without exploiting any extra knowledge. (B)\u00a0Multitask learning learns the target task and source tasks simultaneously. (C)\u00a0Self-supervised learning first leverages a proxy task to learn general knowledge from a large-scale unlabeled dataset and then finetunes the pre-trained model on the dataset of the target task. (D) MoTSE-guided transfer learning first pre-trains a model on the most similar task with the target task according to the task similarity estimated by MoTSE and then finetuned the pre-trained model on the dataset of the target task. stands for the dataset for the target task, and stand for the datasets for the source tasks, and stands for the large-scale unlabeled dataset. The numbers between the datasets represent the similarity estimated by MoTSE between the corresponding tasks.</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/013a06576f26/gr3.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a03\n </strong>\n \n \n <p>MoTSE outperforms baseline methods and alleviates negative transfer on the QM9 and PCBA datasets (A) The prediction performance of MoTSE and eleven baseline methods on the QM9 and PCBA datasets, measured in terms of R<sup>2</sup> and AUPRC, respectively. (B) The prediction performance of eleven transfer learning methods versus that of the Scratch method on the QM9 and PCBA datasets. (C) The comparison results of R<sup>2</sup> between MoTSE and eleven baseline methods on the QM9 dataset after filtering every molecule from the test set if it has a Tanimoto similarity score greater than 0.8 to any molecule in the training set (also see Figure\u00a0S4A). (D) The comparison results of AUPRC between MoTSE and eleven baseline methods on an unbalanced PCBA dataset with only positive samples (also see Figure\u00a0S4B).</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/eae09b5c65cb/gr4.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a04\n </strong>\n \n \n <p>The prediction performance of MoTSE and baseline methods on the FreeSolv, BACE, and HOPV datasets (A) The comparison results between MoTSE and eleven baseline methods on the FreeSolv dataset, measured in terms of root-mean-square-error (RMSE). (B) The comparison results between MoTSE and eleven baseline methods on the BACE dataset, measured in terms of AUPRC. (C) The comparison results between MoTSE and eleven baseline methods on the HOPV dataset, measured in terms of R<sup>2</sup>. (D) The prediction performance of eleven transfer learning methods versus that of the Scratch method on the HOPV dataset, measured in terms of R<sup>2</sup>.</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/9c4763ef9b48/gr5.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a05\n </strong>\n \n \n <p>The task similarity derived from MoTSE is generalizable across models with different architectures and datasets with different data distributions (A-C) The comparison results between MoTSE and baseline methods on the QM9 and PCBA datasets (measured in terms of R<sup>2</sup> and AUPRC, respectively), using the graph attention network (GAT), fully connected network (FCN), and recurrent neural network (RNN) models, respectively. (D) The comparison results between MoTSE and baseline methods on the Alchemy dataset, measured in terms of R<sup>2</sup>. (E) The similarity trees constructed based on the task similarity estimated by MoTSE on the QM9 and Alchemy datasets, respectively.</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/541000a7fe80/gr6.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a06\n </strong>\n \n \n <p>The similarity estimated by MoTSE between four physical chemistry tasks and the example molecules with importance scores assigned by the attribution method employed in MoTSE The numbers between tasks denote the task similarity derived from MoTSE. In the visualized molecules, darker colors represent higher importance scores. See the main text for the definitions of the four physical chemistry task.</p>\n \n </figcaption>\n \n </figure>\n \n <figure>\n <a href=\"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/0dbf/9579493/38e321f8c5d2/gr7.jpg\">\n \n </a>\n \n \n \n \n \n <figcaption>\n \n \n \n <strong>\n Figure\u00a07\n </strong>\n \n \n <p>Measuring and interpreting ...",
      "url": "https://pubmed.ncbi.nlm.nih.gov/36274947"
    },
    {
      "title": "Transfer Learning Graph Representations of Molecules for pKa, 13C-NMR, and Solubility",
      "text": "[Back to\\\nTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\n\nSearch within Theoretical and Computational Chemistry\n\n# Transfer Learning Graph Representations of Molecules for pKa, 13C-NMR, and Solubility\n\n01 September 2023, Version 1\n\nThis is not the most recent version. There is a [newer version](https://chemrxiv.org/engage/chemrxiv/article-details/6584fe019138d231613a8a68) of this content available\n\nWorking Paper\n\n## Authors\n\n- [Amer Marwan El Samman](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Amer%20Marwan%20El%20Samman),\n- [Stefano De Castro](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stefano%20De%20Castro),\n- [Brooke Morton](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Brooke%20Morton),\n- [Stijn De Baerdemacker](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Stijn%20De%20Baerdemacker)\n\n[Show author details](https://chemrxiv.org/chemrxiv.org)\n\nThis content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nWe explore transfer learning models from a pre-trained graph convoluntional neural network representation of molecules, obtained from SchNet, 1 to predict 13 C-NMR, pKa, and logS sol- ubility. SchNet learns a graph representation of a molecule by associating each atom with an \u201cembedding vector\u201d and interacts the atom-embeddings with each other by leveraging graph- convolutional filters on their interatomic distances. We pre-trained SchNet on molecular energy and demonstrate that the pre-trained atomistic embeddings can then be used as a transferable representation for a wide array of properties. On the one hand, for atomic properties such as micro-pK1 and 13 C-NMR, we investigate two models, one linear and one neural net, that inputs pre-trained atom-embeddings of a particular atom (e.g. carbon) and predicts a local property (e.g. 13 C-NMR). On the other hand, for molecular properties such as solubility, a size-extensive graph model is built using the embeddings of all atoms in the molecule as input. For all cases, qualitatively correct predictions are made with relatively little training data (< 1000 training points), showcasing the ease with which pre-trained embeddings pick up on important chemical patterns. The proposed models successfully capture well-understood trends of pK1 and solu- bility. This study advances our understanding of current neural net graph representations and their capacity for transfer learning applications in chemistry.\n\n## Keywords\n\n[machine learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=machine%20learning)\n\n[transfer learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=transfer%20learning)\n\n[pKa](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=pKa)\n\n[NMR](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=NMR)\n\n[logS](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=logS)\n\n## Supplementary weblinks\n\n**Title**\n\n**Description**\n\n**Actions**\n\n**Title**\n\nSchNet Model Embedding Vectors of QM9 Atoms Labelled According to Functional Groups Designation\n\n**Description**\n\nEmbedding vectors for all atoms in the first 10k molecules in the QM9 dataset, generated by a trained SchNet model Also contains the model which the embedding vectors were extracted from . Model was trained on 100k training points (molecules) and 10k validation points of QM9.\n\n**Actions**\n\n[**View**](https://doi.org/10.25545/EK1EQA)\n\n**Title**\n\nTransfer Learning Graph Representations of Molecules for pKa 13C NMR and Solubility.\n\n**Description**\n\nJupyter Notebooks with models used for Transfer Learning Graph Representations of Molecules for pKa 13C NMR and Solubility.\n\n**Actions**\n\n[**View**](https://github.com/amerelsamman/Transfer-Learning-Graph-Representations-%20of-Molecules-for-pKa-13C-NMR-and-Solubility.)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\n[Dec 22, 2023 Version\\\n2](https://chemrxiv.org/engage/chemrxiv/article-details/6584fe019138d231613a8a68)\n\nSep 01, 2023 Version 1\n\n## Metrics\n\n1,355\n\n696\n\n0\n\nViews\n\nDownloads\n\nCitations\n\n## License\n\nCC\n\nBY\n\nNC\n\nThe content is available under\n[CC BY NC 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by-nc/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2023-hfcm5\\\nD O I: 10.26434/chemrxiv-2023-hfcm5 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2023-hfcm5)\n\n## Funding\n\n**Canada Research Chairs**\n\n**Canada Foundation for Innovation**\n\n**New Brunswick Innovation Foundation**\n\n**Natural Sciences and Engineering Research Council of Canada**\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/64f167e379853bbd78d62c1e"
    },
    {
      "title": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting",
      "text": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n\n### Subjects\n\n- [Computational biology and bioinformatics](https://www.nature.com/subjects/computational-biology-and-bioinformatics)\n- [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n- [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n- [Quantum mechanics](https://www.nature.com/subjects/quantum-mechanics)\n\n## Abstract\n\nWe investigate the potential of graph neural networks for transfer learning and improving molecular property prediction on sparse and expensive to acquire high-fidelity data by leveraging low-fidelity measurements as an inexpensive proxy for a targeted property of interest. This problem arises in discovery processes that rely on screening funnels for trading off the overall costs against throughput and accuracy. Typically, individual stages in these processes are loosely connected and each one generates data at different scale and fidelity. We consider this setup holistically and demonstrate empirically that existing transfer learning techniques for graph neural networks are generally unable to harness the information from multi-fidelity cascades. Here, we propose several effective transfer learning strategies and study them in transductive and inductive settings. Our analysis involves a collection of more than 28 million unique experimental protein-ligand interactions across 37 targets from drug discovery by high-throughput screening and 12 quantum properties from the dataset QMugs. The results indicate that transfer learning can improve the performance on sparse tasks by up to eight times while using an order of magnitude less high-fidelity training data. Moreover, the proposed methods consistently outperform existing transfer learning strategies for graph-structured data on drug discovery and quantum mechanics datasets.\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-022-00447-x/MediaObjects/42256_2022_447_Fig1_HTML.png)\n\n### [Molecular contrastive learning of representations via graph neural networks](https://www.nature.com/articles/s42256-022-00447-x?fromPaywallRec=false)\n\nArticle03 March 2022\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-023-00825-5/MediaObjects/42004_2023_825_Fig1_HTML.png)\n\n### [Hierarchical Molecular Graph Self-Supervised Learning for property prediction](https://www.nature.com/articles/s42004-023-00825-5?fromPaywallRec=false)\n\nArticleOpen access17 February 2023\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-024-01169-4/MediaObjects/42004_2024_1169_Fig1_HTML.png)\n\n### [Fast and effective molecular property prediction with transferability map](https://www.nature.com/articles/s42004-024-01169-4?fromPaywallRec=false)\n\nArticleOpen access17 April 2024\n\n## Introduction\n\nWe investigate the potential of graph neural networks (GNNs) for transfer learning and improved molecular property prediction in the context of funnels or screening cascades characteristic of drug discovery and/or molecular design. GNNs have emerged as a powerful and widely-used class of algorithms for molecular property prediction thanks to their natural ability to learn from molecular structures represented as atoms and bonds[1](https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ece5dd68-d0cb-44df-a052-aafc9c6fdfd1#ref-CR1), [2](https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ece5dd68-d0cb-44df-a052-aafc9c6fdfd1#ref-CR2), [3](https://www.nature.com/articles/s41467-024-45566-8#ref-CR3), as well as in the life sciences in general[4](https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ece5dd68-d0cb-44df-a052-aafc9c6fdfd1#ref-CR4), [5](https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ece5dd68-d0cb-44df-a052-aafc9c6fdfd1#ref-CR5), [6](https://www.nature.com/articles/s41467-024-45566-8#ref-CR6). However, their potential for transfer learning is yet to be established. The screening cascade refers to a multi-stage approach where one starts with cheap and relatively noisy methods (high-throughput screening, molecular mechanics calculations, etc.) that allow for screening a large number of molecules. This is followed by increasingly accurate and more expensive evaluations that come with much lower throughput, up to the experimental characterisation of compounds. Individual stages or tiers in the screening funnel are, thus, used to make a reduction of the search space and focus the evaluation of more expensive properties on the promising regions. In this way, the funnel maintains a careful trade-off between the scale, cost, and accuracy. The progression from one tier to another is typically done manually by selecting subsets of molecules from the library screened at the previous stage or via a surrogate model that focuses the screening budget of the next step on the part of the chemical space around the potential hits. Such surrogate models are typically built using the data originating from a single tier and, thus, without leveraging measurements of different fidelity.\n\nFor efficient use of experimental resources, it is beneficial to have good predictive models operating on sparse datasets and guiding the high-fidelity evaluations relative to properties of interest. The latter is the most expensive part of the funnel and to efficiently support it, we consider it in a transfer learning setting designed to leverage low-fidelity observations to improve the effectiveness of predictive models on sparse and high-fidelity experimental data. In drug discovery applications of this setup, low-fidelity measurements can be seen as ground truth values that have been corrupted by noise, experimental or reading artefacts, or are simply performed using less precise but cheaper experiments. For quantum mechanics simulations, low-fidelity data typically corresponds to approximations or truncations of more complex and computationally-expensive calculations, such that the low- and high-fidelity labels are closely related. Thus, it is natural to expect that incorporating low-fidelity measurements as an input feature into a high-fidelity model typically improves the performance on sparse tasks relative to predictors learnt using the high-fidelity data alone. Despite its apparent simplicity, even in the transductive learning setting (i.e., low-fidelity and high-fidelity labels are available for all data points), it is not trivial to define an adequate workflow that jointly uses both low- and high-fidelity labels. For instance, devising an end-to-end training scheme with low- and high-fidelity labels as part of the same model can be challenging for drug discovery applications, where the disparity between the numbers of respective observations is larger than two orders-of magnitude (e.g., the number of high-fidelity observations can be over 500 times lower than that of the low-fidelity ones). Previous work has successfully applied multi-fidelity learning on several problems[7](https://www.nature.com/articles/s41467-024-45566-8#ref-CR7), but as we show in \u201cComparison with the multi-fidelity state embedding algorithm\u201d section that approach is unfortunately not effective in drug discovery. While successfully exploiting multi-fidelity data in the transductive setting is valuable on its own, virtually all high-throughput screening steps in drug discovery are followed by experiments generating high-fidelity measurements for molecules that were not part of the original screening cascade, i.e., lacking low-fidelity labels. Devising an in silico model of the low-fideli...",
      "url": "https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ece5dd68-d0cb-44df-a052-aafc9c6fdfd1"
    },
    {
      "title": "Inductive transfer learning for molecular activity prediction: Next-Gen QSAR Models with MolPMoFiT",
      "text": "Search all BMC articles\n\nSearch\n\nInductive transfer learning for molecular activity prediction: _Next_- _Gen QSAR Models with MolPMoFiT_\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-020-00430-x.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-020-00430-x.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-020-00430-x.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-020-00430-x.epub)\n\n- Research article\n- [Open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 22 April 2020\n\n# Inductive transfer learning for molecular activity prediction: _Next_- _Gen QSAR Models with MolPMoFiT_\n\n- [Xinhao Li](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#auth-Xinhao-Li-Aff1) [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#Aff1) &\n- [Denis Fourches](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#auth-Denis-Fourches-Aff1)[ORCID: orcid.org/0000-0001-5642-8303](http://orcid.org/0000-0001-5642-8303)[1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#Aff1)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a012**, Article\u00a0number:\u00a027 (2020)\n[Cite this article](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#citeas)\n\n- 13k Accesses\n\n- 74 Citations\n\n- 17 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x/metrics)\n\n\n## Abstract\n\nDeep neural networks can directly learn from chemical structures without extensive, user-driven selection of descriptors in order to predict molecular properties/activities with high reliability. But these approaches typically require large training sets to learn the endpoint-specific structural features and ensure reasonable prediction accuracy. Even though large datasets are becoming the new normal in drug discovery, especially when it comes to high-throughput screening or metabolomics datasets, one should also consider smaller datasets with challenging endpoints to model and forecast. Thus, it would be highly relevant to better utilize the tremendous compendium of unlabeled compounds from publicly-available datasets for improving the model performances for the user\u2019s particular series of compounds. In this study, we propose the **Mol** ecular **P** rediction **Mo** del **Fi** ne- **T** uning ( **MolPMoFiT**) approach, an effective transfer learning method based on self-supervised pre-training\u2009+\u2009task-specific fine-tuning for QSPR/QSAR modeling. A large-scale molecular structure prediction model is pre-trained using one million unlabeled molecules from ChEMBL in a self-supervised learning manner, and can then be fine-tuned on various QSPR/QSAR tasks for smaller chemical datasets with specific endpoints. Herein, the method is evaluated on four benchmark datasets (lipophilicity, FreeSolv, HIV, and blood\u2013brain barrier penetration). The results showed the method can achieve strong performances for all four datasets compared to other _state_- _of_- _the_- _art_ machine learning modeling techniques reported in the literature so far.\n\n![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13321-020-00430-x/MediaObjects/13321_2020_430_Figa_HTML.png)\n\n## Introduction\n\nPredicting properties/activities of chemicals from their structures is one of the key objectives in cheminformatics and molecular modeling. Quantitative structure property/activity relationship (QSPR/QSAR) modeling \\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR1), [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR2), [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR3), [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR4), [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR5), [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR6)\\] relies on machine learning techniques to establish quantified links between molecular structures and their experimental properties/activities. When using a classic machine learning approach, the training process is divided into two main steps: feature extraction/calculation and the actual modeling. The features (also called _descriptors_) characterizing the molecular structures are critical for the model performances. They typically encompass 2D molecular fingerprints, topological indices, or substructural fragments, as well as more complex 3D and 4D descriptors \\[ [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR8)\\] directly computed from the molecular structures \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR9)\\].\n\nDeep learning methods have demonstrated remarkable performances in several QSPR/QSAR case studies. In addition to use expert-engineered molecular descriptors as input, those techniques can also directly take molecular structures ( _e.g.,_ molecular graph \\[ [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR10), [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR12), [13](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR13), [14](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR14), [15](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR15), [16](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR16), [17](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR18), [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR20), [21](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR21)\\], SMILES strings \\[ [22](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR22), [23](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR23), [24](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR24)\\], and molecular 2D/3D grid image \\[ [25](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR25), [26](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR26), [27](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR27), [28](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR28), [29](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR29), [30](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR30)\\]) and learn the data-driven feature representations for predicting properties/activities. As a result, this type of approach is potentially able to capture and extract underlying, complex structural patterns and feature \u2194 property relationships given sufficient amount of training data. The knowledge derived from these dataset-specific descriptors can then be used to better interpret and understand the structure\u2013property relationships as well as to design new compounds. In a large scale benchmark study, Yang et al. \\[ [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR12)\\] shown that a graph convolutional model that construct a learned representation from molecular graph consistently matches or outperforms models trained with expert-engineered molecular de...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x"
    }
  ]
}