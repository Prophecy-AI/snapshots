{
  "query": "What are the best techniques for transfer learning in molecular property prediction with very small datasets of 20-30 molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Transfer learning for molecular property prediction when only\u202f20\u201130 labelled molecules are available relies on first building a rich, general\u2011purpose representation on a much larger, related dataset and then adapting it to the tiny target set.  The most successful pipelines combine (i) a **graph\u2011based neural network** such as the message\u2011passing model\u202fPaiNN or a graph\u2011convolutional network (GCN) that is pretrained on millions of molecules with inexpensive labels (e.g., semi\u2011empirical energies) and (ii) **fine\u2011tuning** on the small target set, often with the early layers frozen to preserve the learned chemical features\u202f([arXiv\u202f2024\u201104\u201113393](https://arxiv.org/abs/2404.13393);\u202f[arXiv\u202f2021\u201107\u201110882](https://arxiv.org/abs/2107.10882)).  Normalising both source and target labels to zero mean and unit variance helps align the distributions and avoid negative transfer.  When deep models are still too data\u2011hungry, a hybrid approach that augments the learned embeddings with **hand\u2011crafted descriptors** (e.g., SOAP + simple physicochemical descriptors) fed to a gradient\u2011boosting regressor can match or exceed pure GNN performance on very small sets\u202f([arXiv\u202f2024\u201104\u201113393](https://arxiv.org/abs/2404.13393)).\n\nChoosing an appropriate source task is critical.  Recent work proposes **quantitative task\u2011similarity estimators** such as MoTSE, which compute a similarity score between source and target properties and guide the selection of the most transferable pre\u2011training data\u202f([PMC\u202f2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9579493)).  A related \u201ctransferability map\u201d based on the **principal gradient** of model optimisation (PGM) can rapidly assess the distance between source and target gradients, allowing researchers to pick source datasets that are likely to improve performance without costly experiments\u202f([Nature\u202f2024\u201104\u201117](https://www.nature.com/articles/s42004-024-01169-4)).  Multi\u2011fidelity strategies\u2014pre\u2011training on cheap, low\u2011level quantum calculations and fine\u2011tuning on higher\u2011level or experimental data\u2014have also shown gains, especially when the fidelity gap is modest\u202f([Nature\u202f2024\u201102\u201126](https://www.nature.com/articles/s41467-024-45566-8)).\n\nIn practice, a robust workflow for a 20\u201130\u2011molecule problem would be: (1) select a large public dataset whose property is chemically related (guided by MoTSE or PGM); (2) pre\u2011train a GNN (e.g., PaiNN or a Chemprop model) on that dataset, optionally freezing the first\u202f2\u20113 message\u2011passing layers; (3) fine\u2011tune the remaining layers on the target set, using early\u2011stopping and label normalisation; and (4) if the fine\u2011tuned model still overfits, augment the learned embeddings with SOAP or other descriptors and train a gradient\u2011boosting regressor.  Probabilistic models such as DIONYSUS can further improve calibration on low\u2011data regimes\u202f([RSC\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  This combination of careful source\u2011task selection, multi\u2011task pre\u2011training, and hybrid descriptor\u2011based fine\u2011tuning constitutes the current best practice for transfer learning with only a few dozen molecules.",
      "url": ""
    },
    {
      "title": "Transfer Learning for Molecular Property Predictions from Small Data Sets",
      "text": "[View PDF](https://arxiv.org/pdf/2404.13393) [HTML (experimental)](https://arxiv.org/html/2404.13393v2)\n\n> Abstract:Machine learning has emerged as a new tool in chemistry to bypass expensive experiments or quantum-chemical calculations, for example, in high-throughput screening applications. However, many machine learning studies rely on small data sets, making it difficult to efficiently implement powerful deep learning architectures such as message passing neural networks. In this study, we benchmark common machine learning models for the prediction of molecular properties on two small data sets, for which the best results are obtained with the message passing neural network PaiNN, as well as SOAP molecular descriptors concatenated to a set of simple molecular descriptors tailored to gradient boosting with regression trees. To further improve the predictive capabilities of PaiNN, we present a transfer learning strategy that uses large data sets to pre-train the respective models and allows to obtain more accurate models after fine-tuning on the original data sets. The pre-training labels are obtained from computationally cheap ab initio or semi-empirical models and both data sets are normalized to mean zero and standard deviation one to align the labels' distributions. This study covers two small chemistry data sets, the Harvard Organic Photovoltaics data set (HOPV, HOMO-LUMO-gaps), for which excellent results are obtained, and on the Freesolv data set (solvation energies), where this method is less successful, probably due to a complex underlying learning task and the dissimilar methods used to obtain pre-training and fine-tuning labels. Finally, we find that for the HOPV data set, the final training results do not improve monotonically with the size of the pre-training data set, but pre-training with fewer data points can lead to more biased pre-trained models and higher accuracy after fine-tuning.\n\n## Submission history\n\nFrom: Thorren Kirschbaum \\[ [view email](https://arxiv.org/show-email/d04d3fe2/2404.13393)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2404.13393v1)**\nSat, 20 Apr 2024 14:25:34 UTC (2,023 KB)\n\n**\\[v2\\]**\nSat, 12 Oct 2024 16:25:27 UTC (959 KB)",
      "url": "https://arxiv.org/abs/2404.13393"
    },
    {
      "title": "Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2107.10882** (cs)\n\n\\[Submitted on 22 Jul 2021\\]\n\n# Title:Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules\n\nAuthors: [Kirill Karpov](https://arxiv.org/search/cs?searchtype=author&query=Karpov,+K) (1 and 2), [Artem Mitrofanov](https://arxiv.org/search/cs?searchtype=author&query=Mitrofanov,+A) (1 and 2), [Vadim Korolev](https://arxiv.org/search/cs?searchtype=author&query=Korolev,+V) (1 and 2), [Valery Tkachenko](https://arxiv.org/search/cs?searchtype=author&query=Tkachenko,+V) (2) ((1) Lomonosov Moscow State University, Department of Chemistry, Leninskie gory, 1 bld. 3, Moscow, Russia, (2) Science Data Software, LLC, 14909 Forest Landing Cir, Rockville, USA)\n\nView a PDF of the paper titled Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules, by Kirill Karpov (1 and 2) and 13 other authors\n\n[View PDF](https://arxiv.org/pdf/2107.10882)\n\n> Abstract:The use of machine learning in chemistry has become a common practice. At the same time, despite the success of modern machine learning methods, the lack of data limits their use. Using a transfer learning methodology can help solve this problem. This methodology assumes that a model built on a sufficient amount of data captures general features of the chemical compound structure on which it was trained and that the further reuse of these features on a dataset with a lack of data will greatly improve the quality of the new model. In this paper, we develop this approach for small organic molecules, implementing transfer learning with graph convolutional neural networks. The paper shows a significant improvement in the performance of models for target properties with a lack of data. The effects of the dataset composition on model quality and the applicability domain of the resulting models are also considered.\n\n|     |     |\n| --- | --- |\n| Comments: | 9 pages, 6 figures |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2107.10882](https://arxiv.org/abs/2107.10882) \\[cs.LG\\] |\n|  | (or [arXiv:2107.10882v1](https://arxiv.org/abs/2107.10882v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2107.10882](https://doi.org/10.48550/arXiv.2107.10882)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Kirill Karpov \\[ [view email](https://arxiv.org/show-email/c8afecf4/2107.10882)\\]\n\n**\\[v1\\]**\nThu, 22 Jul 2021 18:57:24 UTC (529 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules, by Kirill Karpov (1 and 2) and 13 other authors\n\n- [View PDF](https://arxiv.org/pdf/2107.10882)\n- [TeX Source](https://arxiv.org/src/2107.10882)\n- [Other Formats](https://arxiv.org/format/2107.10882)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2107.10882&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2107.10882&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-07](https://arxiv.org/list/cs.LG/2021-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2107.10882?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.10882)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.10882)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.10882)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-10882) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-10882)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2107.10882&description=Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2107.10882&title=Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2107.10882) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2107.10882"
    },
    {
      "title": "Fast and effective molecular property prediction with transferability map",
      "text": "Fast and effective molecular property prediction with transferability map\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n### Subjects\n\n- [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n- [Drug discovery and development](https://www.nature.com/subjects/drug-discovery-and-development)\n\n## Abstract\n\nEffective transfer learning for molecular property prediction has shown considerable strength in addressing insufficient labeled molecules. Many existing methods either disregard the quantitative relationship between source and target properties, risking negative transfer, or require intensive training on target tasks. To quantify transferability concerning task-relatedness, we propose Principal Gradient-based Measurement (PGM) for transferring molecular property prediction ability. First, we design an optimization-free scheme to calculate a principal gradient for approximating the direction of model optimization on a molecular property prediction dataset. We have analyzed the close connection between the principal gradient and model optimization through mathematical proof. PGM measures the transferability as the distance between the principal gradient obtained from the source dataset and that derived from the target dataset. Then, we perform PGM on various molecular property prediction datasets to build a quantitative transferability map for source dataset selection. Finally, we evaluate PGM on multiple combinations of transfer learning tasks across 12 benchmark molecular property prediction datasets and demonstrate that it can serve as fast and effective guidance to improve the performance of a target task. This work contributes to more efficient discovery of drugs, materials, and catalysts by offering a task-relatedness quantification prior to transfer learning and understanding the relationship between chemical properties.\n\n### Similar content being viewed by others\n\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\n\nArticleOpen access04 May 2023\n\n### [A knowledge-guided pre-training framework for improving molecular representation learning](https://www.nature.com/articles/s41467-023-43214-1?fromPaywallRec=false)\n\nArticleOpen access21 November 2023\n\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://www.nature.com/articles/s42004-024-01155-w?fromPaywallRec=false)\n\nArticleOpen access05 April 2024\n\n## Introduction\n\nMolecular property prediction, which involves identifying molecules with desired properties[1](https://www.nature.com/articles/s42004-024-01169-4#ref-CR1), [2](https://www.nature.com/articles/s42004-024-01169-4#ref-CR2), poses a critical challenge prevalent across various scientific fields. It holds particular significance in chemistry for designing drugs, catalysts, and materials. In recent years, artificial intelligence (AI) technologies have come mainstream in this area, and AI-guided chemical design can efficiently explore chemical space while improving performance based on experimental feedback, showing promise from laboratory research to real-world industry applications[3](https://www.nature.com/articles/s42004-024-01169-4#ref-CR3). However, it is common that the experimental data size is small as producing labeled data requires time-consuming and expensive experiments[4](https://www.nature.com/articles/s42004-024-01169-4#ref-CR4), [5](https://www.nature.com/articles/s42004-024-01169-4#ref-CR5). In contrast, transfer learning[6](https://www.nature.com/articles/s42004-024-01169-4#ref-CR6) has become a powerful paradigm for addressing data scarcity problem by exploiting the knowledge from related datasets across fields such as natural language processing[7](https://www.nature.com/articles/s42004-024-01169-4#ref-CR7), [8](https://www.nature.com/articles/s42004-024-01169-4#ref-CR8), computer vision[9](https://www.nature.com/articles/s42004-024-01169-4#ref-CR9), [10](https://www.nature.com/articles/s42004-024-01169-4#ref-CR10), and biomedcine[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12). In chemistry, transfer learning leverages pre-trained models on extensive or related datasets to facilitate efficient exploration of vast chemical space[13](https://www.nature.com/articles/s42004-024-01169-4#ref-CR13), [14](https://www.nature.com/articles/s42004-024-01169-4#ref-CR14) for various downstream tasks. It has been used to predict properties[15](https://www.nature.com/articles/s42004-024-01169-4#ref-CR15), [16](https://www.nature.com/articles/s42004-024-01169-4#ref-CR16), plan synthesis[17](https://www.nature.com/articles/s42004-024-01169-4#ref-CR17), [18](https://www.nature.com/articles/s42004-024-01169-4#ref-CR18), and explore the space of chemical reactions[19](https://www.nature.com/www.nature.com#ref-CR19), [20](https://www.nature.com/www.nature.com#ref-CR20), [21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/articles/s42004-024-01169-4#ref-CR22).\n\nTransfer learning can enhance molecular property prediction in limited data sets by borrowing knowledge from sufficient source data sets, thus improving both model accuracy and computation efficiency. Although several previous works have explored the power of transfer learning to enhance molecular property prediction[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/articles/s42004-024-01169-4#ref-CR25), challenges remain. One major challenge is negative transfer, which occurs when the performance after transfer learning is adversely affected due to minimal similarity between the source and target tasks[26](https://www.nature.com/articles/s42004-024-01169-4#ref-CR26), [27](https://www.nature.com/articles/s42004-024-01169-4#ref-CR27). For example, Hu et al.[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23) observed that pretrained GNN (at both node-level and graph-level) performed well but yielded negative transfer when pretrained at the level of either entire graphs or individual nodes. Additionally, some supervised pre-training tasks unrelated to the downstream task of interest can even degrade the downstream performance[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23), [28](https://www.nature.com/articles/s42004-024-01169-4#ref-CR28).\n\nNegative transfer primarily stems from suboptimal model and layer choices, as well as insufficient task relatedness, highlighting the need to evaluate transferability prior to applying transfer learning. In computer vision, some researchers have recently focused on selecting the best model from a pool of options by estimating the transferability of each model[29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/articles/s42004-024-01169-4#ref-CR32). In molecular property prediction, recent efforts involve investigating the relatedness of the source task to the target task. To maximize the performance on a target task and prevent negative transfer, existing methods mainly rely on a molecular distance metric to measure the similarity of molecules, such as Tanimoto coefficient (based on molecular fingerprint)[33](https://www.nature.com/articles/s42004-024-01169-4#ref-CR33), [34](https://www.nature.com/articles/s42004-024-01169-4#ref-CR34) and a chemical distance measure (based on fingerprint and subgraph)[35](https://www.nature.com/articles/s42004-024-01169-...",
      "url": "https://www.nature.com/articles/s42004-024-01169-4"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Improving molecular property prediction through a task similarity enhanced transfer learning strategy",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Summary</h2>\n<p>Deeply understanding the properties (e.g., chemical or biological characteristics) of small molecules plays an essential role in drug development. A large number of molecular property datasets have been rapidly accumulated in recent years. However, most of these datasets contain only a limited amount of data, which hinders deep learning methods from making accurate predictions of the corresponding molecular properties. In this work, we propose a transfer learning strategy to alleviate such a data scarcity problem by exploiting the similarity between molecular property prediction tasks. We introduce an effective and interpretable computational framework, named MoTSE (Molecular Tasks Similarity Estimator), to provide an accurate estimation of task similarity. Comprehensive tests demonstrated that the task similarity derived from MoTSE can serve as useful guidance to improve the prediction performance of transfer learning on molecular properties. We also showed that MoTSE can capture the intrinsic relationships between molecular properties and provide meaningful interpretability for the derived similarity.</p>\n<section><p><strong>Subject areas:</strong> Drugs, Computational chemistry, Bioinformatics, Artificial intelligence</p></section></section><section><h2>Graphical abstract</h2>\n<figure><p></p>\n</figure></section><section><h2>Highlights</h2>\n<ul>\n<li>\n<span>\u2022</span><p>MoTSE accurately measures similarity between molecular property prediction tasks</p>\n</li>\n<li>\n<span>\u2022</span><p>A novel transfer learning strategy to accurately predict molecular properties</p>\n</li>\n<li>\n<span>\u2022</span><p>An interpretable method to help understand relations between molecular properties</p>\n</li>\n</ul></section><section><hr/>\n<p>Drugs; Computational chemistry; Bioinformatics; Artificial intelligence</p></section><section><h2>Introduction</h2>\n<p>With the development of high-throughput experimental techniques in the fields of biology and chemistry (<a href=\"#bib28\">Macarron et\u00a0al., 2011</a>), the number of available datasets of diverse molecular properties has increased significantly over the past few years (<a href=\"#bib33\">Ramakrishnan et\u00a0al., 2014</a>; <a href=\"#bib31\">Papadatos et\u00a0al., 2015</a>; <a href=\"#bib19\">Kim et\u00a0al., 2016</a>). This offers an unprecedented opportunity to design accurate computational models for molecular property prediction, thus facilitating the comprehension of molecular properties and accelerating the drug discovery process. However, as huge experimental efforts are often required for obtaining large-scale molecular property labels, the available data of the majority of the properties are still extremely scarce. For example, although the preprocessed ChEMBL dataset (<a href=\"#bib10\">Gaulton et\u00a0al., 2012</a>; <a href=\"#bib29\">Mayr et\u00a0al., 2018</a>) contains 1,310 bioassays and covers over 400K small molecules, the numbers of available labels of over 90% of the bioassays are below 1K. This data scarcity problem has limited the applications of data-driven computational models, especially deep learning models, in making accurate predictions of the corresponding molecular properties.</p>\n<p>To alleviate the data scarcity problem, transfer learning strategies have been widely applied to improve the prediction performance of tasks with limited data in the field of computer vision (<a href=\"#bib54\">Zamir et\u00a0al., 2018</a>; <a href=\"#bib25\">Li et\u00a0al., 2020</a>; <a href=\"#bib6\">Chen and He, 2021</a>). The general idea of transfer learning strategies is to transfer the knowledge learned from a source task with sufficient data to enhance the learning of a target task with limited data. The superior performance of transfer learning has also been well validated in molecular property prediction tasks (<a href=\"#bib42\">Simoes et\u00a0al., 2018</a>; <a href=\"#bib40\">Shen and Nicolaou, 2020</a>; <a href=\"#bib4\">Cai et\u00a0al., 2020</a>; <a href=\"#bib24\">Li and Fourches, 2020</a>). Nevertheless, the success of transfer learning is not always guaranteed. A number of studies have indicated that transfer learning can harm prediction performance (termed negative transfer) (<a href=\"#bib37\">Rosenstein et\u00a0al., 2005</a>; <a href=\"#bib9\">Fang et\u00a0al., 2015</a>; <a href=\"#bib49\">Wang et\u00a0al., 2019b</a>; <a href=\"#bib56\">Zhuang et\u00a0al., 2021</a>). It has been observed that negative transfer usually occurs when there exists only weak (or even no) similarity between the source and target tasks (<a href=\"#bib55\">Zhang et\u00a0al., 2020</a>). Therefore, to facilitate the effective applications of transfer learning in molecular property prediction and avoid the negative transfer problem, it is necessary to accurately measure the similarity between different molecular property prediction tasks.</p>\n<p>It is generally hard to explicitly and manually measure the similarity between molecular property prediction tasks, even for experienced experts, as fully understanding the behaviors of molecules in the chemical and biological systems is extremely difficult owing to the high complexity of these systems. Fortunately, data-driven computational methods can provide an implicit way to enable us to define and measure task similarity. The seminal work of Taskonomy (<a href=\"#bib54\">Zamir et\u00a0al., 2018</a>) has made a pioneering attempt toward modeling the similarity between computer vision tasks through a deep learning approach. The results have shown that incorporating the similarity derived from Taskonomy can improve the performance of transfer learning on computer vision tasks. In addition, the similarity tree constructed according to the derived similarity is highly consistent with human conceptions, indicating that such approaches can potentially capture the intrinsic relationships between tasks. This thus inspires us to develop a computational method for estimating the similarity between molecular property prediction tasks, which can not only guide the source task selection to avoid negative transfer in transfer learning but also provide useful hints in understanding the relationships between tasks.</p>\n<p>To this end, we propose MoTSE, an interpretable computational framework, to efficiently measure the similarity between molecular property prediction tasks. MoTSE is based on the assumption that two tasks should be similar if the hidden knowledge learned by their task-specific models is close to each other. More specifically, MoTSE first pre-trains a graph neural network (GNN) model for each task. Then an attribution method and a molecular representation similarity analysis (MRSA) method are introduced to represent the hidden knowledge enclosed in the pre-trained GNNs as embedded vectors and project individual tasks into a unified latent space. Finally, MoTSE calculates the distances between the vectors in the latent space to derive the similarity between different tasks. Based on the task similarity derived from MoTSE, we design a novel transfer learning strategy to enhance the learning of the molecular property prediction tasks with limited data.</p>\n<p>Our extensive computational tests demonstrated that the task similarity estimated by MoTSE can successfully guide the source task selection in transfer learning, with superior prediction performance over a number of baseline methods, including multitask learning, training from scratch, and nine state-of-the-art self-supervised learning methods, on several molecular property datasets from various domains. Meanwhile, by applying MoTSE to a dataset measuring the physical chemistry properties and a dataset measuring the bio-activities against cytochrome P450 isozymes, we also demonstrated that MoTSE was able to capture the intrinsic relationships between molecular properties and provide meaningful interpretability for the derived similarity.</p></section><section><h2>Results</h2>\n<section><h3>Overall design of MoTSE</h3>\n<p><a href=\"#fig1\">Figure\u00a01</a> illus...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9579493"
    },
    {
      "title": "Scalable Multi-Task Transfer Learning for Molecular Property Prediction",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2410.00432"
    },
    {
      "title": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting",
      "text": "Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=fa07bcec-8447-4c0c-aa2f-f22d38705f5b)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nTransfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-45566-8.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:26 February 2024# Transfer learning with graph neural networks for improved molecular property prediction in the multi-fidelity setting\n* [David Buterez](#auth-David-Buterez-Aff1)[ORCID:orcid.org/0000-0001-6558-0833](https://orcid.org/0000-0001-6558-0833)[1](#Aff1),\n* [Jon Paul Janet](#auth-Jon_Paul-Janet-Aff2)[ORCID:orcid.org/0000-0001-7825-4797](https://orcid.org/0000-0001-7825-4797)[2](#Aff2),\n* [Steven J. Kiddle](#auth-Steven_J_-Kiddle-Aff3)[3](#Aff3),\n* [Dino Oglic](#auth-Dino-Oglic-Aff4)[4](#Aff4)&amp;\n* \u2026* [Pietro Li\u00f3](#auth-Pietro-Li_-Aff1)[ORCID:orcid.org/0000-0002-0540-5053](https://orcid.org/0000-0002-0540-5053)[1](#Aff1)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:1517(2024)[Cite this article](#citeas)\n* 32kAccesses\n* 77Citations\n* 5Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-45566-8/metrics)\n### Subjects\n* [Computational biology and bioinformatics](https://www.nature.com/subjects/computational-biology-and-bioinformatics)\n* [Drug discovery](https://www.nature.com/subjects/drug-discovery)\n* [Mathematics and computing](https://www.nature.com/subjects/mathematics-and-computing)\n* [Quantum mechanics](https://www.nature.com/subjects/quantum-mechanics)\n## Abstract\nWe investigate the potential of graph neural networks for transfer learning and improving molecular property prediction on sparse and expensive to acquire high-fidelity data by leveraging low-fidelity measurements as an inexpensive proxy for a targeted property of interest. This problem arises in discovery processes that rely on screening funnels for trading off the overall costs against throughput and accuracy. Typically, individual stages in these processes are loosely connected and each one generates data at different scale and fidelity. We consider this setup holistically and demonstrate empirically that existing transfer learning techniques for graph neural networks are generally unable to harness the information from multi-fidelity cascades. Here, we propose several effective transfer learning strategies and study them in transductive and inductive settings. Our analysis involves a collection of more than 28 million unique experimental protein-ligand interactions across 37 targets from drug discovery by high-throughput screening and 12 quantum properties from the dataset QMugs. The results indicate that transfer learning can improve the performance on sparse tasks by up to eight times while using an order of magnitude less high-fidelity training data. Moreover, the proposed methods consistently outperform existing transfer learning strategies for graph-structured data on drug discovery and quantum mechanics datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-022-00501-8/MediaObjects/42256_2022_501_Fig1_HTML.png)\n### [An adaptive graph learning method for automated molecular interactions and properties predictions](https://www.nature.com/articles/s42256-022-00501-8?fromPaywallRec=false)\nArticle23 June 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-023-45269-y/MediaObjects/41598_2023_45269_Fig1_HTML.png)\n### [Binding affinity predictions with hybrid quantum-classical convolutional neural networks](https://www.nature.com/articles/s41598-023-45269-y?fromPaywallRec=false)\nArticleOpen access20 October 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41586-023-05905-z/MediaObjects/41586_2023_5905_Fig1_HTML.png)\n### [Computational approaches streamlining drug discovery](https://www.nature.com/articles/s41586-023-05905-z?fromPaywallRec=false)\nArticle26 April 2023\n## Introduction\nWe investigate the potential of graph neural networks (GNNs) for transfer learning and improved molecular property prediction in the context of funnels or screening cascades characteristic of drug discovery and/or molecular design. GNNs have emerged as a powerful and widely-used class of algorithms for molecular property prediction thanks to their natural ability to learn from molecular structures represented as atoms and bonds[1](#ref-CR1),[2](#ref-CR2),[3](https://www.nature.com/articles/s41467-024-45566-8#ref-CR3), as well as in the life sciences in general[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s41467-024-45566-8#ref-CR6). However, their potential for transfer learning is yet to be established. The screening cascade refers to a multi-stage approach where one starts with cheap and relatively noisy methods (high-throughput screening, molecular mechanics calculations, etc.) that allow for screening a large number of molecules. This is followed by increasingly accurate and more expensive evaluations that come with much lower throughput, up to the experimental characterisation of compounds. Individual stages or tiers in the screening funnel are, thus, used to make a reduction of the search space and focus the evaluation of more expensive properties on the promising regions. In this way, the funnel maintains a careful trade-off between the scale, cost, and accuracy. The progression from one tier to another is typically done manually by selecting subsets of molecules from the library screened at the previous stage or via a surrogate model that focuses the screening budget of the next step on the part of the chemical space around the potential hits. Such surrogate models are typically built using the data originating from a single tier and, thus, without leveraging measurements of different fidelity.\nFor efficient use of experimental resources, it is beneficial to have good predictive models operating on sparse datasets and guiding the high-fidelity evaluations relative to properties of interest. The latter is the most expensive part of the funnel and to efficiently support it, we consider it in a transfer learning setting designed to leverage low-fidelity observations to improve the effectiveness of predictive models on sparse and high-fidelity experimental data. In drug discovery applications of this setup, low-fidelity measurements can be seen as ground truth values that have been corrupted by noise, experimental or reading artefacts, or are simply performed using less precise but cheaper experiments. For quantum mechanics...",
      "url": "https://www.nature.com/articles/s41467-024-45566-8?error=cookies_not_supported&code=ef07e9bb-ac7c-474e-8a9f-c58f545a8a12"
    },
    {
      "title": "Graph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization",
      "text": "Search all BMC articles\n\nSearch\n\nGraph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00904-2.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00904-2.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00904-2.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00904-2.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 23 October 2024\n\n# Graph neural processes for molecules: an evaluation on docking scores and strategies to improve generalization\n\n- [Miguel Garc\u00eda-Orteg\u00f3n](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Miguel-Garc_a_Orteg_n-Aff1-Aff2-Aff3) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3),\n- [Srijit Seal](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Srijit-Seal-Aff4) [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff4),\n- [Carl Rasmussen](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Carl-Rasmussen-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Andreas Bender](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Andreas-Bender-Aff3) [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3) &\n- \u2026\n- [Sergio Bacallado](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Sergio-Bacallado-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a0115 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 1169 Accesses\n\n- 1 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2/metrics)\n\n\nThis article has been [updated](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#change-history)\n\n### Abstract\n\nNeural processes (NPs) are models for meta-learning which output uncertainty estimates. So far, most studies of NPs have focused on low-dimensional datasets of highly-correlated tasks. While these homogeneous datasets are useful for benchmarking, they may not be representative of realistic transfer\u00a0learning. In particular, applications in scientific research may prove especially challenging due to the potential novelty of meta-testing tasks. Molecular property prediction is one such research area that is characterized by sparse datasets of many functions on a shared molecular space. In this paper, we study the application of graph NPs to molecular property prediction with DOCKSTRING, a diverse dataset of docking scores. Graph NPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in chemoinformatics, as well as alternative techniques for transfer learning and meta-learning. In order to increase meta-generalization to divergent test functions, we propose fine-tuning strategies that adapt the parameters of NPs. We find that adaptation can substantially increase NPs' regression performance while maintaining good calibration of uncertainty estimates. Finally, we present a Bayesian optimization experiment which showcases the potential advantages of NPs over Gaussian processes in iterative screening. Overall, our results suggest that NPs on molecular graphs hold great potential for molecular property prediction in the low-data setting.\n\n### Scientific contribution\n\nNeural processes are a family of meta-learning algorithms which deal with data scarcity by transferring information across tasks and making probabilistic predictions. We evaluate their performance on regression and optimization molecular tasks using docking scores, finding them to outperform classical single-task and transfer-learning models. We examine the issue of generalization to divergent test tasks, which is a general concern of meta-learning algorithms in science, and propose strategies to alleviate it.\n\n## Introduction\n\nA major difficulty in the application of machine learning (ML) to molecular property prediction in drug discovery is the scarcity of labeled data. Experimental assays are expensive and time-consuming, and data collection is biased towards certain bioactivities (e.g. protein targets deemed medically relevant or commercially profitable) or molecules (e.g. those that are easier to acquire or synthesize). As a result, chemoinformatic datasets are highly sparse and non-overlapping. In a typical pharmaceutical company\u2019s chemical library, it is estimated that less than 1% of all the compound-assay pairs have been measured\u00a0\\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR1)\\]. Even more strikingly, public databases are as little as 0.05% complete\u00a0\\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR1), [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR2)\\].\n\nMeta-learning, or \u201clearning to learn\u201d, is a machine-learning paradigm that attempts to achieve fast adaptation to novel tasks given a small number of labeled datapoints\u00a0\\[ [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR3)\\]. The meta-learning setting is similar to transfer learning in that it attempts to take advantage of existing information to improve predictions on downstream tasks. However, instead of transferring knowledge from a single pre-training task with many labels, meta-learning attempts to transfer knowledge from multiple meta-training tasks with few labels each. Later, during meta-testing, the model is evaluated on unseen tasks, using a few labeled datapoints from each task as examples. These example points encode information about the meta-test task and are called the contexts. In turn, the query points of interest that we want to predict for each meta-task are called the targets. The ability to learn from meta-training tasks without overfitting and generalizing to novel meta-testing tasks is called meta-generalization.\n\nThe meta-learning setting may be appropriate in molecular property prediction\u00a0\\[ [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR4)\\], since measurements from many different molecular tasks have been collected historically and could be used for meta-training. Examples of molecular tasks are physicochemical properties, protein binding affinities, phenotypic assays or ADMET endpoints\u00a0\\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR5), [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR6)\\]. Typically, each task comprises too few datapoints to train a large neural model, but collectively a large set of bioactivities may be useful to learn biases of molecular functions, as well as molecular representations. However, given the sheer diversity of molecular tasks that are available, extra care should be taken to ensure that the biases learnt during meta-training are adequate for meta-testing. For example, tasks related to physicochemical properties, which are intrinsic to molecules, may be very different from cell assays, which depend on the complex interplay between molecules and a biological system\u00a0\\[ [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#ref-CR7)\\].\n\nIn addition to data efficiency and learning from sparse datasets, another feature that is desirable is the ability to produce uncertainty estimates\u00a0\\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2#re...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00904-2"
    }
  ]
}