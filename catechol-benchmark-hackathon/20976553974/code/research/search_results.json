{
  "query": "What are the best Gaussian Process models for predicting chemical reaction outcomes with small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Gaussian\u2011process (GP)\u2011based models are currently the most reliable tools for reaction\u2011outcome prediction when only a handful of experimental points are available, because they provide calibrated uncertainty estimates that can be exploited for Bayesian optimisation.  The most widely\u2011adopted framework is **GAUCHE**, an open\u2011source library that supplies more than 30 specialised kernels for molecular graphs, SMILES strings and fingerprint vectors, allowing GPs to be applied directly to the discrete representations that chemists use\u202f([arXiv](https://arxiv.org/abs/2212.04450); [documentation](https://leojklarner.github.io/gauche)).  By selecting kernels such as the marginalized graph kernel or the string\u2011kernel\u2011based Tanimoto similarity, GAUCHE achieves strong predictive performance on datasets of fewer than\u202f100 reactions and makes it straightforward to couple the GP with Bayesian optimisation loops for reaction\u2011condition screening.\n\nA complementary \u201cdeep\u2011kernel\u201d approach further improves accuracy on very small data by learning a data\u2011driven feature map with a neural network before feeding the embeddings to a GP.  The **Deep Kernel Learning (DKL)** model introduced by Singh and Hern\u00e1ndez\u2011Lobato (2024/2025) has been shown to outperform vanilla GPs on benchmark reaction\u2011outcome tasks while retaining reliable uncertainty quantification, making it especially useful for optimisation under scarce data\u202f([Nature Communications Chemistry](https://www.nature.com/articles/s42004-024-01219-x); [Cambridge repository](https://www.repository.cam.ac.uk/handle/1810/369753)).  \n\nFinally, the **DIONYSUS** framework explicitly addresses calibration and generalisability on low\u2011data chemical sets.  It evaluates a suite of probabilistic models\u2014including standard GPs, sparse GPs and the DKL variant\u2014and selects the one that best balances predictive error and calibrated uncertainty, which is critical when only a few reaction measurements are available\u202f([RSC Digital Discovery 2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  Together, GAUCHE\u2019s chemistry\u2011specific kernels, deep\u2011kernel GP models, and the DIONYSUS calibration pipeline constitute the state\u2011of\u2011the\u2011art GP toolbox for small\u2011dataset reaction\u2011outcome prediction.",
      "url": ""
    },
    {
      "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2212.04450** (physics)\n\n\\[Submitted on 6 Dec 2022 ( [v1](https://arxiv.org/abs/2212.04450v1)), last revised 21 Feb 2023 (this version, v2)\\]\n\n# Title:GAUCHE: A Library for Gaussian Processes in Chemistry\n\nAuthors: [Ryan-Rhys Griffiths](https://arxiv.org/search/physics?searchtype=author&query=Griffiths,+R), [Leo Klarner](https://arxiv.org/search/physics?searchtype=author&query=Klarner,+L), [Henry B. Moss](https://arxiv.org/search/physics?searchtype=author&query=Moss,+H+B), [Aditya Ravuri](https://arxiv.org/search/physics?searchtype=author&query=Ravuri,+A), [Sang Truong](https://arxiv.org/search/physics?searchtype=author&query=Truong,+S), [Samuel Stanton](https://arxiv.org/search/physics?searchtype=author&query=Stanton,+S), [Gary Tom](https://arxiv.org/search/physics?searchtype=author&query=Tom,+G), [Bojana Rankovic](https://arxiv.org/search/physics?searchtype=author&query=Rankovic,+B), [Yuanqi Du](https://arxiv.org/search/physics?searchtype=author&query=Du,+Y), [Arian Jamasb](https://arxiv.org/search/physics?searchtype=author&query=Jamasb,+A), [Aryan Deshwal](https://arxiv.org/search/physics?searchtype=author&query=Deshwal,+A), [Julius Schwartz](https://arxiv.org/search/physics?searchtype=author&query=Schwartz,+J), [Austin Tripp](https://arxiv.org/search/physics?searchtype=author&query=Tripp,+A), [Gregory Kell](https://arxiv.org/search/physics?searchtype=author&query=Kell,+G), [Simon Frieder](https://arxiv.org/search/physics?searchtype=author&query=Frieder,+S), [Anthony Bourached](https://arxiv.org/search/physics?searchtype=author&query=Bourached,+A), [Alex Chan](https://arxiv.org/search/physics?searchtype=author&query=Chan,+A), [Jacob Moss](https://arxiv.org/search/physics?searchtype=author&query=Moss,+J), [Chengzhi Guo](https://arxiv.org/search/physics?searchtype=author&query=Guo,+C), [Johannes Durholt](https://arxiv.org/search/physics?searchtype=author&query=Durholt,+J), [Saudamini Chaurasia](https://arxiv.org/search/physics?searchtype=author&query=Chaurasia,+S), [Felix Strieth-Kalthoff](https://arxiv.org/search/physics?searchtype=author&query=Strieth-Kalthoff,+F), [Alpha A. Lee](https://arxiv.org/search/physics?searchtype=author&query=Lee,+A+A), [Bingqing Cheng](https://arxiv.org/search/physics?searchtype=author&query=Cheng,+B), [Al\u00e1n Aspuru-Guzik](https://arxiv.org/search/physics?searchtype=author&query=Aspuru-Guzik,+A), [Philippe Schwaller](https://arxiv.org/search/physics?searchtype=author&query=Schwaller,+P), [Jian Tang](https://arxiv.org/search/physics?searchtype=author&query=Tang,+J)\n\nView a PDF of the paper titled GAUCHE: A Library for Gaussian Processes in Chemistry, by Ryan-Rhys Griffiths and Leo Klarner and Henry B. Moss and Aditya Ravuri and Sang Truong and Samuel Stanton and Gary Tom and Bojana Rankovic and Yuanqi Du and Arian Jamasb and Aryan Deshwal and Julius Schwartz and Austin Tripp and Gregory Kell and Simon Frieder and Anthony Bourached and Alex Chan and Jacob Moss and Chengzhi Guo and Johannes Durholt and Saudamini Chaurasia and Felix Strieth-Kalthoff and Alpha A. Lee and Bingqing Cheng and Al\\\\'an Aspuru-Guzik and Philippe Schwaller and Jian Tang\n\n[View PDF](https://arxiv.org/pdf/2212.04450)\n\n> Abstract:We introduce GAUCHE, a library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to chemical representations, however, is nontrivial, necessitating kernels defined over structured inputs such as graphs, strings and bit vectors. By defining such kernels in GAUCHE, we seek to open the door to powerful tools for uncertainty quantification and Bayesian optimisation in chemistry. Motivated by scenarios frequently encountered in experimental chemistry, we showcase applications for GAUCHE in molecular discovery and chemical reaction optimisation. The codebase is made available at [this https URL](https://github.com/leojklarner/gauche)\n\n|     |     |\n| --- | --- |\n| Subjects: | Chemical Physics (physics.chem-ph); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2212.04450](https://arxiv.org/abs/2212.04450) \\[physics.chem-ph\\] |\n|  | (or [arXiv:2212.04450v2](https://arxiv.org/abs/2212.04450v2) \\[physics.chem-ph\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2212.04450](https://doi.org/10.48550/arXiv.2212.04450)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ryan-Rhys Griffiths \\[ [view email](https://arxiv.org/show-email/892178c4/2212.04450)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2212.04450v1)**\nTue, 6 Dec 2022 08:28:21 UTC (557 KB)\n\n**\\[v2\\]**\nTue, 21 Feb 2023 06:16:06 UTC (2,390 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GAUCHE: A Library for Gaussian Processes in Chemistry, by Ryan-Rhys Griffiths and Leo Klarner and Henry B. Moss and Aditya Ravuri and Sang Truong and Samuel Stanton and Gary Tom and Bojana Rankovic and Yuanqi Du and Arian Jamasb and Aryan Deshwal and Julius Schwartz and Austin Tripp and Gregory Kell and Simon Frieder and Anthony Bourached and Alex Chan and Jacob Moss and Chengzhi Guo and Johannes Durholt and Saudamini Chaurasia and Felix Strieth-Kalthoff and Alpha A. Lee and Bingqing Cheng and Al\\\\'an Aspuru-Guzik and Philippe Schwaller and Jian Tang\n\n- [View PDF](https://arxiv.org/pdf/2212.04450)\n- [TeX Source](https://arxiv.org/src/2212.04450)\n- [Other Formats](https://arxiv.org/format/2212.04450)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2212.04450&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2212.04450&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2022-12](https://arxiv.org/list/physics.chem-ph/2022-12)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2212.04450?context=cond-mat)\n\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2212.04450?context=cond-mat.mtrl-sci)\n\n[cs](https://arxiv.org/abs/2212.04450?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2212.04450?context=cs.LG)\n\n[physics](https://arxiv.org/abs/2212.04450?context=physics)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.04450)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.04450)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.04450)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2212.04450&description=GAUCHE: A Library for Gaussian Processes in Chemistry) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2212.04450&title=GAUCHE: A Library for Gaussian Processes in Chemistry)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [Wh...",
      "url": "https://arxiv.org/abs/2212.04450"
    },
    {
      "title": "Deep Kernel learning for reaction outcome prediction and optimization",
      "text": "Deep Kernel learning for reaction outcome prediction and optimization | Communications Chemistry\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Communications Chemistry](https://media.springernature.com/full/nature-cms/uploads/product/commschem/header-3dc28429486e0d2c8f49fd9baf5afa40.svg)](https://www.nature.com/commschem)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42004-024-01219-x?error=cookies_not_supported&code=339c684d-2df4-42bb-a10b-c18beba44b3d)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42004)\n* [RSS feed](https://www.nature.com/commschem.rss)\nDeep Kernel learning for reaction outcome prediction and optimization\n[Download PDF](https://www.nature.com/articles/s42004-024-01219-x.pdf)\n[Download PDF](https://www.nature.com/articles/s42004-024-01219-x.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:14 June 2024# Deep Kernel learning for reaction outcome prediction and optimization\n* [Sukriti Singh](#auth-Sukriti-Singh-Aff1)[ORCID:orcid.org/0000-0003-2286-2974](https://orcid.org/0000-0003-2286-2974)[1](#Aff1)&amp;\n* [Jos\u00e9 Miguel Hern\u00e1ndez-Lobato](#auth-Jos__Miguel-Hern_ndez_Lobato-Aff1)[ORCID:orcid.org/0000-0001-7610-949X](https://orcid.org/0000-0001-7610-949X)[1](#Aff1)\n[*Communications Chemistry*](https://www.nature.com/commschem)**volume7**, Article\u00a0number:136(2024)[Cite this article](#citeas)\n* 7809Accesses\n* 14Citations\n* 8Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42004-024-01219-x/metrics)\n### Subjects\n* [Catalysis](https://www.nature.com/subjects/catalysis)\n* [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n* [Method development](https://www.nature.com/subjects/method-development)\n* [Synthetic chemistry methodology](https://www.nature.com/subjects/methodology)\n* [Structure prediction](https://www.nature.com/subjects/structure-prediction)\n## Abstract\nRecent years have seen a rapid growth in the application of various machine learning methods for reaction outcome prediction. Deep learning models have gained popularity due to their ability to learn representations directly from the molecular structure. Gaussian processes (GPs), on the other hand, provide reliable uncertainty estimates but are unable to learn representations from the data. We combine the feature learning ability of neural networks (NNs) with uncertainty quantification of GPs in a deep kernel learning (DKL) framework to predict the reaction outcome. The DKL model is observed to obtain very good predictive performance across different input representations. It significantly outperforms standard GPs and provides comparable performance to graph neural networks, but with uncertainty estimation. Additionally, the uncertainty estimates on predictions provided by the DKL model facilitated its incorporation as a surrogate model for Bayesian optimization (BO). The proposed method, therefore, has a great potential towards accelerating reaction discovery by integrating accurate predictive models that provide reliable uncertainty estimates with BO.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-021-00144-6/MediaObjects/41598_2021_144_Fig1_HTML.png)\n### [Deep Bayesian Gaussian processes for uncertainty estimation in electronic health records](https://www.nature.com/articles/s41598-021-00144-6?fromPaywallRec=false)\nArticleOpen access19 October 2021\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-57135-6/MediaObjects/41598_2024_57135_Fig1_HTML.png)\n### [Relationship between prediction accuracy and uncertainty in compound potency prediction using deep neural networks and control models](https://www.nature.com/articles/s41598-024-57135-6?fromPaywallRec=false)\nArticleOpen access19 March 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-021-88939-5/MediaObjects/41598_2021_88939_Fig1_HTML.png)\n### [GPCR\\_LigandClassify.py; a rigorous machine learning classifier for GPCR targeting compounds](https://www.nature.com/articles/s41598-021-88939-5?fromPaywallRec=false)\nArticleOpen access04 May 2021\n## Introduction\nChemical reaction optimization is central to organic synthesis and has largely been based on chemical intuition[1](https://www.nature.com/articles/s42004-024-01219-x#ref-CR1). During optimization, the aim is to maximize the reaction outcome (e.g., yield and/or enantiomeric excess) by identifying suitable experimental conditions[2](https://www.nature.com/articles/s42004-024-01219-x#ref-CR2). This involves evaluating a multidimensional chemical space comprising of reaction variables such as catalyst, solvent, substrate, additive, time, temperature, concentration, etc.[3](https://www.nature.com/articles/s42004-024-01219-x#ref-CR3),[4](https://www.nature.com/articles/s42004-024-01219-x#ref-CR4). Owing to the complexity of this problem, several data-driven approaches have been employed for efficient exploration of the chemical space[5](#ref-CR5),[6](#ref-CR6),[7](https://www.nature.com/articles/s42004-024-01219-x#ref-CR7).\nThe estimation of reaction outcome is of great importance in reaction development. It could enable chemists to identify, for instance, low-yield reactions prior to wet-lab experiments, thereby saving time and resources. Machine learning (ML) has shown an impressive degree of success in many areas of chemistry[8](#ref-CR8),[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42004-024-01219-x#ref-CR11). Earlier efforts toward reaction outcome prediction use hand-crafted features such as physical organic descriptors and molecular fingerprints[12](https://www.nature.com/articles/s42004-024-01219-x#ref-CR12),[13](https://www.nature.com/articles/s42004-024-01219-x#ref-CR13). Conventional ML methods, particularly random forests, perform extremely well with these non-learned representations. Recently, the advances in deep learning (DL) have led to the development of new molecular representations[14](https://www.nature.com/articles/s42004-024-01219-x#ref-CR14). These are learned directly from molecular structures like simplified molecular input line entry specifications (SMILES) and molecular graphs. The chemical language models (LMs) and graph neural networks (GNNs) trained using these string or graph-based representations have displayed great potential in reaction outcome prediction[15](#ref-CR15),[16](#ref-CR16),[17](#ref-CR17),[18](https://www.nature.com/articles/s42004-024-01219-x#ref-CR18).\nThe reaction outcome prediction augmented with uncertainty quantification is expected to find superior utility during reaction optimization[19](https://www.nature.com/articles/s42004-024-01219-x#ref-CR19). As an example, Bayesian optimization (BO) works with the uncertainty estimates to suggest new experiments in a search for optimal reaction conditions[20](https://www.nature.com/articles/s42004-024-01219-x#ref-CR20). While the quantification of uncertainty using the above-mentioned ML methods might not be straightforward, the uncertainty-awareness of Gaussian processes (GPs) is well-known[21](https://www.nature.com/articles/s42004-024-01219-x#ref-CR21),[22](https://www.nature.com/articles/s42004-024-01219-x#ref-CR22). G...",
      "url": "https://www.nature.com/articles/s42004-024-01219-x"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Documentation",
      "text": "ContentsMenuExpandLight modeDark modeAuto light/dark mode\n\n[Back to top](https://leojklarner.github.io/gauche/)\n\nToggle Light / Dark / Auto color theme\n\nToggle table of contents sidebar\n\n# Documentation [\\#](https://leojklarner.github.io/gauche/\\#documentation)\n\n[![GAUCHE Logo](https://leojklarner.github.io/gauche/_images/gauche_banner_1.png)](https://leojklarner.github.io/gauche/_images/gauche_banner_1.png)\n\n**GAUCHE** is a collaborative, open-source software library that aims to make state-of-the-art probabilistic modelling and black-box optimisation techniques more easily accessible to scientific experts in chemistry, materials science and beyond. We provide 30+ bespoke kernels for molecules, chemical reactions and proteins and illustrate how they can be used for Gaussian processes and Bayesian optimisation in 10+ easy-to-adapt tutorial notebooks.\n\n[Paper (NeurIPS 2023)](https://arxiv.org/abs/2212.04450)\n\n# Overview [\\#](https://leojklarner.github.io/gauche/\\#overview)\n\nGeneral-purpose Gaussian process (GP) and Bayesian optimisation (BO) libraries do not cater for molecular representations. Likewise, general-purpose molecular machine learning libraries do not consider GPs and BO. To bridge this gap, GAUCHE provides a modular, robust and easy-to-use framework of 30+ parallelisable and batch-GP-compatible implementations of string, fingerprint and graph kernels that operate on a range of widely-used molecular representations.\n\n[![GAUCHE Overview](https://leojklarner.github.io/gauche/_images/gauche_overview.png)](https://leojklarner.github.io/gauche/_images/gauche_overview.png)\n\n## Kernels [\\#](https://leojklarner.github.io/gauche/\\#kernels)\n\nStandard GP packages typically assume continuous input spaces of low and fixed dimensionality. This makes it difficult to apply them to common molecular representations: molecular graphs are discrete objects, SMILES strings vary in length and topological fingerprints tend to be high-dimensional and sparse. To bridge this gap, GAUCHE provides:\n\n- **Fingerprint Kernels** that measure the similarity between bit/count vectors of descriptor by examining the degree to which their elements overlap.\n\n- **String Kernels** that measure the similarity between strings by examining the degree to which their sub-strings overlap.\n\n- **Graph Kernels** that measure between graphs by examining the degree to which certain substructural motifs overlap.\n\n\n## Representations [\\#](https://leojklarner.github.io/gauche/\\#representations)\n\nGAUCHE supports any representation that is based on bit/count vectors, strings or graphs. For rapid prototyping and benchmarking, we also provide a range of standard featurisation techniques for molecules, chemical reactions and proteins:\n\n| Domain | Representation |\n| --- | --- |\n| Molecules | ECFP Fingerprints \\[1\\], rdkit Fragments, Fragprints, Graphs \\[2\\], SMILES \\[3\\], SELFIES \\[4\\] |\n| Chemical Reactions | One-Hot Encoding, Data-Driven Reaction Fingerprints \\[5\\], Differential Reaction Fingerprints \\[6\\], Reaction SMARTS |\n| Proteins | Sequences, Graphs \\[2\\] |\n\n## Getting Started [\\#](https://leojklarner.github.io/gauche/\\#getting-started)\n\nThe easiest way to install Gauche is via pip.\n\npip install gauche\n\nAs not all users will need the full functionality of the package, we provide a range of installation options:\n\n- pip install gauche \\- installs the core functionality of GAUCHE (kernels, representations, data loaders, etc.) and should cover a wide range of use cases.\n\n- pip install gauche\\[rxn\\] \\- additionally installs the rxnfp and drfp fingerprints that can be used to represent chemical reactions.\n\n- pip install gauche\\[graphs\\] \\- installs all dependencies for graph kernels and representations.\n\n\nIf you aren\u2019t sure which installation option is right for you, you can simply install all of them with pip install gauche\\[all\\].\n\nThe best way to get started with GAUCHE is to check out our tutorial notebooks. These notebooks provide a step-by-step introduction to the core functionality of GAUCHE and illustrate how it can be used to solve a range of common problems in molecular property prediction and optimisation.\n\n- [Loading and Featurising Molecular Data](https://leojklarner.github.io/gauche/notebooks/loading_and_featurising_molecules.html)\n- [GP Regression on Molecules](https://leojklarner.github.io/gauche/notebooks/gp_regression_on_molecules.html)\n- [Bayesian Optimisation Over Molecules](https://leojklarner.github.io/gauche/notebooks/bayesian_optimisation_over_molecules.html)\n- [Sparse GP Regression on Molecules](https://leojklarner.github.io/gauche/notebooks/sparse_gp_regression_for_big_molecular_data.html)\n- [Multitask GP Regression on Molecules](https://leojklarner.github.io/gauche/notebooks/multitask_gp_regression_on_molecules.html)\n- [Learning an Objective Function through Interaction with a Human Chemist](https://leojklarner.github.io/gauche/notebooks/molecular_preference_learning.html)\n- [Preferential Bayesian Optimisation](https://leojklarner.github.io/gauche/notebooks/preferential_bayesian_optimisation.html)\n- [GP Regression on Protein Sequences: Bag of Amino Acids](https://leojklarner.github.io/gauche/notebooks/protein_fitness_prediction_bag_of_amino_acids.html)\n- [GP Regression on Protein Sequences: Subsequence String Kernel](https://leojklarner.github.io/gauche/notebooks/protein_fitness_prediction_ssk_gp.html)\n- [Bayesian GNNs for Molecular Property Prediction](https://leojklarner.github.io/gauche/notebooks/bayesian_gnn_on_molecules.html)\n\n## Extensions [\\#](https://leojklarner.github.io/gauche/\\#extensions)\n\nIf there are any specific kernels or representations that you would like to see included in GAUCHE, please reach out or submit an issue/pull request.\n\n# Gauche\u2019s API [\\#](https://leojklarner.github.io/gauche/\\#gauche-s-api)\n\nAPI Reference\n\n- [gauche.kernels](https://leojklarner.github.io/gauche/modules/kernels.html)\n  - [Fingerprint Kernels](https://leojklarner.github.io/gauche/modules/kernels.html#module-gauche.kernels.fingerprint_kernels.tanimoto_kernel)\n    - [`TanimotoKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.tanimoto_kernel.TanimotoKernel)\n    - [`batch_tanimoto_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.tanimoto_kernel.batch_tanimoto_sim)\n    - [`BraunBlanquetKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.braun_blanquet_kernel.BraunBlanquetKernel)\n    - [`batch_braun_blanquet_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.braun_blanquet_kernel.batch_braun_blanquet_sim)\n    - [`DiceKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.dice_kernel.DiceKernel)\n    - [`batch_dice_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.dice_kernel.batch_dice_sim)\n    - [`FaithKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.faith_kernel.FaithKernel)\n    - [`batch_faith_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.faith_kernel.batch_faith_sim)\n    - [`ForbesKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.forbes_kernel.ForbesKernel)\n    - [`batch_forbes_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.forbes_kernel.batch_forbes_sim)\n    - [`InnerProductKernel`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.inner_product_kernel.InnerProductKernel)\n    - [`batch_inner_product_sim()`](https://leojklarner.github.io/gauche/modules/kernels.html#gauche.kernels.fingerprint_kernels.inner_product_kernel.batch_inner_product_sim)\n    - [`IntersectionKernel`](https://leojklarner.github.io/gauche/modules/kernels.htm...",
      "url": "https://leojklarner.github.io/gauche"
    },
    {
      "title": "A Library for Gaussian Processes in Chemistry",
      "text": "#### GAUCHE: A Library for Gaussian Processes in Chemistry\nPart of\n[Advances in Neural Information Processing Systems 36 (NeurIPS 2023)](https://papers.nips.cc/paper_files/paper/2023) Main Conference Track\n[Bibtex](https://papers.nips.cc/paper_files/paper/19762-/bibtex) [Paper](https://papers.nips.cc/paper_files/paper/2023/file/f2b1b2e974fa5ea622dd87f22815f423-Paper-Conference.pdf)\n#### Authors\n_Ryan-Rhys Griffiths, Leo Klarner, Henry Moss, Aditya Ravuri, Sang Truong, Yuanqi Du, Samuel Stanton, Gary Tom, Bojana Rankovic, Arian Jamasb, Aryan Deshwal, Julius Schwartz, Austin Tripp, Gregory Kell, Simon Frieder, Anthony Bourached, Alex Chan, Jacob Moss, Chengzhi Guo, Johannes Peter D\u00fcrholt, Saudamini Chaurasia, Ji Won Park, Felix Strieth-Kalthoff, Alpha Lee, Bingqing Cheng, Alan Aspuru-Guzik, Philippe Schwaller, Jian Tang_\n#### Abstract\nWe introduce GAUCHE, an open-source library for GAUssian processes in CHEmistry. Gaussian processes have long been a cornerstone of probabilistic machine learning, affording particular advantages for uncertainty quantification and Bayesian optimisation. Extending Gaussian processes to molecular representations, however, necessitates kernels defined over structured inputs such as graphs, strings and bit vectors. By providing such kernels in a modular, robust and easy-to-use framework, we seek to enable expert chemists and materials scientists to make use of state-of-the-art black-box optimization techniques. Motivated by scenarios frequently encountered in practice, we showcase applications for GAUCHE in molecular discovery, chemical reaction optimisation and protein design. The codebase is made available at https://github.com/leojklarner/gauche.\nDo not remove: This comment is monitored to verify that the site is working properly",
      "url": "https://papers.nips.cc/paper_files/paper/2023/hash/f2b1b2e974fa5ea622dd87f22815f423-Abstract-Conference.html"
    },
    {
      "title": "Predicting hydrogen atom transfer energy barriers using Gaussian process regression \u2020",
      "text": "Predicting hydrogen atom transfer energy barriers using Gaussian process regression - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00174E\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00174e)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00319e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00314d)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D4DD00174E](https://doi.org/10.1039/D4DD00174E)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 513-522\n# Predicting hydrogen atom transfer energy barriers using Gaussian process regression[\u2020](#fn1)\nEvgeni Ulanov[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0009-0006-9281-9049)\\*ae,Ghulam A. Qadir\\*a,Kai Riedmiller[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-1738-754X)a,Pascal Friederich[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-4465-1465)\\*bcandFrauke Gr\u00e4ter[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2891-3381)\\*ade\naHeidelberg Institute for Theoretical Studies, Heidelberg, Germany. E-mail:[evgeni.ulanov@h-its.org](mailto:evgeni.ulanov@h-its.org);[ghulam.qadir@h-its.org](mailto:ghulam.qadir@h-its.org);[frauke.graeter@h-its.org](mailto:frauke.graeter@h-its.org)\nbInstitute of Theoretical Informatics, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany. E-mail:[pascal.friederich@kit.edu](mailto:pascal.friederich@kit.edu)\ncInstitute of Nanotechnology, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany\ndInterdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany\neMax Planck Institute for Polymer Research, Mainz, Germany\nReceived 24th June 2024, Accepted 6th January 2025\nFirst published on 10th January 2025\n## Abstract\nPredicting reaction barriers for arbitrary configurations based on only a limited set of density functional theory (DFT) calculations would render the design of catalysts or the simulation of reactions within complex materials highly efficient. We here propose Gaussian process regression (GPR) as a method of choice if DFT calculations are limited to hundreds or thousands of barrier calculations. For the case of hydrogen atom transfer in proteins, an important reaction in chemistry and biology, we obtain a mean absolute error of 3.23 kcal mol\u22121for the range of barriers in the data set using SOAP descriptors and similar values using the marginalized graph kernel. Thus, the two GPR models can robustly estimate reaction barriers within the large chemical and conformational space of proteins. Their predictive power is comparable to a graph neural network-based model, and GPR even outcompetes the latter in the low data regime. We propose GPR as a valuable tool for an approximate but data-efficient model of chemical reactivity in a complex and highly variable environment.\n## 1 Introduction\nChemical reactivity in complex chemical or biochemical systems can be assessed typically at very high accuracy using quantum chemical methods such as density functional theory (DFT). Surrogate models built using machine learning have been recently suggested to be able to replace these computationally demanding DFT calculations.[1](#cit1)The trained model serves as a black box approximation of the true mapping between reaction geometries and energy barriers. Machine-learned surrogate models can in principle predict reaction barriers solely based on molecular structures, after being trained on pre-calculated DFT barriers, albeit at lower accuracy than the actual DFT calculation itself.[2,3](#cit2)\nIn molecular machine learning, graph representations for molecular prediction problems have seen great success in recent years, specifically in the form of Graph Neural Networks (GNNs). Prominent recent examples include the frameworks Polarizable Atom Interaction Neural Network (PaiNN),[4](#cit4)Neural Equivariant Interatomic Potentials (NequIP)[5](#cit5)and MACE.[6](#cit6)We have recently shown that a variant of the PaiNN model can be used to predict electronic activation energies, in this paper referred to as energy barriers, of hydrogen atom transfer (HAT) reactions in proteins.[7](#cit7)We have chosen HAT because of its important role in proteins subjected to oxidative stress molecules, light, or, as recently shown by us, mechanical force. Mechanoradicals are formed in type I collagen through homolytic bond scission when subject to mechanical stress.[8](#cit8)The generated radicals migrate through the material, eventually to a site that can stabilize radicals.[9](#cit9)Understanding the mechanisms behind these reactions is especially interesting to get a better insight into the effects stress, such as exercise, can have on protein materials like collagen. Beyond mechanoradicals, migration of protein radicals, originating from light, oxidative stress molecules, or other external factors, often occurs through HAT,e.g.ref.[10](#cit10). Exact radical migration paths are, however, mostly unknown. This renders HAT an interesting test bed to tackle the prediction of biochemical reactivity.\nOur model presented in ref.[7](#cit7)based on PaiNN was able to predict HAT reactions with geometries obtained from molecular dynamics (MD) simulations, with a mean absolute error (MAE) of 2.4 kcal mol\u22121when restricting the prediction to transitions with distances \u22642 \u00c5,i.e.the most relevant transitions in a material.\nGeometries originated directly from molecular dynamics (MD) simulations, without subsequent DFT optimizations, rendering such a method very efficient. The model allows the prediction of a reaction barrier for virtually any reactant pair occurring during an MD simulation of the protein. This can ultimately allow simulating these reactions in a kinetic Monte Carlo setting coupled to the MD simulations,i.e.reactive dynamics of the biochemical system under investigation.[11](#cit11)From a practical perspective, the energy barrier prediction should only rely on the initial geometric configuration as input for it to be useful in a future application.\nOne major drawback of building such a surrogate model is the need to initially compute, using DFT, a large set of energy barriers of the reaction at hand to train the model. Here, large often refers to thousands of barriers, in our case to 19![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)164 barriers for reaching an intermediate accuracy of a PaiNN model for HAT, with further room for improvement by enlarging the dataset. For practitioners, a more data-efficient model would be very advantageous.\nWe propose two alternative variants to model these reactions with Gaussian Process Regression (GPR):[12](#cit12)(1) using the Smooth Overlap of Atomic Positions (SOAP) descriptor[13](#cit13)and (2) using the Marginalized Graph Kernel.[14\u201317](#cit14)We show that these approaches are a useful alternative to the previously developed GNN, especially in the low data regime. This is of particular interest, since DFT can be very computationally demanding for large systems at high levels of accuracy. Therefore, acquiring new training points can be costly. The proposed methods would allow practitioners to achieve good predictions while reducing the time and cost spent generating training data....",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00174e"
    },
    {
      "title": "",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2021/sc/d0sc04896h)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc05084a)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d1sc90015c)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)\nThis Open Access Article is licensed under a\n\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D0SC04896H](https://doi.org/10.1039/D0SC04896H)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2021, **12**, 1163-1175\n\n# Machine learning meets mechanistic modelling for accurate prediction of experimental activation energies [\u2020](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h\\#fn1)\n\nKjell\nJorner\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-4191-6790)a,\nTore\nBrinck\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0003-2673-075X)b,\nPer-Ola\nNorrby\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2419-0705)c and David\nButtar\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-5466-023X)\\*a\n\naEarly Chemical Development, Pharmaceutical Sciences, R&D, AstraZeneca, Macclesfield, UK. E-mail: [david.buttar@astrazeneca.com](mailto:david.buttar@astrazeneca.com)\n\nbApplied Physical Chemistry, Department of Chemistry, CBH, KTH Royal Institute of Technology, Stockholm, Sweden\n\ncData Science & Modelling, Pharmaceutical Sciences, R&D, AstraZeneca, Gothenburg, Sweden\n\nReceived\n4th September 2020\n, Accepted 2nd November 2020\n\nFirst published on 5th November 2020\n\n* * *\n\n## Abstract\n\nAccurate prediction of chemical reactions in solution is challenging for current state-of-the-art approaches based on transition state modelling with density functional theory. Models based on machine learning have emerged as a promising alternative to address these problems, but these models currently lack the precision to give crucial information on the magnitude of barrier heights, influence of solvents and catalysts and extent of regio- and chemoselectivity. Here, we construct hybrid models which combine the traditional transition state modelling and machine learning to accurately predict reaction barriers. We train a Gaussian Process Regression model to reproduce high-quality experimental kinetic data for the nucleophilic aromatic substitution reaction and use it to predict barriers with a mean absolute error of 0.77 kcal mol\u22121 for an external test set. The model was further validated on regio- and chemoselectivity prediction on patent reaction data and achieved a competitive top-1 accuracy of 86%, despite not being trained explicitly for this task. Importantly, the model gives error bars for its predictions that can be used for risk assessment by the end user. Hybrid models emerge as the preferred alternative for accurate reaction prediction in the very common low-data situation where only 100\u2013150 rate constants are available for a reaction class. With recent advances in deep learning for quickly predicting barriers and transition state geometries from density functional theory, we envision that hybrid models will soon become a standard alternative to complement current machine learning approaches based on ground-state physical organic descriptors or structural information such as molecular graphs or fingerprints.\n\n* * *\n\n## Introduction\n\nAccurate prediction of chemical reactions is an important goal both in academic and industrial research. [1\u20133](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit1) Recently, machine learning approaches have had tremendous success in quantitative prediction of reaction yields based on data from high-throughput experimentation [4,5](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit4) and enantioselectivities based on carefully selected universal training sets. [6](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit6) At the same time, traditional quantitative structure\u2013reactivity relationship (QSRR) methods based on linear regression have seen a renaissance with interpretable, holistic models that can generalize across reaction types. [7](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit7) In parallel with these developments of quantitative prediction methods, deep learning models trained on reaction databases containing millions of patent and literature data have made quick qualitative yes/no feasibility prediction routine for almost any reaction type. [8](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit8)\n\nIn the pharmaceutical industry, prediction tools have great potential to accelerate synthesis of prospective drugs ( [Fig. 1a](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#imgfig1)). [9](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit9) Quick prediction is essential in the discovery phase, especially within the context of automation and rapid synthesis of a multitude of candidates for initial activity screening. [3,10,11](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit3) In these circumstances, a simple yes/no as provided by classification models is usually sufficient. More accurate prediction is necessary in the later drug development process, where the synthesis route and formulation of one or a few promising drug candidates is optimized. Here, regression models that give the reaction activation energy can be used to predict both absolute reactivity and selectivity ( [Fig. 1b](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#imgfig1)). Prediction of absolute reactivity can be used to assess feasibility under process-relevant conditions, while prediction of selectivity is key to reducing purification steps. Predictive tools therefore hold great promise for accelerating route and process development, ultimately delivering medicines to patients both faster and at lower costs.\n\n|     |     |     |\n| --- | --- | --- |\n| [![image file: d0sc04896h-f1.tif](https://pubs.rsc.org/image/article/2021/SC/d0sc04896h/d0sc04896h-f1.gif)](https://pubs.rsc.org/image/article/2021/SC/d0sc04896h/d0sc04896h-f1_hi-res.gif) |\n|  | **Fig. 1**(a) Example of synthetic route to a drug compound. Prospects for AI-assisted route design. (b) Accurate prediction of reaction barriers gives both rate and selectivity. (c) The nucleophilic aromatic substitution (SNAr) reaction. |  |\n\nThe current workhorse for computational studies of organic reactions is density functional theory (DFT, [Fig. 2a](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#imgfig2)). Since rising to prominence in the early 90s, DFT has enjoyed extraordinary success in rationalizing reactivity and selectivity across the reaction spectrum by modelling the full reaction mechanism. [12](https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h#cit12) The success of DFT can be traced in part due to a fortuitous cancellation of errors, which makes it particularly suited for properties such as enantioselectivity, which depends on the relative energies of two structurally very similar transition states (TSs). However, this cancellation of errors does not generally extend to the prediction of the absolute magnitude of reactions barriers (activation free energies, \u0394G\u2021). In particular, DFT struggles with one very important class of reactions: ionic reactions in solution. Plata and Singleton even sug...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2021/sc/d0sc04896h"
    },
    {
      "title": "Deep Kernel Learning for Reaction Outcome Prediction",
      "text": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato Deep Kernel Learning for Reaction Outcome Prediction https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8\nDeep Kernel Learning for Reaction Outcome Prediction\nJos\u00e9 Miguel Hern\u00e1ndez-Lobato\n2025-01-26T16:32:00Z\n# Deep Kernel Learning for Reaction Outcome Prediction and Optimization\nS. Singh and J. M. Hern\u00e1ndez-lobato\nDiscover an innovative deep kernel learning model developed by Sukriti Singh and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato that predicts chemical reaction outcomes with remarkable precision. This cutting-edge approach combines the power of neural networks and Gaussian processes, offering not just accurate predictions but also valuable uncertainty estimates, making it an exciting advancement in optimizing reaction conditions.\n00:00\n00:00\n~3 min \u2022 Beginner \u2022 English\nChat with Paper\nIndex\n- [General](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=general)\n- [Summary](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=summary)\n- [Metrics](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=metrics)\n[General](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=general) [Summary](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=summary) [Metrics](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?nav=metrics)\nAbstract\nRecent years have seen a rapid growth in the application of various machine learning methods for reaction outcome prediction. Deep learning models have gained popularity due to their ability to learn representations directly from the molecular structure. Gaussian processes (GPs), on the other hand, provide reliable uncertainty estimates but are unable to learn representations from the data. We combine the feature learning ability of neural networks (NNs) with uncertainty quantification of GPs in a deep kernel learning (DKL) framework to predict the reaction outcome. The DKL model is observed to obtain very good predictive performance across different input representations. It significantly outperforms standard GPs and provides comparable performance to graph neural networks, but with uncertainty estimation. Additionally, the uncertainty estimates on predictions provided by the DKL model facilitated its incorporation as a surrogate model for Bayesian optimization (BO). The proposed method, therefore, has a great potential towards accelerating reaction discovery by integrating accurate predictive models that provide reliable uncertainty estimates with BO.\nPublisher\nCommunications Chemistry\nPublished On\nJun 14, 2024\nAuthors\nSukriti Singh, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato\nTags\ndeep kernel learning\nchemical reactions\nuncertainty quantification\nBayesian optimization\nneural networks\nGaussian processes\noptimization\n#### Related Publications\nExplore these studies to deepen your understanding of the subject.\n[Medicine and Health\nA multimodal deep learning approach for the prediction of cognitive decline and its effectiveness in clinical trials for Alzheimer\u2019s disease\nC. Wang, H. Tachimori, et al.]\n[Medicine and Health\nContext Aware Deep Learning for Brain Tumor Segmentation, Subtype Classification, and Survival Prediction Using Radiology Images\nL. Pei, L. Vidyaratne, et al.]\n[Medicine and Health\nDevelopment and evaluation of deep learning algorithms for assessment of acute burns and the need for surgery\nC. Boissin, L. Laflamme, et al.](https://researchbunny.com/papers/development-and-evaluation-of-deep-learning-algorithms-for-assessment-of-acute-burns-and-the-need-for-surgery-7cta)\n[Medicine and Health\nDesign and Analysis of a Deep Learning Ensemble Framework Model for the Detection of COVID-19 and Pneumonia Using Large-Scale CT Scan and X-ray Image Datasets\nX. Xue, S. Chinnaperumal, et al.]\nListen, Learn & Level Up\nOver 10,000 hours of research content in 25+ fields, available in 12+ languages.\nNo more digging through PDFs, just hit play and absorb the world's latest research in your language, on your time.\n[Start Listening - It's free](https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8?action=showauth)",
      "url": "https://researchbunny.com/papers/deep-kernel-learning-for-reaction-outcome-prediction-and-optimization-pqs8"
    }
  ]
}