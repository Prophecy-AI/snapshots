{
  "query": "What are the best techniques for predicting properties of unseen chemical compounds using molecular descriptors when the test set contains completely new molecules?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Predicting the properties of truly novel molecules is most reliable when the modelling pipeline is built around **robust descriptor\u2011based learners** and **rigorous out\u2011of\u2011distribution (OOD) validation**.  Gradient\u2011boosting decision\u2011tree ensembles (e.g., XGBoost, LightGBM) consistently rank among the top classical methods for descriptor\u2011driven QSAR because they handle high\u2011dimensional, sparse fingerprints (such as ECFP4) and capture non\u2011linear structure\u2011property relationships while remaining fast to train and easy to tune\u30103\u3011.  Random\u2011forest models perform similarly, but boosting often yields better calibration on heterogeneous data.  Crucially, model selection should be guided by OOD splits that mimic the intended use case: scaffold\u2011based splitting (Bemis\u2011Murcko scaffolds) provides a realistic estimate of performance on new chemotypes and shows a strong correlation (Pearson\u202fr\u202f\u2248\u202f0.9) between in\u2011distribution and OOD results, whereas similarity\u2011cluster splits are far more challenging and expose over\u2011optimistic ID scores\u30101\u3011.  Using these splits during hyper\u2011parameter optimisation helps identify models that generalise beyond the training chemical space.\n\nTo further guard against unreliable predictions on unseen scaffolds, **uncertainty quantification** is recommended.  Conformal prediction wraps any deterministic descriptor model (including gradient\u2011boosted trees) and yields calibrated prediction intervals that automatically widen for molecules that are dissimilar to the training set, thereby flagging high\u2011risk forecasts\u301010\u3011.  When data are scarce, **transfer\u2011learning or surrogate\u2011model strategies** that combine a small set of high\u2011quality quantum\u2011mechanical descriptors with a pre\u2011trained deep model can improve data efficiency, but the resulting hidden representations should still be benchmarked against pure descriptor baselines because the latter often remain competitive on low\u2011data OOD tasks\u30105\u3011.  Recent benchmarking studies (e.g., BOOM) confirm that ensembles of well\u2011tuned gradient\u2011boosting models, evaluated with scaffold\u2011aware splits and equipped with conformal uncertainty, achieve the best trade\u2011off between accuracy and reliability for completely new molecules\u30107\u3011.\n\nIn practice, an effective workflow therefore consists of: (1) computing a rich set of molecular descriptors (e.g., ECFP4, physicochemical counts, 3\u2011D\u2011derived features); (2) training a gradient\u2011boosted tree ensemble with hyper\u2011parameters tuned on scaffold\u2011split validation; (3) applying conformal prediction to obtain calibrated confidence intervals; and (4) optionally augmenting the descriptor set with transfer\u2011learned embeddings when a related high\u2011quality source dataset is available.  This combination maximises predictive power while explicitly accounting for the uncertainty inherent in extrapolating to unseen chemical space.",
      "url": ""
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Inductive transfer learning for molecular activity prediction: Next-Gen QSAR Models with MolPMoFiT",
      "text": "Search all BMC articles\n\nSearch\n\nInductive transfer learning for molecular activity prediction: _Next_- _Gen QSAR Models with MolPMoFiT_\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-020-00430-x.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-020-00430-x.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-020-00430-x.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-020-00430-x.epub)\n\n- Research article\n- [Open access](https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 22 April 2020\n\n# Inductive transfer learning for molecular activity prediction: _Next_- _Gen QSAR Models with MolPMoFiT_\n\n- [Xinhao Li](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#auth-Xinhao-Li-Aff1) [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#Aff1) &\n- [Denis Fourches](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#auth-Denis-Fourches-Aff1)[ORCID: orcid.org/0000-0001-5642-8303](http://orcid.org/0000-0001-5642-8303)[1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#Aff1)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a012**, Article\u00a0number:\u00a027 (2020)\n[Cite this article](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#citeas)\n\n- 13k Accesses\n\n- 74 Citations\n\n- 17 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x/metrics)\n\n\n## Abstract\n\nDeep neural networks can directly learn from chemical structures without extensive, user-driven selection of descriptors in order to predict molecular properties/activities with high reliability. But these approaches typically require large training sets to learn the endpoint-specific structural features and ensure reasonable prediction accuracy. Even though large datasets are becoming the new normal in drug discovery, especially when it comes to high-throughput screening or metabolomics datasets, one should also consider smaller datasets with challenging endpoints to model and forecast. Thus, it would be highly relevant to better utilize the tremendous compendium of unlabeled compounds from publicly-available datasets for improving the model performances for the user\u2019s particular series of compounds. In this study, we propose the **Mol** ecular **P** rediction **Mo** del **Fi** ne- **T** uning ( **MolPMoFiT**) approach, an effective transfer learning method based on self-supervised pre-training\u2009+\u2009task-specific fine-tuning for QSPR/QSAR modeling. A large-scale molecular structure prediction model is pre-trained using one million unlabeled molecules from ChEMBL in a self-supervised learning manner, and can then be fine-tuned on various QSPR/QSAR tasks for smaller chemical datasets with specific endpoints. Herein, the method is evaluated on four benchmark datasets (lipophilicity, FreeSolv, HIV, and blood\u2013brain barrier penetration). The results showed the method can achieve strong performances for all four datasets compared to other _state_- _of_- _the_- _art_ machine learning modeling techniques reported in the literature so far.\n\n![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13321-020-00430-x/MediaObjects/13321_2020_430_Figa_HTML.png)\n\n## Introduction\n\nPredicting properties/activities of chemicals from their structures is one of the key objectives in cheminformatics and molecular modeling. Quantitative structure property/activity relationship (QSPR/QSAR) modeling \\[ [1](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR1), [2](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR2), [3](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR3), [4](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR4), [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR5), [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR6)\\] relies on machine learning techniques to establish quantified links between molecular structures and their experimental properties/activities. When using a classic machine learning approach, the training process is divided into two main steps: feature extraction/calculation and the actual modeling. The features (also called _descriptors_) characterizing the molecular structures are critical for the model performances. They typically encompass 2D molecular fingerprints, topological indices, or substructural fragments, as well as more complex 3D and 4D descriptors \\[ [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR8)\\] directly computed from the molecular structures \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR9)\\].\n\nDeep learning methods have demonstrated remarkable performances in several QSPR/QSAR case studies. In addition to use expert-engineered molecular descriptors as input, those techniques can also directly take molecular structures ( _e.g.,_ molecular graph \\[ [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR10), [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR12), [13](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR13), [14](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR14), [15](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR15), [16](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR16), [17](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR18), [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR20), [21](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR21)\\], SMILES strings \\[ [22](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR22), [23](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR23), [24](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR24)\\], and molecular 2D/3D grid image \\[ [25](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR25), [26](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR26), [27](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR27), [28](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR28), [29](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR29), [30](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR30)\\]) and learn the data-driven feature representations for predicting properties/activities. As a result, this type of approach is potentially able to capture and extract underlying, complex structural patterns and feature \u2194 property relationships given sufficient amount of training data. The knowledge derived from these dataset-specific descriptors can then be used to better interpret and understand the structure\u2013property relationships as well as to design new compounds. In a large scale benchmark study, Yang et al. \\[ [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x#ref-CR12)\\] shown that a graph convolutional model that construct a learned representation from molecular graph consistently matches or outperforms models trained with expert-engineered molecular de...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00430-x"
    },
    {
      "title": "Practical guidelines for the use of gradient boosting for molecular property prediction",
      "text": "Practical guidelines for the use of gradient boosting for molecular property prediction | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/s13321-023-00743-7?)\n# Practical guidelines for the use of gradient boosting for molecular property prediction\n* Research\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:28 August 2023\n* Volume\u00a015, article\u00a0number73, (2023)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00743-7.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nPractical guidelines for the use of gradient boosting for molecular property prediction\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/s13321-023-00743-7.pdf)\n* [Davide Boldini](#auth-Davide-Boldini-Aff1)[1](#Aff1),\n* [Francesca Grisoni](#auth-Francesca-Grisoni-Aff2-Aff3)[2](#Aff2),[3](#Aff3),\n* [Daniel Kuhn](#auth-Daniel-Kuhn-Aff4)[4](#Aff4),\n* [Lukas Friedrich](#auth-Lukas-Friedrich-Aff4)[4](#Aff4)&amp;\n* \u2026* [Stephan A. Sieber](#auth-Stephan_A_-Sieber-Aff1)[1](#Aff1)Show authors\n* 9935Accesses\n* 80Citations\n* 13Altmetric\n* 1Mention\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7/metrics)\n## Abstract\nDecision tree ensembles are among the most robust, high-performing and computationally efficient machine learning approaches for quantitative structure\u2013activity relationship (QSAR) modeling. Among them, gradient boosting has recently garnered particular attention, for its performance in data science competitions, virtual screening campaigns, and bioactivity prediction. However, different variants of gradient boosting exist, the most popular being XGBoost, LightGBM and CatBoost. Our study provides the first comprehensive comparison of these approaches for QSAR. To this end, we trained 157,590 gradient boosting models, which were evaluated on 16 datasets and 94 endpoints, comprising 1.4 million compounds in total. Our results show that XGBoost generally achieves the best predictive performance, while LightGBM requires the least training time, especially for larger datasets. In terms of feature importance, the models surprisingly rank molecular features differently, reflecting differences in regularization techniques and decision tree structures. Thus, expert knowledge must always be employed when evaluating data-driven explanations of bioactivity. Furthermore, our results show that the relevance of each hyperparameter varies greatly across datasets and that it is crucial to optimize as many hyperparameters as possible to maximize the predictive performance. In conclusion, our study provides the first set of guidelines for cheminformatics practitioners to effectively train, optimize and evaluate gradient boosting models for virtual screening and QSAR applications.\n### Graphical abstract\n![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs13321-023-00743-7/MediaObjects/13321_2023_743_Figa_HTML.png)\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-99-1435-7?as&#x3D;webp)\n### [Using the Light Gradient Boosting Machine for Prediction in QSAR Models](https://link.springer.com/10.1007/978-981-99-1435-7_10?fromPaywallRec=false)\nChapter\u00a9 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1140%2Fepje%2Fs10189-025-00491-6/MediaObjects/10189_2025_491_Figa_HTML.png)\n### [Improved QSAR methods for predicting drug properties utilizing topological indices and machine learning models](https://link.springer.com/10.1140/epje/s10189-025-00491-6?fromPaywallRec=false)\nArticle09 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs13738-019-01835-8/MediaObjects/13738_2019_1835_Fig1_HTML.png)\n### [Molecular docking and 4D-QSAR model of methanone derivatives by electron conformational-genetic algorithm method](https://link.springer.com/10.1007/s13738-019-01835-8?fromPaywallRec=false)\nArticle21 December 2019\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Cheminformatics](https://jcheminf.biomedcentral.com/subjects/cheminformatics)\n* [Computational Chemistry](https://jcheminf.biomedcentral.com/subjects/computational-chemistry)\n* [Learning algorithms](https://jcheminf.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n* [Structure Prediction](https://jcheminf.biomedcentral.com/subjects/structure-prediction)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Introduction\nQuantitative structure\u2013activity relationship (QSAR) modelling occupies a vital role in cheminformatics research [[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR5)]. QSAR aims to link the molecular structure with experimentally measurable properties, and it is routinely used to predict molecular properties such as bioactivity [[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR9)], toxicity [[10](#ref-CR10),[11](#ref-CR11),[12](#ref-CR12),[13](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR13)] and absorption, distribution, metabolism and excretion (ADME) [[3](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR3),[14](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR14)], thus covering a fundamental role in both hit discovery and hit-to-lead optimization.\nQSAR aims to link the molecular structure (numerically encoded as the so-called molecular descriptors) [[15](#ref-CR15),[16](#ref-CR16),[17](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR17)] with experimentally measurable properties. For this application, decision tree ensembles are among the most used machine learning methods thanks to their excellent performance, ability to rank features in terms of importance and their ability to scale to large datasets [[18](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR18),[19](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR19)], alongside other popular frameworks like support vector machines (SVM) [[20](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR20),[21](https://jcheminf.biomedcentral.com/article/10.1186/s13321-023-00743-7#ref-CR21)].\nAmong decision tree ensembles, gradient boosting machines (GBM) have seen a strong surge in popularity in the last years, driven by excellent results in data science competitions and state-of-the-a...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00743-7"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Harnessing surrogate models for data-efficient predictive chemistry: descriptors  vs.  learned hidden representations",
      "text": "Harnessing surrogate models for data-efficient predictive chemistry: descriptors vs. learned hidden representations - Digital Discovery (RSC Publishing) DOI:10.1039/D5DD00256G\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d5dd00256g)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d5dd00163c)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d5dd00245a)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D5DD00256G](https://doi.org/10.1039/D5DD00256G)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 3227-3237\n# Harnessing surrogate models for data-efficient predictive chemistry: descriptorsvs.learned hidden representations\nGuanming Chen[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0009-0004-2581-1712)andThijs Stuyver[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8322-0572)\\*\nEcole Nationale Sup\u00e9rieure de Chimie de Paris, Universit\u00e9 PSL, CNRS, i-CLeHS, 75 005 Paris, France. E-mail:[thijs.stuyver@chimieparistech.psl.eu](mailto:thijs.stuyver@chimieparistech.psl.eu)\nReceived 9th June 2025, Accepted 13th September 2025\nFirst published on 22nd September 2025\n## Abstract\nPredictive chemistry often faces data scarcity, limiting the performance of machine learning (ML) models. This is particularly the case for specialized tasks such as reaction rate or selectivity prediction. A common solution is to use quantum mechanical (QM) descriptors\u2014physically meaningful features derived from electronic structure calculations\u2014to enhance model robustness in low-data regimes. However, computing these descriptors is costly. Surrogate models address this by predicting QM descriptors directly from molecular structure, enabling fast and scalable input generation for data-efficient downstream ML models. In this study, we compare two strategies for using surrogate models: one that feeds predicted QM descriptors into downstream models, and another that leverages the surrogate's internal hidden representations instead. Across a diverse set of chemical prediction tasks, we find that hidden representations often outperform QM descriptors, particularly when descriptor selection is not tightly aligned with the downstream task. Only for extremely small datasets or when using carefully selected, task-specific descriptors do the predicted values yield better performance. Our findings highlight that the hidden space of surrogate models captures rich, transferable chemical information, offering a robust and efficient alternative to explicit descriptor use. We recommend this strategy for building data-efficient models in predictive chemistry, especially when feature importance analysis is not a primary goal.\n## Introduction\nA common issue faced when designing data-driven models in chemistry is data scarcity.[1,2](#cit1)For many specialized predictive tasks,e.g., prediction of reaction rates, enantiomeric excess, and solvation energies, only limited amounts of relevant and accurate data can be mined from the literature, and high-throughput experimentation is technically challenging and/or prohibitively expensive.[3,4](#cit3)As a consequence, datasets for these types of tasks typically only contain several hundred, up to a couple of thousand, data points at best. In such a data-limited regime, conventional machine learning (ML) algorithms tend to perform poorly.\nOne strategy to address the issue of data scarcity consists of representation engineering,i.e., describing the molecules/reactions through a limited set of (carefully selected) informative, physically meaningful descriptors, so that a robust relationship between ML model input and output can be learned.[5\u20137](#cit5)Quantum Mechanical (QM) descriptors are a particularly popular choice in this regard. Unfortunately, the calculation of such descriptors typically requires resource-intensive density functional theory (DFT) calculations,[8\u201312](#cit8)which limits the applicability of this approach to big datasets, as well as to use cases where inference is expected at a high-throughput speed.\nAn alternative strategy that has been pioneered in recent years is to avoid the explicit calculation of QM descriptors, by predicting their values for unseen molecules and/or reactions with the help of a surrogate ML model.[1,13\u201316](#cit1)Taking this approach, QM descriptors can be inferred on-the-fly, so that the generation of the input representation of the downstream model can be seamlessly integrated into a single end-to-end model. This aggregate model then rivals regular ML models in terms of inference speed and computational resource footprint, while enabling higher accuracy and increased robustness due to the physical information encoded in the intermediately predicted descriptors.\nOf course, setting up such a surrogate model still requires an initial training dataset, constructed through high-throughput QM calculations. However, once generated, this data, as well as the resulting surrogate models, can be applied to \u2013and repurposed for \u2013different downstream prediction tasks. Consequently, a range of high-throughput QM datasets have been released in recent years. In addition to the prototypical QM9 dataset,[17](#cit17)one of the earliest large-scale examples was the QMugs[18,19](#cit18)dataset, which contains a wide range of quantum mechanically (QM) computed descriptors and properties for 665k biologically and pharmacologically relevant molecules extracted from the ChEMBL[20](#cit20)database. Other examples include the QM40 (ref.[21](#cit21)) dataset, which contains QM properties for 163k compounds extracted from the ZINC[22](#cit22)database, the QCDGE[23](#cit23)dataset, which contains both ground- and excited-state properties for 450k C, H, N, O, F containing compounds, the BDE-db[24](#cit24)dataset, which contains QM descriptors for more than 200k organic radicals, and the tmQM[25](#cit25)dataset, which contains properties for 86k transition metal complexes.\nThe growing availability and diversity of public QM descriptor datasets indicate that surrogate modeling will become increasingly accessible\u2014and presumably more widely adopted as a consequence\u2014in the years to come. As such, it is important to establish guidelines on how the QM information, captured in these datasets, can be leveraged optimally for downstream tasks.\nTaking a closer look at the typical architecture of the surrogate models used so far,[1,14,15](#cit1)one can conclude that they generally start from a SMILES string from which a molecular graph is deduced. The atomic vectors of this graph are subsequently embedded into a learned (hidden) representation, after which multiple feed-forward neural networks (FFNN), or readout functions, lead to the actual QM descriptors, that is, the surrogate model targets ([Fig. 1](#imgfig1)).\n[![image file: d5dd00256g-f1.tif](https://pubs.rsc.org/image/article/2025/DD/d5dd00256g/d5dd00256g-f1.gif)](https://pubs.rsc.org/image/article/2025/DD/d5dd00256g/d5dd00256g-f1_hi-res.gif)|\n|**Fig. 1**Predictive chemistry with surrogate models: scheme of the surrogate model and the downstream reactivity prediction model architectures employed in this study, taking reaction prediction as an example. The surrogate model is trained with external quantum chemical descriptor datasets to generate informative representations on-the-fly. The SMILES representation of a molecule is first converted into a 2-D molecula...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d5dd00256g"
    },
    {
      "title": "Transfer learning for a foundational chemistry model \u2020",
      "text": "Transfer learning for a foundational chemistry model - Chemical Science (RSC Publishing) DOI:10.1039/D3SC04928K\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2024/sc/d3sc04928k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d4sc01122h)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2024/sc/d4sc00090k)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D3SC04928K](https://doi.org/10.1039/D3SC04928K)(Edge Article)[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2024,**15**, 5143-5151\n# Transfer learning for a foundational chemistry model[\u2020](#fn1)\nEmma King-Smith[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2999-0955)\\*\nCavendish Laboratory, University of Cambridge, Cambridge, UK. E-mail:[esk34@cam.ac.uk](mailto:esk34@cam.ac.uk)\nReceived 19th September 2023, Accepted 15th November 2023\nFirst published on 24th November 2023\n## Abstract\nData-driven chemistry has garnered much interest concurrent with improvements in hardware and the development of new machine learning models. However, obtaining sufficiently large, accurate datasets of a desired chemical outcome for data-driven chemistry remains a challenge. The community has made significant efforts to democratize and curate available information for more facile machine learning applications, but the limiting factor is usually the laborious nature of generating large-scale data. Transfer learning has been noted in certain applications to alleviate some of the data burden, but this protocol is typically carried out on a case-by-case basis, with the transfer learning task expertly chosen to fit the finetuning. Herein, I develop a machine learning framework capable of accurate chemistry-relevant prediction amid general sources of low data. First, a chemical \u201cfoundational model\u201d is trained using a dataset of \u223c1 million experimental organic crystal structures. A task specific module is then stacked atop this foundational model and subjected to finetuning. This approach achieves state-of-the-art performance on a diverse set of tasks: toxicity prediction, yield prediction, and odor prediction.\n## Introduction\nThe implementation of computerized algorithms into organic chemistry has had a rich history, with early emphasis centered around deriving linear relationships from observed results.[1](#cit1)By the 1970s, synthetic chemists had turned their attention to utilizing more complex functions to model more abstract observations. In 1977, Coreyet al.published the first recognized retrosynthetic analyzer, LHASA (Logics and Heuristics Applied to Synthetic Analysis) which featured hand coded expert rules resulting in over 30![[thin space (1/6-em)]](https://www.rsc.org/images/entities/char_2009.gif)000 lines of FORTRAN code.[2](#cit2)With the turn of the century, improvements in computational hardware and the development of new computer learning algorithms, including machine learning (ML), have seeded new avenues for algorithm-based predictive chemistry.[3,4](#cit3)ML is the process of taking complex inputs, abstracting their relevant features through non-linear equations, and correlating those features to a given output. Despite its simplistic framework, variations upon this theme have yielded advances in numerous areas including fundamental molecular property prediction (e.g., quantum chemical, ADMET), reaction property prediction (e.g., regioselectivity, yield), and generative modeling.[5\u20137](#cit5)With these more powerful algorithms come higher data requirements. Where the first data-driven chemistry models may have necessitated a few experimental results, ML often demands tens of thousands of data points. Purely computational datasets from density functional theory (DFT) or semi-empirical methods have been generated and utilized in ML for prediction of quantum chemical properties (e.g., HOMO\u2013LUMO gaps, dipole moments)[8](#cit8)and in molecular scaffold generation.[9](#cit9)Benefits of using these datasets include larger sizes and less noise present within each observation. Whilst experimental data is inherently noisier, more expensive, and often more laborious to generate than computational data, it presents a more holistic representation of a chemical system, even if we do not fully understand the intricacies present within that system. It is therefore important for the community to find ways to incorporate these smaller, experimental datasets as a key feature into ML tools.\nOne method that has seen potential towards the utilization of smaller datasets in deep ML is transfer learning.[10](#cit10)In this process, a model is first trained on a large dataset. The target prediction of the first task (pretraining task) does not need to be directly related to the desired final task (finetuning task), however, the initial knowledge gained from pretraining must have some relevancy to the finetuning. In a neural network, each non-linear function is referred to as a layer; each layer in the neural network is responsible for extracting relevant chemical features for a given task. The first layer is defined as the input layer, the final layer as the output layer, and for this manuscript, the penultimate layer is defined as the latent space ([Fig. 1](#imgfig1)). One may imagine the latent space as a complete digitization of a molecule, whereby each molecular feature has been assigned a series of numbers. The key to effective transfer learning hinges around this latent space, whereby each molecule's chemically relevant features are so well characterized that the substitution of one output layer for another output layer results in accurate prediction of a different chemical property (see ESI: A non-expert's guide to transfer learning (CliffsNotes version) on p. S3 for further explanation of transfer learning[\u2020](#fn1)). In essence, the model transfers the knowledge it learnt from pretraining to finetuning. To date, transfer learning in data-driven chemistry has been applied on a case-by-case basis, where pretraining tasks are expertly chosen for specific finetunings to minimize domain mismatch.[10](#cit10)This limits the ML possibilities for smaller datasets.\n[![image file: d3sc04928k-f1.tif](https://pubs.rsc.org/image/article/2024/SC/d3sc04928k/d3sc04928k-f1.gif)](https://pubs.rsc.org/image/article/2024/SC/d3sc04928k/d3sc04928k-f1_hi-res.gif)|\n|**Fig. 1**Graphical overview of the framework. Top panel illustrates the pretraining process and the structure of deep learning neural networks. The bottom panel shows how the top large model (foundational model) can be used for new chemistry-relevant predictionsviathe foundational model's latent space.||\nHerein, I report the development of a general chemistry-centric foundational model utilizing transfer learning, capitalizing upon the molecular featurization from the resultant latent space. Rather than concocting a molecular representation through manual descriptor selection like in traditional QSAR, an underlying model, dubbed the \u201cfoundational model\u201d is utilized to generate the molecular representation, from which further training can be carried out to predict any endpoint properties of choice in a modular fashion, re-using (transferring) knowledge acquired in the first step. The goal of the foundational model is to ensure that enough relevant chemical information is present in the molecular representation (See ESI: A non-expert's guide to transfer learning (CliffsNotes version) on p. S3 for further explana...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04928k"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2505.01912] BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2505.01912\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2505.01912**(cs)\n[Submitted on 3 May 2025 ([v1](https://arxiv.org/abs/2505.01912v1)), last revised 19 Dec 2025 (this version, v2)]\n# Title:BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nAuthors:[Evan R. Antoniuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Antoniuk,+E+R),[Shehtab Zaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaman,+S),[Tal Ben-Nun](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Nun,+T),[Peggy Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[James Diffenderfer](https://arxiv.org/search/cs?searchtype=author&amp;query=Diffenderfer,+J),[Busra Sahin](https://arxiv.org/search/cs?searchtype=author&amp;query=Sahin,+B),[Obadiah Smolenski](https://arxiv.org/search/cs?searchtype=author&amp;query=Smolenski,+O),[Tim Hsu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsu,+T),[Anna M. Hiszpanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Hiszpanski,+A+M),[Kenneth Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+K),[Bhavya Kailkhura](https://arxiv.org/search/cs?searchtype=author&amp;query=Kailkhura,+B),[Brian Van Essen](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Essen,+B)\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n[View PDF](https://arxiv.org/pdf/2505.01912)[HTML (experimental)](https://arxiv.org/html/2505.01912v2)> > Abstract:\n> Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\\mathbf{BOOM}$, $\\mathbf{b}$enchmarks for $\\mathbf{o}$ut-$\\mathbf{o}$f-distribution $\\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at [> this https URL\n](https://github.com/FLASK-LLNL/BOOM)> Subjects:|Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2505.01912](https://arxiv.org/abs/2505.01912)[cs.LG]|\n|(or[arXiv:2505.01912v2](https://arxiv.org/abs/2505.01912v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2505.01912](https://doi.org/10.48550/arXiv.2505.01912)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Evan Antoniuk [[view email](https://arxiv.org/show-email/6abb5e36/2505.01912)]\n**[[v1]](https://arxiv.org/abs/2505.01912v1)**Sat, 3 May 2025 19:51:23 UTC (35,134 KB)\n**[v2]**Fri, 19 Dec 2025 23:00:10 UTC (16,070 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2505.01912)\n* [HTML (experimental)](https://arxiv.org/html/2505.01912v2)\n* [TeX Source](https://arxiv.org/src/2505.01912)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2505.01912&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2505.01912&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-05](https://arxiv.org/list/cs.LG/2025-05)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/2505.01912?context=cond-mat)\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2505.01912?context=cond-mat.mtrl-sci)\n[cs](https://arxiv.org/abs/2505.01912?context=cs)\n[cs.AI](https://arxiv.org/abs/2505.01912?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.01912)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.01912)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.01912)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2505.01912&amp;description=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2505.01912&amp;title=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*...",
      "url": "https://arxiv.org/abs/2505.01912"
    },
    {
      "title": "Machine learning methods in chemoinformatics",
      "text": "Machine learning methods in chemoinformatics - PMC[Skip to main content](#main-content)\n![](https://ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1002/wcms.1183)\n* [](pdf/wcms0004-0468.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://ncbi.nlm.nih.gov/about/copyright/)\n![Wiley Open Access Collection logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-blackwellopen.png)\nWiley Interdiscip Rev Comput Mol Sci\n. 2014 Feb 24;4(5):468\u2013481. doi:[10.1002/wcms.1183](https://doi.org/10.1002/wcms.1183)\n# Machine learning methods in chemoinformatics\n[John B O Mitchell](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Mitchell JBO\"[Author]>)\n### John B O Mitchell\n1School of Chemistry, University of St Andrews, St Andrews, UK\nFind articles by[John B O Mitchell](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Mitchell JBO\"[Author]>)\n1,\\*\n* Author information\n* Article notes\n* Copyright and License information\n1School of Chemistry, University of St Andrews, St Andrews, UK\n\u2709\\*Correspondence to:jbom@st-andrews.ac.uk\nConflict of interest: The author has declared no conflicts of interest for this article.\nIssue date 2014 Sep.\n\u00a92014 The Authors.*WIREs Computational Molecular Science*published by John Wiley &amp; Sons,\nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.\n[PMC Copyright notice](https://ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC4180928\u00a0\u00a0EMSID:[EMS57556](https://ncbi.nlm.nih.gov/articles/mid/EMS57556/)PMID:[25285160](https://pubmed.ncbi.nlm.nih.gov/25285160/)\n## Abstract\nMachine learning algorithms are generally developed in computer science or adjacent disciplines and find their way into chemical modeling by a process of diffusion. Though particular machine learning methods are popular in chemoinformatics and quantitative structure\u2013activity relationships (QSAR), many others exist in the technical literature. This discussion is methods-based and focused on some algorithms that chemoinformatics researchers frequently use. It makes no claim to be exhaustive. We concentrate on methods for supervised learning, predicting the unknown property values of a test set of instances, usually molecules, based on the known values for a training set. Particularly relevant approaches include Artificial Neural Networks, Random Forest, Support Vector Machine, k-Nearest Neighbors and na\u00efve Bayes classifiers.*WIREs Comput Mol Sci*2014, 4:468\u2013481.\n**How to cite this article:***WIREs Comput Mol Sci*2014, 4:468\u2013481. doi:[10.1002/wcms.1183](https://doi.org/10.1002/wcms.1183)\n## INTRODUCTION\nThe field known as chemoinformatics, or sometimes cheminformatics, can be considered as that part of computational chemistry whose models are*not*based on reproducing the real physics and chemistry by which the world works at the molecular scale. Unlike quantum chemistry or molecular simulation, which are designed to model physical reality, chemoinformatics is intended simply to produce useful models that can predict chemical and biological properties of compounds given the two-dimensional (or sometimes three, see[Box 1](#box1)chemical structure of a molecule.\n### REPRESENTING MOLECULES: TWO OR THREE-DIMENSIONAL?\nIn chemoinformatics, the researcher is presented with a fundamental dilemma\u2014should the molecules be described with two- or three-dimensional representations? A two-dimensional representation is essentially a molecular graph with the atoms as nodes and the bonds as edges. Onto this may be added extra information, such as bond orders, and the stereochemistry about double bonds and at chiral centers. Such a representation of chemical structure is essentially a digitized form of the structural diagrams familiar to chemists, and lacks the explicit spatial coordinates of the atoms.\nAn alternative approach is to generate a three-dimensional structure. This can be done from the molecular graph or connection table using a program such as CORINA,[7](#b7),[8](#b8)from a crystal structure, or from a quantum chemical calculation. Although a three-dimensional structure carries additional information, the difficulty is that molecules generally exist as an equilibrium between multiple conformers. Even if our structure correctly represents the lowest energy, and hence most abundant, conformer, alternative conformations may be critical for biological functions such as protein binding. Nonetheless, using three-dimensional structure opens up possibilities like scaffold hopping in drug design, where molecules with diverse two-dimensional structures but similar three-dimensional shapes may bind the same target. Sheridan and Kearsley's review[9](#b9)and an article by the Ritchie group[10](#b10)are two of the numerous papers discussing the relative merits of two and three-dimensional molecular representations.\nThe history of chemoinformatics began with local models, typically for quantitative structure\u2013activity relationships (QSAR) or quantitative structure\u2013property relationships (QSPR). Popular versions of this history usually begin with Hammett or Hansch,[1](#b1),[2](#b2)though Borman has followed the trail of QSAR back into the 19th century.[3](#b3)Early models were generally based on linear, and later multilinear, regression. These were typically built using only a very few features, and were valid only for a small series of closely related compounds. Interestingly, machine learning and pattern recognition methods have an association with chemistry going back more than four decades, with methods like the linear learning machine being applied to problems such as the interpretation of spectroscopic data, as discussed in an early review by Kowalski.[4](#b4)\nIn contrast to the very small applicability domains of early QSAR studies, much recent work has concentrated on global models, by which we mean models trained on and hence valid for a wide range of organic or drug-like compounds. A number of factors, most notably the availability of data for molecules spanning a much wider chemical space, the use of a large and diverse selection of descriptors, and the development of sophisticated nonlinear machine-learning algorithms have increased the use of such global models in recent years.\n## CHEMOINFORMATICS\n### From Molecules to Features to Properties\nAlthough to some extent a postrationalization, it is helpful to consider chemoinformatics model building as a two-part process.[5](#b5)Firstly a molecular structure, typically represented as a molecular graph or co...",
      "url": "https://ncbi.nlm.nih.gov/pmc/articles/PMC4180928"
    }
  ]
}