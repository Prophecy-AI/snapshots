{
  "query": "What neural network architectures achieve the best performance on small molecule property prediction with limited training data of 20-30 samples?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Small\u2011molecule property prediction in the ultra\u2011low\u2011data regime (\u2248\u202f20\u201330 labeled compounds) is dominated by **graph\u2011based neural networks that are equipped with meta\u2011learning or transfer\u2011learning strategies**.  Benchmark studies published after 2023 show that a **message\u2011passing neural network (MPNN) such as PaiNN** consistently outperforms other deep\u2011learning baselines when only a few dozen samples are available, especially when the model is first pre\u2011trained on large, inexpensive quantum\u2011chemical or semi\u2011empirical datasets and then fine\u2011tuned on the target set\u202f([arXiv\u202f2024](https://arxiv.org/abs/2404.13393)).  Complementary to pure MPNNs, **heterogeneous meta\u2011learning frameworks** that treat standard graph encoders (e.g., Graph Isomorphism Network\u202f\u2013\u202fGIN, Pre\u2011GNN) as \u201cproperty\u2011specific\u201d modules and pair them with a self\u2011attention encoder for shared generic knowledge achieve the highest few\u2011shot accuracy across diverse benchmarks\u202f([ScienceDirect\u202f2025](https://www.sciencedirect.com/science/article/pii/S2001037025003381)).  A related **few\u2011shot meta\u2011learning pipeline that fuses graph\u2011neural embeddings with convolutional\u2011network embeddings** also reports sizable gains (\u2248\u202f10\u202f% ROC\u2011AUC improvement) over conventional graph baselines on 10\u2011shot tasks\u202f([Springer\u202f2023](https://link.springer.com/content/pdf/10.1007/s00521-023-08403-5.pdf)).\n\nAcross these studies, the common recipe for the 20\u201330\u2011sample setting is: (1) start from a **pre\u2011trained GNN/MPNN** (PaiNN, GIN, or other message\u2011passing variants), (2) apply **meta\u2011learning or task\u2011adaptive fine\u2011tuning** (heterogeneous meta\u2011learning, few\u2011shot adaptation), and (3) optionally augment the structural representation with **self\u2011attention or convolutional encoders** to capture contextual chemical knowledge.  This combination yields the best reported predictive performance under severe data scarcity, outperforming standard feed\u2011forward or purely descriptor\u2011based models.  ",
      "url": ""
    },
    {
      "title": "Context-informed few-shot molecular property prediction via ...",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- View\u00a0**PDF**\n- Download full issue\n\nSearch ScienceDirect\n\n## [Computational and Structural Biotechnology Journal](https://www.sciencedirect.com/journal/computational-and-structural-biotechnology-journal)\n\n[Volume 27](https://www.sciencedirect.com/journal/computational-and-structural-biotechnology-journal/vol/27/suppl/C), 2025, Pages 4173-4182\n\n# Research Article  Context-informed few-shot molecular property prediction via heterogeneous meta-learning\n\nAuthor links open overlay panelJunhaoXuea, JunLiubc, KaiChena\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.csbj.2025.08.016](https://doi.org/10.1016/j.csbj.2025.08.016) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S2001037025003381&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nOpen access\n\n## Highlights\n\n- \u2022\nWe take into account the diverse substructures present in molecules and reconceptualize graph-based embed- dings, such as GIN and Pre-GNN, as encoders of property-specific knowledge to capture contextual information effectively.\n\n- \u2022\nWe focus on the fundamental structures and commonalities of molecules, employing self-attention encoders as an extractor of generic knowledge for shared property.\n\n- \u2022\nWe present a general meta-learning algorithm that optimizes property-share and property-specific knowledge en- coders heterogeneously. This approach enables our algorithm to capture both general and contextual knowledge more effectively.\n\n- \u2022\nWe perform extensive experiments on real molecular datasets, comparing our model's performance with various alternatives. Our results demonstrate enhanced predictive accuracy, with a more significant performance improvement achieved using fewer training samples Highlights.\n\n\n## Abstract\n\nMolecular property prediction is essential in diversified applications, as it helps identify molecules with the desired characteristics. However, the task often suffers from limited data, making the few-shot learning challenging. We introduce a Context-informed Few-shot Molecular Property Prediction via a Heterogeneous Meta-Learning approach, which employs graph neural networks combined with self-attention encoders to effectively extract and integrate both property-specific and property-shared molecular features, respectively. Based on the property-shared molecular features, we further infer molecular relations by using an adaptive relational learning module. The final molecular embedding is improved by aligning with the property label in the property-specific classifier. Furthermore, we employ a heterogeneous meta-learning strategy that updates parameters of the property-specific features within individual tasks in the inner loop and jointly updates all parameters in the outer loop. This enhances the model's ability to effectively capture both general and contextual information, leading to a substantial improvement in predictive accuracy. The model's performance was rigorously evaluated across various real molecular datasets, showcasing its superiority over current methods, especially in challenging few-shot learning scenarios.\n\n## Graphical abstract\n\n1. [Download: Download high-res image (130KB)](https://ars.els-cdn.com/content/image/1-s2.0-S2001037025003381-gr001_lrg.jpg)\n2. [Download: Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S2001037025003381-gr001.jpg)\n\n- Previous article in issue\n- Next article in issue\n\n## Keywords\n\nMolecular property prediction\n\nFew-shot learning\n\nHeterogeneous meta-learning\n\nContext-informed learning\n\nRecommended articles\n\n## Data availability\n\nThe raw data used in this article can be obtained at [MoleculeNet](https://moleculenet.org/), a Benchmark for Molecular Machine Learning. You can also get the data at [here](https://drive.google.com/file/d/1K3c4iCFHEKUuDVSGBtBYr8EOegvIJulO/view?usp=sharing), which is shared by PAR [\\[19\\]](https://www.sciencedirect.com/www.sciencedirect.com#br0190). The source code of our method is accessible at [https://github.com/xuejunhao123/CFS-HML](https://github.com/xuejunhao123/CFS-HML).\n\n\u00a9 2025 Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.",
      "url": "https://www.sciencedirect.com/science/article/pii/S2001037025003381"
    },
    {
      "title": "Transfer Learning for Molecular Property Predictions from Small Data Sets",
      "text": "[View PDF](https://arxiv.org/pdf/2404.13393) [HTML (experimental)](https://arxiv.org/html/2404.13393v2)\n\n> Abstract:Machine learning has emerged as a new tool in chemistry to bypass expensive experiments or quantum-chemical calculations, for example, in high-throughput screening applications. However, many machine learning studies rely on small data sets, making it difficult to efficiently implement powerful deep learning architectures such as message passing neural networks. In this study, we benchmark common machine learning models for the prediction of molecular properties on two small data sets, for which the best results are obtained with the message passing neural network PaiNN, as well as SOAP molecular descriptors concatenated to a set of simple molecular descriptors tailored to gradient boosting with regression trees. To further improve the predictive capabilities of PaiNN, we present a transfer learning strategy that uses large data sets to pre-train the respective models and allows to obtain more accurate models after fine-tuning on the original data sets. The pre-training labels are obtained from computationally cheap ab initio or semi-empirical models and both data sets are normalized to mean zero and standard deviation one to align the labels' distributions. This study covers two small chemistry data sets, the Harvard Organic Photovoltaics data set (HOPV, HOMO-LUMO-gaps), for which excellent results are obtained, and on the Freesolv data set (solvation energies), where this method is less successful, probably due to a complex underlying learning task and the dissimilar methods used to obtain pre-training and fine-tuning labels. Finally, we find that for the HOPV data set, the final training results do not improve monotonically with the size of the pre-training data set, but pre-training with fewer data points can lead to more biased pre-trained models and higher accuracy after fine-tuning.\n\n## Submission history\n\nFrom: Thorren Kirschbaum \\[ [view email](https://arxiv.org/show-email/d04d3fe2/2404.13393)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2404.13393v1)**\nSat, 20 Apr 2024 14:25:34 UTC (2,023 KB)\n\n**\\[v2\\]**\nSat, 12 Oct 2024 16:25:27 UTC (959 KB)",
      "url": "https://arxiv.org/abs/2404.13393"
    },
    {
      "title": "Molecular property prediction in the ultra\u2010low data regime - Nature",
      "text": "Molecular property prediction in the ultra\u2010low data regime | Communications Chemistry\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Communications Chemistry](https://media.springernature.com/full/nature-cms/uploads/product/commschem/header-3dc28429486e0d2c8f49fd9baf5afa40.svg)](https://www.nature.com/commschem)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s42004-025-01592-1?error=cookies_not_supported&code=080dccb6-b4aa-47ba-840b-aba438a0f70a)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;42004)\n* [RSS feed](https://www.nature.com/commschem.rss)\nMolecular property prediction in the ultra\u2010low data regime\n[Download PDF](https://www.nature.com/articles/s42004-025-01592-1.pdf)\n[Download PDF](https://www.nature.com/articles/s42004-025-01592-1.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:08 July 2025# Molecular property prediction in the ultra\u2010low data regime\n* [Basem A. Eraqi](#auth-Basem_A_-Eraqi-Aff1)[ORCID:orcid.org/0000-0003-2911-221X](https://orcid.org/0000-0003-2911-221X)[1](#Aff1),\n* [Dmitrii Khizbullin](#auth-Dmitrii-Khizbullin-Aff2)[2](#Aff2),\n* [Shashank S. Nagaraja](#auth-Shashank_S_-Nagaraja-Aff1)[ORCID:orcid.org/0000-0003-4930-6513](https://orcid.org/0000-0003-4930-6513)[1](#Aff1)&amp;\n* \u2026* [S. Mani Sarathy](#auth-S__Mani-Sarathy-Aff1)[ORCID:orcid.org/0000-0002-3975-6206](https://orcid.org/0000-0002-3975-6206)[1](#Aff1)Show authors\n[*Communications Chemistry*](https://www.nature.com/commschem)**volume8**, Article\u00a0number:201(2025)[Cite this article](#citeas)\n* 8813Accesses\n* 1Citations\n* 21Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s42004-025-01592-1/metrics)\n### Subjects\n* [Chemical engineering](https://www.nature.com/subjects/chemical-engineering)\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n## Abstract\nData scarcity remains a major obstacle to effective machine learning in molecular property prediction and design, affecting diverse domains such as pharmaceuticals, solvents, polymers, and energy carriers. Although multi-task learning (MTL) can leverage correlations among properties to improve predictive performance, imbalanced training datasets often degrade its efficacy through negative transfer. Here, we present adaptive checkpointing with specialization (ACS), a training scheme for multi-task graph neural networks that mitigates detrimental inter-task interference while preserving the benefits of MTL. We validate ACS on multiple molecular property benchmarks, where it consistently surpasses or matches the performance of recent supervised methods. To illustrate its practical utility, we deploy ACS in a real-world scenario of predicting sustainable aviation fuel properties, showing that it can learn accurate models with as few as 29 labeled samples. By enabling reliable property prediction in low-data regimes, ACS broadens the scope and accelerates the pace of artificial intelligence-driven materials discovery and design.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-55082-4/MediaObjects/41467_2024_55082_Fig1_HTML.png)\n### [Multi-channel learning for integrating structural hierarchies into context-dependent molecular representation](https://www.nature.com/articles/s41467-024-55082-4?fromPaywallRec=false)\nArticleOpen access06 January 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41524-025-01836-7/MediaObjects/41524_2025_1836_Fig1_HTML.png)\n### [Attention-based functional-group coarse-graining: a deep learning framework for molecular prediction and design](https://www.nature.com/articles/s41524-025-01836-7?fromPaywallRec=false)\nArticleOpen access21 November 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-022-00790-5/MediaObjects/42004_2022_790_Fig1_HTML.png)\n### [Transferring chemical and energetic knowledge between molecular systems with machine learning](https://www.nature.com/articles/s42004-022-00790-5?fromPaywallRec=false)\nArticleOpen access13 January 2023\n## Introduction\nMachine learning (ML)-based molecular property prediction models can significantly accelerate the de novo design of high-performance molecules and mixtures by providing accurate property predictions. This data-driven approach explores the chemical space defined by learned model representations, enabling the discovery of materials that fulfill specific application requirements. However, the efficacy of such models relies heavily on predictive accuracy, which is constrained by the availability and quality of training data[1](https://www.nature.com/articles/s42004-025-01592-1#ref-CR1),[2](https://www.nature.com/articles/s42004-025-01592-1#ref-CR2). Across many practical domains\u2014including pharmaceutical drugs[3](https://www.nature.com/articles/s42004-025-01592-1#ref-CR3), chemical solvents[2](https://www.nature.com/articles/s42004-025-01592-1#ref-CR2), polymers[4](https://www.nature.com/articles/s42004-025-01592-1#ref-CR4), and green energy carriers[5](https://www.nature.com/articles/s42004-025-01592-1#ref-CR5)\u2014the scarcity of reliable, high-quality labels impedes the development of robust molecular property predictors.\nMulti-task learning (MTL) has been proposed to alleviate data bottlenecks by exploiting correlations among related molecular properties (hereafter termed*tasks*)[6](#ref-CR6),[7](#ref-CR7),[8](https://www.nature.com/articles/s42004-025-01592-1#ref-CR8). Through inductive transfer, MTL leverages the training signals or learned representations from one task to improve another, allowing the model to discover and utilize shared structures for more accurate predictions across all tasks. In practice, however, MTL is frequently undermined by negative transfer (NT)[7](https://www.nature.com/articles/s42004-025-01592-1#ref-CR7): performance drops that occur when updates driven by one task are detrimental to another. Prior studies linked NT primarily to*low task relatedness*and the associated*gradient conflicts*in shared parameters[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s42004-025-01592-1#ref-CR11). The resulting gradient conflicts can reduce the overall benefits of MTL or even degrade performance.\nBeyond task dissimilarity, NT can also arise from architectural or optimization mismatches[12](https://www.nature.com/articles/s42004-025-01592-1#ref-CR12). Capacity mismatch occurs when the shared backbone lacks sufficient flexibility to support divergent task demands, leading to overfitting on some tasks and underfitting on others. Similarly, when tasks exhibit different optimal learning rates, shared training may update parameters at incompatible magnitudes, destabilizing convergence[11](https://www.nature.com/articles/s42004-025-01592-1#ref-CR11). Additionally, data distribution differences, such as temporal and spatial disparities, can impede effective knowledge transfer[13](https://www.nature.com/articles/s42004-025-01592-1#ref-CR13),[14](https://www.nature.com/articles/s42004-025-01592-1#ref-CR14). Temporal differences\u2014such as variations in the measurement years of molecular data\u2014can lead to inflated performance estimates if not properly accounted for. This inflation has been shown to re...",
      "url": "https://www.nature.com/articles/s42004-025-01592-1"
    },
    {
      "title": "Efficient Learning of Molecular Properties Using Graph Neural ...",
      "text": "Efficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge | ACS Omega\nOpens in a new windowOpens an external websiteOpens an external website in a new window\nClose this dialog\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising.To learn more, view the following link:[Privacy Policy](http://www.acs.org/content/acs/en/privacy.html)\nManage Preferences\nClose Cookie Preferences\nRecently Viewed[**close modal](javascript:void(0))\n[Skip to article](#article__left-side)[Skip to sidebar](#article_content-right)\n* [ACS](http://www.acs.org)\n* [ACS Publications](https://pubs.acs.org/)\n* [C&amp;EN](https://cen.acs.org)\n* [CAS](https://www.cas.org)\n[Access through institution](https://pubs.acs.org/action/ssostart?redirectUri=/doi/10.1021/acsomega.5c07178)\n[Log In](https://pubs.acs.org/action/ssoRequestForLoginPage)\n[![ACS Publications. Most Trusted. Most Cited. Most Read](https://pubs.acs.org/specs/products/achs/images/pubslogo-big.gif)](https://pubs.acs.org/)\n[![ACS Omega](https://pubs.acs.org/cms/10.1021/acsomega/asset/16adb841-3216-db84-0321-adb84103216a/title.png)](https://pubs.acs.org/journal/acsodf)\nEfficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)\n* **Share\nShare on\n* [**Facebook](https://pubs.acs.org/#facebook)\n* [**X](https://pubs.acs.org/#x)\n* [**Wechat](https://pubs.acs.org/#wechat)\n* [**LinkedIn](https://pubs.acs.org/#linkedin)\n* [**Reddit](https://pubs.acs.org/#reddit)\n* [**Email](https://pubs.acs.org/#email)\n* [**Bluesky](https://pubs.acs.org/#bluesky)\n* **Jump to\n* [Abstract](#Abstract)\n* [Introduction](#_i2)\n* [Previous Work](#_i3)\n* [The Graph Neural Network Model](#_i4)\n* [Results](#_i9)\n* [Discussion](#_i21)\n* [Conclusion](#_i22)\n* [Data Availability](#dataAvailabilityNotesSection)\n* [Author Information](#authorInformationSection)\n* [Acknowledgments](#_i24)\n* [Additional Notes](#_i25)\n* [References](#_i26)\n* [Cited By](#citeThis)\n* **ExpandCollapse\n[**More](#)\n**Back to top\n**Close quick search form\n**clear**search\nACS OmegaAll Publications/Website\n[Advanced Search](https://pubs.acs.org/search/advanced)\n**Close\nPublications[**](#)\n## CONTENT TYPES\n* All Types\n## SUBJECTS\nPublications: All Types[**](#)\nClose## NextPrevious\n![Figure 1]()![Loading Img](https://pubs.acs.org/specs/products/achs/releasedAssets/images/loading/loading-018624cffd023ad5641b8e99931a80e6.gif)\n[**Download Hi-Res Image](#)[**Download to MS-PowerPoint](#)[****Cite This:**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)*ACS Omega*2025, 10, 45, 54421-54429\n[ADVERTISEMENT](http://acsmediakit.org)\n[![Journal Logo](specs/products/achs/releasedAssets/images/loading/loader.gif)](https://pubs.acs.org/journal/acsodf)\n[******Get e-Alerts](#)\n* [![Open Access](https://pubs.acs.org/specs/products/achs/releasedAssets/images/access-control/open-access.svg)](https://acsopenscience.org/researchers/open-access/)\nThis publication is Open Access under the license indicated.[Learn More](https://pubs.acs.org/page/access-types)\n* **Cite\n* [Citation](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and abstract](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=abs&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [Citation and references](https://pubs.acs.org/action/downloadCitation?doi=10.1021/acsomega.5c07178&amp;include=ref&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178)\n* [More citation options**](https://pubs.acs.org/action/showCitFormats?doi=10.1021/acsomega.5c07178&amp;include=cit&amp;format=ris&amp;direct=true&amp;downloadFileName=acsomega.5c07178&amp;href=/doi/10.1021/acsomega.5c07178)\n* **Share\nShare on\n* [**Facebook](https://pubs.acs.org/#facebook)\n* [**X](https://pubs.acs.org/#x)\n* [**WeChat](https://pubs.acs.org/#wechat)\n* [**LinkedIn](https://pubs.acs.org/#linkedin)\n* [**Reddit](https://pubs.acs.org/#reddit)\n* [**Email](https://pubs.acs.org/#email)\n* [**Bluesky](https://pubs.acs.org/#bluesky)\n* **Jump to\n* [Abstract](#Abstract)\n* [Introduction](#_i2)\n* [Previous Work](#_i3)\n* [The Graph Neural Network Model](#_i4)\n* [Results](#_i9)\n* [Discussion](#_i21)\n* [Conclusion](#_i22)\n* [Data Availability](#dataAvailabilityNotesSection)\n* [Author Information](#authorInformationSection)\n* [Acknowledgments](#_i24)\n* [Additional Notes](#_i25)\n* [References](#_i26)\n* [Cited By](#citeThis)\n* **ExpandCollapse\nArticleNovember 6, 2025\n# Efficient Learning of Molecular Properties Using Graph Neural Networks Enhanced with Chemistry Knowledge\n**Click to copy article linkArticle link copied!\n* Tetiana Lutchyn\nTetiana Lutchyn\nDepartment of Physics and Technology, The Arctic University of Norway, Troms\u00f8 9019, Norway\nMore by[Tetiana Lutchyn](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Tetiana%20Lutchyn)\n* Marie Mardal\nMarie Mardal\nDepartment of Chemistry, The Arctic University of Norway, Troms\u00f8 9019, Norway\nDepartment of Forensic Medicine, University of Copenhagen, 1172 K\u00f8benhavn, Denmark\nMore by[Marie Mardal](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Marie%20Mardal)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-3203-7305](https://orcid.org/0000-0002-3203-7305)\n* Benjamin Ricaud**\\***\nBenjamin Ricaud\nDepartment of Physics and Technology, The Arctic University of Norway, Troms\u00f8 9019, Norway\n**\\***Email:[firstname.lastname@uit.no](mailto:firstname.lastname@uit.no)\nMore by[Benjamin Ricaud](https://pubs.acs.org/action/doSearch?field1=Contrib&amp;text1=Benjamin%20Ricaud)\n![Orcid](https://pubs.acs.org/products/achs/releasedAssets/images/orchid-2856f829046fbda55b90e1582edf0e9a.png)[https://orcid.org/0000-0002-7611-4448](https://orcid.org/0000-0002-7611-4448)\n[**Open PDF](https://pubs.acs.org/doi/pdf/10.1021/acsomega.5c07178?ref=article_openPDF)\n## ACS Omega\nCite this:*ACS Omega*2025, 10, 45, 54421\u201354429\n**Click to copy citationCitation copied!\n[https://pubs.acs.org/doi/10.1021/acsomega.5c07178](https://pubs.acs.org/doi/10.1021/acsomega.5c07178)\n[https://doi.org/10.1021/acsomega.5c07178](https://doi.org/10.1021/acsomega.5c07178)\nPublishedNovember 6, 2025\n[****](#)\n### Publication History\n* **\nReceived\n22 July 2025\n* **\nAccepted\n27 October 2025\n* **\nRevised\n21 October 2025\n* **\nPublished\nonline6 November 2025\n* **\nPublished\nin issue18 November 2025\nresearch-article\nCopyright \u00a92025 The Authors. Published by American Chemical Society. This publication is licensed under\n[CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n### License Summary\\*\nYou are free toshare(copy and redistribute) this article in any medium or format and toadapt(remix, transform, and build upon) the material for any purpose, even commercially within the parameters below:\n* ![cc licence](https://pubs.acs.org/specs/products/achs/releasedAssets/images/cc-license/cc.svg)\nCreative Com...",
      "url": "https://pubs.acs.org/doi/10.1021/acsomega.5c07178"
    },
    {
      "title": "Publishing neural networks in drug discovery might compromise ...",
      "text": "Publishing neural networks in drug discovery might compromise training data privacy - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1186/s13321-025-00982-w)\n* [](pdf/13321_2025_Article_982.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![Journal of Cheminformatics logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-jcheminfo.png)\nJ Cheminform\n. 2025 Mar 26;17:38. doi:[10.1186/s13321-025-00982-w](https://doi.org/10.1186/s13321-025-00982-w)\n# Publishing neural networks in drug discovery might compromise training data privacy\n[Fabian P Kr\u00fcger](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kr\u00fcger FP\"[Author]>)\n### Fabian P Kr\u00fcger\n1Discovery Sciences, Molecular AI, AstraZeneca R&amp;D, M\u00f6lndal, 431 83 Sweden\n2TUM School of Computation, Information and Technology, Department of Mathematics, Technical University of Munich, Munich, 80333 Germany\n3Molecular Targets and Therapeutics Center, Institute of Structural Biology, Helmholtz Munich - Deutsches Forschungszentrum F\u00fcr Gesundheit Und Umwelt (GmbH), Neuherberg, 85764 Germany\nFind articles by[Fabian P Kr\u00fcger](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Kr\u00fcger FP\"[Author]>)\n1,2,3,\u2709,[Johan \u00d6stman](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00d6stman J\"[Author]>)\n### Johan \u00d6stman\n4AI Sweden, Gothenburg, 41756 Sweden\nFind articles by[Johan \u00d6stman](<https://pubmed.ncbi.nlm.nih.gov/?term=\"\u00d6stman J\"[Author]>)\n4,[Lewis Mervin](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Mervin L\"[Author]>)\n### Lewis Mervin\n5Discovery Sciences, Molecular AI, AstraZeneca R&amp;D, Cambridge, CB2 0AA UK\nFind articles by[Lewis Mervin](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Mervin L\"[Author]>)\n5,[Igor V Tetko](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tetko IV\"[Author]>)\n### Igor V Tetko\n3Molecular Targets and Therapeutics Center, Institute of Structural Biology, Helmholtz Munich - Deutsches Forschungszentrum F\u00fcr Gesundheit Und Umwelt (GmbH), Neuherberg, 85764 Germany\nFind articles by[Igor V Tetko](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Tetko IV\"[Author]>)\n3,[Ola Engkvist](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Engkvist O\"[Author]>)\n### Ola Engkvist\n1Discovery Sciences, Molecular AI, AstraZeneca R&amp;D, M\u00f6lndal, 431 83 Sweden\n6Department of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, 412 96 Sweden\nFind articles by[Ola Engkvist](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Engkvist O\"[Author]>)\n1,6\n* Author information\n* Article notes\n* Copyright and License information\n1Discovery Sciences, Molecular AI, AstraZeneca R&amp;D, M\u00f6lndal, 431 83 Sweden\n2TUM School of Computation, Information and Technology, Department of Mathematics, Technical University of Munich, Munich, 80333 Germany\n3Molecular Targets and Therapeutics Center, Institute of Structural Biology, Helmholtz Munich - Deutsches Forschungszentrum F\u00fcr Gesundheit Und Umwelt (GmbH), Neuherberg, 85764 Germany\n4AI Sweden, Gothenburg, 41756 Sweden\n5Discovery Sciences, Molecular AI, AstraZeneca R&amp;D, Cambridge, CB2 0AA UK\n6Department of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, 412 96 Sweden\n\u2709Corresponding author.\nReceived 2024 Dec 10; Accepted 2025 Mar 4; Collection date 2025.\n\u00a9The Author(s) 2025\n**Open Access**This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit[http://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC11948693\u00a0\u00a0PMID:[40140934](https://pubmed.ncbi.nlm.nih.gov/40140934/)\n## Abstract\nThis study investigates the risks of exposing confidential chemical structures when machine learning models trained on these structures are made publicly available. We use membership inference attacks, a common method to assess privacy that is largely unexplored in the context of drug discovery, to examine neural networks for molecular property prediction in a black-box setting. Our results reveal significant privacy risks across all evaluated datasets and neural network architectures. Combining multiple attacks increases these risks. Molecules from minority classes, often the most valuable in drug discovery, are particularly vulnerable. We also found that representing molecules as graphs and using message-passing neural networks may mitigate these risks. We provide a framework to assess privacy risks of classification models and molecular representations, available at[https://github.com/FabianKruger/molprivacy](https://github.com/FabianKruger/molprivacy). Our findings highlight the need for careful consideration when sharing neural networks trained on proprietary chemical structures, informing organisations and researchers about the trade-offs between data confidentiality and model openness.\n### Supplementary Information\nThe online version contains supplementary material available at 10.1186/s13321-025-00982-w.\n**Keywords:**Membership inference attack, Privacy, Drug discovery, Cheminformatics, QSAR, Machine learning\n## Scientific contribution\nThis study presents the first systematic assessment of the privacy risks associated with the sharing of neural networks trained to predict molecular properties. We are the first to develop a comprehensive framework for assessing these privacy risks in the context of cheminformatics, enabling the evaluation of vulnerabilities across different molecular representations and model architectures. Our work bridges the gap between privacy research and cheminformatics, providing a foundation for safer data sharing practices in drug discovery.\n### Supplementary Information\nThe online version contains supplementary material available at 10.1186/s13321-025-00982-w.\n## Introduction\nThe use of neural networks has gained sig...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11948693"
    },
    {
      "title": "Enhancing Molecular Property Prediction with Knowledge ... - arXiv",
      "text": "Enhancing Molecular Property Prediction with Knowledge from Large Language Models\n# Enhancing Molecular Property Prediction\nwith Knowledge from Large Language Models\nPeng Zhou1, 2,\nLai Hou Tim2,\nZhixiang Cheng1,\nKun Xie2, 3,\nChaoyi Li1,\nWei Liu2,\nXiangxiang Zeng1\n###### Abstract\nPredicting molecular properties is a critical component of drug discovery. Recent advances in deep learning\u2014particularly Graph Neural Networks (GNNs)\u2014have enabled end-to-end learning from molecular structures, reducing reliance on manual feature engineering. However, while GNNs and self-supervised learning approaches have advanced molecular property prediction (MPP), the integration of human prior knowledge remains indispensable, as evidenced by recent methods that leverage large language models (LLMs) for knowledge extraction. Despite their strengths, LLMs are constrained by knowledge gaps and hallucinations, particularly for less-studied molecular properties. In this work, we propose a novel framework that, for the first time, integrates knowledge extracted from LLMs with structural features derived from pre-trained molecular models to enhance MPP. Our approach prompts LLMs to generate both domain-relevant knowledge and executable code for molecular vectorization, producing knowledge-based features that are subsequently fused with structural representations. We employ three state-of-the-art LLMs\u2014GPT-4o, GPT-4.1, and DeepSeek-R1\u2014for knowledge extraction. Extensive experiments demonstrate that our integrated method outperforms existing approaches, confirming that the combination of LLM-derived knowledge and structural information provides a robust and effective solution for MPP.\n![Refer to caption](figs/fr2.png)Figure 1:(a) Traditional machine learning algorithms rely heavily on expert knowledge and feature engineering, limiting their adaptability and generalization. (b) Graph neural networks, while directly mapping molecular graphs to properties, often ignore expert insights. (c) We bridges these gaps by using LLMs to incorporate expert knowledge with graph structures, enhancing the prediction of molecular properties.\n## Introduction\nPredicting molecular properties is an essential task in the drug discovery process. Computational approaches to MPP not only accelerate drug screening but also improve cost efficiency. Traditional computational methods for molecular property prediction (MPP) typically involve extracting molecular fingerprints or carefully engineered features, followed by the application of machine learning algorithms such as Support Vector Machines (SVM)> (Cortes and Vapnik [> 1995\n](https://arxiv.org/html/2509.20664v1#bib.bib6)> )\nand Random Forests (RF)> (Ho [> 1998\n](https://arxiv.org/html/2509.20664v1#bib.bib15)> )\n. However, these methods heavily depend on domain experts for feature extraction and are susceptible to human knowledge biases> (Merkwirth and Lengauer [> 2005\n](https://arxiv.org/html/2509.20664v1#bib.bib32)> ; Degen et\u00a0al. [> 2008\n](https://arxiv.org/html/2509.20664v1#bib.bib8)> )\n. The advent of deep learning has partially addressed these limitations. Deep learning approaches can better leverage the growing availability of data and are less reliant on manual feature engineering. In particular, the application of Graph Neural Networks (GNNs) in the molecular domain has shown significant promise, as molecules can naturally be represented as graph structures. These models can be trained end-to-end directly on molecular graphs, enabling them to capture higher-order nonlinear relationships more effectively, eliminate human biases, and dynamically adapt to different tasks> (Wu et\u00a0al. [> 2020\n](https://arxiv.org/html/2509.20664v1#bib.bib44)> ; Mayr et\u00a0al. [> 2018\n](https://arxiv.org/html/2509.20664v1#bib.bib31)> ; Wieder et\u00a0al. [> 2020\n](https://arxiv.org/html/2509.20664v1#bib.bib43)> )\n.\nIn recent years, numerous GNN-based methods have been proposed, many of which focus on developing improved self-supervised learning techniques> (Liu et\u00a0al. [> 2021b\n](https://arxiv.org/html/2509.20664v1#bib.bib27)> )\n, leveraging contrastive learning> (Khosla et\u00a0al. [> 2020\n](https://arxiv.org/html/2509.20664v1#bib.bib20)> )\n, and deriving more robust molecular representations from large volumes of unlabeled molecular data> (Liu et\u00a0al. [> 2022\n](https://arxiv.org/html/2509.20664v1#bib.bib26)> ; Wang et\u00a0al. [> 2022\n](https://arxiv.org/html/2509.20664v1#bib.bib41)> )\n. However, we contend that human prior knowledge will remain indispensable for the foreseeable future. For example, by explicitly constraining chemical bonds between molecules and targets, Interformer> (Lai et\u00a0al. [> 2024\n](https://arxiv.org/html/2509.20664v1#bib.bib22)> )\ncan predict more plausible chemical conformations. Similarly, the incorporation of multiple sequence alignments enables AlphaFold to more accurately predict molecular and protein conformations> (Jumper et\u00a0al. [> 2021\n](https://arxiv.org/html/2509.20664v1#bib.bib18)> )\n. More relevantly, LLM4SD> (Zheng et\u00a0al. [> 2025\n](https://arxiv.org/html/2509.20664v1#bib.bib52)> )\nleverages LLMs to extract human knowledge for molecular vectorization, subsequently employing Random Forests for MPP, and outperforms GNN-based methods on several tasks. LLMs such as ChatGPT> (OpenAI [> 2022\n](https://arxiv.org/html/2509.20664v1#bib.bib33)> )\nand DeepSeek> (Guo et\u00a0al. [> 2025\n](https://arxiv.org/html/2509.20664v1#bib.bib12)> )\nhave been trained on vast amounts of human data and are finely aligned with human knowledge through techniques like reinforcement learning from human feedback> (Bai et\u00a0al. [> 2022\n](https://arxiv.org/html/2509.20664v1#bib.bib1)> )\n, endowing them with a breadth and depth of knowledge that surpasses most individuals. Nevertheless, the molecular knowledge acquired by LLMs intuitively follows a long-tail distribution. For well-studied molecular properties, LLMs may have accumulated sufficient experience, but for less-explored areas, they may lack adequate reference rules. Furthermore, due to the well-known phenomenon of hallucination, LLMs often provide seemingly plausible answers even when they lack sufficient knowledge. While LLM4SD offers an effective means of utilizing the knowledge embedded in LLMs, it cannot fully eliminate hallucinations and biases. Therefore, we argue that integrating molecular structural information\u2014particularly representations learned from pre-trained structural models\u2014with human prior knowledge represents a highly promising direction for advancing MPP.\nIn this work, we propose the integration of knowledge from LLMs with molecular structural information to enhance MPP. Similar to LLM4SD, we extract prior knowledge from LLMs based on different types of molecular properties and further prompt LLMs to infer potential knowledge by providing molecular samples related to the target properties. We instruct the LLMs to generate both relevant knowledge and executable function code, which are then used to vectorize the molecules and obtain knowledge-based molecular features. These extracted knowledge features are subsequently fused with structural features obtained from pre-trained molecular structure models. By combining knowledge features with structural features, our model not only leverages the breadth of human expertise but also learns direct mappings between structure and properties from the structural features. We employ three state-of-the-art LLMs for knowledge extraction, including GPT-4o, GPT-4.1> (OpenAI [> 2022\n](https://arxiv.org/html/2509.20664v1#bib.bib33)> )\n, and DeepSeek-R1> (Guo et\u00a0al. [> 2025\n](https://arxiv.org/html/2509.20664v1#bib.bib12)> )\n. Extensive comparative experiments demonstrate the effectiveness of our approach and confirm that LLMs can provide reliable knowledge for MPP.\n## Related work\n### Expert-crafted feature based MPP.\nMany earlier MPP models were based on expert-crafted features, primarily including two categories: molecular descriptors and molecular fingerprints. Molecular descri...",
      "url": "https://arxiv.org/html/2509.20664v1"
    },
    {
      "title": "Chain-aware graph neural networks for molecular property prediction",
      "text": "Just a moment...\n# academic.oup.com\nVerify you are human by completing the action below.\nacademic.oup.com needs to review the security of your connection before proceeding.\nVerification successful\nWaiting for academic.oup.com to respond...\nRay ID:`9bd2c4865f27ad01`\nPerformance &amp; security by[Cloudflare](https://www.cloudflare.com?utm_source=challenge&amp;utm_campaign=m)",
      "url": "https://academic.oup.com/bioinformatics/article/40/10/btae574/7818417"
    },
    {
      "title": "Few-shot learning via graph embeddings with convolutional networks for low-data molecular property prediction",
      "text": "ORIGINAL ARTICLE\nFew-shot learning via graph embeddings with convolutional networks\nfor low-data molecular property prediction\nLuis Torres1 \u2022 Joel P. Arrais1 \u2022 Bernardete Ribeiro1\nReceived: 3 July 2022 / Accepted: 13 February 2023 / Published online: 10 March 2023\n\u0002 The Author(s) 2023\nAbstract\nGraph neural networks and convolutional architectures have proven to be pivotal in improving the prediction of molecular\nproperties in drug discovery. However, this is fundamentally a low data problem that is incompatible with regular deep\nlearning approaches. Contemporary deep networks require large amounts of training data, which severely limits the\nprediction of new molecular entities from limited available data. In this paper, we address the challenge of low data in\nmolecular property prediction by: (1) defining a set of deep learning architectures that accept compound chemical\nstructures in the form of molecular graphs, (2) creating a few-shot learning strategy across graph neural networks and\nconvolutional neural networks to leverage the rich information of graph embeddings, and (3) proposing a two-module\nmeta-learning framework to learn from task-transferable knowledge and predict molecular properties on few-shot data.\nFurthermore, we conduct multiple experiments on two benchmark multiproperty datasets to demonstrate a superior\nperformance over conventional graph-based baselines. ROC-AUC results for 10-shot experiments show an average\nimprovement of \u00fe11:37% on Tox21 and \u00fe0:53% on SIDER, which are representative small-sized biological datasets for\nmolecular property prediction.\nKeywords Few-shot learning \u0002 Convolutional neural networks \u0002 Graph neural networks \u0002 Molecular property prediction\n1 Introduction\nDrug discovery and development is an extremely long and\nexpensive process that aims to find innovative medical\ncompounds ready to be formulated, synthesized and\nadministered to a patient [1]. Despite the most recent sci\u0002entific advances and ever-increasing understanding of\nbiological systems, most of these compounds fail to be\nselected due to a lack of desirable molecular properties [2].\nIn lead optimization, only a small fraction of the\nmolecules can pass virtual screening and enter clinical\ndevelopment. The higher the quality of these preclinical\ncandidates, the higher the probability of successful drug\ndevelopment [3]. However, the major cost of this operation\nstems from exploring the entire chemical space to synthe\u0002size only a few drug candidates. Thus, the search for new\nclasses of compounds with a suitable pharmacological\nprofile from a small amount of labeled data is paramount\n[4].\nCurrently, artificial intelligence assists almost every step\nof drug discovery including target identification, lead dis\u0002covery and optimization or preclinical data generation.\nThese methods reduce the number of iterations required to\ndiscover novel and active compounds while eliminating\nthose that are inactive, reactive and toxic [5].\nWith the evolution of artificial intelligence, develop\u0002ments in deep learning (DL) have played a crucial role in\noptimizing drug discovery. These algorithms motivated the\napplication of new graph representation learning tech\u0002niques to model systems of drug interaction and prediction.\nHowever, with only a few labeled molecules available,\ndeep networks struggle to generalize well and achieve\nacceptable performance [6].\n& Luis Torres\nluistorres@dei.uc.pt\nJoel P. Arrais\njpa@dei.uc.pt\nBernardete Ribeiro\nbribeiro@dei.uc.pt\n1 Centre for Informatics and Systems of the University of\nCoimbra, Univ Coimbra, Coimbra, Portugal\n123\nNeural Computing and Applications (2023) 35:13167\u201313185\nhttps://doi.org/10.1007/s00521-023-08403-5(0123456789().,-volV)(0123456789(). ,- volV)\nWell-validated biological datasets (e.g., Tox21, SIDER)\n[7] are limited in size and very expensive to obtain. These\nscarce drug repositories include only a few compounds that\nshare the same set of molecular properties. The resulting\nlack of biological information, including molecules sharing\nsimilar properties, bounds the performance of conventional\napproaches. This precondition sets the challenge of\ndeveloping models to effectively predict small molecules\nin few-shot learning scenarios [8, 9].\nRecent research has demonstrated that simple machine\nlearning algorithms and random forest predictors are\neffective in learning meaningful structural information\nfrom just a few labelled compounds [10, 11]. On the other\nhand, transfer learning and data augmentation techniques\nalso provide the domain knowledge required in cases\nwhere examples with supervised information are hard or\nimpossible to obtain [12, 13].\nNonetheless, these techniques are often too expensive\nand resource intensive to perform in drug discovery cam\u0002paigns. More recently, non-trivial few-shot learning pre\u0002dictors have been proposed to discover the properties of\nnew molecules and recognize potential drug candidates for\nfurther development [14, 15]. These methods attempt to\nlearn from a set of molecular property prediction tasks and\ngeneralize to new chemical properties given a just a few\nmolecules available.\nSmall molecules can be viewed as comprehensive graph\nstructures, where atoms are represented as nodes and\nchemical bonds as edges shared by neighbors in a graph\n[16\u201318]. These graph-level representations account for the\nspatial arrangement of atoms and bonds as well as inter\u0002actions between neighboring nodes and edges. This\napproach is more suitable for representation learning than\nsequence-based methods that describe molecules as\nsequential features such as SMILES (Simplified Molecular\nInput Line Entry System) strings [19].\nThese unique graph features can be used by deep\nlearning pipelines, which fail to predict molecular proper\u0002ties with limited available data. This limitation prompts the\nneed to explore models that quickly adapt across tasks to\npredict new properties on few-shot data [20, 21].\n2 Related work\nFew-shot learning methods have emerged as critical tools\nto accelerate and optimize drug discovery. These are\nalgorithms that target at generalizing from small data col\u0002lections to predict new systems from a limited amount of\nlabeled information. Recently, few-shot models have pro\u0002ven effective in modeling molecules as comprehensive\ngraph structures used for graph-based representation\nlearning. Graph neural networks leverage this information\nto build molecular embeddings by treating atoms as nodes\nand chemical bonds as edges. Node and edge embeddings\ncan later be used to support the prediction of molecular\nproperties. Deep networks such as convolutional neural\nnetworks also manipulate these continuous vectorial rep\u0002resentations to encode molecular graphs in a form suit\u0002able for few-shot molecule prediction.\n2.1 Few-shot learning\nHumans have an innate ability to recognize new objects\nand representations quickly from just a few examples. Prior\nknowledge helps to distinguish new concepts based on a\ngeneralized perception of an extensive and diverse set of\nrepresentations. Thus, the ability to few-shot learn different\nrepresentations by observing a concept and generating\nmeaningful and diverse variations is very important when\nclassifying new instances of unknown concepts [22].\nThis strategy attempts to adapt from previously seen\nclasses to predict unseen representations from just a few\nlabeled examples. This idea of few-shot generalization\ngave rise to few-shot learning methods [23].\nFew-shot learning (FSL) was introduced by Fei-Fei et al.\n[24] in the field of computer vision and image processing.\nThis approach presents a fundamental feature, which is the\nability to predict based on prior experience by transferring\nknowledge across tasks.\nIn drug discovery, molecular property prediction is a\nfew-shot learning problem since only a few molecules can\npass virtual screening to be further evaluated in lead opti\u0002mization. At this stage, few-shot models attempt to learn a\npredictor from a set of molecular property tasks an...",
      "url": "https://link.springer.com/content/pdf/10.1007/s00521-023-08403-5.pdf?error=cookies_not_supported&code=7a7d3d75-7162-450d-ad30-d540826e6434"
    }
  ]
}