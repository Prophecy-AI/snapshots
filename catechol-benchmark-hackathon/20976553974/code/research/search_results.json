{
  "query": "How to reduce overfitting when predicting properties of completely unseen molecules in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent studies of molecular\u2011property models and general machine\u2011learning best practices, the most effective way to curb overfitting for completely unseen molecules is to combine robust data handling, regularization, and advanced learning strategies\u202f([AWS](https://aws.amazon.com/what-is/overfitting), [PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9449894)).\n\n**Step\u2011by\u2011step guide**\n\n1. **Split the data with an out\u2011of\u2011distribution (OOD) aware scheme** \u2013 use scaffold\u2011based or k\u2011fold *n\u2011step forward* cross\u2011validation so that test folds contain chemotypes not present in training. This mimics the \u201ccompletely unseen\u201d scenario and gives a realistic estimate of generalization\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006)).\n\n2. **Apply regularization and early stopping** \u2013 add L1 (Lasso) or L2 (Ridge) penalties to the loss and monitor validation error to stop training before the model memorizes noise. Both techniques shrink weights and reduce variance\u202f([AWS](https://aws.amazon.com/what-is/overfitting), [CodeSignal](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/regularization-techniques-in-machine-learning-enhancing-model-generalization)).\n\n3. **Use ensembling of diverse learners** \u2013 train several models (e.g., different architectures, random seeds, or feature sets) and average their predictions. Ensembles tend to cancel individual overfits and improve OOD performance\u202f([AWS](https://aws.amazon.com/what-is/overfitting)).\n\n4. **Leverage transfer learning from large, related chemical datasets** \u2013 pre\u2011train a representation (e.g., graph\u2011neural network or transformer) on abundant public data, then fine\u2011tune on the small target set. Transfer learning mitigates overfitting when labeled data are sparse\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9449894)).\n\n5. **Incorporate meta\u2011learning with unlabeled molecules** \u2013 use a meta\u2011learning framework that learns to adapt between in\u2011distribution and out\u2011of\u2011distribution data by exploiting abundant unlabeled compounds. This approach explicitly trains the model to generalize beyond the training chemistry\u202f([arXiv\u202f2025\u201106\u201113](https://arxiv.org/abs/2506.11877)).\n\n6. **Augment the training set with chemically plausible variations** \u2013 generate multiple SMILES strings, conformers, or stereoisomers for each molecule, or apply random perturbations to descriptors. Data augmentation expands the effective sample size and discourages memorization\u202f([AWS](https://aws.amazon.com/what-is/overfitting)).\n\n7. **Quantify prediction uncertainty and prune low\u2011confidence outputs** \u2013 compute uncertainty estimates (e.g., via Bayesian layers or ensemble variance) and discard or flag predictions with high uncertainty. Focusing on confident regions reduces the impact of overfit predictions on downstream decisions\u202f([PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9449894)).\n\nFollowing these seven actions\u2014OOD\u2011aware validation, regularization\u202f+\u202fearly stopping, ensembling, transfer learning, meta\u2011learning with unlabeled data, chemical augmentation, and uncertainty\u2011driven pruning\u2014will substantially lower overfitting and improve the reliability of molecular property predictions on completely new compounds.",
      "url": ""
    },
    {
      "title": "Assigning Confidence to Molecular Property Prediction - PMC",
      "text": "<div><div>\n \n <main>\n \n <article><section><section><div>\n<p>. Author manuscript; available in PMC: 2022 Sep 7.</p></div>\n<p><em>Published in final edited form as: </em>Expert Opin Drug Discov. 2021 Jun 15;16(9):1009\u20131023. doi: <a href=\"https://doi.org/10.1080/17460441.2021.1925247\">10.1080/17460441.2021.1925247</a>\n</p>\n</section></section><section><section><h2>Abstract</h2>\n<section><h3>Introduction:</h3>\n<p>Computational modeling has rapidly advanced over the last decades, especially to predict molecular properties for chemistry, material science and drug design. Recently, machine learning techniques have emerged as a powerful and cost-effective strategy to learn from existing datasets and perform predictions on unseen molecules. Accordingly, the explosive rise of data-driven techniques raises an important question: What confidence can be assigned to molecular property predictions and what techniques can be used for that purpose?</p></section><section><h3>Areas covered:</h3>\n<p>In this work, we discuss popular strategies for predicting molecular properties relevant to drug design, their corresponding uncertainty sources and methods to quantify uncertainty and confidence. First, our considerations for assessing confidence begin with dataset bias and size, data-driven property prediction and feature design. Next, we discuss property simulation via molecular docking, and free-energy simulations of binding affinity in detail. Lastly, we investigate how these uncertainties propagate to generative models, as they are usually coupled with property predictors.</p></section><section><h3>Expert opinion:</h3>\n<p>Computational techniques are paramount to reduce the prohibitive cost and timing of brute-force experimentation when exploring the enormous chemical space. We believe that assessing uncertainty in property prediction models is essential whenever closed-loop drug design campaigns relying on high-throughput virtual screening are deployed. Accordingly, considering sources of uncertainty leads to better-informed experimental validations, more reliable predictions and to more realistic expectations of the entire workflow. Overall, this increases confidence in the predictions and designs and, ultimately, accelerates drug design.</p></section><section><p><strong>Keywords:</strong> Neural networks, deep learning, drug discovery, generative models, artificial intelligence, model uncertainty estimation, docking, molecular dynamics, generative models</p></section></section><section><h2>Graphical Abstract</h2>\n<p></p></section><section><h2>I. INTRODUCTION</h2>\n<p>While data-driven modeling has made large advances in image processing and speech recognition, ground-breaking contributions in drug discovery are harder to find. Often, only limited amounts of reliable <em>in vitro</em> and <em>in vivo</em> data suitable for supervised learning tasks are available. Additionally, uncertainties of experimental data, for instance regarding effectiveness and toxicity, are common and significant as the experimental design is complex, prone to noise and the outcome is dependent on many parameters, in particular compound dosage. Hence, these uncertainties are propagated to property prediction workflows when data-driven models are trained on that data.</p>\n<p>While it is evident that machine learning (ML) prediction accuracy and data quality are intrinsically related, there are more subtle aspects that are sometimes neglected. Dataset size, composition, coverage and training-test split have considerable influence on the final model performance but are insufficient to assess uncertainty alone without suitable models and methodologies for that purpose. For instance, using random or scaffold-based train/test splits does not lead to reliable measures of model performance and generalizability. Other common problems include high dataset bias, small sample size or low chemical diversity. However, some of these issues currently do not have a straightforward remedy as there is no commonly accepted way to describe chemical subspaces rendering it difficult to uncover all inherent biases. Nevertheless, when the underlying data is not investigated appropriately model performance and generalizability tend to be overestimated leading to a significant number of false predictions and, ultimately, low confidence in the property prediction workflows.</p>\n<p>In this article, we discuss methodologies and datasets for chemical properties important in drug design. In particular, we focus on the main sources of uncertainties for both simulation-based and data-driven property prediction. In doing so, we discuss uncertainties inherent in datasets, outputs of data-driven models, input features, and simulation of binding affinities. In addition, we discuss the importance of uncertainty in generative models, especially for property-based molecule design. Finally, we close with our personal opinion on the most important aspects of uncertainty and confidence focusing on important problems and future avenues that will lead to higher predictive ability and models that naturally take uncertainty into account.</p></section><section><h2>II. DATASET UNCERTAINTY</h2>\n<p>One of the most common challenges for data-driven molecular property prediction is dataset size and composition. For many important properties in drug discovery, especially pharmacologic properties of molecules such as absorption, distribution, metabolism, excretion and toxicity (ADMET), only a limited amount of high-quality data is available and it is usually only available for certain classes of molecules, which inherently introduces biases. To provide an overview of the amount and type of data available, <a href=\"#T1\">Table 1</a> lists popular datasets for molecular property prediction. Accordingly, in this section, we discuss dataset characteristics that are important to consider for data-driven property prediction tasks along with methods and procedures to minimize their impact on property prediction performance.</p>\n<section><h3>TABLE I.</h3>\n<p>Some of the common datasets used for molecular machine learning and data-driven property prediction.</p>\n<div><table>\n<colgroup>\n<col/>\n<col/>\n<col/>\n<col/>\n</colgroup>\n<thead><tr>\n<th>Name</th>\n<th>Description</th>\n<th>Number of molecules</th>\n<th>Possible bias</th>\n</tr></thead>\n<tbody>\n<tr>\n<td>ZINC [<a href=\"#R1\">1</a>]</td>\n<td>Database of commercially available compounds together with very simple estimated molecular properties for virtual screening.</td>\n<td>1.4 billion</td>\n<td>Inherently biased by currently synthesizable chemical space. Consequently, the molecular shapes have been shown to be highly biased against sphere-like molecules.</td>\n</tr>\n<tr>\n<td>QM9 [<a href=\"#R2\">2</a>]</td>\n<td>Electronic properties estimated using density functional theory (DFT) simulations.</td>\n<td>134 thousand</td>\n<td>Biased towards small molecules only containing the elements C, H, N, O and F.</td>\n</tr>\n<tr>\n<td>PubChemQC [<a href=\"#R3\">3</a>, <a href=\"#R4\">4</a>]</td>\n<td>Geometries and electronic properties of molecules with short string representations taken from PubChem.</td>\n<td>221 million</td>\n<td>Biased towards small molecules that have been reported in the literature before.</td>\n</tr>\n<tr>\n<td>Tox21 [<a href=\"#R5\">5</a>]</td>\n<td>Toxicologic properties of molecules with respect to 12 different assays</td>\n<td>13 thousand</td>\n<td>Biased towards environmental compounds and approved drugs.</td>\n</tr>\n<tr>\n<td>ToxCast [<a href=\"#R6\">6</a>]</td>\n<td>High-throughput screening and computational data for the toxicology of molecules from industry, consumer products and the food industry based on cell assays.</td>\n<td>1.8 thousand</td>\n<td>Biased towards molecules used in industry, consumer products and the food industry.</td>\n</tr>\n<tr>\n<td>ClinTox [<a href=\"#R7\">7</a>]</td>\n<td>Drugs and drug candidates that made it to clinical trials and were either approved or failed.</td>\n<td>1.5 thousand</td>\n<td>Bi...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9449894"
    },
    {
      "title": "What is Overfitting? - Overfitting in Machine Learning Explained - AWS",
      "text": "What is Overfitting? - Overfitting in Machine Learning Explained - AWS[Skip to main content](#aws-page-content-main)\n* [What is Cloud Computing?](https://aws.amazon.com/what-is-cloud-computing/)\u203a\n* [Cloud Computing Concepts Hub](https://aws.amazon.com/what-is/)\u203a\n* [Artificial Intelligence](https://aws.amazon.com/ai/)\u203a\n* Machine Learning\n# What is Overfitting?\n[Create an AWS Account](https://portal.aws.amazon.com/gp/aws/developer/registration/index.html?pg=what_is_header)\n## Page topics\n* [What is Overfitting?](#what-is-overfitting--1d5tx13)\n* [Why does overfitting occur?](#why-does-overfitting-occur--1d5tx13)\n* [How can you detect overfitting?](#how-can-you-detect-overfitting--1d5tx13)\n* [How can you prevent overfitting?](#how-can-you-prevent-overfitting--1d5tx13)\n* [What is underfitting?](#what-is-underfitting--1d5tx13)\n* [How can AWS minimize overfitting errors in your machine learning models?](#how-can-aws-minimize-overfitting-errors-in-your-machine-learning-models--1d5tx13)\n## What is Overfitting?\nOverfitting is an undesirable[machine learning](https://aws.amazon.com/what-is/machine-learning/)behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data. When data scientists use machine learning models for making predictions, they first train the model on a known data set. Then, based on this information, the model tries to predict outcomes for new data sets. An overfit model can give inaccurate predictions and cannot perform well for all types of new data.\n## Why does overfitting occur?\nYou only get accurate predictions if the machine learning model generalizes to all types of data within its domain. Overfitting occurs when the model cannot generalize and fits too closely to the training dataset instead. Overfitting happens due to several reasons, such as:\n\u2022The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.\n\u2022The training data contains large amounts of irrelevant information, called noisy data.\n\u2022The model trains for too long on a single sample set of data.\n\u2022The model complexity is high, so it learns the noise within the training data.\n**Overfitting examples**\nConsider a use case where a machine learning model has to analyze photos and identify the ones that contain dogs in them. If the machine learning model was trained on a data set that contained majority photos showing dogs outside in parks , it may may learn to use grass as a feature for classification, and may not recognize a dog inside a room.\nAnother overfitting example is a machine learning algorithm that predicts a university student's academic performance and graduation outcome by analyzing several factors like family income, past academic performance, and academic qualifications of parents. However, the test data only includes candidates from a specific gender or ethnic group. In this case, overfitting causes the algorithm's prediction accuracy to drop for candidates with gender or ethnicity outside of the test dataset.\n## How can you detect overfitting?\nThe best method to detect overfit models is by testing the machine learning models on more data with with comprehensive representation of possible input data values and types. Typically, part of the training data is used as test data to check for overfitting. A high error rate in the testing data indicates overfitting. One method of testing for overfitting is given below.\n**K-fold cross-validation**\nCross-validation is one of the testing methods used in practice. In this method, data scientists divide the training set into K equally sized subsets or sample sets called folds. The training process consists of a series of iterations. During each iteration, the steps are:\n1. Keep one subset as the validation data and train the machine learning model on the remaining K-1 subsets.\n2. Observe how the model performs on the validation sample.\n3. Score model performance based on output data quality.\nIterations repeat until you test the model on every sample set. You then average the scores across all iterations to get the final assessment of the predictive model.\n## How can you prevent overfitting?\nYou can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\n**Early stopping**\nEarly stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.\n**Pruning**\nYou might identify several features or parameters that impact the final prediction when you build a model. Feature selection\u2014or pruning\u2014identifies the most important features within the training set and eliminates irrelevant ones. For example, to predict if an image is an animal or human, you can look at various input parameters like face shape, ear position, body structure, etc. You may prioritize face shape and ignore the shape of the eyes.\n**Regularization**\nRegularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. For example, mathematical calculations apply a penalty value to features with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in 20 years. Regularization would give a lower penalty value to features like population growth and average annual income but a higher penalty value to the average annual temperature of the city.\n**Ensembling**\nEnsembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. They use multiple models to analyze sample data and pick the most accurate outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine learning models one after another to get the final result, while bagging trains them in parallel.\n**Data augmentation**\nData augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics. For example, applying transformations such as translation, flipping, and rotation to input images.\n## What is underfitting?\nUnderfitting is another type of error that occurs when the model cannot determine a meaningful relationship between the input and output data. You get underfit models if they have not trained for the appropriate length of time on a large number of data points.\n**Underfitting vs. overfitting**\nUnderfit models experience high bias\u2014they give inaccurate results for both the training data and test set. On the other hand, overfit models experience high variance\u2014they give accurate results for the training set but not for the test set. More model training results in less bias but variance can increase. Data scientists aim to find the sweet spot between underfitting and overfitting when fitting a model. A well-fitted model can quickly establish the dominant trend for seen and unseen data sets.\n## How can AWS minimize overfitting errors in your machine learning models?\nYou can use[Amazon SageMaker](https://aws.amazon.com/sagemaker/)to build, train, and deploy machine learning models for any use case with fully managed infrastructure, tools, and workflows. Amazon SageMaker has a built-in feature called[Amazon SageMaker Model Training](https://aws.amazon.com/sagemaker/train/)that automatically analyzes data generated during training, such as input, output, and transformations. As a result, it can detect and repor...",
      "url": "https://aws.amazon.com/what-is/overfitting"
    },
    {
      "title": "Regularization Techniques in Machine Learning - CodeSignal",
      "text": "[Skip to main content](https://codesignal.com/codesignal.com#main-content)\n\nTopic Overview\n\nWelcome to our lesson on **regularization**, a pivotal concept in machine learning. Regularization is a technique used to prevent **overfitting**, a common issue that arises when our model learns too much detail from the training data and performs poorly on unseen data. In this lesson, we will focus on learning and applying L1 and L2 regularization techniques to Logistic Regression and Decision Tree models.\n\nConcept of Regularization\n\nIn this section, we'll explore how to tackle overfitting through regularization. Overfitting is like memorizing the answers to a test rather than understanding the subject. It happens when a model learns the training data too well, including its noise and outliers, which hampers its performance on new, unseen data. Regularization helps to prevent this by simplifying the model in a controlled way.\n\nThere are two main types of regularization techniques we will focus on: `L1 (Lasso)` and `L2 (Ridge)` regularization. Both methods add a penalty to the model, but they do so in different ways, leading to different outcomes.\n\nL1 Regularization (Lasso)\n\nImagine you're painting a picture but decide to use only the essential colors. This is what `L1` regularization does. It simplifies the model by forcing some feature weights to be exactly zero, effectively removing those features from the model. This can lead to a model that's easier to interpret and less prone to overfitting. In technical terms, `L1` adds a penalty equal to the absolute value of the magnitude of coefficients.\n\nL2 Regularization (Ridge)\n\nJoin the 1M+ learners on CodeSignal\n\nBe a part of our community of 1M+ users who develop and demonstrate their skills on CodeSignal\n\nStart learning today!\n\nNow, think of tuning a musical instrument to ensure no single note overpowers the others. `L2` regularization works similarly. It reduces the model's complexity by penalizing large coefficients but doesn't zero them out. This method is useful when many features contribute small effects, and you don't want to eliminate them entirely. `L2` adds a penalty equal to the square of the magnitude of coefficients.\n\nVisualizing Regularization\n\nHere is a chart that illustrates these concepts. In the chart, you can see how `L1` regularization can completely remove some features (by setting their importance to zero), while `L2` regularization uniformly reduces the importance of all features.\n\nRegularization is a powerful tool in machine learning, striking a balance between simplicity and predictive power in models. By applying `L1` or `L2` regularization, we can create models that generalize better to new data, avoiding the pitfalls of overfitting.\n\nNow, let's see how we can apply these regularization techniques in different types of models like Logistic Regression and Decision Trees.\n\nImplementing Regularization in Logistic Regression\n\nNow, let's apply L1 and L2 regularization in the context of **Logistic Regression** using the popular Python library, `Sklearn`. For this, we'll use the same **Breast Cancer Wisconsin Dataset** we've been using throughout this course. After loading the data and splitting it into a training set and a test set, we'll use `Sklearn's LogisticRegression()` class, which has a `penalty` parameter for applying regularization.\n\nLet's see it in action:\n\nThe hyperparameter `'C'` operates as the inverse of the regularization strength. Smaller values indicate stronger regularization.\n\nAfter fitting the Logistic Regression models with L1 and L2 regularization, it's important to evaluate their performance. We'll do this by computing the accuracy of each model on the test set. The accuracy score is a straightforward way to measure how often the model predicts correctly. In Sklearn, this can be done using the `accuracy_score` function from the `metrics` module. Let's calculate and compare the accuracies of our regularized models:\n\noutput:\n\nWe can see that L2 regularization is working better in this case than L1.\n\nWhen does it make sense to use L1 or L2 regularization\n\nIn machine learning, choosing between L1 (Lasso) and L2 (Ridge) regularization depends on your model and data. L1 is beneficial for models with numerous features, as it helps in feature selection by shrinking some coefficients to zero. This is particularly useful in Logistic and Linear Regression when you want a simpler, more interpretable model. On the other hand, L2 regularization, which reduces the impact of all features more uniformly without eliminating them, is suitable when dealing with correlated features. It's commonly used in Logistic Regression, Linear Regression, and Neural Networks to prevent overfitting, especially when the dataset has fewer samples than features.\n\nImportantly, regularization techniques like L1 and L2 are not used in models such as Decision Trees. These models have their own methods of controlling complexity and preventing overfitting, like tree depth and pruning, making external regularization unnecessary.\n\nLesson Summary and Practice\n\nGreat job for making it through this lesson! You've learned a fundamental technique that will help prevent your machine learning models from overfitting to your training data. Importantly, you can now model with confidence, knowing that you're equipped to reduce the risk of overfitting by carefully applying L1 and L2 regularization techniques to Logistic Regressions and other models.\n\nNow, it's time to cement these concepts into your practice. Up next, we have some hands-on exercises designed to help you apply what you've just learned. It's time to level up your machine learning models, and remember - practice makes perfect!\n\n[Previous Lesson](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/optimizing-decision-trees-with-hyperparameter-tuning)\n\n[Previous](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/optimizing-decision-trees-with-hyperparameter-tuning)\n\n[Next Lesson: Elevating Predictive Models with RandomForest and GradientBoosting Techniques](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/elevating-predictive-models-with-randomforest-and-gradientboosting-techniques)\n\n[Next Lesson: Elevating Predictive Models with RandomForest and GradientBoosting Techniques](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/elevating-predictive-models-with-randomforest-and-gradientboosting-techniques)\n\n[Next](https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/elevating-predictive-models-with-randomforest-and-gradientboosting-techniques)\n\nLog in\n\nStart learning\n\nCompany\n\nCollections\n\nPlatform\n\nRoles\n\nResources\n\nSupport",
      "url": "https://codesignal.com/learn/courses/intro-to-model-optimization-in-machine-learning/lessons/regularization-techniques-in-machine-learning-enhancing-model-generalization"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "A systematic study of key elements underlying molecular property ...",
      "text": "A systematic study of\u00a0key elements underlying molecular property prediction | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-023-41948-6?error=cookies_not_supported&code=5236daa0-b785-4f43-9ab0-aeb6859995eb)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nA systematic study of\u00a0key elements underlying molecular property prediction\n[Download PDF](https://www.nature.com/articles/s41467-023-41948-6.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-023-41948-6.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:13 October 2023# A systematic study of\u00a0key elements underlying molecular property prediction\n* [Jianyuan Deng](#auth-Jianyuan-Deng-Aff1)[ORCID:orcid.org/0000-0003-0647-1287](https://orcid.org/0000-0003-0647-1287)[1](#Aff1),\n* [Zhibo Yang](#auth-Zhibo-Yang-Aff2)[2](#Aff2),\n* [Hehe Wang](#auth-Hehe-Wang-Aff3)[3](#Aff3),\n* [Iwao Ojima](#auth-Iwao-Ojima-Aff3)[ORCID:orcid.org/0000-0002-3628-1161](https://orcid.org/0000-0002-3628-1161)[3](#Aff3),\n* [Dimitris Samaras](#auth-Dimitris-Samaras-Aff2)[ORCID:orcid.org/0000-0002-1373-0294](https://orcid.org/0000-0002-1373-0294)[2](#Aff2)&amp;\n* \u2026* [Fusheng Wang](#auth-Fusheng-Wang-Aff1-Aff2)[ORCID:orcid.org/0000-0002-9369-9361](https://orcid.org/0000-0002-9369-9361)[1](#Aff1),[2](#Aff2)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume14**, Article\u00a0number:6395(2023)[Cite this article](#citeas)\n* 28kAccesses\n* 127Citations\n* 19Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-023-41948-6/metrics)\n### Subjects\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n* [Machine learning](https://www.nature.com/subjects/machine-learning)\n## Abstract\nArtificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, key elements underlying molecular property prediction remain largely unexplored, which impedes further advancements in this field. Herein, we conduct an extensive evaluation of representative models using various representations on the MoleculeNet datasets, a suite of opioids-related datasets and two additional activity datasets from the literature. To investigate the predictive power in low-data and high-data space, a series of descriptors datasets of varying sizes are also assembled to evaluate the models. In total, we have trained 62,820 models, including 50,220 models on fixed representations, 4200 models on SMILES sequences and 8400 models on molecular graphs. Based on extensive experimentation and rigorous comparison, we show that representation learning models exhibit limited performance in molecular property prediction in most datasets. Besides, multiple key elements underlying molecular property prediction can affect the evaluation results. Furthermore, we show that activity cliffs can significantly impact model prediction. Finally, we explore into potential causes why representation learning models can fail and show that dataset size is essential for representation learning models to excel.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-55082-4/MediaObjects/41467_2024_55082_Fig1_HTML.png)\n### [Multi-channel learning for integrating structural hierarchies into context-dependent molecular representation](https://www.nature.com/articles/s41467-024-55082-4?fromPaywallRec=false)\nArticleOpen access06 January 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-021-90259-7/MediaObjects/41598_2021_90259_Fig1_HTML.png)\n### [A merged molecular representation learning for molecular properties prediction with a web-based service](https://www.nature.com/articles/s41598-021-90259-7?fromPaywallRec=false)\nArticleOpen access26 May 2021\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42004-024-01155-w/MediaObjects/42004_2024_1155_Fig1_HTML.png)\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://www.nature.com/articles/s42004-024-01155-w?fromPaywallRec=false)\nArticleOpen access05 April 2024\n## Introduction\nDrug discovery is an expensive process in both time and cost with a daunting attrition rate. As revealed by a recent study[1](https://www.nature.com/articles/s41467-023-41948-6#ref-CR1), the average cost of developing a new drug was \\~1 billion dollars and has been ever increasing[2](https://www.nature.com/articles/s41467-023-41948-6#ref-CR2). In the past decade, the practice of drug discovery has been undergoing radical transformations in light of the advancements in artificial intelligence (AI)[3](#ref-CR3),[4](#ref-CR4),[5](https://www.nature.com/articles/s41467-023-41948-6#ref-CR5), which, at its core, is molecular representation learning. Molecules are typically represented in three ways: fixed representations, including fingerprints and structural keys, that signify the presence of specific structural patterns; linear notations, such as Simplified Molecular Input Line Entry System (SMILES) strings; and molecular graphs[6](https://www.nature.com/articles/s41467-023-41948-6#ref-CR6). With the advent of deep learning, various neural networks have been proposed for molecular representation learning, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs) and graph neural networks (GNNs), among others[5](https://www.nature.com/articles/s41467-023-41948-6#ref-CR5). One major task for AI in drug discovery is molecular property prediction, which seeks to learn a function that maps a structure to a property value. In the literature, deep representation learning has been reported as a promising approach for molecular property prediction, outperforming fixed molecular representations[7](https://www.nature.com/articles/s41467-023-41948-6#ref-CR7),[8](https://www.nature.com/articles/s41467-023-41948-6#ref-CR8). More recently, to address the lack of labeled data in drug discovery, self-supervised learning has been proposed to leverage large-scale, unlabeled corpus on both SMILES strings[9](#ref-CR9),[10](#ref-CR10),[11](https://www.nature.com/articles/s41467-023-41948-6#ref-CR11)and molecular graphs[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](https://www.nature.com/articles/s41467-023-41948-6#ref-CR15), which has enabled state-of-the-art performance on the MoleculeNet benchmark datasets[16](https://www.nature.com/articles/s41467-023-41948-6#ref-CR16).\nDespite the current prosperity, AI-driven drug discovery is not without its critiques. Usually, when a new technique is developed for molecular property prediction, improved metrics by experimenting on the MoleculeNet benchmark datasets[16](https://www.nature.com/articles/s41467-023-41948-6#ref-CR16)are used to substantiate the claim that the model achieves chemical space generalization. Althou...",
      "url": "https://www.nature.com/articles/s41467-023-41948-6"
    },
    {
      "title": "Robust Quantum Reservoir Computing for Molecular Property ...",
      "text": "[Back to all](https://www.quera.com/blog)\n\nTechnology\n\n# Robust Quantum Reservoir Computing for Molecular Property Prediction: A Collaborative Study\n\nMarch 18, 2025\n\nmin read\n\n6 min read\n\nWhen it comes to small, complex datasets\u2014think rare diseases, early-stage clinical trials, or finely tuned molecular design\u2014classical machine learning methods often struggle. Overfitting, limited accuracy, and high variability make extracting valuable insights a challenge. A recent study presented by a collaborative team from Merck, Amgen, Deloitte, and QuEra explores a promising alternative: [**quantum reservoir computing**](https://www.quera.com/glossary/quantum-reservoir-computing) **(QRC)**. Below is a summary of why they chose this approach, how they applied it, and the results they observed. See the [arXiv paper](https://arxiv.org/abs/2412.06758) detailing the study and the [recording of our webinar](https://www.quera.com/events/science-with-quera-robust-quantum-reservoir-computing-for-molecular-property-prediction) covering it.\n\n## **1\\. The Motivation**\n\n### **Small-Data Problem**\n\nIn industries like biopharma, oncology, and personalized medicine, data is often scarce. Traditional machine learning models can overfit quickly on small datasets (e.g., 100\u2013300 samples), and their predictive performance deteriorates on new, unseen data. Moreover, the variability of their performance across multiple data splits can be prohibitively high.\n\n### **Why Quantum?**\n\nThe team sought a quantum-based method to address these challenges\u2014particularly with data riddled with complex correlations and nonlinearity. Quantum reservoir computing offered a compelling solution because:\n\n- It **doesn\u2019t rely on large-scale training** (in contrast to many variational quantum algorithms).\n- It can handle **nonlinear dynamics** well, drawing on the rich interactions within the quantum hardware.\n- It holds the promise of **scalability**, especially important as quantum devices grow in qubit count.\n\n### **Focusing on Molecular Properties**\n\nMolecular property prediction is a natural test case for small-sample research: drug discovery pipelines, for example, often produce datasets that are too limited for standard machine learning techniques. By showcasing results on a molecular dataset, the group aimed to illustrate how QRC could generalize to other small-data challenges in pharmaceuticals, healthcare, and beyond.\n\n## **2\\. The Process**\n\n### **Neutral-Atom Hardware and Reservoir Computing**\n\nQuEra\u2019s [neutral-atom quantum hardware](https://www.quera.com/neutral-atom-platform) provides a highly scalable platform where individual atoms act as qubits. Unlike some quantum technologies, neutral-atom systems can potentially reach tens or even hundreds of thousands of qubits without the complexity of massive wiring or cryogenic refrigeration. This is key for **reservoir computing**, where the \u201creservoir\u201d is a physical system through which data is passed to generate richer feature representations (called embeddings).\n\n### **Embedding Workflow**\n\n1\\. **Data Preprocessing and Encoding**\n\nSmall, high-value datasets (e.g., molecular properties) are cleaned, clustered, or reduced to ensure they capture the essential features.\n\nThe numerical values are then encoded into the quantum computer. For neutral-atom hardware, data can be embedded by adjusting local parameters (e.g., atom positions, pulse strengths).\n\n2\\. **Quantum Evolution**\n\nOnce the data is encoded, the atoms undergo quantum dynamics. Because the system is analog, these interactions occur naturally, generating complex, nonlinear transformations without requiring heavy parameter optimization.\n\n3\\. **Measurement and Embedding Extraction**\n\nThe quantum states are measured multiple times. These measurement outcomes form a new set of \u201cquantum-processed\u201d features\u2014often showing patterns difficult to replicate with purely classical methods.\n\n4\\. **Classical Post-Processing**\n\nRather than train the quantum system itself, the team trains a **classical** model (such as a random forest) on these quantum-derived embeddings. The process circumvents some known challenges in hybrid quantum-classical training\u2014like vanishing gradients\u2014by limiting training to the classical side.\n\n### **Comparisons to Classical Methods**\n\nTo ensure robustness, the study compared:\n\n- **Raw Data** fed into a variety of classical machine learning models.\n- **Classical Embeddings** (e.g., standard kernel methods like Gaussian processes).\n- **Quantum Reservoir Embeddings** from the neutral-atom system.\n\nThe team also ran experiments for multiple dataset sizes\u2014ranging from about 100 records to several hundred\u2014to simulate the real-world progression from \u201csmall data\u201d to more mature data samples.\n\n## **3\\. The Results**\n\n### **Strong Performance on Small Data**\n\nFor datasets containing **100\u2013200 samples**, the QRC-based approach consistently outperformed purely classical methods. This mattered in two critical ways:\n\n- **Higher Accuracy**: Predictions of molecular properties were notably more accurate with quantum-derived embeddings.\n- **Lower Variability**: Performance across different train-test splits was more stable, a major concern when data volume is limited.\n\n**Convergence with Larger Datasets**\n\nAs the number of samples rose (e.g., 800+), the gap between quantum and classical methods **narrowed**. In other words, with more data, conventional machine learning caught up. However, the quantum approach demonstrated an edge in \u201clearning more\u201d from fewer data points\u2014an advantage that could be transformational in early-stage or niche applications where data is, by nature, limited.\n\n**Interpretable Embeddings**\n\nVisualizations using techniques like UMAP underscored why the QRC approach can excel. The quantum embeddings often formed **more distinct clusters**, revealing clearer, more separated patterns in the data\u2014essential for both classification and regression tasks.\n\n**Hardware Scalability**\n\nIn test experiments, the QRC method scaled up to **over 100 qubits** on QuEra\u2019s hardware\u2014among the largest quantum machine learning demonstrations published so far. As hardware capacity grows, the team expects further gains in addressing complex, high-dimensional problems that classical simulators cannot easily replicate.\n\n## **Future Outlook**\n\n**Beyond Molecular Properties**\n\nThe team believes QRC can be extended to several other arenas:\n\n- **Clinical Trials**: Particularly in rare diseases or early-phase trials with very few patient samples.\n- **Anomaly Detection**: Nonlinearities and small volumes make quantum approaches appealing for healthcare data (like ECG or voice analysis).\n- **Large Language Models (LLMs)**: Some early research indicates that quantum methods could enhance attention mechanisms or guide prompt engineering in AI.\n- **Time-Series Prediction**: Audio signals, EKG readings, and real-world sensor data often have hidden correlations that QRC might capture more elegantly than classical techniques.\n\n**Collaboration Is Key**\n\nThis project exemplifies how **deep partnerships** among quantum hardware developers, pharmaceutical experts, and data science teams can break new ground. Real-world data, specialized domain knowledge, and quantum engineering expertise come together to tackle problems that simply weren\u2019t tractable a few years ago.\n\n## **Conclusion**\n\nThis collaborative study underscores the emerging role of **quantum reservoir computing** in unlocking value from small but critical datasets. By harnessing the inherent dynamics of neutral-atom quantum systems, the team showed improved predictive performance, especially where classical machine learning tends to falter\u2014namely, in low-sample, high-complexity scenarios.\n\nWhile classical methods remain competitive for large datasets, quantum reservoir computing demonstrates a powerful new capability to \u201cdo more with less.\u201d As the technology scales, the research community will likely uncover even more applications where QRC can deliv...",
      "url": "https://www.quera.com/blog-posts/robust-quantum-reservoir-computing-for-molecular-property-prediction-a-collaborative-study"
    },
    {
      "title": "3. Regression & Model Assessment",
      "text": "[**deep learning for molecules & materials**](https://dmol.pub/index.html)\n\nBy [Andrew White](https://twitter.com/andrewwhite01)\n\n- [Colab](https://colab.research.google.com/github/whitead/dmol-book/blob/master/ml/regression.ipynb)\n\n- [repository](https://github.com/whitead/dmol-book)\n- [open issue](https://github.com/whitead/dmol-book/issues/new?title=Issue%20on%20page%20%2Fml/regression.html&body=Your%20issue%20content%20here.)\n\n- [.ipynb](https://dmol.pub/_sources/ml/regression.ipynb)\n- .pdf\n\nContents\n\n# Regression & Model Assessment\n\n## Contents\n\n# 3\\. Regression & Model Assessment [\\#](https://dmol.pub/dmol.pub\\#regression-model-assessment)\n\nRegression is supervised learning with continuous (or sometimes discrete) labels. You are given labeled data consisting of features and labels \\\\(\\\\{\\\\vec{x}\\_i, y\\_i\\\\}\\\\). The goal is to find a function that describes their relationship, \\\\(\\\\hat{f}(\\\\vec{x}) = \\\\hat{y}\\\\). A more formal discussion of the concepts discussed here can be found in Chapter 3 of Bishop\u2019s Pattern Recognition and Machine Learning\\[ [Bis06](https://dmol.pub/dmol.pub#id18)\\].\n\nAudience & Objectives\n\nThis lecture introduces some probability theory, especially expectations. You can get a refresher of [probability of random variables](https://whitead.github.io/numerical_stats/) and/or [expectations](https://whitead.github.io/numerical_stats/unit_4/lectures/lecture_2.pdf). Recall an expectation is \\\\(E\\[x\\] = \\\\sum P(x)x\\\\) and variance is \\\\(E\\[\\\\left(x - E\\[x\\]\\\\right)^2\\]\\\\). We also use and discuss [linear regression techniques](https://nbviewer.jupyter.org/github/whitead/numerical_stats/blob/master/unit_12/lectures/lecture_1.ipynb#Extending-Least-Squares-to-Multiple-Dimensions-in-Domain---OLS-ND). After completing this chapter, you should be able to:\n\n- Perform multi-dimensional regression with a loss function\n\n- Understand how to and why we batch\n\n- Understand splitting of data\n\n- Reason about model bias and model variance\n\n- Assess model fit and generalization error\n\n\n## 3.1. Running This Notebook [\\#](https://dmol.pub/dmol.pub\\#running-this-notebook)\n\nClick the \u00a0\u00a0 above to launch this page as an interactive Google Colab. See details below on installing packages.\n\nTip\n\nTo install packages, execute this code in a new cell.\n\n```\n!pip install dmol-book\n\n```\n\nIf you find install problems, you can get the latest working versions of packages used in [this book here](https://github.com/whitead/dmol-book/blob/main/package/setup.py)\n\nAs usual, the code below sets-up our imports.\n\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nfrom jax.example_libraries import optimizers\nimport jax\nimport dmol\n```\n\n```\n# soldata = pd.read_csv('https://dataverse.harvard.edu/api/access/datafile/3407241?format=original&gbrecs=true')\nsoldata = pd.read_csv(\n    \"https://github.com/whitead/dmol-book/raw/main/data/curated-solubility-dataset.csv\"\n)\nfeatures_start_at = list(soldata.columns).index(\"MolWt\")\nfeature_names = soldata.columns[features_start_at:]\n```\n\n## 3.2. Overfitting [\\#](https://dmol.pub/dmol.pub\\#overfitting)\n\nWe\u2019ll be working again with the AqSolDB\\[ [SKE19](https://dmol.pub/dmol.pub#id26)\\] dataset. It has about 10,000 unique compounds with measured solubility in water (label) and 17 molecular descriptors (features). We need to create a better assessment of our supervised ML models. The goal of our ML model is to predict solubility of new unseen molecules. Therefore, to assess we should test on unseen molecules. We will split our data into two subsets: **training data** and **testing data**. Typically this is done with an 80%/20%, so that you train on 80% of your data and test on the remaining 20%. In our case, we\u2019ll just do 50%/50% because we have plenty of data and thus do not need to take 80% for training. We\u2019ll be using a subset, 50 molecules chosen randomly, rather than the whole dataset. So we\u2019ll have 25 training molecules and 25 testing molecules.\n\nLet\u2019s begin by seeing what effect the split of train/test has on our linear model introduced in the previous chapter.\n\n```\n# Get 50 points and split into train/test\nsample = soldata.sample(50, replace=False)\ntrain = sample[:25]\ntest = sample[25:]\n\n# standardize the features using only train\ntest[feature_names] -= train[feature_names].mean()\ntest[feature_names] /= train[feature_names].std()\ntrain[feature_names] -= train[feature_names].mean()\ntrain[feature_names] /= train[feature_names].std()\n\n# convert from pandas dataframe to numpy arrays\nx = train[feature_names].values\ny = train[\"Solubility\"].values\ntest_x = test[feature_names].values\ntest_y = test[\"Solubility\"].values\n```\n\nWe will again use a linear model, \\\\( \\\\hat{y} = \\\\vec{w}\\\\vec{x} + b \\\\). One change we\u2019ll make is using the [@jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit) decorator from `jax`. This decorator will tell `jax` to inspect our function, simplify it, and compile it to run quickly on a GPU (if available) or CPU. The rest of our work is the same as the previous chapter. We begin with defining our loss, which is mean squared error (MSE) again.\n\n```\n# define our loss function\n@jax.jit\ndef loss(w, b, x, y):\n    return jnp.mean((y - jnp.dot(x, w) - b) ** 2)\n\nloss_grad = jax.grad(loss, (0, 1))\nw = np.random.normal(size=x.shape[1])\nb = 0.0\nloss_grad(w, b, x, y)\n```\n\n```\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n```\n\n```\n(Array([-2.8831835 ,  1.3794075 , -3.0262635 , -3.1956701 , -4.007525  ,\n        -0.83476734, -3.0994835 , -4.0455275 , -3.675129  ,  2.5466921 ,\n        -3.11543   , -4.171584  , -1.5834932 , -3.3554041 , -3.1797354 ,\n         0.86207145, -2.0010393 ], dtype=float32),\n Array(5.7721677, dtype=float32, weak_type=True))\n\n```\n\nNow we will train our model, again using gradient descent. This time we will not batch, since our training data only has 25 points. Can you see what the learning rate is? Why is it so different from the last chapter when we used the whole dataset?\n\n```\nloss_progress = []\ntest_loss_progress = []\neta = 0.05\nfor i in range(2000):\n    grad = loss_grad(w, b, x, y)\n    w -= eta * grad[0]\n    b -= eta * grad[1]\n    loss_progress.append(loss(w, b, x, y))\n    test_loss_progress.append(loss(w, b, test_x, test_y))\nplt.plot(loss_progress, label=\"Training Loss\")\nplt.plot(test_loss_progress, label=\"Testing Loss\")\n\nplt.xlabel(\"Step\")\nplt.yscale(\"log\")\nplt.legend()\nplt.ylabel(\"Loss\")\nplt.show()\n```\n\n```\nyhat = x @ w + b\nplt.plot(y, y, \":\", linewidth=0.2)\nplt.plot(y, x @ w + b, \"o\")\nplt.xlim(min(y), max(y))\nplt.ylim(min(y), max(y))\nplt.text(min(y) + 1, max(y) - 2, f\"correlation = {np.corrcoef(y,yhat)[0,1]:.3f}\")\nplt.text(min(y) + 1, max(y) - 3, f\"loss = {np.sqrt(np.mean((y-yhat)**2)):.3f}\")\nplt.title(\"Training Data\")\nplt.show()\n```\n\n```\nyhat = test_x @ w + b\nplt.plot(test_y, test_y, \":\", linewidth=0.2)\nplt.plot(test_y, yhat, \"o\")\nplt.xlim(min(test_y), max(test_y))\nplt.ylim(min(test_y), max(test_y))\nplt.text(\n    min(test_y) + 1,\n    max(test_y) - 2,\n    f\"correlation = {np.corrcoef(test_y,yhat)[0,1]:.3f}\",\n)\nplt.text(\n    min(test_y) + 1,\n    max(test_y) - 3,\n    f\"loss = {np.sqrt(np.mean((test_y-yhat)**2)):.3f}\",\n)\nplt.title(\"Testing Data\")\nplt.show()\n```\n\nWe\u2019ve plotted above the loss on our training data and testing data. The loss on training goes down after each step, as we would expect for gradient descent. However, the testing loss goes down and then starts to go back up. This is called **overfitting**. This is one of the key challenges in ML and we\u2019ll often be discussing it.\n\nOverfitting is a result of training for too many steps or with too many parameters, resulting in our model learning the **noise** in the training data. The noise is specific for the training data and when computing loss on the test data there is poor performance.\n\nTo understand this, let\u2019s first define noise. Assume that there is a \u201cperfect\u201d function \\\\(f(\\\\vec{x})\\\\) that can compute labels from features. Our model is a...",
      "url": "https://dmol.pub/ml/regression.html"
    }
  ]
}