{
  "query": "What are the best techniques for graph neural networks to predict chemical reaction yields with solvent effects?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Graph neural networks (GNNs) have become the leading architecture for reaction\u2011yield prediction because they can directly encode the molecular topology of reactants, products and, crucially, the surrounding solvent.  The most effective pipelines now combine three complementary ideas.  First, the reaction is represented as a set of molecular graphs\u202f\u2014\u202fone for each reactant, catalyst, product and the solvent\u202f\u2014\u202fand a **molecular\u2011interaction module** learns pairwise attention between atoms belonging to different components, allowing the model to capture synergistic steric and electronic effects of the solvent on the transition state\u202f([nature.com](https://www.nature.com/articles/s41467-023-39283-x)).  Second, the solvent\u2011specific interactions are modeled explicitly by **merging the solute and solvent graphs** into a single heterogeneous graph and applying an attention\u2011based message\u2011passing network, as demonstrated in the MMGNN framework for solvation free\u2011energy prediction; this approach can be transferred to yield prediction by treating the solvent as an additional node set\u202f([ijcai.org](https://www.ijcai.org/proceedings/2024/0642.pdf)).  Third, **pre\u2011training on large reaction corpora** (e.g., USPTO or high\u2011throughput screening datasets) followed by fine\u2011tuning on solvent\u2011annotated reactions improves generalisation, especially when the solvent space is sparse\u202f([jcheminf.biomedcentral.com](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z)).\n\nTo obtain reliable predictions, modern GNNs also incorporate **uncertainty\u2011aware learning**, where the model outputs a predictive distribution rather than a point estimate; this helps identify reactions where solvent effects are poorly captured and guides data acquisition\u202f([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC8750748)).  For interpretability, **graph\u2011additive models** decompose the predicted yield into contributions from individual atoms or functional groups, making it possible to trace which solvent\u2011induced electronic or steric features drive the prediction\u202f([iopscience.iop.org](https://iopscience.iop.org/article/10.1088/2632-2153/addfaa)).  Together, these techniques\u2014interaction\u2011aware message passing, merged solute\u2011solvent graphs, large\u2011scale pre\u2011training, uncertainty quantification, and additive interpretability\u2014constitute the current best practice for GNN\u2011based chemical\u2011reaction\u2011yield prediction that explicitly accounts for solvent effects.",
      "url": ""
    },
    {
      "title": "Interpretation of chemical reaction yields with graph neural additive ...",
      "text": "<div><div>\n<h2>We apologize for the inconvenience...</h2>\n<p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p>\n<p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>\n<p>\n<a href=\"https://ioppublishing.org/contacts/\">https://ioppublishing.org/contacts/</a>\n</p>\n</div></div>",
      "url": "https://iopscience.iop.org/article/10.1088/2632-2153/addfaa"
    },
    {
      "title": "Uncertainty-aware prediction of chemical reaction yields with graph ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>In this paper, we present a data-driven method for the uncertainty-aware prediction of chemical reaction yields. The reactants and products in a chemical reaction are represented as a set of molecular graphs. The predictive distribution of the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance. Uncertainty-aware learning and inference are applied to the model to make accurate predictions and to evaluate their uncertainty. We demonstrate the effectiveness of the proposed method on benchmark datasets with various settings. Compared to the existing methods, the proposed method improves the prediction and uncertainty quantification performance in most settings.</p>\n<section><p><strong>Keywords:</strong> Chemical reaction yield prediction, Uncertainty-aware prediction, Graph neural network, Deep learning</p></section></section><section><h2>Introduction</h2>\n<p>In organic chemistry, the prediction of chemical reaction yields is an important research topic in chemical synthesis planning [<a href=\"#CR1\">1</a>, <a href=\"#CR2\">2</a>]. This enables the estimation of the overall yield of a complex synthetic pathway and the detection of low-yield reactions that negatively affect the overall yield. It also provides clues for designing new reactions that provide higher yields to save on the time and cost required for experimental syntheses.</p>\n<p>Machine learning has achieved remarkable success in the data-driven prediction of chemical reaction yields [<a href=\"#CR1\">1</a>, <a href=\"#CR3\">3</a>\u2013<a href=\"#CR7\">7</a>]. The main concept is to construct a prediction model that predicts the yield of a chemical reaction by learning from previously accumulated data comprising a number of chemical reactions annotated with their experimentally measured yields. The successful application of a prediction model enables fast and efficient estimation of chemical reaction yields without performing experimental syntheses, which are costly and time-consuming.</p>\n<p>Early studies represented each chemical reaction as a fixed-size vector of handcrafted features, such as molecular fingerprints and chemical property descriptors, and constructed an off-the-shelf prediction model on top of the vector representation [<a href=\"#CR3\">3</a>\u2013<a href=\"#CR5\">5</a>, <a href=\"#CR8\">8</a>]. The limitation of this approach is that the choice of adequate features relies on chemical knowledge and intuition, and some inherent information to the original reaction may be lost in the representation. With advances in deep learning [<a href=\"#CR9\">9</a>], recent studies have applied deep neural networks constructed on a more informative representation of a chemical reaction. Schwaller et al. [<a href=\"#CR6\">6</a>, <a href=\"#CR10\">10</a>] used simplified molecular-input line-entry system (SMILES) to represent a chemical reaction. To predict the reaction yield, they fine-tuned a bidirectional encoder representations from transformers (BERT) model pre-trained using a reaction SMILES database [<a href=\"#CR11\">11</a>] to predict the yield. Saebi et al. [<a href=\"#CR7\">7</a>] represented a chemical reaction as a set of graphs, on which a graph neural network was constructed to predict the yield.</p>\n<p>In this paper, we present an alternative method for predicting chemical reaction yields. As a prediction model, we adapt a graph neural network that directly operates on the graph representation of a chemical reaction in a permutation-invariant fashion. We use uncertainty-aware learning and inference in the model to make accurate predictions of yields and determine the confidence of predictions.</p></section><section><h2>Methods</h2>\n<section><h3>Data representation</h3>\n<p>We suppose that a chemical reaction consists of a number of reactants and a single product. This chemical reaction is labeled with its reaction yield. Each instance is represented as <span></span>, where <span></span> and <span></span> are the set of <em>m</em> reactants and the resulting product in the reaction, respectively, and <em>y</em> is the reaction yield. The number of reactants <em>m</em> can be different for each reaction.</p>\n<p>Each molecule in <span></span> and <span></span> is defined as an undirected graph <span></span>, where <span></span> and <span></span> represent the set of nodes and the set of edges, respectively. The node feature vectors <span></span> and edge feature vectors <span></span> are associated with heavy atoms (e.g., C, N, O, and F) and their bonds (e.g., single, double, triple, and aromatic), respectively. Hydrogen atoms are treated implicitly. The number of heavy atoms and bonds in each molecule is the same as the number of node feature vectors and edge feature vectors in the corresponding graph representation, respectively. Figure\u00a0<a href=\"#Fig1\">1</a> illustrates an example of the graph representation of a molecule.</p>\n<figure><h4>Fig. 1.</h4>\n<p></p>\n<figcaption><p>Illustrative example of the graph representation for a molecule</p></figcaption></figure><p>For the <em>j</em>-th atom, <span></span> is a vector indicating the atom type, formal charge, degree, hybridization, number of hydrogens, valence, chirality, whether it accepts or donates electrons, whether it is aromatic, whether it is in a ring, and associated ring sizes. For the bond between the <em>j</em>-th and <em>k</em>-th atoms, <span></span> is a vector indicating the bond type, stereochemistry, whether it is in a ring, and whether it is conjugated.</p></section><section><h3>Prediction model</h3>\n<p>To predict the reaction yield <em>y</em>, we introduce a predictive distribution for <em>y</em> conditioned on the set of reactants <span></span> and product <span></span>, denoted by <span></span>, which is modeled as a normal distribution as follows:</p>\n<p>where <span></span> and <span></span> are the mean and variance of the distribution, respectively. We parameterize the predictive distribution <span></span> using a neural network <em>f</em> that produces <span></span> and <span></span> as a function of <span></span> and <span></span> with a set of parameters <span></span>:</p>\n<p>To construct the neural network <em>f</em>, we adapt the architecture presented by Saebi <em>et al.</em> [<a href=\"#CR7\">7</a>] to process two sets of molecular graphs with advanced neural network modules. Figure\u00a0<a href=\"#Fig2\">2</a> illustrates the architecture used in this study. The architectural details of each component are presented next.</p>\n<figure><h4>Fig. 2.</h4>\n<p></p>\n<figcaption><p>Architecture of the prediction model</p></figcaption></figure><p>A message passing neural network (MPNN) [<a href=\"#CR12\">12</a>] is used as the GNN component of <em>f</em> to process each molecular graph <span></span> in <span></span> and <span></span>. The GNN is designed to take <span></span> as the input and return the graph representation vector <span></span> as the output:</p>\n<p>In the GNN, we apply multiple message passing steps using an edge network as a message function and a gated recurrent unit (GRU) network as an update function to generate node representation vectors. We then apply a set2set model [<a href=\"#CR13\">13</a>] as a readout function for global pooling over the node representation vectors to obtain a graph-level embedding that is invariant to the order of the nodes. The embedding is sparsified by a fully-connected layer to obtain the graph representation vector <span></span>. The use of the GNN renders the representation invariant to graph isomorphism.</p>\n<p>We summate the graph representation vectors for <span></span>. This makes the representation invariant with respect to the order of the reactants. The summated vector is concatenated with the graph representation vector <span></span> to generate the reaction representation vector <span></span>:</p>\n<p>The reaction representation vector <span></s...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8750748"
    },
    {
      "title": "Reaction performance prediction with an extrapolative and ... - Nature",
      "text": "Reaction performance prediction with an extrapolative and interpretable graph model based on chemical knowledge\n\n[Download PDF](https://www.nature.com/articles/s41467-023-39283-x.pdf)\n\n[Download PDF](https://www.nature.com/articles/s41467-023-39283-x.pdf)\n\n### Subjects\n\n- [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n- [Method development](https://www.nature.com/subjects/method-development)\n- [Reaction mechanisms](https://www.nature.com/subjects/reaction-mechanisms)\n\n## Abstract\n\nAccurate prediction of reactivity and selectivity provides the desired guideline for synthetic development. Due to the high-dimensional relationship between molecular structure and synthetic function, it is challenging to achieve the predictive modelling of synthetic transformation with the required extrapolative ability and chemical interpretability. To meet the gap between the rich domain knowledge of chemistry and the advanced molecular graph model, herein we report a knowledge-based graph model that embeds the digitalized steric and electronic information. In addition, a molecular interaction module is developed to enable the learning of the synergistic influence of reaction components. In this study, we demonstrate that this knowledge-based graph model achieves excellent predictions of reaction yield and stereoselectivity, whose extrapolative ability is corroborated by additional scaffold-based data splittings and experimental verifications with new catalysts. Because of the embedding of local environment, the model allows the atomic level of interpretation of the steric and electronic influence on the overall synthetic performance, which serves as a useful guide for the molecular engineering towards the target synthetic function. This model offers an extrapolative and interpretable approach for reaction performance prediction, pointing out the importance of chemical knowledge-constrained reaction modelling for synthetic purpose.\n\n### Similar content being viewed by others\n\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\n\nArticleOpen access04 May 2023\n\n### [A generalized-template-based graph neural network for accurate organic reactivity prediction](https://www.nature.com/articles/s42256-022-00526-z?fromPaywallRec=false)\n\nArticle15 September 2022\n\n### [A meta-learning approach for selectivity prediction in asymmetric catalysis](https://www.nature.com/articles/s41467-025-58854-8?fromPaywallRec=false)\n\nArticleOpen access16 April 2025\n\n## Introduction\n\nThe chemical comprehension and accurate prediction of reactivity and selectivity provide the foundation for the rational and efficient exploration of massive synthetic space[1](https://www.nature.com/articles/s41467-023-39283-x#ref-CR1), [2](https://www.nature.com/articles/s41467-023-39283-x#ref-CR2).\u00a0This establishment of the structure\u2013performance relationship (SPR) has been focused on the reaction mechanism study and elucidation of the determining transition state model[3](https://www.nature.com/articles/s41467-023-39283-x#ref-CR3). Using the transition state model, chemists can elucidate the origins of the observed reactivity/selectivity trend and make synthetic judgments based on chemical theory and empirical experience[4](https://www.nature.com/articles/s41467-023-39283-x#ref-CR4). This classic knowledge-driven strategy has reached remarkable success in synthetic chemistry and continues to provide strong support for the discovery of new catalysts, reagents, and reaction[5](https://www.nature.com/articles/s41467-023-39283-x#ref-CR5). Despite the advantage of offering qualitative guidance in the synthetic universe, it is challenging for the knowledge-driven strategy to handle the high-dimensional SPR without a clear mechanistic basis and analytic equation. The seemingly subtle change in catalyst, additive, or even solvent may result in significant perturbation of the overall synthetic performance[6](https://www.nature.com/articles/s41467-023-39283-x#ref-CR6), [7](https://www.nature.com/articles/s41467-023-39283-x#ref-CR7). This is why laborious and repetitive condition optimization is still inevitably required, limiting the efficiency of synthetic development[8](https://www.nature.com/articles/s41467-023-39283-x#ref-CR8).\n\nThe data-driven approach has recently emerged as a powerful strategy for SPR establishment[9](https://www.nature.com/articles/s41467-023-39283-x#ref-CR9), [10](https://www.nature.com/articles/s41467-023-39283-x#ref-CR10). By harnessing the interrelationship within the synthetic data, modern machine learning (ML) algorithms can create powerful models for synthetic prediction. Accurate predictions of reaction yield[11](https://www.nature.com/www.nature.com#ref-CR11), [12](https://www.nature.com/www.nature.com#ref-CR12), [13](https://www.nature.com/www.nature.com#ref-CR13), [14](https://www.nature.com/articles/s41467-023-39283-x#ref-CR14), kinetic rate[15](https://www.nature.com/articles/s41467-023-39283-x#ref-CR15), [16](https://www.nature.com/articles/s41467-023-39283-x#ref-CR16) and activation energy[17](https://www.nature.com/www.nature.com#ref-CR17), [18](https://www.nature.com/www.nature.com#ref-CR18), [19](https://www.nature.com/articles/s41467-023-39283-x#ref-CR19), chemo-[20](https://www.nature.com/articles/s41467-023-39283-x#ref-CR20), regio-[21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/www.nature.com#ref-CR22), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/articles/s41467-023-39283-x#ref-CR25), and stereoselectivity[26](https://www.nature.com/www.nature.com#ref-CR26), [27](https://www.nature.com/www.nature.com#ref-CR27), [28](https://www.nature.com/www.nature.com#ref-CR28), [29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/articles/s41467-023-39283-x#ref-CR32). have been achieved in a wide array of organic transformations, which validated the exciting concept of ML prediction of synthetic performances. However, the ML prediction and design of synthetic transformation are still far from mature. One of the major bottlenecks is the availability of the molecular encoding approach and the ML framework that are suitable for SPR prediction (Fig.\u00a0[1a](https://www.nature.com/articles/s41467-023-39283-x#Fig1)). Quantum chemical descriptors[27](https://www.nature.com/articles/s41467-023-39283-x#ref-CR27) are known for their solid physical basis and high descriptive ability, but their application typically requires a sophisticated understanding of the underlying reaction mechanism, and the descriptor generation can be time- and resource-consuming for large-scale screening. The string- and topological structure-based encodings (i.e., SMILES, molecular fingerprints, etc.)[33](https://www.nature.com/articles/s41467-023-39283-x#ref-CR33), [34](https://www.nature.com/articles/s41467-023-39283-x#ref-CR34) do not require expert knowledge of the studied transformation and can be efficiently generated, while it is difficult to trace the physical organic origins of the synthetic performance. In addition, the extrapolation problem presents additional challenges for SPR prediction[35](https://www.nature.com/articles/s41467-023-39283-x#ref-CR35), [36](https://www.nature.com/articles/s41467-023-39283-x#ref-CR36). Current synthetic models still lack sufficient guidance for developing new catalysts and transformations.\n\n**Fig. 1: Machine learning prediction of synthetic performance and molecular property.**\n\n**a** Representative strategy of synthetic prediction by concatenating the molecular encodings of reactant 1 (orange), reactant 2 (blue), additive (green), and product (yellow). **b** Previous work of quantum chemistry-augmente...",
      "url": "https://www.nature.com/articles/s41467-023-39283-x"
    },
    {
      "title": "[PDF] MMGNN: A Molecular Merged Graph Neural Network for ... - IJCAI",
      "text": "Wenjie Du, Shuai Zhang, Di Wu, Jun Xia, Ziyuan Zhao, Junfeng Fang, Yang Wang MMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation Free Energy Prediction https://ijcai.org/proceedings/2024/0642.pdf\nMMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation Free Energy Prediction\nWenjie Du, Shuai Zhang, Di Wu, Jun Xia, Ziyuan Zhao, Junfeng Fang, Yang Wang\n2024-07-25\nMMGNN: A Molecular Merged Graph Neural Network for Explainable Solvation\nFree Energy Prediction\nWenjie Du1,2, Shuai Zhang1,2, Di Wu1, Jun Xia3, Ziyuan Zhao4, Junfeng Fang1,\u2217,\nYang Wang1,2,\u2217\n1University of Science and Technology of China (USTC), Hefei, China\n2Suzhou Institute for Advanced Research, USTC, Suzhou, China\n3Zhejiang University, Hangzhou, China\n4Agency for Science, Technology and Research (A*STAR), Singapore\n{duwenjie, shuaizhang, fjf, wdcxy}@mail.ustc.edu.cn, zhaoz@i2r.a-star.edu.sg, junxia@zju.edu.cn,\nangyan@ustc.edu.cn\nAbstract\nIn this paper, we address the challenge of ac\u0002curately modeling and predicting Gibbs free en\u0002ergy in solute-solvent interactions, a pivotal yet\ncomplex aspect in the feld of chemical model\u0002ing. Traditional approaches, primarily relying on\ndeep learning models, face limitations in captur\u0002ing the intricate dynamics of these interactions. To\novercome these constraints, we introduce a novel\nframework, Molecular Modeling Graph Neural\nNetwork (MMGNN), which more closely mir\u0002rors real-world chemical processes. Specifcally,\nMMGNN exquisitely models atomic interactions\nsuch as hydrogen bonds by initially forming in\u0002discriminate connections between intermolecular\natoms, which are then refned using an attention\u0002based aggregation method, tailoring to specifc\nsolute-solvent pairs. To address the challenges\nof non-interactive or repulsive atomic interactions,\nMMGNN incorporates interpreters for nodes and\nedges in the merged graph, enhancing explainabil\u0002ity and reducing redundancy. MMGNN stands as\nthe frst framework to exquisitely align with real\nchemical processes, providing a more accurate and\nscientifcally sound approach to modeling solute\u0002solvent interactions. The infusion of explainability\nallows for the extraction of key subgraphs, which\nare pivotal for further research in solute-solvent\ndynamics. Extensive experimental validation con\u0002frms the effcacy and enhanced explainability of\nMMGNN.\n1 Introduction\nUnderstanding solute-solvent interactions in specifc solvents\nis pivotal for areas in physical chemistry, including chemical\nreactions and electrochemistry [Varghese and Mushrif, 2019;\nD\u2019Souza et al., 2011]. A comprehensive grasp of these in\u0002teractions is vital not only for explanatory experimental out\u0002comes but also for guiding the design and control of reac\u0002tions and properties [Chung et al., 2022; Fang et al., 2024b;\n\ud835\udc27 \ud835\udc29\ud835\udc27\n\ud835\udc27\nGraph Merge\n\ud835\udfcf\n\ud835\udfd0\n\ud835\udc29\ud835\udc27\n\u2032\n\u2032\n\n\ud835\udc27\n\ud835\udc27\n\ud835\udfcf\n\ud835\udfd0\n+ \n\ud835\udc27\n\ud835\udc27\nMerge\n\ud835\udfcf\n\ud835\udfd0\n+ \nEthanol\n(Solvent)\nAcetonitrile\n(Solute)\nDissolution\nInitial state\nHomogeneous \nsolution\n(a) Method based on Concatenation\n(b) Method based on Merging\n(c) Chemical Processes\n(d) Our Method based on Merged Graph\nConcatenate Prediction\nPrediction\nPrediction\nFigure 1: Comparison of different paradigms for Gibbs free energy\nprediction. (a) Method by concatenation; (b) method by merging;\n(c) a schematic diagram of the process where acetonitrile (solute) is\ndissolved in ethanol (solvent); and (d) the illustration of our method.\nBest viewed in color.\nXia et al., 2023b]. In this context, the solvation Gibbs free\nenergy \u2206Gsolv emerges as a critical physicochemical prop\u0002erty, dictating a molecule\u2019s behavior in solution [Low et al.,\n2022]. This property is intricately linked to the solute\u2019s parti\u0002tion coeffcient between gas and solvent phases [Chung et al.,\n2022]. However, empirically testing solute-solvent free ener\u0002gies for all combinations is impractical due to high costs and\nextensive time requirements. This challenge necessitates an\nincreased reliance on deep learning models to predict these\nenergies more effciently [Varghese and Mushrif, 2019].\nThe landscape of solute-solvent interaction modeling is\ncurrently dominated by two primary approaches. The frst,\ntermed embedding concatenation, employs two separate\nGraph Neural Networks (GNNs) to represent solute and\nsolvent molecules. The individual embeddings from these\nGNNs are then concatenated for subsequent prediction tasks,\nProceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI-24)\n5808\nas depicted in Figure 1 (a). While effective, this method may\nnot adequately capture the intricate coupling between solute\nand solvent. In contrast, the embedding merging approach,\nshown in Figure 1 (b), seeks to address this limitation. It\nalso utilizes two Graph Encoders for initial representation\nbut differs in the subsequent processing of these embeddings.\nHere, the embeddings are merged through advanced interac\u0002tion strategies, such as Transformer-based techniques or in\u0002teractive pruning algorithms, to better refect the complex in\u0002teractions before making predictions.\nAlthough the embedding merging approach marks an ad\u0002vancement, it falls short in accurately simulating the real\nchemical processes involved in dissolution, as depicted in\nFigure 1 (c). The dissolution process is characterized by the\nsubstitution of intramolecular forces within solute and solvent\nwith intermolecular forces between them. This is exempli\u0002fed by the interaction of acetonitrile and ethanol, where in\u0002dividual acetonitrile molecules are gradually surrounded by\nethanol molecules. The formation of hydrogen bonds be\u0002tween atoms (nodes) of these molecules leads to a homo\u0002geneous solution. This natural dissolution process suggests\na need for an algorithm that explicitly models these inter\u0002molecular node interactions, rather than relying solely on\nembedding-level interactions. Such a model, by more closely\nmirroring actual chemical processes, is crucial for both scien\u0002tifc accuracy and the effectiveness of the predictions.\nIn response to the limitations of existing models, we\npropose the Molecular Modeling Graph Neural Network\n(MMGNN), a novel framework designed to closely align\nwith the actual chemical processes in solute-solvent interac\u0002tions. This framework signifcantly improves the prediction\naccuracy of Gibbs free energy. Illustrated in Figure 1 (d),\nMMGNN begins by indiscriminately connecting intermolec\u0002ular atoms to enhance interactions, such as hydrogen bonds,\nbetween molecules. After that, MMGNN assigns variable\nweights to these connections, refecting the different con\u0002straints of various chemical bonds. An attention-based ag\u0002gregation method is then employed, enabling the framework\nto adaptively learn from diverse solute-solvent combinations.\nThe result is a merged graph representation for each pair,\nhighlighting the most signifcant atomic interactions.\nMeanwhile, this framework also addresses potential chal\u0002lenges: (1) the presence of non-existent or repulsive atomic\ninteractions, and (2) increased complexity and convergence\ndiffculties in merged complete graphs for large molecular\npairs. Inspired by graph explainability algorithms, MMGNN\nincorporates interpreters for both nodes and solute-solvent\nedges in the merged graph. This approach effectively reduces\nredundancy, focusing only on relevant interactions. The ex\u0002planatory subgraph is then encoded and utilized in regression\nmodels, such as Fully Connected Neural Networks (FCNN),\nto predict Gibbs free energy with greater precision.\nIn conclusion, the main contributions of this paper can be\nsummarized as follows:\n\u2022 Introduction of MMGNN: We present the Molecular\nModeling Graph Neural Network (MMGNN), a pioneering\nframework designed to explicitly align with actual chemi\u0002cal processes, enabling more accurate modeling of solute\u0002solvent interactions.\n\u2022 Advancement in Explainability: Our approach integrates\nexplainability into the model, allowing for the extraction of\nkey subgraphs. This feature not only enhances the under\u0002standing of MMGNN\u2019s predictions but a...",
      "url": "https://www.ijcai.org/proceedings/2024/0642.pdf"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph neural networks",
      "text": "Search all BMC articles\n\nSearch\n\nImproving chemical reaction yield prediction using pre-trained graph neural networks\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 01 March 2024\n\n# Improving chemical reaction yield prediction using pre-trained graph neural networks\n\n- [Jongmin Han](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Jongmin-Han-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1),\n- [Youngchun Kwon](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youngchun-Kwon-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Youn-Suk Choi](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youn_Suk-Choi-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2) &\n- \u2026\n- [Seokho Kang](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Seokho-Kang-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a025 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 5588 Accesses\n\n- 9 Citations\n\n- 2 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z/metrics)\n\n\n## Abstract\n\nGraph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.\n\n## Introduction\n\nA chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner \\[ [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR3), [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR4), [5](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR5), [6](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR6), [7](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\].\n\nRecently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR6), [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR7)\\] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR9), [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR10)\\].\n\nDespite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.\n\nTo alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR12)\\]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart \\[ [13](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR13), [14](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR14), [15](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR15), [16](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR16), [17](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR18)\\]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented \\[ [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR20)\\]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them \\[ [21](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR21), [22](https://jcheminf.biomedcentral.com/jcheminf.biome...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z"
    },
    {
      "title": "Leveraging graph neural networks to predict Hammett's constants ...",
      "text": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- [View\u00a0**PDF**](https://www.sciencedirect.com/science/article/pii/S294974772400037X/pdfft?md5=335a88ffe3f92c7e45d0c719a8b49fd9&pid=1-s2.0-S294974772400037X-main.pdf)\n- Download full issue\n\nSearch ScienceDirect\n\n## [Artificial Intelligence Chemistry](https://www.sciencedirect.com/journal/artificial-intelligence-chemistry)\n\n[Volume 2, Issue 2](https://www.sciencedirect.com/journal/artificial-intelligence-chemistry/vol/2/issue/2), December 2024, 100079\n\n# Leveraging graph neural networks to predict Hammett\u2019s constants for benzoic acid derivatives\n\nAuthor links open overlay panelVaneetSaini1, RanjeetKumar\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.aichem.2024.100079](https://doi.org/10.1016/j.aichem.2024.100079) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S294974772400037X&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by-nc/4.0/)\n\nOpen access\n\n## Highlights\n\n- \u2022\nUtilization of graph-based molecular encoding derived from SMILES notations for organic molecules.\n\n- \u2022\nPioneering study using a large-scale dataset to predict Hammett's constant parameters.\n\n- \u2022\nDataset is publicly available, supporting reproducibility and further research.\n\n- \u2022\nThe Attentive FP algorithm demonstrated high predictive accuracy, achieving an R\u00b2 score of 0.93 on the test set.\n\n- \u2022\nThis method offers a rapid and efficient solution for predicting Hammett's constants.\n\n\n## Abstract\n\nThe Hammett constants, \u03c3m and \u03c3p, reflect the electron-withdrawing and electron-donating abilities of substituents on aromatic compounds, and have been successfully used in various structure-activity relationship studies. However, determining these constants experimentally is both resource-intensive and time-consuming approach. In this study, we explore the use of graph neural networks (GNNs) to predict Hammett constant parameters using graph-based features. This innovative approach aims to provide rapid and efficient predictions of \u03c3m and \u03c3p values, eliminating the need for extensive computational and experimental setups. By leveraging the power of GNNs, we hope to streamline the process of obtaining these critical parameters, thereby facilitating more efficient reaction design and enhancing the applicability of linear free energy relationship studies in chemical research.\n\n## Graphical Abstract\n\n1. [Download: Download high-res image (115KB)](https://ars.els-cdn.com/content/image/1-s2.0-S294974772400037X-ga1_lrg.jpg)\n2. [Download: Download full-size image](https://ars.els-cdn.com/content/image/1-s2.0-S294974772400037X-ga1.jpg)\n\nThis study employs graph neural networks (GNNs) to predict Hammett\u2019s constants, aiming for rapid, efficient predictions without extensive experimental setups, enhancing reaction design and chemical research.\n\n- Previous article in issue\n- Next article in issue\n\n## Keywords\n\nHammett constants\n\nMachine learning\n\nGraph neural network, Kinetic parameters\n\nMolecular modelling\n\nLoading...\n\nRecommended articles\n\n## Data availability statement\n\nThe datasets and model algorithms can be accessed from this link: [https://github.com/v-saini/hammet-gnn.git](https://github.com/v-saini/hammet-gnn.git)\n\n[1](https://www.sciencedirect.com/www.sciencedirect.com#bfn1)\n\nOrcid ID: 0000\u20130002-8186\u20135166\n\n\u00a9 2024 The Author(s). Published by Elsevier B.V.",
      "url": "https://www.sciencedirect.com/science/article/pii/S294974772400037X"
    },
    {
      "title": "[PDF] Prediction of Chemical Reaction Yields using Deep Learning",
      "text": "Prediction of Chemical Reaction Yields using Deep\nLearning\nPhilippe Schwaller\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nE-mail: phs@zurich.ibm.com\nAlain C. Vaucher, Teodoro Laino\nIBM Research \u2013 Europe, S\u00a8aumerstrasse 4, 8803 R\u00a8uschlikon, Switzerland\nJean-Louis Reymond\nDepartment of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012\nBern, Switzerland\nAbstract. Artificial intelligence is driving one of the most important revolutions\nin organic chemistry. Multiple platforms, including tools for reaction prediction and\nsynthesis planning based on machine learning, successfully became part of the organic\nchemists\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike\nreaction prediction and retrosynthetic models, the prediction of reaction yields has\nreceived less attention in spite of the enormous potential of accurately predicting\nreaction conversion rates. Reaction yields models, describing the percentage of\nthe reactants converted to the desired products, could guide chemists and help\nthem select high-yielding reactions and score synthesis routes, reducing the number\nof attempts. So far, yield predictions have been predominantly performed for\nhigh-throughput experiments using a categorical (one-hot) encoding of reactants,\nconcatenated molecular fingerprints, or computed chemical descriptors. Here, we\nextend the application of natural language processing architectures to predict reaction\nproperties given a text-based representation of the reaction, using an encoder\ntransformer model combined with a regression layer. We demonstrate outstanding\nprediction performance on two high-throughput experiment reactions sets. An analysis\nof the yields reported in the open-source USPTO data set shows that their distribution\ndi\u21b5ers depending on the mass scale, limiting the dataset applicability in reaction yields\npredictions.\n1. Introduction\nChemical reactions in organic chemistry are described by writing the structural\nformula of reactants and products separated by an arrow, representing the chemical\ntransformation by specifying how the atoms rearrange between one or several reactant\nPrediction of Chemical Reaction Yields using Deep Learning 2\nmolecules and one or several product molecules [1]. Economic, logistic, and energetic\nconsiderations drive chemists to prefer chemical transformations capable of converting\nall reactant molecules into products with the highest yield possible. However, side\u0002reactions, degradation of reactants, reagents or products in the course of the reaction,\nequilibrium processes with incomplete conversion to a product, or simply by product\nisolation and purification undermine the quantitative conversion of reactants into\nproducts, rarely reaching optimal performance.\nReaction yields are usually reported as a percentage of the theoretical chemical\nconversion, i.e., the percentage of the reactant molecules successfully converted to the\ndesired product compared to the theoretical value. It is not uncommon for chemists\nto synthesise a molecule in a dozen or more reaction steps. Hence, low-yield reactions\nmay have a disastrous e\u21b5ect on the overall route yield because of the individual steps\u2019\nmultiplicative e\u21b5ect. Therefore, it is not surprising that designing new reactions with\nyields higher than existing ones attracts much e\u21b5ort in organic chemistry research.\nIn practice, specific chemical reaction classes are characterised by lower or higher\nyields, with the actual value depending on the reaction conditions (temperature,\nconcentrations, etc.) and on the specific substrates.\nEstimating the reaction yield can be a game-changing asset for synthesis planning.\nIt provides chemists with the ability to evaluate the overall yield of complex reaction\npaths, addressing possible shortcomings well ahead of investing hours and materials in\nwet-lab experiments. Computational models predicting reaction yields could support\nsynthetic chemists in choosing an appropriate synthesis route among many predicted\nby data-driven algorithms. Moreover, reaction yields prediction models could also be\nemployed as scoring functions in computer-assisted retrosynthesis route planning tools\n[2, 3, 4, 5], to complement forward prediction models [6, 4] and in-scope filters [2].\nMost of the existing e\u21b5orts in constructing models for the prediction of reactivity or\nof reaction yields focused on a particular reaction class: oxidative dehydrogenations of\nethylbenzene with tin oxide catalysts [7], reactions of vanadium selenites [8], Buchwald\u2013\nHartwig aminations [9, 10, 11], and Suzuki\u2013Miyaura cross-coupling reactions [12, 13, 14].\nTo the best of our knowledge, there was only one attempt to design a general-purpose\nprediction model for reactivity and yields, without applicability constraints to a specific\nreaction class [15]. In this work, the authors design a model predicting whether the\nreaction yield is above or below a threshold value and conclude that the models and\ndescriptors they consider cannot deliver satisfactory results.\nHere, we build on our legacy of treating organic chemistry as a language to introduce\na new model that predicts reaction yields starting from reaction SMILES [16]. More\nspecifically, we fine-tune the rxnfp models by Schwaller et al. [17] based on a BERT\u0002encoder [18] by extending it with a regression layer to predict reaction yields. BERT\nencoders belong to the transformer model family, which has revolutionised natural\nlanguage processing [19, 18]. These models take sequences of tokens as input to compute\ncontextualised representations of all the input tokens, and can be applied to reactions\nrepresented in the SMILES [20] format. In this work, we demonstrate for the first\nPrediction of Chemical Reaction Yields using Deep Learning 3\ntime, that these natural language architectures are very useful not only when working\nwith language tokens, but also to provide descriptors of high quality to predict reaction\nproperties such as reaction yields.\nIt is possible to train our approach both on data specific to a given reaction class\nor on data representing di\u21b5erent reaction types. Thus, we initially trained the model on\ntwo high-throughput experimentation (HTE) data sets. Among the few HTE reaction\ndata sets published in recent years, we selected the data sets for palladium-catalysed\nBuchwald\u2013Hartwig reactions provided by Ahneman et al. [9] and for Suzuki\u2013Miyaura\ncoupling reactions provided by Perera et al. [21]. Finally, we trained our model on\npatent data available in the USPTO data set [22, 23].\nHTE and Patent data sets are very di\u21b5erent in terms of content and quality. HTE\ndata sets typically cover a very narrow region in the chemical reaction space, with\nchemical reaction data related to one or a few reaction templates applied to large\ncombinations of selected precursors (reactants, solvents, bases, catalysts, etc.). In\ncontrast, patent reactions cover a much wider reaction space. In terms of quality, HTE\ndata sets report reactions represented uniformly and with yields measured using the\nsame analytical equipment, thus providing a consistent and high quality collection of\nknowledge. In comparison, the yields from patents were measured by di\u21b5erent scientists\nusing di\u21b5erent equipments. Incomplete information in the original documents, such\nas unreported reagents or reaction conditions, and the extensive limitation in text\nmining technologies makes the entire set of patent reactions quite noisy and sparse.\nAn extensive analysis of the USPTO data set revealed that the experimental conditions\nand reaction parameters, such as scale of the reaction, concentrations, temperature,\npressure, or reaction duration, may have a significant e\u21b5ect on the measured reaction\nyields. The functional dependency of the yields from the reaction conditions poses\nadditional constraints, as the model presented in this w...",
      "url": "https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/60c750f2ee301c70b1c7a973/original/prediction-of-chemical-reaction-yields-using-deep-learning.pdf"
    },
    {
      "title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
      "text": "Kwon\u00a0et\u00a0al. Journal of Cheminformatics (2022) 14:2 \nhttps://doi.org/10.1186/s13321-021-00579-z\nRESEARCH ARTICLE\nUncertainty-aware prediction of\u00a0chemical \nreaction yields with\u00a0graph neural networks\nYoungchun Kwon1,2, Dongseon Lee1, Youn\u2011Suk Choi1* and Seokho Kang3*\nAbstract\nIn this paper, we present a data-driven method for the uncertainty-aware prediction of chemical reaction yields. The \nreactants and products in a chemical reaction are represented as a set of molecular graphs. The predictive distribution \nof the yield is modeled as a graph neural network that directly processes a set of graphs with permutation invariance. \nUncertainty-aware learning and inference are applied to the model to make accurate predictions and to evaluate their \nuncertainty. We demonstrate the efectiveness of the proposed method on benchmark datasets with various settings. \nCompared to the existing methods, the proposed method improves the prediction and uncertainty quantifcation \nperformance in most settings.\nKeywords: Chemical reaction yield prediction, Uncertainty-aware prediction, Graph neural network, Deep learning\n\u00a9 The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativeco\nmmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nIn organic chemistry, the prediction of chemical reac\u0002tion yields is an important research topic in chemical \nsynthesis planning [1, 2]. Tis enables the estimation of \nthe overall yield of a complex synthetic pathway and the \ndetection of low-yield reactions that negatively afect \nthe overall yield. It also provides clues for designing new \nreactions that provide higher yields to save on the time \nand cost required for experimental syntheses.\nMachine learning has achieved remarkable success in \nthe data-driven prediction of chemical reaction yields \n[1, 3\u20137]. Te main concept is to construct a prediction \nmodel that predicts the yield of a chemical reaction by \nlearning from previously accumulated data compris\u0002ing a number of chemical reactions annotated with \ntheir experimentally measured yields. Te successful \napplication of a prediction model enables fast and ef\u0002cient estimation of chemical reaction yields without \nperforming experimental syntheses, which are costly and \ntime-consuming.\nEarly studies represented each chemical reaction as a \nfxed-size vector of handcrafted features, such as molecu\u0002lar fngerprints and chemical property descriptors, and \nconstructed an of-the-shelf prediction model on top of \nthe vector representation [3\u20135, 8]. Te limitation of this \napproach is that the choice of adequate features relies \non chemical knowledge and intuition, and some inher\u0002ent information to the original reaction may be lost \nin the representation. With advances in deep learning \n[9], recent studies have applied deep neural networks \nconstructed on a more informative representation of a \nchemical reaction. Schwaller et\u00a0al. [6, 10] used simplifed \nmolecular-input line-entry system (SMILES) to represent \na chemical reaction. To predict the reaction yield, they \nfne-tuned a bidirectional encoder representations from \ntransformers (BERT) model pre-trained using a reaction \nSMILES database [11] to predict the yield. Saebi et\u00a0al. [7] \nrepresented a chemical reaction as a set of graphs, on \nwhich a graph neural network was constructed to predict \nthe yield.\nIn this paper, we present an alternative method for pre\u0002dicting chemical reaction yields. As a prediction model, \nwe adapt a graph neural network that directly operates \nOpen Access\nJournal of Cheminformatics\n*Correspondence: ysuk.choi@samsung.com; s.kang@skku.edu\n1\n Samsung Advanced Institute of Technology, Samsung Electronics Co. \nLtd., 130 Samsung\u2011ro, Yeongtong\u2011gu, Suwon, Republic of Korea 3\n Department of Industrial Engineering, Sungkyunkwan University, 2066 \nSeobu\u2011ro, Jangan\u2011gu, Suwon, Republic of Korea\nFull list of author information is available at the end of the article\nKwon\u00a0et\u00a0al. Journal of Cheminformatics (2022) 14:2 Page 2 of 10\non the graph representation of a chemical reaction in \na permutation-invariant fashion. We use uncertainty\u0002aware learning and inference in the model to make accu\u0002rate predictions of yields and determine the confdence of \npredictions.\nMethods\nData representation\nWe suppose that a chemical reaction consists of a num\u0002ber of reactants and a single product. Tis chemical \nreaction is labeled with its reaction yield. Each instance \nis represented as (R,P, y), where R = {GR,1, ... , GR,m}\nand P = {GP} are the set of m reactants and the resulting \nproduct in the reaction, respectively, and y is the reac\u0002tion yield. Te number of reactants m can be diferent for \neach reaction.\nEach molecule in R and P is defned as an undirected \ngraph G = (V, E), where V and E represent the set of \nnodes and the set of edges, respectively. Te node fea\u0002ture vectors vj \u2208 V and edge feature vectors ej,k \u2208 E\nare associated with heavy atoms (e.g., C, N, O, and F) \nand their bonds (e.g., single, double, triple, and \naromatic), respectively. Hydrogen atoms are treated \nimplicitly. Te number of heavy atoms and bonds in each \nmolecule is the same as the number of node feature vec\u0002tors and edge feature vectors in the corresponding graph \nrepresentation, respectively. Figure\u00a01 illustrates an exam\u0002ple of the graph representation of a molecule.\nFor the j-th atom, vj = (vj,1, ... , vj,p) is a vec\u0002tor indicating the atom type, formal charge, degree, \nhybridization, number of hydrogens, valence, chiral\u0002ity, whether it accepts or donates electrons, whether it \nis aromatic, whether it is in a ring, and associated ring \nsizes. For the bond between the j-th and k-th atoms, \nej,k = (ej,k,1, ... , ej,k,q) is a vector indicating the bond \ntype, stereochemistry, whether it is in a ring, and whether \nit is conjugated.\nPrediction model\nTo predict the reaction yield y, we introduce a predictive \ndistribution for y conditioned on the set of reactants R\nand product P, denoted by p\u03b8 (y|R,P), which is modeled \nas a normal distribution as follows:\nwhere \u00b5 and \u03c32\n are the mean and variance of the distri\u0002bution, respectively. We parameterize the predictive dis\u0002tribution p\u03b8 using a neural network f that produces \u00b5 and \n\u03c32\n as a function of R and P with a set of parameters \u03b8:\nTo construct the neural network f, we adapt the architec\u0002ture presented by Saebi et al. [7] to process two sets of \nmolecular graphs with advanced neural network mod\u0002ules. Figure\u00a0 2 illustrates the architecture used in this \nstudy. Te architectural details of each component are \npresented next.\nA message passing neural network (MPNN) [12] is \nused as the GNN component of f to process each molec\u0002ular graph G in R and P. Te GNN is designed to take G\nas the input and return the graph representation vector r\nas the output:\nIn the GNN, we apply multiple message passing steps \nusing an edge network as a message function and a gated \nrecurrent unit (GRU) network as an update function to \ngenerate node representation vectors. We then apply a \nset2set model [13] as a readout function for g...",
      "url": "https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-021-00579-z.pdf"
    }
  ]
}