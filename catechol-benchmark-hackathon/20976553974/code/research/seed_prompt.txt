## Current Status
- Best CV score: 0.0623 from exp_004 (PerTarget HGB+ETR with LOO)
- Best LB score: 0.0956 from exp_004
- CV-LB gap: +53% â†’ LOO CV is optimistic for unseen solvents
- Target: 0.01727 (5.5x lower than best LB)
- Remaining submissions: 3

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_014 - template compliance is correct
- Evaluator's top priority: Combine MLP + GBDT ensemble with per-target approach and Optuna-optimized weights. **AGREE** - this is the key missing piece
- Key concerns raised:
  1. CV is worse than exp_004 (0.0834 vs 0.0623) - **VALID** - exp_014 used Spange only, exp_004 used COMBINED features
  2. Missing MLP component - **AGREE** - MLP may capture non-linear patterns
  3. Limited Optuna trials (50) - **VALID** - may need more trials
- How I'm addressing: Next experiment should use COMBINED features (like exp_004) + MLP + Optuna for ensemble weights

## CRITICAL INSIGHT FROM ANALYSIS
**WHY exp_014 (Optuna) WAS WORSE THAN exp_004:**
1. exp_014 used Spange-only features, exp_004 used COMBINED features (0.8*ACS_PCA + 0.2*Spange)
2. Optuna found SHALLOW models (depth 3-6) which underfit
3. GroupKFold CV during Optuna may not correlate with LOO CV

**SOLUTION:**
1. Use COMBINED features (0.8*ACS_PCA + 0.2*Spange) like exp_004
2. Use DEEPER models (depth=None or 10-20)
3. Add MLP component for non-linear patterns
4. Optuna for ensemble WEIGHTS (not just hyperparameters)

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop14_analysis.ipynb` for CV-LB gap analysis
- Key patterns:
  1. exp_004 used COMBINED features (0.8*ACS_PCA + 0.2*Spange) - THIS IS CRITICAL
  2. exp_014 used Spange only - THIS IS WHY IT WAS WORSE
  3. More regularization made LB WORSE (0.0956 â†’ 0.0991) - NOT traditional overfitting
  4. Per-target models (HGB for SM, ETR for Products) achieved best CV (0.0623)
  5. TTA hurts performance - confirmed in exp_005

## CRITICAL CONSTRAINT
**ðŸš¨ DO NOT CHANGE THE VALIDATION STRATEGY IN TEMPLATE CELLS ðŸš¨**
- The template expects LOO splits with specific fold counts
- Task 0: 24 folds (one per solvent)
- Task 1: 13 folds (one per solvent ramp)
- Changing to GroupKFold breaks the submission format

## Recommended Approaches (Priority Order)

### 1. HIGHEST PRIORITY: Per-Target + MLP Hybrid with COMBINED Features
**Why**: exp_004 achieved best CV (0.0623) with COMBINED features. Adding MLP may capture additional patterns.
**Implementation**:
```python
# Use COMBINED features like exp_004
SPANGE_DF = load_features('spange_descriptors')
ACS_PCA_DF = load_features('acs_pca_descriptors')

# Feature weighting: 0.8 acs_pca + 0.2 spange (from exp_004)
spange_weight = 0.2
acs_weight = 0.8

# Per-target models (from exp_004)
# HGB for SM (target 2) - captures gradient patterns
# ETR for Products (targets 0, 1) - robust to outliers

# ADD MLP component
# MLP: [128, 64, 32] with BatchNorm, ReLU, Dropout(0.2), Sigmoid output
# Ensemble: weighted average of MLP + HGB + ETR
```

### 2. Optuna for Ensemble Weights (NOT just hyperparameters)
**Why**: Fixed weights are suboptimal. Optuna can find optimal weights.
**Implementation**:
```python
def objective(trial):
    # Ensemble weights to optimize
    mlp_weight = trial.suggest_float('mlp_weight', 0.1, 0.5)
    hgb_weight = trial.suggest_float('hgb_weight', 0.1, 0.5)
    etr_weight = trial.suggest_float('etr_weight', 0.1, 0.5)
    
    # Normalize weights
    total = mlp_weight + hgb_weight + etr_weight
    weights = [mlp_weight/total, hgb_weight/total, etr_weight/total]
    
    # Use GroupKFold for internal CV (faster)
    gkf = GroupKFold(n_splits=5)
    errors = []
    for train_idx, val_idx in gkf.split(X, y, groups=solvent_ids):
        # Train MLP, HGB, ETR
        # Ensemble predictions with weights
        # Calculate MAE
        errors.append(mae)
    return np.mean(errors)

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)  # More trials than exp_014
```

### 3. Feature Engineering for Solvent Generalization
**Why**: The problem is NOT overfitting - we need features that generalize to unseen solvents
**Implementation**:
- Use COMBINED features (0.8*ACS_PCA + 0.2*Spange) - PROVEN to work in exp_004
- Add Arrhenius kinetics features: 1/T, ln(t), t*T interaction
- Consider: polarity ratios, hydrogen bonding capacity

### 4. Deeper Models (NOT shallow like exp_014)
**Why**: exp_014 found shallow models (depth 3-6) which may underfit
**Implementation**:
- HGB: depth=None (default, unlimited) or depth=10-15
- ETR: depth=None (default, unlimited) or depth=15-20
- MLP: [128, 64, 32] or deeper [256, 128, 64, 32]

## What NOT to Try
- âŒ GroupKFold in template cells (breaks submission format)
- âŒ TTA (hurts performance - confirmed in exp_005)
- âŒ Simple Ridge regression (CV 0.0896, worse than PerTarget)
- âŒ More regularization (made LB worse in exp_006)
- âŒ Spange-only features (exp_014 showed this is worse than COMBINED)
- âŒ Shallow models (exp_014 found depth 3-6 underfit)

## Validation Notes
- Use LOO for submission (REQUIRED by evaluation metric)
- Use GroupKFold ONLY for internal hyperparameter tuning (OPTIONAL)
- The CV-LB gap is ~50% with LOO - this is expected for unseen solvents
- Don't chase LOO CV - focus on approaches that generalize

## Submission Strategy
With 3 submissions remaining:
1. **DO NOT submit exp_014** - CV is worse than exp_004, unlikely to improve LB
2. **Implement Per-Target + MLP Hybrid with COMBINED features**
3. **Submit if CV improves significantly (>5%) over exp_004 (0.0623)**
4. **Keep 1-2 submissions for final iterations**

## Gap Analysis
- Target: 0.01727
- Best LB: 0.0956
- Gap: 5.5x (HUGE)

This gap suggests we need fundamentally different approaches. The target IS reachable - we just need to find the right combination of:
1. COMBINED features (0.8*ACS_PCA + 0.2*Spange) - PROVEN in exp_004
2. Per-target models (HGB for SM, ETR for Products) - PROVEN in exp_004
3. MLP component for non-linear patterns - NOT YET TRIED with per-target
4. Optuna for ensemble WEIGHTS - NOT YET TRIED