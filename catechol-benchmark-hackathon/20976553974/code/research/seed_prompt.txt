## Current Status
- Best CV score: 0.0623 from exp_004/017/019
- Best LB score: 0.0956 from exp_004/016
- CV-LB gap: +53% ‚Üí Test set has fundamentally different solvents
- Target: 0.01727 (5.5x away from best LB)
- Submissions remaining: 2

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. Agreed - exp_019 is correctly implemented.
- Evaluator's top priority: DO NOT SUBMIT exp_019, implement GNN instead. AGREED.
- Key concerns raised: 
  1. exp_019 has same CV as exp_004/017, likely same LB
  2. Tree-based + MLP has hit ceiling (~0.095 LB)
  3. GNN is the only approach with demonstrated target-level performance
- How I'm addressing: Recommending GNN implementation as next experiment

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop19_analysis.ipynb` for strategic analysis
- Key patterns:
  1. CV-LB gap is CONSISTENT (~50%) across all models - test set has unseen solvents
  2. Public kernels achieve similar LB (~0.09-0.10) with tree-based/MLP approaches
  3. Paper arxiv:2512.19530 achieved MSE 0.0039 using GNN (25x better than our LB)
  4. RDKit and PyTorch Geometric are now available for GNN implementation

## Recommended Approaches

### PRIORITY 1: Graph Neural Network (GNN) for Molecular Property Prediction
**Why:** Only approach with demonstrated target-level performance. The paper shows GNN can achieve MSE 0.0039.

**Implementation Strategy:**
1. Use RDKit to convert solvent SMILES to molecular graphs
2. Use PyTorch Geometric for GNN implementation
3. Architecture: Message Passing Neural Network (MPNN) or Graph Attention Network (GAT)
4. Encode solvent as molecular graph with atom/bond features
5. Combine with process conditions (Temperature, Residence Time, SolventB%)

**Key Features:**
- Atom features: atom type, formal charge, degree, hybridization, aromaticity
- Bond features: bond type, stereochemistry, ring membership
- Global features: Temperature, Residence Time, SolventB%

**Template Compliance:**
- Define GNNModel class that inherits from BaseModel
- train_model() takes X_train, y_train as pandas DataFrames
- predict() returns torch tensor of shape [N, 3]
- Only change model definition line in last 3 cells

### PRIORITY 2: Enhanced MLP with Better Training (Fallback)
**Why:** 'System Malfunction' kernel uses 300 epochs + LR scheduling, may improve generalization.

**Changes from exp_019:**
- 300 epochs instead of 200
- ReduceLROnPlateau scheduler (factor=0.5, patience=20)
- Gradient clipping (1.0)
- MSE loss instead of MAE

### PRIORITY 3: Ensemble of GNN + Tree-based Models
**Why:** Diversity may help generalization.

**Architecture:**
- GNN for molecular structure understanding
- HGB/ETR for process conditions
- Weighted ensemble: 0.5 * GNN + 0.5 * tree-based

## What NOT to Try
- ‚ùå exp_019 submission - same CV as exp_004, likely same LB
- ‚ùå More tree-based hyperparameter tuning - ceiling reached
- ‚ùå DRFP features - exp_018 showed they hurt performance
- ‚ùå GroupKFold validation - changes fold structure, causes submission errors

## Validation Notes
- Use LOO validation (24 folds for single, 13 for full) - required by template
- CV-LB gap is ~50%, so CV 0.06 ‚Üí LB ~0.09
- To reach target (0.01727), need CV ~0.011 (assuming 50% gap)
- GNN may have smaller CV-LB gap due to better molecular understanding

## Critical Implementation Notes

**üö® TEMPLATE COMPLIANCE (MANDATORY):**
```python
# Last 3 cells MUST remain exactly as template
# Only change: model = GNNModel(data='single')  # or 'full'
```

**GNN Implementation Skeleton:**
```python
from rdkit import Chem
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GCNConv, global_mean_pool
import torch.nn as nn

class GNNModel(BaseModel):
    def __init__(self, data='single'):
        self.data_type = data
        self.mixed = (data == 'full')
        # Define GNN layers
        
    def _smiles_to_graph(self, smiles):
        # Convert SMILES to PyG Data object
        mol = Chem.MolFromSmiles(smiles)
        # Extract atom features, bond features
        # Return Data(x=atom_features, edge_index=bonds, edge_attr=bond_features)
        
    def train_model(self, X_train, y_train):
        # Convert solvents to graphs
        # Train GNN on graphs + process conditions
        
    def predict(self, X):
        # Convert solvents to graphs
        # Return predictions as torch tensor [N, 3]
```

**SMILES Lookup Required:**
The data contains solvent NAMES, not SMILES. You need to:
1. Check if there's a SMILES lookup table in the data
2. Or create a mapping from solvent names to SMILES
3. Use RDKit to convert SMILES to molecular graphs

**Check for SMILES data:**
```bash
ls /home/data/*.csv
head /home/data/spange_descriptors_lookup.csv
```

## Submission Strategy
- DO NOT submit exp_019 - save submissions for GNN
- If GNN achieves CV < 0.05, submit immediately
- If GNN fails, try enhanced MLP as fallback
- Save last submission for best performing approach