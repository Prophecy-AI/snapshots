## Current Status
- Best CV score: 0.0623 from exp_004 (PerTarget HGB+ETR with LOO)
- Best LB score: 0.0956 from exp_004
- CV-LB gap: +53% ‚Üí LOO CV is optimistic, but we MUST use LOO for submission
- Target: 0.01727 (5.5x lower than best LB)

## Response to Evaluator
- Technical verdict was TRUSTWORTHY for exp_012
- **CRITICAL ISSUE DISCOVERED**: exp_012 submission FAILED because GroupKFold changed the fold structure
- The evaluation metric expects LOO fold structure (24 folds for task 0, 13 folds for task 1)
- GroupKFold with 5 folds causes "Evaluation metric raised an unexpected error"
- **SOLUTION**: We MUST use LOO for submission, even if CV is optimistic

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop12_analysis.ipynb` for fold structure analysis
- Key insight: The competition evaluation metric is tied to the LOO fold structure
- The top kernel (lishellliang) uses GroupKFold but may have same submission issue

## CRITICAL CONSTRAINT
**üö® DO NOT CHANGE THE VALIDATION STRATEGY IN TEMPLATE CELLS üö®**
- The template expects LOO splits with specific fold counts
- Task 0: 24 folds (one per solvent)
- Task 1: 13 folds (one per solvent ramp)
- Changing to GroupKFold breaks the submission format
- If you want to use GroupKFold for internal CV estimation, do it SEPARATELY

## Recommended Approaches (Priority Order)

### 1. IMMEDIATE: Fix exp_012 to use LOO
- Revert to LOO validation in template cells
- Keep the MLP + GBDT ensemble architecture
- This should give similar LB to exp_004 (~0.0956)

### 2. Optuna Hyperparameter Optimization (HIGH PRIORITY)
- The top kernel uses Optuna to optimize hyperparameters
- Optimize: lr (1e-4 to 1e-2), dropout (0.1-0.5), hidden_dims, xgb_depth (3-8), rf_depth (5-15), lgb_leaves (15-63), weights
- Use a SEPARATE GroupKFold validation for Optuna (not in template cells)
- Then use the best hyperparameters with LOO for final submission

### 3. Per-Target Models with Better Features
- exp_004/005 showed per-target works well (HGB for SM, ETR for Products)
- Try combining with Optuna tuning
- Try different feature combinations (DRFP + Spange, ACS_PCA + Spange)

### 4. Feature Engineering for Unseen Solvents
- The test set contains completely new solvents
- Focus on features that generalize (chemical properties, not solvent-specific patterns)
- Consider: polarity, dielectric constant, hydrogen bonding capacity

## What NOT to Try
- ‚ùå GroupKFold in template cells (breaks submission format)
- ‚ùå TTA (hurts performance - confirmed in exp_005)
- ‚ùå Simple Ridge regression (CV 0.0896, worse than PerTarget)

## Validation Notes
- Use LOO for submission (REQUIRED by evaluation metric)
- Use GroupKFold ONLY for internal hyperparameter tuning (OPTIONAL)
- The CV-LB gap is ~50% with LOO - this is expected for unseen solvents
- Don't chase LOO CV - focus on approaches that generalize

## Submission Strategy
- We have 3 submissions remaining
- Next submission should be a fixed version of exp_012 (LOO instead of GroupKFold)
- Or try Optuna-optimized model with LOO

## Key Insight
The target (0.01727) is 5.5x lower than our best LB (0.0956). This is a HUGE gap.
We need fundamentally better approaches, not just hyperparameter tuning.
Consider:
1. Better feature engineering for unseen solvents
2. Domain-specific features (chemical properties)
3. Different model architectures (transformers, GNNs)
4. Transfer learning from larger chemical datasets