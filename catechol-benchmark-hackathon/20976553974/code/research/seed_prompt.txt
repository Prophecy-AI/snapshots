# Catechol Reaction Yield Prediction - Evolved Seed Prompt (Loop 5)

## Current Status
- Best CV score: 0.0623 from exp_004 (005_no_tta_per_target)
- Best LB score: 0.0956 (from submission of exp_004)
- **CV-LB gap: -0.0333 (53% worse on LB!)** → MASSIVE OVERFITTING
- Target: 0.01727 (5.5x better than our LB)
- Submissions: 1/5 used, 4 remaining

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - execution is sound, but the CV-LB gap reveals our validation is too optimistic.

**Evaluator's top priority**: Submit to verify CV-LB correlation, then focus on closing the 3.6x gap to target.

**We submitted and got LB 0.0956** - this confirms:
1. Our CV (0.0623) is NOT representative of test performance
2. The test set has MORE challenging solvents than our leave-one-out captures
3. We need fundamentally different approaches, not incremental improvements

**Key concerns raised by evaluator**:
1. Gap to target remains large (3.6x) → NOW 5.5x based on LB!
2. Hardest solvents still problematic → Confirmed: HFIP (0.145 MAE), Ethylene Glycol (0.122)
3. Model complexity vs data size → Need stronger regularization
4. Feature engineering ceiling → Need higher-dimensional features

## CRITICAL INSIGHT: CV-LB Gap Analysis

The 53% CV-LB gap indicates MASSIVE OVERFITTING. Our leave-one-solvent-out CV is too optimistic because:
- Training on 23/24 solvents still captures most chemical space
- Models learn solvent-specific patterns that don't generalize
- Test set likely has chemically unique solvents not represented in training

## Data Understanding

**Reference notebooks:**
- `exploration/evolver_loop5_lb_feedback.ipynb` - CV-LB gap analysis, per-solvent errors
- `exploration/evolver_loop4_analysis.ipynb` - TTA hurts performance discovery
- `exploration/evolver_loop2_analysis.ipynb` - Model comparison, hardest solvents

**Key patterns:**
1. **Hardest solvents**: HFIP (0.145 MAE), Ethylene Glycol (0.122), Water.Acetonitrile (0.112)
2. **These are chemically unique** with high Spange distances from centroid
3. **Temperature dominates**: Strong correlation with SM (-0.82), P2 (0.72)

## Recommended Approaches (Priority Order)

### 1. STRONGER REGULARIZATION (HIGHEST PRIORITY)
The 53% CV-LB gap screams overfitting. Try:
```python
# Simple Ridge Regression - may generalize better
from sklearn.linear_model import Ridge
model = Ridge(alpha=10.0)  # Strong regularization

# Or regularized trees
ExtraTreesRegressor(
    n_estimators=100,  # Fewer trees
    max_depth=5,       # Shallower (was 10)
    min_samples_leaf=5, # More samples per leaf (was 2)
    min_samples_split=10
)
```

### 2. HIGHER-DIMENSIONAL FEATURES (HIGH PRIORITY)
Current Spange (13-dim) + ACS_PCA (5-dim) may not capture enough chemistry:
```python
# Load DRFP (2048-dim) or fragprints (2133-dim)
drfp = pd.read_csv(f'{DATA_PATH}/drfp_lookup.csv', index_col=0)
# Apply PCA to reduce to 50-100 dimensions
from sklearn.decomposition import PCA
pca = PCA(n_components=50)
drfp_reduced = pca.fit_transform(drfp.values)
```
These may capture chemical similarity better for truly unseen solvents.

### 3. ENSEMBLE OF DIVERSE MODELS (MEDIUM PRIORITY)
Combine fundamentally different approaches:
- Ridge regression (linear baseline)
- Random Forest with strong regularization
- Gradient Boosting with low learning rate
- Average predictions to reduce variance

### 4. SIMPLER BASELINE FIRST
Before complex models, establish what a simple linear model achieves:
```python
# Ridge regression baseline
from sklearn.linear_model import Ridge
from sklearn.multioutput import MultiOutputRegressor

class SimpleRidgeModel(BaseModel):
    def __init__(self, data='single'):
        self.model = MultiOutputRegressor(Ridge(alpha=10.0))
        # ... feature engineering
```

## What NOT to Try

1. **TTA** - Confirmed to hurt performance (30% worse)
2. **Complex neural networks** - Dataset too small
3. **Deeper trees** - Will increase overfitting
4. **More estimators** - Diminishing returns, more overfitting

## Template Compliance (MANDATORY)

- Last 3 cells MUST remain unchanged from template
- ONLY change: `model = YourModel()` line
- Same hyperparameters across ALL folds
- Model class needs: `__init__`, `train_model(X_train, y_train)`, `predict(X_test)` → torch tensor [N, 3]

## Next Experiment: 006_regularized_baseline

**Goal**: Establish a strongly regularized baseline that may generalize better to unseen solvents.

**Approach**:
1. Use Ridge regression with alpha=10.0 (strong regularization)
2. Combined features: Spange + ACS_PCA + Arrhenius kinetics
3. Per-target models (separate for SM vs Products)
4. NO TTA

**Expected outcome**: 
- CV may be worse than current 0.0623
- But LB may be BETTER due to less overfitting
- This establishes a baseline for comparison

**If Ridge works well**, try:
- DRFP features with PCA reduction
- Ensemble of Ridge + regularized trees

## Key Insight

The target (0.01727) is 5.5x better than our LB (0.0956). This is a HUGE gap. We need to:
1. Stop optimizing CV (it's misleading)
2. Focus on approaches that generalize to unseen solvents
3. Use simpler models with stronger regularization
4. Try higher-dimensional features that capture chemistry better

**DO NOT GIVE UP** - The target IS achievable. The solution likely involves:
- Fundamentally different feature engineering
- Much stronger regularization
- Ensemble of diverse simple models