# Catechol Reaction Yield Prediction - Seed Prompt (Loop 29)

## Current Status
- **Best CV score**: 0.0623 from exp_004/exp_005/exp_028 (HGB+ETR per-target, NO TTA)
- **Best LB score**: 0.0956 from exp_004/exp_016
- **CV-LB gap**: +53% (LB = 0.986*CV + 0.0333, R²=0.988)
- **Target**: 0.01727 (5.5x better than best LB)
- **Submissions**: 0 remaining today (RESET AT 00:00 UTC - DO NOT SUBMIT)

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** exp_028 successfully replicated exp_005's CV 0.0623.

**Evaluator's top priority**: Implement a proper GNN approach since tree-based models have plateaued.

**My synthesis**: 
- The evaluator is CORRECT that tree-based models have plateaued
- Analysis shows CV-LB relationship predicts we need NEGATIVE CV to reach target - IMPOSSIBLE
- This confirms we need a FUNDAMENTALLY DIFFERENT approach
- The target IS achievable (someone did it), so there must be an approach with a DIFFERENT CV-LB relationship

**Key insight from analysis**: 
- Strong correlations between Spange physics features and targets (ET(30) ~0.7 with products, -0.84 with SM)
- These physics-based features capture generalizable chemical principles
- Models that leverage these principles should generalize better to unseen solvents

## Data Understanding

**Reference notebooks:**
- `exploration/evolver_loop28_analysis.ipynb` - CV-LB relationship analysis, feature correlations
- `exploration/evolver_loop27_lb_feedback.ipynb` - Previous CV-LB analysis
- `exploration/eda.ipynb` - Initial EDA

**Key patterns:**
1. **CV-LB correlation is 0.988** - Lower CV reliably predicts lower LB
2. **BUT**: The intercept (0.0333) means even CV=0 gives LB=0.0333 > target
3. **Test set has COMPLETELY NEW solvents** - Models memorize solvent-specific patterns
4. **Best features**: ET(30), alpha, SA from Spange descriptors correlate strongly with targets
5. **Per-target architecture works best**: HGB for SM, ETR for Products

**The fundamental problem:**
- Our models learn solvent-specific patterns that don't generalize
- The test set has solvents NEVER seen in training
- We need models that learn GENERALIZABLE chemical principles
- The winning approach must have a DIFFERENT CV-LB relationship (lower intercept)

## Recommended Approaches

### Priority 1: Improved GNN with Pretrained Embeddings
**Rationale**: exp_020 GNN achieved CV 0.099 (worse than trees), but the implementation was basic. Research shows pretrained GNNs achieve best results in low-data regimes.

**Key improvements over exp_020**:
1. Use Graph Attention Networks (GAT) instead of GCN - attention helps focus on important atoms
2. Add edge features (bond type, conjugation) - exp_020 only had node features
3. Use pretrained molecular embeddings if available
4. Longer training with proper learning rate schedule (cosine annealing)
5. Multi-task learning across all 3 targets

**Implementation guidance**:
```python
# Key architecture improvements
class ImprovedGNN(nn.Module):
    def __init__(self):
        # Use GAT instead of GCN
        self.conv1 = GATConv(node_features, hidden_dim, heads=4)
        self.conv2 = GATConv(hidden_dim*4, hidden_dim, heads=4)
        self.conv3 = GATConv(hidden_dim*4, hidden_dim, heads=1)
        
        # Add edge features
        self.edge_encoder = nn.Linear(edge_features, hidden_dim)
        
        # Multi-task heads with shared representation
        self.shared = nn.Linear(hidden_dim + process_dim, 128)
        self.head_sm = nn.Linear(128, 1)
        self.head_p2 = nn.Linear(128, 1)
        self.head_p3 = nn.Linear(128, 1)
```

### Priority 2: Physics-Informed Neural Network
**Rationale**: The Spange features (ET(30), alpha, etc.) have strong correlations with targets. A model that explicitly uses these physics-based features should generalize better.

**Key idea**: Instead of learning arbitrary patterns, constrain the model to learn physically meaningful relationships.

**Implementation**:
1. Use Spange features as primary inputs (not just concatenated)
2. Add physics-based interaction terms (ET(30) × Temperature, alpha × Residence Time)
3. Use monotonicity constraints where appropriate (higher temperature → more products)
4. Regularize to prefer simpler, more generalizable relationships

### Priority 3: Ensemble with Diversity
**Rationale**: If different model types have different CV-LB relationships, ensembling might help.

**Key idea**: Combine tree-based (exp_005) with neural network approaches.

**Implementation**:
1. Keep best tree model (HGB+ETR per-target)
2. Add improved GNN
3. Add physics-informed NN
4. Weight by validation performance on held-out solvents

### Priority 4: Domain Adaptation / Adversarial Training
**Rationale**: The CV-LB gap suggests distribution shift. Adversarial training can learn features that are invariant to solvent identity.

**Key idea**: Train a discriminator to predict which solvent the data came from, and train the main model to fool the discriminator while predicting yields.

## What NOT to Try

1. **More tree-based variations** - 28 experiments show this approach plateaus at CV ~0.062
2. **Simple MLP without pretrained embeddings** - exp_010/019 showed this underperforms trees
3. **GroupKFold validation** - Breaks submission structure (exp_011/012 failed)
4. **Data augmentation (flip)** - Proven to hurt full data performance
5. **Stronger regularization alone** - exp_021 showed this HURTS performance

## Validation Notes

**CV scheme**: LOO validation (24 folds single, 13 folds full) - MUST use this for submission
**CV-LB calibration**: LB ≈ CV + 0.033 (53% relative gap)
**Target CV needed**: To beat target LB 0.01727, need approach with DIFFERENT CV-LB relationship

## CRITICAL REMINDERS

1. **NO SUBMISSIONS TODAY** - 0 remaining, reset at 00:00 UTC
2. **Template compliance**: Last 3 cells MUST match template exactly
3. **Focus on approaches that might have DIFFERENT CV-LB relationship**
4. **The target IS achievable** - someone did it, we need to find how

## MANDATORY SUBMISSION STRUCTURE (from competition rules):
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- The line `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same

## Experiment Naming
Next experiment: `exp_029_improved_gnn`

## Key Questions to Answer
1. Can an improved GNN achieve better CV than trees (< 0.062)?
2. Does the GNN have a different CV-LB relationship (lower intercept)?
3. Can physics-informed features improve generalization?