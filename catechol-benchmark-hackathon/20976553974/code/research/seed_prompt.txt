# Catechol Reaction Yield Prediction - Seed Prompt (Loop 8)

## Current Status
- **Best CV score**: 0.0623 from exp_004 (PerTarget HGB+ETR NO TTA)
- **Best LB score**: 0.0956 from exp_004
- **CV-LB gap**: +53% (0.0333 absolute) → LARGE gap indicates test set has chemically unique solvents
- **Target**: 0.01727 (5.5x better than best LB)
- **Submissions**: 2/5 used, 3 remaining

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator correctly verified the GP experiment.

**Evaluator's top priority**: Submit exp_006 to test regularization hypothesis.
- **DONE**: We submitted exp_006 and got LB 0.0991 (WORSE than exp_004's 0.0956)
- **CRITICAL INSIGHT**: More regularization made LB WORSE! This means:
  1. The problem is NOT traditional overfitting
  2. We need BETTER features that generalize to new solvents
  3. Simpler models lose signal without reducing the gap

**Key concerns raised by evaluator**:
1. GP underperforms on CV - CONFIRMED (0.0721 vs 0.0623)
2. Kernel choice for GP - Valid concern, but GP alone won't beat target
3. Gap to target remains enormous - TRUE, need fundamentally different approach

**My synthesis**: The regularization experiment was valuable - it DISPROVED the overfitting hypothesis. We now know we need BETTER features and model diversity, not simpler models.

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Basic data characteristics
- `exploration/evolver_loop5_lb_feedback.ipynb` - CV-LB gap analysis
- `exploration/evolver_loop6_analysis.ipynb` - GroupKFold vs LOO, feature analysis
- `exploration/evolver_loop7_analysis.ipynb` - GP analysis, hardest solvents
- `exploration/evolver_loop8_lb_feedback.ipynb` - Regularization disproved overfitting hypothesis

**Key patterns to exploit:**
1. **Per-target models work** - HGB for SM, ETR for Products (exp_004 best CV)
2. **TTA hurts performance** - Removing TTA improved full data MAE by 33%
3. **Combined features best** - DRFP-PCA(15) + Spange + ACS_PCA
4. **Hardest solvents**: HFIP, Cyclohexane, Water - chemically unique

## Recommended Approaches

### Priority 1: ENSEMBLE OF DIVERSE MODEL FAMILIES (HIGH IMPACT)
**Why**: Top kernels (lishellliang) use MLP + XGBoost + RF + LightGBM ensemble with learned weights. We haven't tried combining fundamentally different model families.

**Implementation**:
```python
# Combine predictions from diverse models
class DiverseEnsemble(BaseModel):
    def __init__(self, data='single'):
        self.models = [
            PerTargetModel(data),  # Our best CV model (HGB+ETR)
            RandomForestModel(data),  # Different model family
            XGBModel(data),        # Gradient boosting
            LGBModel(data),        # Another gradient boosting variant
        ]
        self.weights = [0.4, 0.2, 0.2, 0.2]  # Tune these
    
    def predict(self, X):
        preds = [m.predict(X) for m in self.models]
        return sum(w * p for w, p in zip(self.weights, preds))
```

### Priority 2: STACKING/BLENDING (MEDIUM IMPACT)
**Why**: Use predictions from multiple models as features for a meta-learner. This can capture complementary patterns.

**Implementation**:
- Train base models on training data
- Generate OOF predictions
- Train meta-learner (Ridge/XGB) on stacked predictions
- Final prediction = meta-learner(base_predictions)

### Priority 3: FEATURE ENGINEERING FOR GENERALIZATION (MEDIUM IMPACT)
**Why**: The test set has chemically unique solvents. We need features that capture chemical similarity better.

**Ideas**:
- Interaction features: temp * solvent_polarity, time * solvent_viscosity
- Arrhenius kinetics: 1/T, ln(t), exp(-Ea/RT)
- Chemical distance features: distance to nearest training solvent

## What NOT to Try

1. **More regularization** - DISPROVED by exp_006 (made LB worse)
2. **Simpler models** - Ridge (exp_005) had worst CV (0.0896)
3. **GP alone** - CV worse than tree-based (0.0721 vs 0.0623)
4. **TTA** - Hurts performance on mixed solvents

## Validation Notes

- **CV scheme**: Leave-one-solvent-out for single, leave-one-ramp-out for full
- **CV-LB gap**: ~50% (0.0623 → 0.0956) - test set has unique solvents
- **Calibration**: CV improvements may not translate to LB improvements
- **Strategy**: Focus on model diversity and feature generalization, not CV optimization

## Competition-Specific Constraints (MUST FOLLOW)

**MANDATORY SUBMISSION STRUCTURE:**
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- The line `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same

**Template compliance example:**
```python
# Cell -3 (third from last)
model = YourModel(data='single')  # ONLY THIS LINE CAN CHANGE

# Cell -2 (second from last)
model = YourModel(data='full')    # ONLY THIS LINE CAN CHANGE

# Cell -1 (last cell) - DO NOT MODIFY
submission = pd.concat([submission_single_solvent, submission_full_data])
submission = submission.reset_index()
submission.index.name = "id"
submission.to_csv("submission.csv", index=True)
```

## Next Experiment Recommendation

**Experiment 009: Diverse Ensemble**
- Combine PerTarget (best CV) + RandomForest + XGBoost + LightGBM
- Use weighted averaging with tuned weights (start with [0.4, 0.2, 0.2, 0.2])
- Use combined features (Spange + Arrhenius kinetics)
- Focus on model diversity, not individual model optimization
- Expected CV: ~0.065 (may be slightly worse than best)
- Expected LB: Potentially better due to diversity

**Rationale**: The regularization experiment showed that simpler models don't help. We need model diversity to capture different patterns. Top kernels use this approach successfully.

**Alternative Experiment 010: Stacking with Meta-Learner**
If ensemble doesn't improve LB, try stacking:
- Base models: ETR, HGB, RF, XGB
- Meta-learner: Ridge or simple average
- This can capture complementary patterns from different models