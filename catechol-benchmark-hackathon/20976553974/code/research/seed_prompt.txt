# Catechol Reaction Yield Prediction - Seed Prompt (Loop 28)

## Current Status
- **Best CV score**: 0.0623 from exp_004/exp_005 (HGB+ETR per-target, NO TTA)
- **Best LB score**: 0.0956 from exp_004/exp_016
- **CV-LB gap**: +53% (LB = 0.986*CV + 0.0333, R²=0.988)
- **Target**: 0.01727 (5.5x better than best LB)
- **Submissions**: 0 remaining today (RESET AT 00:00 UTC - DO NOT SUBMIT)

## Response to Evaluator

**Technical verdict was CONCERNS.** The evaluator correctly identified:
1. **Data augmentation bug in exp_027**: The model had flip augmentation that wasn't in exp_005, causing CV 0.0810 instead of 0.0623. This is a valid finding.
2. **Template compliance issue**: Cell 10 exists after the "FINAL CELL" - must be removed before submission.

**Evaluator's top priority**: Fix the implementation bug to match exp_005's CV 0.0623.

**My synthesis**: 
- Fixing the bug would only get us to CV 0.0623 → predicted LB ~0.093
- This is still 5.4x from target (0.01727)
- The evaluator is correct that the bug should be fixed, BUT fixing it won't reach the target
- We need a FUNDAMENTALLY DIFFERENT approach that breaks the CV-LB linear relationship

**Key insight**: The CV-LB relationship (LB = 0.986*CV + 0.0333) predicts we need CV = -0.016 to reach target. This is IMPOSSIBLE. The target IS achievable (someone did it), so there must be an approach that has a DIFFERENT CV-LB relationship - one where the model generalizes better to unseen solvents.

## Data Understanding

**Reference notebooks:**
- `exploration/evolver_loop27_lb_feedback.ipynb` - CV-LB relationship analysis
- `exploration/evolver_loop24_analysis.ipynb` - Feature importance analysis
- `exploration/evolver_loop21_analysis.ipynb` - CV-LB gap analysis across models

**Key patterns:**
1. **CV-LB correlation is 0.988** - Lower CV reliably predicts lower LB
2. **All models have ~50% CV-LB gap** - This is data-specific, not model-specific
3. **Test set has COMPLETELY NEW solvents** - Models memorize solvent-specific patterns
4. **Best features**: 0.8*ACS_PCA + 0.2*Spange with Arrhenius kinetics
5. **Per-target architecture works best**: HGB for SM, ETR for Products

**The fundamental problem:**
- Our models learn solvent-specific patterns that don't generalize
- The test set has solvents NEVER seen in training
- We need models that learn GENERALIZABLE chemical principles

## Recommended Approaches

### Priority 1: Graph Neural Network with Pretrained Embeddings
**Rationale**: Research shows GNNs with transfer learning achieve best results in ultra-low data regimes (20-30 samples). The key is using pretrained molecular embeddings.

**Implementation**:
```python
import torch
import torch_geometric
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool

class MolecularGNN(torch.nn.Module):
    def __init__(self, node_features, hidden_dim=64, num_layers=3):
        super().__init__()
        self.convs = torch.nn.ModuleList()
        self.convs.append(GATConv(node_features, hidden_dim))
        for _ in range(num_layers - 1):
            self.convs.append(GATConv(hidden_dim, hidden_dim))
        self.fc = torch.nn.Linear(hidden_dim + process_features, 3)
    
    def forward(self, x, edge_index, batch, process_features):
        for conv in self.convs:
            x = conv(x, edge_index)
            x = F.relu(x)
        x = global_mean_pool(x, batch)
        x = torch.cat([x, process_features], dim=1)
        return torch.sigmoid(self.fc(x))
```

**Key considerations**:
- Use RDKit to convert solvent SMILES to molecular graphs
- Node features: atom type, hybridization, aromaticity, etc.
- Edge features: bond type, conjugation, ring membership
- Combine graph embedding with process features (RT, Temp)

### Priority 2: Multi-Task Learning with Shared Representation
**Rationale**: The three targets (SM, Product 2, Product 3) are chemically related. Learning a shared representation could improve generalization.

**Implementation**:
- Single encoder for all targets
- Separate prediction heads per target
- Joint training with task-specific loss weighting

### Priority 3: Physics-Informed Features
**Rationale**: The Arrhenius kinetics features helped. More physics-based features could improve generalization.

**New features to try**:
- Activation energy proxy: exp(-Ea/RT) where Ea is estimated from solvent properties
- Solvent polarity × temperature interaction
- Residence time × solvent viscosity interaction
- Hansen solubility parameters

### Priority 4: Ensemble of Diverse Architectures
**Rationale**: If GNN has different CV-LB relationship than tree models, ensembling could help.

**Implementation**:
- Train GNN model (new approach)
- Keep best tree model (exp_005)
- Weighted average based on validation performance

## What NOT to Try

1. **More tree-based variations** - 27 experiments show this approach plateaus at CV ~0.062
2. **Stronger regularization** - exp_021 showed this HURTS performance
3. **GroupKFold validation** - Breaks submission structure (exp_011/012 failed)
4. **Data augmentation (flip)** - Proven to hurt full data performance
5. **Simple MLP without pretrained embeddings** - exp_010/019 showed this underperforms trees

## Validation Notes

**CV scheme**: LOO validation (24 folds single, 13 folds full) - MUST use this for submission
**CV-LB calibration**: LB ≈ CV + 0.033 (53% relative gap)
**Target CV needed**: To beat target LB 0.01727, need approach with DIFFERENT CV-LB relationship

## CRITICAL REMINDERS

1. **NO SUBMISSIONS TODAY** - 0 remaining, reset at 00:00 UTC
2. **Template compliance**: Last 3 cells MUST match template exactly
3. **Focus on approaches that might have DIFFERENT CV-LB relationship**
4. **The target IS achievable** - someone did it, we need to find how

## MANDATORY SUBMISSION STRUCTURE (from competition rules):
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- The line `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same

## Experiment Naming
Next experiment: `exp_028_gnn_pretrained`