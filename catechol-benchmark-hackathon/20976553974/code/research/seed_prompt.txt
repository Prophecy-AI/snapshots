## Current Status
- Best CV score: 0.0623 from exp_004/016/017/018 (HGB+ETR per-target with Spange descriptors)
- Best LB score: 0.0956 from exp_004/016
- CV-LB gap: +53% ‚Üí Test set has fundamentally different solvents
- Target: 0.01727 (5.5x away from best LB)
- Submissions remaining: 2

## Response to Evaluator
- Technical verdict was CONCERNS due to template compliance (14 cells after "FINAL CELL"). AGREED.
- Evaluator's top priority: DO NOT SUBMIT exp_020, return to exp_004 architecture. AGREED.
- Key concerns raised:
  1. GNN CV 0.099 is 59% worse than best (0.0623) - CONFIRMED
  2. Basic GNN with 24 solvents has insufficient data - CONFIRMED
  3. Template compliance violated - CONFIRMED
- How I'm addressing: Recommending return to proven architecture with focus on generalization

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop20_analysis.ipynb` for strategic analysis
- Key patterns:
  1. CV-LB gap is CONSISTENT at ~53% across ALL models (exp_004, exp_006, exp_016)
  2. GNN failed because 24 solvents is insufficient for learning molecular patterns
  3. Research shows pre-trained GNNs + fine-tuning work, but basic GNN doesn't
  4. Best approach remains HGB+ETR per-target with Spange descriptors

## Critical Analysis: Why GNN Failed
1. **Insufficient data**: Only 24 unique solvents - GNN needs thousands to learn patterns
2. **No pre-training**: Paper's success was with pre-trained representations
3. **Mixed solvent handling**: Only used Solvent A's graph, ignored Solvent B
4. **Basic architecture**: 3-layer GCN is too simple for this task

## Recommended Approaches

### PRIORITY 1: Ensemble of Best Models for Better Generalization
**Why:** Diversity may reduce CV-LB gap. Different models capture different patterns.

**Implementation:**
```python
class EnsembleModel(BaseModel):
    def __init__(self, data='single'):
        self.models = [
            PerTargetModel(data),  # exp_004 architecture
            DiverseEnsembleModel(data),  # exp_008 architecture
            MLPGBDTEnsemble(data),  # exp_009 architecture
        ]
        self.weights = [0.5, 0.25, 0.25]  # Weight best model higher
    
    def train_model(self, X_train, y_train):
        for model in self.models:
            model.train_model(X_train, y_train)
    
    def predict(self, X):
        preds = [m.predict(X) for m in self.models]
        return sum(w * p for w, p in zip(self.weights, preds))
```

### PRIORITY 2: Stronger Regularization on Best Architecture
**Why:** exp_006 had smaller CV-LB gap (44% vs 53%) with more regularization.

**Changes from exp_004:**
- Reduce max_depth from 10 to 6-7
- Increase min_samples_leaf from 2 to 5
- Add L2 regularization to MLP components
- Use dropout 0.3-0.4 instead of 0.2

### PRIORITY 3: Feature Selection for Generalization
**Why:** Some features may overfit to training solvents.

**Approach:**
- Use only Spange descriptors (most generalizable)
- Remove ACS descriptors (may be solvent-specific)
- Add only physics-based features (Arrhenius kinetics)
- Avoid high-dimensional features (DRFP)

### PRIORITY 4: Uncertainty-Weighted Predictions
**Why:** Predictions with high uncertainty should be regularized toward mean.

**Implementation:**
```python
# Use GP or ensemble variance to estimate uncertainty
# Blend predictions toward mean for high-uncertainty samples
pred_final = alpha * pred_model + (1 - alpha) * mean_target
# where alpha = 1 / (1 + uncertainty)
```

## What NOT to Try
- ‚ùå exp_020 submission - CV 0.099 is much worse than baseline
- ‚ùå Basic GNN without pre-training - insufficient data
- ‚ùå More tree-based hyperparameter tuning - ceiling reached
- ‚ùå DRFP features - exp_017 showed they hurt performance
- ‚ùå GroupKFold validation - changes fold structure, causes submission errors
- ‚ùå Complex architectures - more parameters = more overfitting

## Validation Notes
- Use LOO validation (24 folds for single, 13 for full) - required by template
- CV-LB gap is ~50%, so CV 0.06 ‚Üí LB ~0.09
- To reach target (0.01727), need CV ~0.011 (assuming 50% gap)
- Focus on approaches that REDUCE the CV-LB gap, not just CV

## Template Compliance (CRITICAL)
```python
# Last 3 cells MUST remain exactly as template
# Only change: model = YourModel(data='single')  # or 'full'
# Notebook must have EXACTLY 11 cells (or fewer) with last 3 being template cells
# DO NOT add cells after the "FINAL CELL"
```

## Submission Strategy
- DO NOT submit exp_020 - it's worse than baseline
- If new experiment achieves CV < 0.055, submit immediately
- If CV is similar to 0.0623 but approach is fundamentally different, consider submitting
- Save last submission for best performing approach

## Key Insight from Research
For small molecular datasets (20-30 molecules), the best approaches are:
1. **Pre-trained GNN + fine-tuning** (not available without external data)
2. **SOAP + simple descriptors + gradient boosting** (hybrid approach)
3. **Task similarity estimation** to avoid negative transfer

Since we can't use pre-trained models, focus on:
- Robust descriptor-based learners (Spange descriptors work well)
- Strong regularization to prevent overfitting
- Ensemble diversity for better generalization

## The Path Forward
The 53% CV-LB gap is the fundamental problem. To close it:
1. Use simpler models with stronger regularization
2. Focus on generalizable features (physics-based, not solvent-specific)
3. Ensemble diverse models for robustness
4. Accept that CV will be higher but LB may be better

**Remember:** A model with CV 0.07 and 30% gap (LB 0.091) is BETTER than a model with CV 0.06 and 53% gap (LB 0.092)!

## üö® CRITICAL COMPETITION-SPECIFIC RULES üö®

**MANDATORY SUBMISSION STRUCTURE:**
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- The line `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same
- DO NOT add cells after the "FINAL CELL" - this violates template compliance

**DATA CONTAMINATION RULES:**
- Pre-training on solvent mixture data to predict full solvent data = NOT ALLOWED
- Hyper-parameters must be SAME across every fold (unless clear rationale exists)
- Test: "If I had to predict a NEW unseen solvent, would this method work without knowing results?"
- DO NOT use any features derived from solvent-specific statistics computed on full data