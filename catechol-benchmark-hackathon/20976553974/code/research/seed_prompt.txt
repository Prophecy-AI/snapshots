## Current Status
- Best CV score: 0.0623 from exp_004/017 (Per-target HGB+ETR with prediction combination)
- Best LB score: 0.0956 from exp_004/016
- CV-LB gap: 53% → Test set has fundamentally different solvents
- Target: 0.01727 (5.5x away from best LB)
- Remaining submissions: 2

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - exp_018's results can be trusted. The DRFP features genuinely hurt performance.

**Evaluator's top priority**: Don't submit exp_018 (worse than exp_004). Try Option A (optimize exp_004) or Option D (GNN).

**My response**: I AGREE with not submitting exp_018. However, I DISAGREE with the priority order:
- Option A (optimize exp_004) has LOW potential - we've already hit the tree-based ceiling
- Option D (GNN) has HIGH potential but is complex to implement
- **I recommend Option C (focus on generalization)** - the 53% CV-LB gap is the real problem

**Key concerns raised by evaluator**:
1. DRFP + tree models is a mismatch → CONFIRMED by exp_018
2. We're stuck at ~0.06 CV / ~0.095 LB ceiling → AGREE, need fundamentally different approach
3. Limited submissions (2) → CRITICAL constraint

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop18_analysis.ipynb` - CV-LB gap analysis
- `exploration/evolver_loop13_analysis.ipynb` - Top kernel insights
- `exploration/evolver_loop14_analysis.ipynb` - OOD prediction research

Key patterns:
1. **53% CV-LB gap is CONSISTENT** - exp_004 and exp_016 both had exactly the same gap
2. **Test set has UNSEEN solvents** - our models memorize training solvents, don't generalize
3. **Tree-based models have hit ceiling** - best CV is 0.0623, unlikely to improve much
4. **DRFP features don't help trees** - 97.43% sparse, needs different model family

## Recommended Approaches

### PRIORITY 1: MLP with Strong Regularization (HIGHEST POTENTIAL)
**Rationale**: 
- MLP can learn non-linear patterns that trees miss
- Strong regularization (dropout, weight decay) helps generalization
- Different model family may have smaller CV-LB gap
- exp_009 (MLP + GBDT) achieved CV 0.0669 - not far from best

**Implementation**:
```python
class RegularizedMLP(BaseModel):
    def __init__(self, data='single'):
        # Architecture: [input] -> [256] -> [128] -> [64] -> [3]
        # Strong regularization:
        # - Dropout: 0.3-0.5 between layers
        # - Weight decay: 1e-3 to 1e-2
        # - Early stopping with patience=20
        # - Batch normalization for stability
        
        # Features: Combine ALL feature sets
        # - Spange (13 dims) + ACS_PCA (5 dims) + DRFP-PCA (20 dims)
        # - Arrhenius kinetics (inv_temp, log_time, interaction)
        
        # Training:
        # - Adam optimizer, lr=1e-3 with cosine annealing
        # - 300 epochs max, early stopping
        # - MSE loss (not MAE - smoother gradients)
```

### PRIORITY 2: Ensemble with Prediction Uncertainty Weighting
**Rationale**:
- GP models provide uncertainty estimates
- Down-weight predictions for OOD solvents
- May reduce CV-LB gap by being conservative on uncertain predictions

**Implementation**:
- Train GP + HGB + ETR ensemble
- Use GP variance to weight predictions
- Higher variance → lower weight on that prediction

### PRIORITY 3: Chemical Class-Based Models
**Rationale**:
- Solvents can be grouped by chemical class (alcohols, ethers, etc.)
- Models trained on chemical classes may generalize better
- Test solvents likely belong to known chemical classes

**Implementation**:
- Classify solvents by functional group
- Train separate models per class
- Use class membership for prediction routing

## What NOT to Try

1. **DRFP with tree-based models** - exp_018 confirmed this doesn't work
2. **More hyperparameter tuning on exp_004** - we've hit the ceiling
3. **GroupKFold validation** - exp_011/012 showed this doesn't help LB
4. **TTA (Test-Time Augmentation)** - exp_004 showed this hurts performance
5. **Feature combination** - exp_016 showed prediction combination is better

## Validation Notes

- Use LOO validation (24 folds single, 13 folds full) - this is the correct scheme
- CV-LB gap is ~53% - expect LB to be ~1.5x worse than CV
- If CV improves to 0.05, expect LB ~0.075 (still far from target)
- **Key insight**: To reach target (0.01727), we need CV ~0.011 (assuming 53% gap)

## Critical Constraints

**MANDATORY SUBMISSION STRUCTURE:**
- Last 3 cells MUST match template exactly
- Only change: `model = YourModel()` line
- Include 'row' column in submission

**Template compliance check:**
```python
# Third-to-last cell: Single solvent LOO (24 folds)
model = YourModel(data='single')

# Second-to-last cell: Full data LOO (13 folds)  
model = YourModel(data='full')

# Last cell: Combine and save submission
```

## Strategic Summary

The 53% CV-LB gap is the REAL problem. Our models are memorizing training solvents, not learning generalizable patterns. To reach the target:

1. **Try MLP with strong regularization** - different model family, may generalize better
2. **Focus on features that generalize** - Arrhenius kinetics, chemical properties
3. **Don't chase CV** - a model with CV 0.07 but smaller CV-LB gap is better than CV 0.06 with 53% gap

The target IS reachable - the paper shows GNN achieves MSE 0.0039. We need to find the right approach.