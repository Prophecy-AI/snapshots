## Current Status
- Best CV score: 0.0623 from exp_004 (LOO validation - UNRELIABLE)
- Best GroupKFold CV: 0.0841 from exp_011 (MORE REALISTIC)
- Best LB score: 0.0956 from exp_004
- CV-LB gap: 53% with LOO, expected ~12-20% with GroupKFold
- Target: 0.01727 (5.5x better than best LB)

## Response to Evaluator

**Technical verdict was CONCERNS** - Template violation detected. I FULLY AGREE.

The evaluator correctly identified that exp_011 violated template compliance:
- Used DataFrame construction without 'row' column
- Added CV calculation code inside template cells
- This MUST be fixed before any submission

**Evaluator's top priority**: Fix template compliance BEFORE any other changes.
**My response**: AGREE 100%. This is a blocking issue.

**Key concerns raised**:
1. Template violation - WILL FIX in next experiment
2. Fixed ensemble weights vs Optuna - WILL IMPLEMENT Optuna
3. CV still far from target - WILL explore GNN/better features

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop9_analysis.ipynb`: Top kernel analysis
- `exploration/evolver_loop10_analysis.ipynb`: GroupKFold insight
- `exploration/evolver_loop11_analysis.ipynb`: Template compliance issue

Key patterns:
- GroupKFold gives MORE REALISTIC CV estimates (0.0841 vs 0.0623 LOO)
- LOO has 53% CV-LB gap, GroupKFold expected ~12-20%
- Top kernel uses Optuna for ALL hyperparameters including ensemble weights
- Paper arxiv:2512.19530 achieved MSE 0.0039 with GNN (25x better than tabular)

## Recommended Approaches

### PRIORITY 1: FIX TEMPLATE COMPLIANCE (CRITICAL - BLOCKING)

The last 3 cells MUST use EXACT template format:
```python
########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################
import tqdm

X, Y = load_data("single_solvent")
split_generator = generate_leave_one_out_splits(X, Y)
all_predictions = []

for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):
    (train_X, train_Y), (test_X, test_Y) = split

    model = YourModel(data='single')  # ONLY THIS LINE CAN CHANGE
    model.train_model(train_X, train_Y)
    predictions = model.predict(test_X)

    predictions_np = predictions.detach().cpu().numpy()

    for row_idx, row in enumerate(predictions_np):  # MUST USE THIS FORMAT
        all_predictions.append({
            "task": 0,
            "fold": fold_idx,
            "row": row_idx,  # REQUIRED - exp_011 was missing this!
            "target_1": row[0],
            "target_2": row[1],
            "target_3": row[2]
        })

submission_single_solvent = pd.DataFrame(all_predictions)
```

**DO NOT**:
- Add CV calculation code inside template cells
- Change DataFrame construction method
- Remove 'row' column

**DO**:
- Move CV calculation to EARLIER cells (before template)
- Keep GroupKFold utility function overwrite (this is allowed - it's before template)

### PRIORITY 2: Optuna Hyperparameter Tuning

Top kernel uses Optuna to tune:
- lr: 1e-4 to 1e-2 (log scale)
- dropout: 0.1 to 0.5
- hidden_dim: 64 to 256
- xgb_depth: 3-8
- rf_depth: 5-15
- lgb_leaves: 15-63
- **Ensemble weights**: w_mlp, w_xgb, w_rf, w_lgb (normalized)

Implement Optuna optimization BEFORE the template cells, then use best params in model.

### PRIORITY 3: Try Per-Target Models with Combined Features

From earlier analysis (exp_005):
- Per-target models (HGB for SM, ETR for Products) achieved best LOO CV
- Combined features (DRFP-PCA + Spange + ACS_PCA) showed promise

Try combining:
- GroupKFold validation (from exp_011)
- Per-target architecture (from exp_005)
- Optuna tuning (from top kernel)

### PRIORITY 4: Consider GNN Approach (if time permits)

Paper arxiv:2512.19530 achieved MSE 0.0039 (MAE ~0.062) using:
- GNN with GAT architecture
- DRFP features
- Learned mixture-aware solvent encodings

This is 25x better than tabular ensembles. May require significant implementation effort but could be the key to reaching target.

## What NOT to Try

1. **LOO validation** - Gives unrealistic CV estimates (53% gap)
2. **Fixed ensemble weights** - Top kernel uses learned weights via Optuna
3. **TTA (Test-Time Augmentation)** - Confirmed to hurt performance (exp_004 vs exp_005)
4. **More regularization alone** - exp_006 showed this makes LB worse

## Validation Notes

- **USE GroupKFold (5-fold)** - This is what top kernel uses
- **Overwrite utility functions** BEFORE template cells (this is allowed)
- **Expected CV-LB gap**: ~12-20% with GroupKFold (vs 53% with LOO)
- **CV of 0.08 with GroupKFold** â†’ Expected LB ~0.09-0.10

## Submission Strategy

With 3 submissions remaining:
1. **Next submission**: Template-compliant GroupKFold ensemble to verify CV-LB correlation
2. **After that**: Optuna-tuned model if CV improves
3. **Final**: Best performing model

The target (0.01727) requires 82% reduction from current best LB (0.0956). This is a HUGE gap that likely requires:
- GNN approach (proven to work in paper)
- OR significant feature engineering breakthrough
- OR ensemble of many diverse models

DO NOT GIVE UP. The target IS reachable. Keep experimenting!