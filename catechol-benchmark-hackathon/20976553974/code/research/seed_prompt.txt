# Catechol Reaction Yield Prediction - Evolved Seed Prompt (Loop 1)

## Current Status
- Best CV score: 0.081393 from exp_000 (baseline ensemble)
- Best LB score: Not yet submitted (0/5 submissions used)
- Target: 0.017270 (4.7x improvement needed)

## Response to Evaluator
- Technical verdict was CONCERNS due to template non-compliance. **CRITICAL: Must fix template structure.**
- Evaluator's top priority: Fix template compliance, then try Gaussian Process models.
- Key concerns raised: (1) Template non-compliance, (2) Large gap to target (4.7x), (3) Missing error analysis, (4) Linear mixture assumption.
- Addressing: Template compliance is mandatory. Error analysis done in Loop 1 - identified high-variance solvents. Will try fundamentally different approaches.

## Data Understanding
**Reference notebooks:** See `exploration/eda.ipynb` and `exploration/evolver_loop1_analysis.ipynb`

Key findings from analysis:
1. **Strong target correlations**: SM negatively correlated with P2 (-0.89) and P3 (-0.77). P2 and P3 highly correlated (0.92). Multi-task learning could exploit this.
2. **High-variance solvents** (hardest to predict): IPA, Decanol, Ethylene Glycol, Water.Acetonitrile, Ethanol
3. **Unusual solvents** (outliers in feature space): Water, Hexafluoropropan-2-ol, Cyclohexane
4. **Temperature dominates**: Temp corr with SM=-0.82, P2=0.72, P3=0.57. Time correlations weaker (~0.2-0.3)

## CRITICAL: Template Compliance
**The last 3 cells MUST match the template exactly. Only change the model definition line.**

```python
# Cell -3: Single solvent task
model = YourModel(data='single')  # ONLY THIS LINE CAN CHANGE

# Cell -2: Full data task  
model = YourModel(data='full')  # ONLY THIS LINE CAN CHANGE

# Cell -1: Save submission (DO NOT CHANGE)
```

## Recommended Approaches (Priority Order)

### 1. Per-Target Models with Different Algorithms (HIGH PRIORITY)
From kernel `dabansherwani_catechol-strategy-to-get-0-11161`:
- Use **different models for different targets**:
  - SM: HistGradientBoostingRegressor (max_depth=7, max_iter=700, lr=0.04)
  - Product 2, Product 3: ExtraTreesRegressor (n_estimators=900, min_samples_leaf=2)
- Ensemble with multiple feature sets (acs_pca + spange)
- Weights: 0.65 * acs_pca + 0.35 * spange

```python
class PerTargetEnsembleModel:
    def __init__(self):
        self.targets = ["Product 2", "Product 3", "SM"]
        self.models = {}
        for t in self.targets:
            if t == "SM":
                self.models[t] = [
                    BetterCatecholModel("acs_pca_descriptors", "hgb"),
                    BetterCatecholModel("spange_descriptors", "hgb"),
                ]
            else:
                self.models[t] = [
                    BetterCatecholModel("acs_pca_descriptors", "etr"),
                    BetterCatecholModel("spange_descriptors", "etr"),
                ]
```

### 2. Enhanced Feature Engineering (HIGH PRIORITY)
Current baseline uses Arrhenius features. Add more:
- **Polynomial features**: rt², temp², rt*temp
- **Log features**: log(rt), log(temp-273.15)
- **Interaction with solvent properties**: temp * dielectric_constant, rt * alpha

From kernel `paritoshtripathi5_alchemy-baseline`:
```python
def _numeric_block(X):
    rt, temp = X['Residence Time'], X['Temperature']
    feats = [rt, temp, rt**2, temp**2, np.log1p(rt), np.log1p(temp), rt * temp]
    return np.concatenate(feats, axis=1)
```

### 3. Higher-Dimensional Features (MEDIUM PRIORITY)
Try DRFP (2048-dim) or fragprints (2133-dim) instead of Spange (13-dim):
- May capture more chemical information
- Need dimensionality reduction or regularization to avoid overfitting

### 4. Gaussian Process Models (MEDIUM PRIORITY)
From web research, GPs are excellent for small chemical datasets:
- Use GPyTorch or sklearn's GaussianProcessRegressor
- Consider Tanimoto kernel for molecular similarity
- Provides uncertainty estimates

### 5. Multi-Task Learning (LOWER PRIORITY)
Exploit target correlations:
- Predict SM first, then use SM prediction as feature for P2/P3
- Or use shared representation learning

## What NOT to Try
- Simple hyperparameter tuning of current ensemble (won't close 4.7x gap)
- Normalizing targets to sum to 1 (they naturally don't)
- Complex neural architectures without proper regularization

## Validation Notes
- Single solvent: Leave-one-solvent-out (24 folds) - correct
- Full data: Leave-one-ramp-out (13 folds) - correct
- No CV-LB gap data yet (no submissions)

## Implementation Checklist
1. ✅ Ensure template compliance (last 3 cells unchanged except model line)
2. ✅ Use correct validation splits
3. ✅ Same hyperparameters across all folds
4. ✅ Return torch tensor [N, 3] from predict()
5. ✅ Clip predictions to [0, 1]

## Reference Kernels
- `dabansherwani_catechol-strategy-to-get-0-11161`: Per-target ensemble (Score: 0.11161)
- `paritoshtripathi5_alchemy-baseline`: Enhanced numeric features
- `tobimichigan_catechol-pipeline-with-lightgbm`: LightGBM pipeline
- `sanidhyavijay24_arrhenius-kinetics-tta-0-09831`: Arrhenius + TTA (Score: 0.09831)
- `lishellliang_mixall-runtime-is-only-2m-15s-but-good-cv-lb`: Ensemble approach

## Next Experiment Priorities
1. **Experiment 002**: Per-target ensemble with HGB for SM, ETR for products (from dabansherwani kernel)
2. **Experiment 003**: Enhanced feature engineering with polynomial/log features
3. **Experiment 004**: Try DRFP or fragprints features
4. **Experiment 005**: Gaussian Process model

## Target Score
Beat **0.017270** (lower is better). Current gap is 4.7x - need fundamentally different approaches, not incremental improvements.