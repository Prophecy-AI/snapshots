# Catechol Reaction Yield Prediction - Evolved Seed Prompt (Loop 8)

## Current Status
- **Best CV score**: 0.0623 from exp_004 (Per-target HGB+ETR NO TTA)
- **Best LB score**: 0.0956 from exp_004
- **CV-LB gap**: +53% (LB is 53% worse than CV)
- **Target**: 0.01727 (5.5x better than our best LB)
- **Submissions used**: 1/5 (4 remaining)

## Response to Evaluator

**Technical verdict**: TRUSTWORTHY. The evaluator confirmed exp_007 (GP) has sound validation, no leakage, and verified scores.

**Evaluator's top priority**: "SUBMIT exp_006 (intermediate regularization, CV 0.0689) to test the regularization hypothesis."
- **I AGREE** - We need LB feedback to calibrate our strategy
- **Reasoning**: With 4 submissions remaining, we can afford to test this hypothesis
- **Action**: Submit exp_006 to verify if regularization reduces CV-LB gap

**Key concerns raised**:
1. **GP Underperforms on CV**: GP CV (0.0721) is 16% worse than best tree-based CV (0.0623) - ACKNOWLEDGED
2. **Submission Strategy Critical**: We need LB feedback to guide strategy - AGREED
3. **Gap to Target is Enormous**: 3.6x gap from best CV to target - ACKNOWLEDGED

**My synthesis**: 
- Submit exp_006 to test regularization hypothesis
- If LB improves proportionally, continue with regularization + ensemble
- If LB doesn't improve, pivot to fundamentally different approaches

## Data Understanding

**Reference notebooks:**
- `exploration/evolver_loop8_analysis.ipynb` - Strategic assessment and kernel analysis
- `exploration/evolver_loop7_analysis.ipynb` - GP feasibility test
- `exploration/evolver_loop5_lb_feedback.ipynb` - CV-LB gap analysis

**Key patterns to exploit:**
1. **Top kernels use ensemble of MLP+XGB+RF+LGB** with learned weights
2. **GroupKFold (5-fold)** gives more realistic CV than Leave-One-Out
3. **TTA HURTS mixed solvents**: DO NOT use TTA
4. **Per-target models work**: Different models for SM vs Products

## Recommended Approaches

### Priority 1: SUBMIT exp_006 (IMMEDIATE)
**Why**: We need LB feedback to calibrate our strategy. exp_006 has intermediate regularization (CV 0.0689).

**Expected outcome**: 
- If LB improves proportionally (e.g., LB ~0.09), regularization helps
- If LB doesn't improve (e.g., LB ~0.10), we need different approaches

### Priority 2: Diverse Ensemble (GP + ETR + MLP)
**Why**: Top kernels use ensemble of diverse model families. We have GP (0.0721), ETR (0.0623), and can add MLP.

**Implementation**:
```python
class DiverseEnsemble(BaseModel):
    def __init__(self, data='single'):
        self.data_type = data
        self.mixed = (data == 'full')
        self.gp_model = GPModel(data=data)
        self.etr_model = ETRModel(data=data)
        self.mlp_model = MLPModel(data=data)
        self.weights = [0.3, 0.4, 0.3]  # GP, ETR, MLP
    
    def train_model(self, X_train, y_train):
        self.gp_model.train_model(X_train, y_train)
        self.etr_model.train_model(X_train, y_train)
        self.mlp_model.train_model(X_train, y_train)
    
    def predict(self, X):
        gp_pred = self.gp_model.predict(X)
        etr_pred = self.etr_model.predict(X)
        mlp_pred = self.mlp_model.predict(X)
        
        final = (self.weights[0] * gp_pred + 
                 self.weights[1] * etr_pred + 
                 self.weights[2] * mlp_pred)
        return torch.tensor(final, dtype=torch.double)
```

### Priority 3: Stacking with Meta-Learner
**Why**: Stacking can capture complementary information from diverse base models.

**Implementation**:
- Train base models (GP, ETR, MLP) on folds
- Use base model predictions as features for meta-learner (Ridge)
- May improve generalization to unseen solvents

### Priority 4: MLP with Strong Regularization
**Why**: Top kernels use MLP with BatchNorm + Dropout + LR scheduler.

**Implementation**:
```python
class RegularizedMLP(nn.Module):
    def __init__(self, input_dim, output_dim=3):
        super().__init__()
        self.net = nn.Sequential(
            nn.BatchNorm1d(input_dim),
            nn.Linear(input_dim, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, output_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x)
```

## What NOT to Try

1. **TTA for mixed solvents** - Confirmed to hurt performance
2. **Deep trees (depth > 10)** - Overfits to training solvents
3. **GP alone** - CV 0.0721 is worse than ETR 0.0623
4. **Complex ensembles without diversity** - Need fundamentally different models

## Validation Notes

- **CV scheme**: Leave-one-solvent-out for single, leave-one-ramp-out for full
- **CV-LB gap**: 53% - CV is too optimistic
- **Hardest solvents**: HFIP (0.174 MAE), Cyclohexane (0.154), Acetonitrile.Acetic Acid (0.117)

## CRITICAL CONSTRAINTS (MUST FOLLOW)

1. **Template compliance**: Last 3 cells must match template exactly
2. **Only change model definition line**: `model = YourModel(data='single')` and `model = YourModel(data='full')`
3. **Same hyperparameters across all folds**
4. **No data leakage**: Scalers fit on training data only within each fold
5. **No TTA for mixed solvents**

## Experiments to Run

**Experiment 009: Diverse Ensemble (GP + ETR + MLP)**
- Combine GP, ExtraTrees, and MLP predictions
- Weighted average: 0.3*GP + 0.4*ETR + 0.3*MLP
- May reduce variance and improve generalization

**Experiment 010: Stacking with Meta-Learner**
- Train base models (GP, ETR, MLP) on folds
- Use base model predictions as features for Ridge meta-learner
- May capture complementary information

**Expected outcome**: CV ~0.06-0.07, potentially better LB due to ensemble diversity.

The target IS reachable. Ensemble diversity and stacking may be the key to closing the CV-LB gap.
