# Catechol Reaction Yield Prediction - Seed Prompt (Loop 9)

## Current Status
- **Best CV score**: 0.0623 from exp_004 (PerTarget HGB+ETR NO TTA)
- **Best LB score**: 0.0956 from exp_004
- **CV-LB gap**: +53% (0.0333 absolute) → LARGE gap indicates test set has chemically unique solvents
- **Target**: 0.01727 (5.5x better than best LB)
- **Submissions**: 2/5 used, 3 remaining

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator correctly verified the diverse ensemble experiment.

**Evaluator's top priority**: Try GroupKFold (5-fold) validation like top kernel.
- **AGREE**: This is a key difference from top kernels. However, I want to first try adding MLP to the ensemble since that's the more impactful change.

**Key concerns raised by evaluator**:
1. GroupKFold vs Leave-One-Out - Valid concern, but MLP is higher priority
2. Ensemble CV is worse than per-target alone - TRUE, need to investigate why
3. Fixed weights may be suboptimal - AGREE, should try Optuna

**My synthesis**: The diverse ensemble (exp_009) had WORSE CV than per-target (exp_004). This suggests:
1. The fixed weights [0.4,0.2,0.2,0.2] are suboptimal
2. We're missing MLP which is a key component in top kernels
3. Combined features may be causing overfitting

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Basic data characteristics
- `exploration/evolver_loop5_lb_feedback.ipynb` - CV-LB gap analysis
- `exploration/evolver_loop8_lb_feedback.ipynb` - Regularization disproved overfitting hypothesis
- `exploration/evolver_loop9_analysis.ipynb` - Top kernel analysis, MLP importance

**Key patterns to exploit:**
1. **Top kernel uses MLP** - We're missing this critical component
2. **Spange descriptors only** - Top kernel uses simpler features
3. **Learned weights** - Optuna finds optimal ensemble weights
4. **Per-target models work** - HGB for SM, ETR for Products (exp_004 best CV)

## Recommended Approaches

### Priority 1: MLP + GBDT ENSEMBLE (HIGH IMPACT)
**Why**: Top kernel (lishellliang) uses MLP + XGBoost + RF + LightGBM. We're missing MLP which captures non-linear patterns that trees miss.

**Implementation**:
```python
class MLPGBDTEnsemble(BaseModel):
    def __init__(self, data='single'):
        self.mlp = EnhancedMLP(input_dim, output_dim=3)
        self.xgb = MultiOutputRegressor(XGBRegressor(...))
        self.rf = MultiOutputRegressor(RandomForestRegressor(...))
        self.lgb = MultiOutputRegressor(LGBMRegressor(...))
        self.weights = [0.35, 0.25, 0.25, 0.15]  # MLP, XGB, RF, LGB
    
    def predict(self, X):
        mlp_pred = self.mlp(X)
        xgb_pred = self.xgb.predict(X)
        rf_pred = self.rf.predict(X)
        lgb_pred = self.lgb.predict(X)
        return sum(w * p for w, p in zip(self.weights, [mlp_pred, xgb_pred, rf_pred, lgb_pred]))
```

**Key MLP architecture (from top kernel)**:
- Hidden dims: [128, 64, 32]
- BatchNorm + ReLU + Dropout(0.2) after each layer
- Sigmoid output (yields are 0-1)
- Adam optimizer with lr=5e-4, weight_decay=1e-5
- ReduceLROnPlateau scheduler

### Priority 2: SIMPLIFY FEATURES (MEDIUM IMPACT)
**Why**: Top kernel uses Spange descriptors only. Our combined features (Spange + ACS_PCA + DRFP) may be causing overfitting.

**Implementation**:
- Try Spange-only features first
- If that works, compare with combined features
- Simpler features may generalize better to unseen solvents

### Priority 3: OPTUNA FOR ENSEMBLE WEIGHTS (MEDIUM IMPACT)
**Why**: Fixed weights [0.4,0.2,0.2,0.2] are suboptimal. Optuna can find optimal weights.

**Implementation**:
```python
def objective(trial):
    w_mlp = trial.suggest_float('w_mlp', 0.1, 1.0)
    w_xgb = trial.suggest_float('w_xgb', 0.1, 1.0)
    w_rf = trial.suggest_float('w_rf', 0.1, 1.0)
    w_lgb = trial.suggest_float('w_lgb', 0.1, 1.0)
    total = w_mlp + w_xgb + w_rf + w_lgb
    weights = [w_mlp/total, w_xgb/total, w_rf/total, w_lgb/total]
    # Train and evaluate with these weights
    return cv_score
```

### Priority 4: STACKING WITH META-LEARNER (LOWER PRIORITY)
**Why**: If weighted averaging doesn't work, try stacking where a meta-learner combines base model predictions.

## What NOT to Try

1. **More regularization** - DISPROVED by exp_006 (made LB worse)
2. **Simpler models alone** - Ridge (exp_005) had worst CV (0.0896)
3. **GP alone** - CV worse than tree-based (0.0721 vs 0.0623)
4. **TTA** - Hurts performance on mixed solvents
5. **PerTarget + RF/XGB/LGB without MLP** - exp_009 showed this doesn't help

## Validation Notes

- **CV scheme**: Leave-one-solvent-out for single, leave-one-ramp-out for full
- **CV-LB gap**: ~50% (0.0623 → 0.0956) - test set has unique solvents
- **Calibration**: CV improvements may not translate to LB improvements
- **Strategy**: Focus on MLP + GBDT ensemble like top kernel

## Competition-Specific Constraints (MUST FOLLOW)

**MANDATORY SUBMISSION STRUCTURE:**
- The submission must have the same last three cells as in the notebook template
- The ONLY allowed change is the line where the model is defined
- The line `model = MLPModel()` can be replaced with a new model definition
- Everything else in the last three cells MUST remain exactly the same

**Template compliance example:**
```python
# Cell -3 (third from last)
model = YourModel(data='single')  # ONLY THIS LINE CAN CHANGE

# Cell -2 (second from last)
model = YourModel(data='full')    # ONLY THIS LINE CAN CHANGE

# Cell -1 (last cell) - DO NOT MODIFY
submission = pd.concat([submission_single_solvent, submission_full_data])
submission = submission.reset_index()
submission.index.name = "id"
submission.to_csv("submission.csv", index=True)
```

## Next Experiment Recommendation

**Experiment 010: MLP + GBDT Ensemble (Like Top Kernel)**

Key implementation details:
1. **MLP Architecture**:
   - Input: Spange features + Time + Temp (15 dims for single, 16 for full)
   - Hidden: [128, 64, 32] with BatchNorm + ReLU + Dropout(0.2)
   - Output: 3 (Sigmoid for yields 0-1)
   - Training: Adam(lr=5e-4, weight_decay=1e-5), 200 epochs, batch_size=32

2. **GBDT Models**:
   - XGBoost: n_estimators=200, max_depth=6, learning_rate=0.05
   - RandomForest: n_estimators=200, max_depth=10
   - LightGBM: n_estimators=200, max_depth=6, learning_rate=0.05

3. **Ensemble Weights**: [0.35, 0.25, 0.25, 0.15] for MLP, XGB, RF, LGB

4. **Features**: Spange descriptors only (simpler may generalize better)

**Expected outcome**:
- CV may be similar to exp_004 (~0.06-0.07)
- LB may improve due to MLP capturing different patterns
- This is what top kernels use successfully

**Alternative Experiment 011: Optuna Weight Optimization**
If exp_010 doesn't improve, use Optuna to find optimal ensemble weights.