# Catechol Reaction Yield Prediction - Techniques Guide

## Problem Overview
This is a chemistry reaction yield prediction problem. The goal is to predict three yield outputs (SM, Product 2, Product 3) based on solvent conditions, temperature, and residence time. The evaluation uses a custom cross-validation procedure with two tasks:
- Task 0: Single solvent data (leave-one-solvent-out CV, 24 folds)
- Task 1: Full data with solvent mixtures (leave-one-ramp-out CV, 13 folds)

**Target score to beat: 0.017270** (lower is better)

## Data Understanding
**Reference notebook:** `exploration/eda.ipynb`
- Full data: 1227 samples, 19 columns
- Single solvent data: 656 samples, 13 columns
- 24 unique solvents, 13 unique solvent ramps
- Targets are yields in range [0, 1] (some slightly exceed 1.0)
- Temperature range: 175-225°C
- Residence Time range: 2-15 minutes
- SolventB% range: 0-1

## Key Techniques from Top Solutions

### 1. Physics-Informed Feature Engineering (CRITICAL)
**Arrhenius Kinetics Features** - The most important technique from the top solution (0.09831 score):
- Convert temperature to Kelvin: `temp_k = temp_c + 273.15`
- Inverse temperature: `inv_temp = 1000.0 / temp_k`
- Log time: `log_time = np.log(time + 1e-6)`
- Interaction term: `interaction = inv_temp * log_time`

This is based on the Arrhenius equation for reaction kinetics: k = A * exp(-Ea/RT)

### 2. Chemical Symmetry and Test Time Augmentation (TTA)
For mixed solvents, a mixture of "Solvent A + Solvent B" is physically identical to "Solvent B + Solvent A". 
**TTA Strategy:**
- During inference for mixed solvents, predict twice:
  - Prediction 1: Input as (A, B)
  - Prediction 2: Input as (B, A) with flipped concentration
  - Final = (Pred1 + Pred2) / 2
- Also train on both symmetries (data augmentation)

### 3. Solvent Featurization
Available lookup tables:
- **spange_descriptors** (13 features): Best for linear mixing, most commonly used
- **acs_pca_descriptors** (5 features): PCA-based from ACS Green Chemistry
- **drfps_catechol** (2048 features): Differential reaction fingerprints
- **fragprints** (2133 features): Fragment + fingerprint concatenation

**Linear mixing for solvent mixtures:**
```
mixed_features = A_features * (1 - pct) + B_features * pct
```

**Feature Combination Strategy:**
- Try concatenating multiple featurizations (e.g., spange + acs_pca)
- Use PCA to reduce dimensionality if using high-dim features like drfps/fragprints

### 4. Additional Numeric Feature Engineering
- Polynomial features: rt², temp², rt*temp
- Log transforms: log1p(rt), log1p(temp)
- Reaction energy proxy: Temperature * Residence Time
- Concentration-temperature interaction: SolventB% * Temperature
- Inverse features: 1/rt, 1/temp (for Arrhenius-like relationships)

### 5. Model Architecture
**MLP (Most successful):**
- BatchNorm at input (critical for normalization)
- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)
- Output: 3 neurons with Sigmoid activation (for bounded outputs)
- Loss: HuberLoss (robust to outliers) or MSELoss
- Optimizer: Adam with lr=5e-4, weight_decay=1e-5
- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)
- Gradient clipping: max_norm=1.0
- Epochs: 250-300

**Alternative Architectures to Try:**
- Deeper networks: [256, 128, 64, 32]
- Wider networks: [256, 256, 128]
- Residual connections for deeper networks
- Multi-task learning with shared representations and task-specific heads

**Model Bagging:**
- Train 5-7 models with different seeds
- Average predictions for robustness
- Consider weighted averaging based on validation performance

### 6. Advanced Techniques for Small Datasets

**Probabilistic Models:**
- Monte Carlo Dropout: Keep dropout active during inference, average multiple predictions
- Bayesian Neural Networks: Provide uncertainty estimates
- Deep Kernel Learning: Combine NN feature extraction with GP uncertainty

**Regularization (Critical for small data):**
- Strong dropout (0.2-0.3)
- L2 weight decay (1e-4 to 1e-5)
- Early stopping with patience
- Data augmentation (symmetry flipping for mixtures)

**Feature Scaling:**
- StandardScaler on all numeric features
- Fit scaler on training data only, transform test data

### 7. Post-Processing
- Clip predictions to [0, 1]
- Optional: Normalize rows so yields sum to 1 (chemical constraint - SM + P2 + P3 ≈ 1)
- Consider softmax-like normalization for multi-output

### 8. Validation Strategy
The competition uses a specific CV structure:
- Single solvent: Leave-one-solvent-out (24 folds)
- Full data: Leave-one-ramp-out (13 folds)
- Must use the exact split generators from utils.py

**Best Practices:**
- Nest hyperparameter tuning inside each fold to avoid leakage
- Track per-fold performance to identify difficult solvents/ramps
- Use consistent random seeds for reproducibility

### 9. Ensemble Strategies
**Seed Ensemble:**
- Train same model with 5-7 different random seeds
- Average predictions

**Feature Ensemble:**
- Train separate models on different featurizations
- Combine predictions (average or weighted)

**Model Ensemble:**
- Combine MLP with gradient boosting (XGBoost/LightGBM)
- Stack models with a meta-learner

**Stacking:**
- Use out-of-fold predictions as features for a second-level model
- Can significantly improve performance

### 10. Alternative Approaches

**XGBoost/LightGBM:**
- Can work but typically underperforms MLP for this problem
- Use MultiOutputRegressor wrapper
- Enable categorical features for solvent names
- Try with Arrhenius features

**Gaussian Processes:**
- Could leverage uncertainty quantification
- Deep Kernel Learning combines NN feature learning with GP
- May be computationally expensive but good for small data

**Transformer-based Models:**
- If using reaction SMILES, consider BERT-based models
- Data augmentation via SMILES randomization can help

### 11. Hyperparameter Tuning Priority
1. Learning rate (1e-4 to 1e-3)
2. Hidden layer sizes
3. Dropout rate (0.1 to 0.3)
4. Weight decay (1e-5 to 1e-4)
5. Batch size (16, 32, 64)
6. Number of epochs with early stopping

### 12. Multi-Output Regression Considerations
- Multi-task neural networks with shared representations outperform single-task models
- The three outputs (SM, Product 2, Product 3) are chemically related - exploit this
- Consider correlation between outputs when designing loss functions
- Distributional regression (predicting distributions instead of point estimates) can improve robustness

## Implementation Priority (Recommended Order)
1. **Baseline:** Basic MLP + spange_descriptors + StandardScaler
2. **Physics:** Add Arrhenius kinetics features (1/T, ln(t), interaction)
3. **Symmetry:** Implement TTA for chemical symmetry (mixed solvents)
4. **Bagging:** Add model bagging (5-7 seeds)
5. **Features:** Try combining multiple featurizations
6. **Architecture:** Experiment with deeper/wider networks
7. **Ensemble:** Combine different model types
8. **Advanced:** Try probabilistic models or transformers

## Code Structure Notes
- The submission must follow the template structure with specific last 3 cells
- Only the model definition line can be changed
- Model must have train_model(X_train, Y_train) and predict(X) methods
- Use torch.set_default_dtype(torch.double) for numerical stability

## Key Insights from Top Kernels
1. Arrhenius kinetics features are the single most impactful improvement
2. Chemical symmetry TTA provides consistent gains for mixed solvents
3. Spange descriptors work best for linear mixing
4. Model bagging with 5-7 seeds reduces variance significantly
5. HuberLoss is more robust than MSELoss for this data
6. BatchNorm at input is critical for stable training

## Potential Breakthrough Approaches
To beat the target score of 0.017270 (which is very low), consider:
1. **Ensemble of ensembles:** Combine multiple bagged models with different architectures
2. **Feature engineering depth:** Explore more physics-based features beyond Arrhenius
3. **Per-solvent models:** Train specialized models for different solvent families
4. **Output correlation modeling:** Explicitly model the correlation between SM, P2, P3
5. **Uncertainty-weighted predictions:** Use uncertainty estimates to weight predictions
6. **Transfer learning:** Pre-train on related chemistry datasets if available
