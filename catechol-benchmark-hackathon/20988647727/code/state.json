{
  "phase": "explorer",
  "loop_count": 0,
  "last_submission": null,
  "competition_id": "catechol-benchmark-hackathon",
  "seed_prompt": "",
  "evaluator_feedback": "",
  "latest_experiment": {},
  "competitive_intel": "## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`",
  "target_score": 0.01727,
  "manual_instructions": "",
  "messages": [],
  "experiments": [],
  "submissions": [],
  "candidates": [],
  "findings": [],
  "metric_lower_is_better": true,
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics feature engineering: Use 1/Temperature (in Kelvin), ln(Time), and their interaction as features. This is physics-informed and improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Test Time Augmentation (TTA) for chemical symmetry: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped - then average. This respects physical symmetry of mixtures.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors provide best base for linear mixing of solvents. Use linear interpolation: A*(1-pct) + B*pct for mixed solvent features.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model bagging with 7 models improves robustness. Use HuberLoss for robustness to outliers. MLP architecture: BatchNorm -> Linear(128) -> ReLU -> Dropout(0.2) repeated, ending with Sigmoid.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Numeric feature engineering: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Use StandardScaler on all features.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV structure: Task 0 = single solvent (leave-one-solvent-out), Task 1 = full data with mixtures (leave-one-ramp-out). Target columns are Product 2, Product 3, SM (yields 0-1).",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "XGBoost approach: Feature engineering with Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip to [0,1] and normalize rows to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available solvent featurizations: spange_descriptors (best for linear mixing), acs_pca_descriptors, drfps_catechol, fragprints. Spange descriptors are most commonly used in top solutions.",
      "source": "../research/description.md",
      "agent": "explorer"
    },
    {
      "finding": "Deep Kernel Learning (DKL) combines neural network feature learning with Gaussian Process uncertainty quantification - good for reaction yield prediction with uncertainty estimates.",
      "source": "WebSearch: ML techniques for chemical reaction yields",
      "agent": "explorer"
    },
    {
      "finding": "Transformer/BERT models on reaction SMILES achieve state-of-the-art for yield prediction. Data augmentation (molecule order permutations, SMILES randomizations) improves results.",
      "source": "WebSearch: ML techniques for chemical reaction yields",
      "agent": "explorer"
    },
    {
      "finding": "For small chemistry datasets (<2000 samples): Use probabilistic models (Bayesian NN, MC Dropout) for uncertainty quantification. Strong regularization (dropout, L2 weight decay) and data augmentation (SMILES randomization) are critical.",
      "source": "WebSearch: NN predictions for small chemistry datasets",
      "agent": "explorer"
    },
    {
      "finding": "Leave-one-cluster-out CV (LOCO-CV) is recommended for measuring extrapolatory power. Nest hyperparameter tuning inside each LOO fold to avoid information leakage.",
      "source": "WebSearch: NN predictions for small chemistry datasets",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning techniques for predicting chemical reaction yields?",
      "finding": "Predicting the yield of a chemical reaction is now treated as a supervised regression problem that can be tackled with a variety of modern machine\u2011learning models.  The most successful approaches combine rich, text\u2011based representations of the reaction (e.g., reaction SMILES) with deep\u2011learning architectures originally developed for natural\u2011language processing.  Encoder\u2011transformer models that treat the whole reaction string as a \u201csentence\u201d and attach a regression head have shown \u201coutstanding\u201d performance on high\u2011throughput experiment datasets, often surpassing traditional fingerprint\u2011based methods that rely on one\u2011hot encodings or concatenated molecular descriptors\u202f([research.ibm.com](https://research.ibm.com/publications/prediction-of-chemical-reaction-yields-using-deep-learning))\u202f([rxn4chemistry.github.io](https://rxn4chemistry.github.io/rxn_yields)).  Variants of the Molecular Transformer, fine\u2011tuned on yield data, are now a standard baseline for yield prediction.\n\nWhen only a few ",
      "agent": "explorer"
    },
    {
      "query": "How to improve neural network predictions for small chemistry datasets with leave-one-out cross validation?",
      "finding": "Improving neural\u2011network predictions on very small chemistry datasets while using leave\u2011one\u2011out (LOO) cross\u2011validation can be done by combining careful data handling, probabilistic modeling, and rigorous validation\u202f\u2013\u202fall of which are highlighted in recent cheminformatics studies ([Thomas\u202fet\u202fal.,\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10);\u202f[Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b);\u202f[Rosseinsky\u202fet\u202fal.,\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c);\u202f[Vehtari\u202fet\u202fal.,\u202f2019](https://proceedings.mlr.press/v97/magnusson19a.html)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Select robust molecular representations**  \n   - Encode each molecule with several complementary descriptors (e.g., circular fingerprints, graph\u2011based embeddings, physicochemical descriptors) and test them separately; diverse representations help compensate for the limited number of samples ([Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org",
      "agent": "explorer"
    },
    {
      "query": "What are the best techniques for multi-output regression with neural networks for chemistry applications?",
      "finding": "Multi\u2011output regression in chemistry is most often tackled with **multi\u2011task neural architectures** that share a common representation of the molecule while learning separate heads for each property.  Early work showed that such networks can outperform single\u2011task models on QSAR datasets by jointly fitting several assays and using regularisation tricks from deep learning\u202f([arxiv.org](https://arxiv.org/abs/1406.1231)).  A modern, highly effective variant replaces fully\u2011connected layers with **graph convolutional networks (GCNs)** that operate directly on molecular graphs; a shared GCN encoder followed by multiple task\u2011specific read\u2011outs has been shown to improve prediction of ADMET endpoints such as log\u202fD, solubility and melting point\u202f([mdpi-res.com](https://mdpi-res.com/d_attachment/molecules/molecules-25-00044/article_deploy/molecules-25-00044.pdf)).  When experimental data are scarce, **multi\u2011fidelity learning** can be added: a low\u2011fidelity quantum\u2011chemical dataset (e.g., COSMO\u2011RS) i",
      "agent": "explorer"
    }
  ],
  "remaining_submissions": 0,
  "max_submissions": 5,
  "last_reset_date_utc": "2026-01-14",
  "start_time": "2026-01-14T09:17:13.972797",
  "time_limit_minutes": 2100,
  "_saved_at": "2026-01-14T09:22:28.993625"
}