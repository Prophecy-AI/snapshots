{
  "query": "What are the best techniques for multi-output regression with neural networks for chemistry applications?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Multi\u2011output regression in chemistry is most often tackled with **multi\u2011task neural architectures** that share a common representation of the molecule while learning separate heads for each property.  Early work showed that such networks can outperform single\u2011task models on QSAR datasets by jointly fitting several assays and using regularisation tricks from deep learning\u202f([arxiv.org](https://arxiv.org/abs/1406.1231)).  A modern, highly effective variant replaces fully\u2011connected layers with **graph convolutional networks (GCNs)** that operate directly on molecular graphs; a shared GCN encoder followed by multiple task\u2011specific read\u2011outs has been shown to improve prediction of ADMET endpoints such as log\u202fD, solubility and melting point\u202f([mdpi-res.com](https://mdpi-res.com/d_attachment/molecules/molecules-25-00044/article_deploy/molecules-25-00044.pdf)).  When experimental data are scarce, **multi\u2011fidelity learning** can be added: a low\u2011fidelity quantum\u2011chemical dataset (e.g., COSMO\u2011RS) is first used to pre\u2011train the GCN, then the model is fine\u2011tuned on a small high\u2011fidelity experimental set, optionally augmenting features or using transfer\u2011learning heads\u202f([chemrxiv.org](https://chemrxiv.org/engage/chemrxiv/article-details/66b359bdc9c6a5c07a4978f5)).\n\nBeyond standard multi\u2011task GCNs, several specialized designs have emerged.  **Distributional regression** replaces point estimates with predicted probability distributions; the Distributional Mixture of Experts (DMoE) trains any backbone (SchNet, GemNet, Graphormer) to output soft target distributions and uses a loss that mixes cross\u2011entropy with an L1 term on expected values, yielding more robust performance on QM9, OC20 and MD17\u202f([arxiv.org](https://arxiv.org/abs/2407.20475)).  **Two\u2011branch networks** provide a generic \u201cplug\u2011and\u2011play\u201d architecture where one branch learns a shared representation and the other learns task\u2011specific transformations, simplifying the deployment of multi\u2011target models across diverse chemical domains\u202f([arxiv.org](https://arxiv.org/abs/2104.09967)).  Finally, **large\u2011scale foundational models** trained on massive multi\u2011task datasets (e.g., millions of molecules and dozens of properties) can be fine\u2011tuned to new regression tasks, offering state\u2011of\u2011the\u2011art accuracy with minimal additional data\u202f([arxiv.org](https://arxiv.org/abs/2310.04292)).\n\nIn practice, the most reliable workflow combines a **graph\u2011based encoder**, **shared hidden layers** for capturing inter\u2011property correlations, and **task\u2011specific output heads** (or distributional heads when uncertainty is needed).  When only a few high\u2011quality measurements exist, augment the training with **low\u2011fidelity quantum data** via transfer or feature\u2011augmentation, and consider **distributional loss functions** to mitigate bias.  This combination\u2014multi\u2011task GCNs with multi\u2011fidelity pre\u2011training and, optionally, distributional experts\u2014has emerged as the leading technique for multi\u2011output regression in modern chemistry applications.",
      "url": ""
    },
    {
      "title": "",
      "text": "We use cookies to distinguish you from other users and to provide you with a better experience on our websites.Close this message to accept cookies or find out how to manage your cookie settings. [Learn more about our Privacy Notice...\\\n\\[opens in a new tab\\]](https://www.cambridge.org/about-us/legal-notices/privacy-notice/)\n\n[Back to\\\nChemical Engineering and Industrial Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470db)\n\nSearch within Chemical Engineering and Industrial Chemistry\n\n# Multi-fidelity graph neural networks for predicting toluene/water partition coefficients\n\n08 August 2024, Version 1\n\nWorking Paper\n\n## Authors\n\n- [Thomas Nevolianis](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Thomas%20Nevolianis),\n- [Jan Gerald Rittig](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Jan%20Gerald%20Rittig),\n- [Alexander Mitsos](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Alexander%20Mitsos),\n- [Kai Leonhard](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Kai%20Leonhard)\n\n[Show author details](https://chemrxiv.org/chemrxiv.org)\n\nThis content is a preprint and has not undergone peer review at the time of posting.\n\nDownload\n\nCite\n\nComment\n\n## Abstract\n\nAccurate prediction of toluene/water partition coefficients of neutral species is crucial in drug discovery and separation processes; however, data-driven modeling of these coefficients remains challenging due to limited available experimental data. To address the limitation of available data, we apply multi-fidelity learning approaches leveraging a quantum chemical dataset (low fidelity) of approximately 9000 entries generated by COSMO-RS and an experimental dataset (high fidelity) of about 250 entries collected from the literature. We explore the transfer learning, feature-augmented learning, and multi-target learning approaches in combination with graph neural networks, validating them on two external datasets: one with molecules similar to training data (EXT-Zamora) and one with more challenging molecules (EXT-SAMPL9). Our results show that multi-target learning significantly improves predictive accuracy, achieving a Root-Mean-Square Error (RMSE) of 0.44 logP units for the EXT-Zamora, compared to an RMSE of 0.63 logP units for single-task models. For the EXT-SAMPL9 dataset, multi-target learning achieves an RMSE of 1.02 logP units, indicating reasonable performance even for more complex molecular structures. These findings highlight the potential of multi-fidelity learning approaches that leverage quantum chemical data to improve toluene/water partition coefficient predictions and address challenges posed by limited experimental data. We expect applicability of the methods used beyond just toluene/water partition coefficients.\n\n## Keywords\n\n[graph neural network](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=graph%20neural%20network)\n\n[multifidelity learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=multifidelity%20learning)\n\n[partition coefficients](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=partition%20coefficients)\n\n## Comments\n\nYou are signed in as . Your name will appear\nwith any comment you post.\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\n\u200b\n\n300 words allowed\n\nYou can enter up to 300 words.\nPost comment\n\nLog in or register with\nORCID to comment\n\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our\n[Commenting Policy\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)\n\\- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can\n[find more information about how to use the commenting feature here\\\n\\[opens in a new tab\\]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs)\n.\n\nThis site is protected by reCAPTCHA and the Google\n[Privacy Policy\\\n\\[opens in a new tab\\]](https://policies.google.com/privacy)\nand\n[Terms of Service\\\n\\[opens in a new tab\\]](https://policies.google.com/terms)\napply.\n\n## Version History\n\nAug 08, 2024 Version 1\n\n## Metrics\n\n1,055\n\n536\n\n2\n\nViews\n\nDownloads\n\nView article\nCitations\n\n## License\n\nCC\n\nBY\n\nThe content is available under\n[CC BY 4.0\\[opens in a new tab\\]](https://creativecommons.org/licenses/by/4.0/)\n\n## DOI\n\n[10.26434/chemrxiv-2024-3t818\\\nD O I: 10.26434/chemrxiv-2024-3t818 \\[opens in a new tab\\]](https://doi.org/10.26434/chemrxiv-2024-3t818)\n\n## Funding\n\n**Deutsche Forschungsgemeinschaft**\n\n191948804\n\n**Deutsche Forschungsgemeinschaft**\n\n466417970\n\n## Author\u2019s competing interest statement\n\nThe author(s) have declared they have no conflict of interest with regard\nto this content\n\n## Ethics\n\nThe author(s) have declared ethics committee/IRB approval is not relevant\nto this content\n\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/66b359bdc9c6a5c07a4978f5"
    },
    {
      "title": "Distribution Learning for Molecular Regression",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2407.20475** (cs)\n\n\\[Submitted on 30 Jul 2024\\]\n\n# Title:Distribution Learning for Molecular Regression\n\nAuthors: [Nima Shoghi](https://arxiv.org/search/cs?searchtype=author&query=Shoghi,+N), [Pooya Shoghi](https://arxiv.org/search/cs?searchtype=author&query=Shoghi,+P), [Anuroop Sriram](https://arxiv.org/search/cs?searchtype=author&query=Anuroop), [Abhishek Das](https://arxiv.org/search/cs?searchtype=author&query=Das,+A)\n\nView a PDF of the paper titled Distribution Learning for Molecular Regression, by Nima Shoghi and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2407.20475) [HTML (experimental)](https://arxiv.org/html/2407.20475v1)\n\n> Abstract:Using \"soft\" targets to improve model performance has been shown to be effective in classification settings, but the usage of soft targets for regression is a much less studied topic in machine learning. The existing literature on the usage of soft targets for regression fails to properly assess the method's limitations, and empirical evaluation is quite limited. In this work, we assess the strengths and drawbacks of existing methods when applied to molecular property regression tasks. Our assessment outlines key biases present in existing methods and proposes methods to address them, evaluated through careful ablation studies. We leverage these insights to propose Distributional Mixture of Experts (DMoE): A model-independent, and data-independent method for regression which trains a model to predict probability distributions of its targets. Our proposed loss function combines the cross entropy between predicted and target distributions and the L1 distance between their expected values to produce a loss function that is robust to the outlined biases. We evaluate the performance of DMoE on different molecular property prediction datasets -- Open Catalyst (OC20), MD17, and QM9 -- across different backbone model architectures -- SchNet, GemNet, and Graphormer. Our results demonstrate that the proposed method is a promising alternative to classical regression for molecular property prediction tasks, showing improvements over baselines on all datasets and architectures.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Quantitative Methods (q-bio.QM) |\n| Cite as: | [arXiv:2407.20475](https://arxiv.org/abs/2407.20475) \\[cs.LG\\] |\n|  | (or [arXiv:2407.20475v1](https://arxiv.org/abs/2407.20475v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2407.20475](https://doi.org/10.48550/arXiv.2407.20475)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Nima Shoghi \\[ [view email](https://arxiv.org/show-email/d2c461e3/2407.20475)\\]\n\n**\\[v1\\]**\nTue, 30 Jul 2024 00:21:51 UTC (399 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Distribution Learning for Molecular Regression, by Nima Shoghi and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2407.20475)\n- [HTML (experimental)](https://arxiv.org/html/2407.20475v1)\n- [TeX Source](https://arxiv.org/src/2407.20475)\n- [Other Formats](https://arxiv.org/format/2407.20475)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2407.20475&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2407.20475&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-07](https://arxiv.org/list/cs.LG/2024-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2407.20475?context=cs)\n\n[q-bio](https://arxiv.org/abs/2407.20475?context=q-bio)\n\n[q-bio.QM](https://arxiv.org/abs/2407.20475?context=q-bio.QM)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2407.20475)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2407.20475)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2407.20475)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2407.20475&description=Distribution Learning for Molecular Regression) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2407.20475&title=Distribution Learning for Molecular Regression)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2407.20475) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2407.20475"
    },
    {
      "title": "Multi-task Neural Networks for QSAR Predictions",
      "text": "# Statistics > Machine Learning\n\n**arXiv:1406.1231** (stat)\n\n\\[Submitted on 4 Jun 2014\\]\n\n# Title:Multi-task Neural Networks for QSAR Predictions\n\nAuthors: [George E. Dahl](https://arxiv.org/search/stat?searchtype=author&query=Dahl,+G+E), [Navdeep Jaitly](https://arxiv.org/search/stat?searchtype=author&query=Jaitly,+N), [Ruslan Salakhutdinov](https://arxiv.org/search/stat?searchtype=author&query=Salakhutdinov,+R)\n\nView a PDF of the paper titled Multi-task Neural Networks for QSAR Predictions, by George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov\n\n[View PDF](https://arxiv.org/pdf/1406.1231)\n\n> Abstract:Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural networks in a recent QSAR competition, we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time. We conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature. We compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |\n| Cite as: | [arXiv:1406.1231](https://arxiv.org/abs/1406.1231) \\[stat.ML\\] |\n|  | (or [arXiv:1406.1231v1](https://arxiv.org/abs/1406.1231v1) \\[stat.ML\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.1406.1231](https://doi.org/10.48550/arXiv.1406.1231)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: George Dahl \\[ [view email](https://arxiv.org/show-email/f89aa69d/1406.1231)\\]\n\n**\\[v1\\]**\nWed, 4 Jun 2014 23:00:05 UTC (34 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Multi-task Neural Networks for QSAR Predictions, by George E. Dahl and Navdeep Jaitly and Ruslan Salakhutdinov\n\n- [View PDF](https://arxiv.org/pdf/1406.1231)\n- [TeX Source](https://arxiv.org/src/1406.1231)\n- [Other Formats](https://arxiv.org/format/1406.1231)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1406.1231&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1406.1231&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2014-06](https://arxiv.org/list/stat.ML/2014-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1406.1231?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1406.1231?context=cs.LG)\n\n[cs.NE](https://arxiv.org/abs/1406.1231?context=cs.NE)\n\n[stat](https://arxiv.org/abs/1406.1231?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1406.1231)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1406.1231)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1406.1231)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1406.1231&description=Multi-task Neural Networks for QSAR Predictions) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1406.1231&title=Multi-task Neural Networks for QSAR Predictions)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1406.1231) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/1406.1231"
    },
    {
      "title": "Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2310.04292** (cs)\n\n\\[Submitted on 6 Oct 2023 ( [v1](https://arxiv.org/abs/2310.04292v1)), last revised 18 Oct 2023 (this version, v3)\\]\n\n# Title:Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets\n\nAuthors: [Dominique Beaini](https://arxiv.org/search/cs?searchtype=author&query=Beaini,+D), [Shenyang Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+S), [Joao Alex Cunha](https://arxiv.org/search/cs?searchtype=author&query=Cunha,+J+A), [Zhiyi Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Z), [Gabriela Moisescu-Pareja](https://arxiv.org/search/cs?searchtype=author&query=Moisescu-Pareja,+G), [Oleksandr Dymov](https://arxiv.org/search/cs?searchtype=author&query=Dymov,+O), [Samuel Maddrell-Mander](https://arxiv.org/search/cs?searchtype=author&query=Maddrell-Mander,+S), [Callum McLean](https://arxiv.org/search/cs?searchtype=author&query=McLean,+C), [Frederik Wenkel](https://arxiv.org/search/cs?searchtype=author&query=Wenkel,+F), [Luis M\u00fcller](https://arxiv.org/search/cs?searchtype=author&query=M%C3%BCller,+L), [Jama Hussein Mohamud](https://arxiv.org/search/cs?searchtype=author&query=Mohamud,+J+H), [Ali Parviz](https://arxiv.org/search/cs?searchtype=author&query=Parviz,+A), [Michael Craig](https://arxiv.org/search/cs?searchtype=author&query=Craig,+M), [Micha\u0142 Koziarski](https://arxiv.org/search/cs?searchtype=author&query=Koziarski,+M), [Jiarui Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+J), [Zhaocheng Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+Z), [Cristian Gabellini](https://arxiv.org/search/cs?searchtype=author&query=Gabellini,+C), [Kerstin Klaser](https://arxiv.org/search/cs?searchtype=author&query=Klaser,+K), [Josef Dean](https://arxiv.org/search/cs?searchtype=author&query=Dean,+J), [Cas Wognum](https://arxiv.org/search/cs?searchtype=author&query=Wognum,+C), [Maciej Sypetkowski](https://arxiv.org/search/cs?searchtype=author&query=Sypetkowski,+M), [Guillaume Rabusseau](https://arxiv.org/search/cs?searchtype=author&query=Rabusseau,+G), [Reihaneh Rabbany](https://arxiv.org/search/cs?searchtype=author&query=Rabbany,+R), [Jian Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+J), [Christopher Morris](https://arxiv.org/search/cs?searchtype=author&query=Morris,+C), [Ioannis Koutis](https://arxiv.org/search/cs?searchtype=author&query=Koutis,+I), [Mirco Ravanelli](https://arxiv.org/search/cs?searchtype=author&query=Ravanelli,+M), [Guy Wolf](https://arxiv.org/search/cs?searchtype=author&query=Wolf,+G), [Prudencio Tossou](https://arxiv.org/search/cs?searchtype=author&query=Tossou,+P), [Hadrien Mary](https://arxiv.org/search/cs?searchtype=author&query=Mary,+H), [Therence Bois](https://arxiv.org/search/cs?searchtype=author&query=Bois,+T), [Andrew Fitzgibbon](https://arxiv.org/search/cs?searchtype=author&query=Fitzgibbon,+A), [B\u0142a\u017cej Banaszewski](https://arxiv.org/search/cs?searchtype=author&query=Banaszewski,+B), [Chad Martin](https://arxiv.org/search/cs?searchtype=author&query=Martin,+C), [Dominic Masters](https://arxiv.org/search/cs?searchtype=author&query=Masters,+D)\n\nView a PDF of the paper titled Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets, by Dominique Beaini and 34 other authors\n\n[View PDF](https://arxiv.org/pdf/2310.04292)\n\n> Abstract:Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2310.04292](https://arxiv.org/abs/2310.04292) \\[cs.LG\\] |\n|  | (or [arXiv:2310.04292v3](https://arxiv.org/abs/2310.04292v3) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2310.04292](https://doi.org/10.48550/arXiv.2310.04292)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dominic Masters \\[ [view email](https://arxiv.org/show-email/bd4f54df/2310.04292)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2310.04292v1)**\nFri, 6 Oct 2023 14:51:17 UTC (2,737 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2310.04292v2)**\nMon, 9 Oct 2023 10:22:11 UTC (2,735 KB)\n\n**\\[v3\\]**\nWed, 18 Oct 2023 11:06:43 UTC (2,735 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets, by Dominique Beaini and 34 other authors\n\n- [View PDF](https://arxiv.org/pdf/2310.04292)\n- [TeX Source](https://arxiv.org/src/2310.04292)\n- [Other Formats](https://arxiv.org/format/2310.04292)\n\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2310.04292&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2310.04292&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-10](https://arxiv.org/list/cs.LG/2023-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2310.04292?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.04292)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.04292)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.04292)\n\n### [1 blog link](https://arxiv.org/tb/2310.04292)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2310.04292&description=Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2310.04292&title=Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.co...",
      "url": "https://arxiv.org/abs/2310.04292"
    },
    {
      "title": "Multi-target prediction for dummies using two-branch neural networks",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2104.09967** (cs)\n\n\\[Submitted on 19 Apr 2021 ( [v1](https://arxiv.org/abs/2104.09967v1)), last revised 25 Oct 2021 (this version, v2)\\]\n\n# Title:Multi-target prediction for dummies using two-branch neural networks\n\nAuthors: [Dimitrios Iliadis](https://arxiv.org/search/cs?searchtype=author&query=Iliadis,+D), [Bernard De Baets](https://arxiv.org/search/cs?searchtype=author&query=De+Baets,+B), [Willem Waegeman](https://arxiv.org/search/cs?searchtype=author&query=Waegeman,+W)\n\nView a PDF of the paper titled Multi-target prediction for dummies using two-branch neural networks, by Dimitrios Iliadis and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2104.09967)\n\n> Abstract:Multi-target prediction (MTP) serves as an umbrella term for machine learning tasks that concern the simultaneous prediction of multiple target variables. Classical instantiations are multi-label classification, multivariate regression, multi-task learning, dyadic prediction, zero-shot learning, network inference, and matrix completion. Despite the significant similarities, all these domains have evolved separately into distinct research areas over the last two decades. This led to the development of a plethora of highly-engineered methods, and created a substantially-high entrance barrier for machine learning practitioners that are not experts in the field. In this work we present a generic deep learning methodology that can be used for a wide range of multi-target prediction problems. We introduce a flexible multi-branch neural network architecture, partially configured via a questionnaire that helps end-users to select a suitable MTP problem setting for their needs. Experimental results for a wide range of domains illustrate that the proposed methodology manifests a competitive performance compared to methods from specific MTP domains.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2104.09967](https://arxiv.org/abs/2104.09967) \\[cs.LG\\] |\n|  | (or [arXiv:2104.09967v2](https://arxiv.org/abs/2104.09967v2) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2104.09967](https://doi.org/10.48550/arXiv.2104.09967)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dimitrios Iliadis \\[ [view email](https://arxiv.org/show-email/8a163c09/2104.09967)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2104.09967v1)**\nMon, 19 Apr 2021 12:44:20 UTC (2,522 KB)\n\n**\\[v2\\]**\nMon, 25 Oct 2021 15:18:55 UTC (3,339 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Multi-target prediction for dummies using two-branch neural networks, by Dimitrios Iliadis and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2104.09967)\n- [TeX Source](https://arxiv.org/src/2104.09967)\n- [Other Formats](https://arxiv.org/format/2104.09967)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2104.09967&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2104.09967&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-04](https://arxiv.org/list/cs.LG/2021-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2104.09967?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2104.09967)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2104.09967)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2104.09967)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2104.html#abs-2104-09967) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2104-09967)\n\n[Bernard De Baets](https://dblp.uni-trier.de/search/author?author=Bernard%20De%20Baets)\n\n[Willem Waegeman](https://dblp.uni-trier.de/search/author?author=Willem%20Waegeman)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2104.09967&description=Multi-target prediction for dummies using two-branch neural networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2104.09967&title=Multi-target prediction for dummies using two-branch neural networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2104.09967) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2104.09967"
    },
    {
      "title": "Neural multi-task learning in drug design",
      "text": "<div><div><h2>References</h2><div><ol><li><p>Kirkpatrick, P. &amp; Ellis, C. Chemical space. <i>Nature</i> <b>432</b>, 823\u2013823 (2004).</p><p><a href=\"https://doi.org/10.1038%2F432823a\">Article</a>\u00a0\n <a href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2004Natur.432..823K\">ADS</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2cXhtVOht7rI\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Chemical%20space&amp;journal=Nature&amp;doi=10.1038%2F432823a&amp;volume=432&amp;pages=823-823&amp;publication_year=2004&amp;author=Kirkpatrick%2CP&amp;author=Ellis%2CC\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Reymond, J.-L. The chemical space project. <i>Acc. Chem. Res.</i> <b>48</b>, 722\u2013730 (2015).</p><p><a href=\"https://doi.org/10.1021%2Far500432k\">Article</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXivFWisb4%3D\">CAS</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25687211\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20chemical%20space%20project&amp;journal=Acc.%20Chem.%20Res.&amp;doi=10.1021%2Far500432k&amp;volume=48&amp;pages=722-730&amp;publication_year=2015&amp;author=Reymond%2CJ-L\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Ertl, P. Cheminformatics analysis of organic substituents: identification of the most common substituents, calculation of substituent properties, and automatic identification of drug-like bioisosteric groups. <i>J. Chem. Inf. Comput. Sci.</i> <b>43</b>, 374\u2013380 (2003).</p><p><a href=\"https://doi.org/10.1021%2Fci0255782\">Article</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD38Xpt1Cmtbs%3D\">CAS</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=12653499\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Cheminformatics%20analysis%20of%20organic%20substituents%3A%20identification%20of%20the%20most%20common%20substituents%2C%20calculation%20of%20substituent%20properties%2C%20and%20automatic%20identification%20of%20drug-like%20bioisosteric%20groups&amp;journal=J.%20Chem.%20Inf.%20Comput.%20Sci.&amp;doi=10.1021%2Fci0255782&amp;volume=43&amp;pages=374-380&amp;publication_year=2003&amp;author=Ertl%2CP\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Bohacek, R. S., McMartin, C. &amp; Guida, W. C. The art and practice of structure-based drug design: a molecular modeling perspective. <i>Med. Res. Rev.</i> <b>16</b>, 3\u201350 (1996).</p><p><a href=\"https://doi.org/10.1002%2F%28SICI%291098-1128%28199601%2916%3A1%3C3%3A%3AAID-MED1%3E3.0.CO%3B2-6\">Article</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK28XhtFyls78%3D\">CAS</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8788213\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20art%20and%20practice%20of%20structure-based%20drug%20design%3A%20a%20molecular%20modeling%20perspective&amp;journal=Med.%20Res.%20Rev.&amp;doi=10.1002%2F%28SICI%291098-1128%28199601%2916%3A1%3C3%3A%3AAID-MED1%3E3.0.CO%3B2-6&amp;volume=16&amp;pages=3-50&amp;publication_year=1996&amp;author=Bohacek%2CRS&amp;author=McMartin%2CC&amp;author=Guida%2CWC\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Lipinski, C. A., Lombardo, F., Dominy, B. W. &amp; Feeney, P. J. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. <i>Adv. Drug Deliv. Rev.</i> <b>23</b>, 3\u201325 (1997).</p><p><a href=\"https://doi.org/10.1016%2FS0169-409X%2896%2900423-1\">Article</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2sXktlKlsQ%3D%3D\">CAS</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Experimental%20and%20computational%20approaches%20to%20estimate%20solubility%20and%20permeability%20in%20drug%20discovery%20and%20development%20settings&amp;journal=Adv.%20Drug%20Deliv.%20Rev.&amp;doi=10.1016%2FS0169-409X%2896%2900423-1&amp;volume=23&amp;pages=3-25&amp;publication_year=1997&amp;author=Lipinski%2CCA&amp;author=Lombardo%2CF&amp;author=Dominy%2CBW&amp;author=Feeney%2CPJ\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Schneider, G. <i>De novo Molecular Design</i> (John Wiley and Sons, 2013).</p></li><li><p>Sadybekov, A. V. &amp; Katritch, V. Computational approaches streamlining drug discovery. <i>Nature</i> <b>616</b>, 673\u2013685 (2023).</p><p><a href=\"https://doi.org/10.1038%2Fs41586-023-05905-z\">Article</a>\u00a0\n <a href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2023Natur.616..673S\">ADS</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3sXoslaku7g%3D\">CAS</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37100941\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Computational%20approaches%20streamlining%20drug%20discovery&amp;journal=Nature&amp;doi=10.1038%2Fs41586-023-05905-z&amp;volume=616&amp;pages=673-685&amp;publication_year=2023&amp;author=Sadybekov%2CAV&amp;author=Katritch%2CV\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Schneider, P. et al. Rethinking drug design in the artificial intelligence era. <i>Nat. Rev. Drug Discov.</i> <b>19</b>, 353\u2013364 (2019).</p><p><a href=\"https://doi.org/10.1038%2Fs41573-019-0050-3\">Article</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=31801986\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Rethinking%20drug%20design%20in%20the%20artificial%20intelligence%20era&amp;journal=Nat.%20Rev.%20Drug%20Discov.&amp;doi=10.1038%2Fs41573-019-0050-3&amp;volume=19&amp;pages=353-364&amp;publication_year=2019&amp;author=Schneider%2CP\">\n Google Scholar</a>\u00a0\n </p></li><li><p>LeCun, Y., Bengio, Y. &amp; Hinton, G. Deep learning. <i>Nature</i> <b>521</b>, 436\u2013444 (2015).</p><p><a href=\"https://doi.org/10.1038%2Fnature14539\">Article</a>\u00a0\n <a href=\"http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&amp;bibcode=2015Natur.521..436L\">ADS</a>\u00a0\n <a href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2MXht1WlurzP\">CAS</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26017442\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning&amp;journal=Nature&amp;doi=10.1038%2Fnature14539&amp;volume=521&amp;pages=436-444&amp;publication_year=2015&amp;author=LeCun%2CY&amp;author=Bengio%2CY&amp;author=Hinton%2CG\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Schmidhuber, J. Deep learning in neural networks: an overview. <i>Neural Networks</i> <b>61</b>, 85\u2013117 (2015).</p><p><a href=\"https://doi.org/10.1016%2Fj.neunet.2014.09.003\">Article</a>\u00a0\n <a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25462637\">PubMed</a>\u00a0\n <a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20in%20neural%20networks%3A%20an%20overview&amp;journal=Neural%20Networks&amp;doi=10.1016%2Fj.neunet.2014.09.003&amp;volume=61&amp;pages=85-117&amp;publication_year=2015&amp;author=Schmidhuber%2CJ\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Krizhevsky, A., Sutskever, I. &amp; Hinton, G. E. ImageNet classification with deep convolutional neural networks. <i>Adv. Neural Inf. Process. Syst.</i> <b>25</b>, 1106\u20131114 (2012).</p></li><li><p>Vaswani, A. et al. Attention is all you need. <i>Adv. Neural Inf. Process. Syst</i>. <b>30</b>, 5998\u20136008 (2017).</p></li><li><p>Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. <i>Nature</i> <b>596</b>, 583\u2013589 (2021).</p><p><a href=\"https://doi.org/10.1038%2Fs41586-021-03819-2\">Article</a>\u00a0\n <a href=\"http://adsabs.harvard.edu/cgi-bin/nph-d...",
      "url": "https://www.nature.com/articles/s42256-023-00785-4"
    },
    {
      "title": "Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?",
      "text": "# Statistics > Machine Learning\n\n**arXiv:2201.05340** (stat)\n\n\\[Submitted on 14 Jan 2022\\]\n\n# Title:Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?\n\nAuthors: [Lena Schmid](https://arxiv.org/search/stat?searchtype=author&query=Schmid,+L), [Alexander Gerharz](https://arxiv.org/search/stat?searchtype=author&query=Gerharz,+A), [Andreas Groll](https://arxiv.org/search/stat?searchtype=author&query=Groll,+A), [Markus Pauly](https://arxiv.org/search/stat?searchtype=author&query=Pauly,+M)\n\nView a PDF of the paper titled Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?, by Lena Schmid and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2201.05340)\n\n> Abstract:Tree-based ensembles such as the Random Forest are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether we separately fit univariate models or directly follow a multivariate approach. For the latter, several possibilities exist that are, e.g. based on modified splitting or stopping rules for multi-output regression. In this work we compare these methods in extensive simulations to help in answering the primary question when to use multivariate ensemble techniques.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2201.05340](https://arxiv.org/abs/2201.05340) \\[stat.ML\\] |\n|  | (or [arXiv:2201.05340v1](https://arxiv.org/abs/2201.05340v1) \\[stat.ML\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2201.05340](https://doi.org/10.48550/arXiv.2201.05340)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Lena Schmid \\[ [view email](https://arxiv.org/show-email/6ebcdec5/2201.05340)\\]\n\n**\\[v1\\]**\nFri, 14 Jan 2022 08:44:25 UTC (341 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?, by Lena Schmid and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2201.05340)\n- [TeX Source](https://arxiv.org/src/2201.05340)\n- [Other Formats](https://arxiv.org/format/2201.05340)\n\n[![license icon](https://arxiv.org/icons/licenses/by-nc-nd-4.0.png)view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2201.05340&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2201.05340&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2022-01](https://arxiv.org/list/stat.ML/2022-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2201.05340?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2201.05340?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2201.05340?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2201.05340)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2201.05340)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2201.05340)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2201.05340&description=Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2201.05340&title=Machine Learning for Multi-Output Regression: When should a holistic multivariate approach be preferred over separate univariate ones?)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2201.05340) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2201.05340"
    },
    {
      "title": "Distribution Learning for Molecular Regression",
      "text": "Now on home page\n## ADS\n## Distribution Learning for Molecular Regression\n- [Shoghi, Nima](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Shoghi%2C+Nima%22);\n- [Shoghi, Pooya](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Shoghi%2C+Pooya%22);\n- [Sriram, Anuroop](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Sriram%2C+Anuroop%22);\n- [Das, Abhishek](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Das%2C+Abhishek%22)\n#### Abstract\nUsing \"soft\" targets to improve model performance has been shown to be effective in classification settings, but the usage of soft targets for regression is a much less studied topic in machine learning. The existing literature on the usage of soft targets for regression fails to properly assess the method's limitations, and empirical evaluation is quite limited. In this work, we assess the strengths and drawbacks of existing methods when applied to molecular property regression tasks. Our assessment outlines key biases present in existing methods and proposes methods to address them, evaluated through careful ablation studies. We leverage these insights to propose Distributional Mixture of Experts (DMoE): A model-independent, and data-independent method for regression which trains a model to predict probability distributions of its targets. Our proposed loss function combines the cross entropy between predicted and target distributions and the L1 distance between their expected values to produce a loss function that is robust to the outlined biases. We evaluate the performance of DMoE on different molecular property prediction datasets -- Open Catalyst (OC20), MD17, and QM9 -- across different backbone model architectures -- SchNet, GemNet, and Graphormer. Our results demonstrate that the proposed method is a promising alternative to classical regression for molecular property prediction tasks, showing improvements over baselines on all datasets and architectures.\nPublication:\narXiv e-prints\nPub Date:July 2024DOI:\n[10.48550/arXiv.2407.20475](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240720475S/doi:10.48550/arXiv.2407.20475)\narXiv:[arXiv:2407.20475](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240720475S/arXiv:2407.20475)Bibcode:[2024arXiv240720475S](https://ui.adsabs.harvard.edu/abs/2024arXiv240720475S/abstract)Keywords:\n- Computer Science - Machine Learning;\n- Quantitative Biology - Quantitative Methods\nfull text sources\nPreprint\n\\|\n\ud83c\udf13",
      "url": "https://ui.adsabs.harvard.edu/abs/2024arXiv240720475S/abstract"
    }
  ]
}