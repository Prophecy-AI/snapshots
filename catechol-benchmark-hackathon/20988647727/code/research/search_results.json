{
  "query": "How to improve neural network predictions for small chemistry datasets with leave-one-out cross validation?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving neural\u2011network predictions on very small chemistry datasets while using leave\u2011one\u2011out (LOO) cross\u2011validation can be done by combining careful data handling, probabilistic modeling, and rigorous validation\u202f\u2013\u202fall of which are highlighted in recent cheminformatics studies ([Thomas\u202fet\u202fal.,\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10);\u202f[Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b);\u202f[Rosseinsky\u202fet\u202fal.,\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c);\u202f[Vehtari\u202fet\u202fal.,\u202f2019](https://proceedings.mlr.press/v97/magnusson19a.html)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Select robust molecular representations**  \n   - Encode each molecule with several complementary descriptors (e.g., circular fingerprints, graph\u2011based embeddings, physicochemical descriptors) and test them separately; diverse representations help compensate for the limited number of samples ([Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  \n\n2. **Use a probabilistic neural network**  \n   - Train a Bayesian or Monte\u2011Carlo dropout NN so that each prediction is accompanied by an uncertainty estimate. Probabilistic models have been shown to give better calibrated outputs on low\u2011data chemical sets ([Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  \n\n3. **Apply exact LOO or PS\u2011LOO for evaluation**  \n   - For each molecule, fit the model on the remaining\u202fN\u20111 points and predict the held\u2011out sample. When exact LOO is computationally heavy, use Pareto\u2011smoothed importance sampling LOO (PS\u2011LOO) to obtain reliable estimates without refitting the full model ([Vehtari\u202fet\u202fal.,\u202f2019](https://proceedings.mlr.press/v97/magnusson19a.html)).  \n\n4. **Guard against similarity bias with leave\u2011one\u2011cluster\u2011out (LOCO\u2011CV)**  \n   - Cluster the dataset by chemical scaffold or similarity (e.g., using Bemis\u2011Murcko scaffolds) and perform LOO at the cluster level. This mimics real\u2011world \u201cnovel scaffold\u201d prediction and prevents overly optimistic performance caused by near\u2011duplicate molecules ([Rosseinsky\u202fet\u202fal.,\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)).  \n\n5. **Nest hyper\u2011parameter tuning inside each LOO fold**  \n   - Within every training split, run an inner CV (e.g., 5\u2011fold) to select learning\u2011rate, dropout rate, weight decay, etc. This avoids leaking information from the test point into model selection, a common pitfall in small\u2011data studies ([Thomas\u202fet\u202fal.,\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10)).  \n\n6. **Regularize and augment the data**  \n   - Apply strong regularization (dropout, L2 weight decay) and augment SMILES strings by randomizing atom order or generating tautomeric variants. Augmentation effectively enlarges the training set and improves generalization on tiny datasets (a practice endorsed in low\u2011data cheminformatics work).  \n\n7. **Assess calibration and report uncertainty metrics**  \n   - After LOO, plot reliability diagrams or compute expected calibration error (ECE). Discard models whose uncertainties are poorly calibrated, as calibrated predictions are essential for trustworthy decision\u2011making on small chemical sets ([Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  \n\nFollowing these seven steps\u2014robust representations, probabilistic modeling, proper LOO/LOCO evaluation, nested hyper\u2011parameter search, regularization/augmentation, and calibration checks\u2014will markedly improve the reliability and accuracy of neural\u2011network predictions on small chemistry datasets.",
      "url": ""
    },
    {
      "title": "Cross-validation pitfalls when selecting and assessing regression and classification models",
      "text": "Cross-validation pitfalls when selecting and assessing regression and classification models | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/1758-2946-6-10?)\n# Cross-validation pitfalls when selecting and assessing regression and classification models\n* Methodology\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:29 March 2014\n* Volume\u00a06, article\u00a0number10, (2014)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nCross-validation pitfalls when selecting and assessing regression and classification models\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n* [Damjan Krstajic](#auth-Damjan-Krstajic-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3),\n* [Ljubomir J Buturovic](#auth-Ljubomir_J-Buturovic-Aff3)[3](#Aff3),\n* [David E Leahy](#auth-David_E-Leahy-Aff4)[4](#Aff4)&amp;\n* \u2026* [Simon Thomas](#auth-Simon-Thomas-Aff5)[5](#Aff5)Show authors\n* 150kAccesses\n* 902Citations\n* 42Altmetric\n* 1Mention\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10/metrics)\n## Abstract\n### Background\nWe address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.\n### Methods\nWe describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case.\n### Results\nWe show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models.\n### Conclusions\nWe demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-024-06630-y/MediaObjects/10994_2024_6630_Fig1_HTML.png)\n### [Reducing cross-validation variance through seed blocking in hyperparameter tuning](https://link.springer.com/10.1007/s10994-024-06630-y?fromPaywallRec=false)\nArticle17 February 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-31041-7?as&#x3D;webp)\n### [Enhancement of Cross Validation Using Hybrid Visual and Analytical Means with Shannon Function](https://link.springer.com/10.1007/978-3-030-31041-7_29?fromPaywallRec=false)\nChapter\u00a9 2020\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42979-022-01051-x/MediaObjects/42979_2022_1051_Figa_HTML.png)\n### [An Efficient Ridge Regression Algorithm with Parameter Estimation for Data Analysis in Machine Learning](https://link.springer.com/10.1007/s42979-022-01051-x?fromPaywallRec=false)\nArticle23 February 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Compound Screening](https://jcheminf.biomedcentral.com/subjects/compound-screening)\n* [Linear Models and Regression](https://jcheminf.biomedcentral.com/subjects/linear-models-and-regression)\n* [Learning algorithms](https://jcheminf.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Molecular Target Validation](https://jcheminf.biomedcentral.com/subjects/molecular-target-validation)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Background\nAllen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)], Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] and Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)], independently introduced cross-validation as a way of estimating parameters for predictive models in order to improve predictions. Allen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)] proposed the PRESS (Prediction Sum of Squares) criteria, equivalent to leave-one-out cross-validation, for problems with selection of predictors and suggested it for general use. Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] suggested the use of leave-one-out cross-validation for estimating model parameters and for assessing their predictive error. It is important to note that Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] was the first to clearly differentiate between the use of cross-validation to select the model (\u201ccross-validatory choice\u201d) and to assess the model (\u201ccross-validatory assessment\u201d). Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)] introduced the Predictive Sample Reuse Method, a method equivalent to V-fold cross-validation, arguing that it improves predictive performance of the cross-validatory choice, at a cost of introducing pseudo-randomness in the process. Since then, cross-validation, with its different varieties, has been investigated extensively and, due to its universality, gained popularity in statistical modelling.\nIn an ideal situation we would have enough data to train and validate our models (training samples) and have separate data for assessing the quality of our model (test samples). Both training and test samples would need to be sufficiently large and diverse in order to be represenatitive. However such data rich situations are rare in life sciences, including QSAR. A major problem with selection and assessment of models is that we usually only have information from the training samples, and it is therefore not feasible to calculate a test error. However, even though we cannot calculate the test error, it is possible to estimate the expected test error using training samples. It can be shown that the expected test error is the...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/dd/d2dd00039c)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00046f)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00056c)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D2DD00039C](https://doi.org/10.1039/D2DD00039C)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2022, **1**, 763-778\n\n# Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties [\u2020](https://pubs.rsc.org/pubs.rsc.org\\#fn1)\n\nSamantha\nDurdy\n\\*ab,\nMichael W.\nGaultois\nbc,\nVladimir V.\nGusev\nbc,\nDanushka\nBollegala\nab and Matthew J.\nRosseinsky\nbcaDepartment of Computer Science, University of Liverpool, Ashton Street, Liverpool, L69 3BX, UK. E-mail: [samantha.durdy@liverpool.ac.uk](mailto:samantha.durdy@liverpool.ac.uk)bLeverhulme Research Centre for Functional Materials Design, University of Liverpool, 51 Oxford Street, Liverpool, L7 3NY, UKcDepartment of Chemistry, University of Liverpool, Crown St, Liverpool, L69 7ZD, UK\n\nReceived\n9th May 2022\n, Accepted 31st August 2022\n\nFirst published on 2nd September 2022\n\n## Abstract\n\nWith machine learning being a popular topic in current computational materials science literature, creating representations for compounds has become common place. These representations are rarely compared, as evaluating their performance \u2013 and the performance of the algorithms that they are used with \u2013 is non-trivial. With many materials datasets containing bias and skew caused by the research process, leave one cluster out cross validation (LOCO-CV) has been introduced as a way of measuring the performance of an algorithm in predicting previously unseen groups of materials. This raises the question of the impact, and control, of the range of cluster sizes on the LOCO-CV measurement outcomes. We present a thorough comparison between composition-based representations, and investigate how kernel approximation functions can be used to better separate data to enhance LOCO-CV applications. We find that domain knowledge does not improve machine learning performance in most tasks tested, with band gap prediction being the notable exception. We also find that the radial basis function improves the linear separability of chemical datasets in all 10 datasets tested and provides a framework for the application of this function in the LOCO-CV process to improve the outcome of LOCO-CV measurements regardless of machine learning algorithm, choice of metric, and choice of compound representation. We recommend kernelised LOCO-CV as a training paradigm for those looking to measure the extrapolatory power of an algorithm on materials data.\n\n## 1 Introduction\n\nRecent advances in materials science have seen a plethora of research into application of machine learning (ML) algorithms. Much of this research has focused on supervised ML methods, such as random forests (RFs) and neural networks. More recently, authors have laid out the best practices to help unify and progress this field. [1\u20134](https://pubs.rsc.org/pubs.rsc.org#cit1)\n\nData representation can play a large role in the performance of ML algorithms; however, optimum choice of representation is not always apparent. In materials science it is often difficult to choose an appropriate representation due to variability in the ML task and in the nature of the chemistry, composition and structures of the materials studied. Additionally, some properties of a material, such its crystal structure in the case of crystalline materials, may not be known until its synthesis. Accordingly, many studies derive representations from either the ratios of elements in the chemical composition, or from domain knowledge-based properties (referred to as features) of these elements, or both, in a process called \u201cfeaturisation\u201d.\n\nGiven the ubiquity of featurisation methods such as those presented here in materials applications, it is important to evaluate the statistical advantage of specific feature sets. [5](https://pubs.rsc.org/pubs.rsc.org#cit5) Section 2.1 overviews different featurisation techniques and how their effectiveness has been previously reported. We expand on this evaluation in Section 3.1, in which seven representations are investigated across five case studies from the literature to explore how these representations perform in published ML tasks. These cases thus represent practical applications, rather than constructed tasks. Each of these representations is also compared to a random projection of equal size to establish the performance benefit of domain knowledge over random noise.\n\nEvaluating the generalisability of ML models is a known challenge across data science, and is of particular concern in materials science, where data sets are of limited size compared with other application areas for ML, and often biased towards historically interesting materials or those closely related to known high-performance materials for certain performance metrics. Typically, models are evaluated on test sets separate from their training data, through a consistent train:test split or N-fold cross validation. However, this does not consider skew in a dataset. In chemical datasets, families of promising materials are often explored more thoroughly than the domain as a whole, which introduces bias and reduces the generalisability of ML models because the data they are trained and tested on are not sampled in a way representative of the domain of target chemistries to be screened with these models. Investigations into how such skew can affect ML models has seen that this skew can result in overfitting [6](https://pubs.rsc.org/pubs.rsc.org#cit6) and that more skewed datasets require more data points in order to train models to achieve similar predictive performance when compared to models trained on less skewed datasets. [7](https://pubs.rsc.org/pubs.rsc.org#cit7)\n\nLeave one cluster out cross validation (LOCO-CV) was suggested to combat this, [8](https://pubs.rsc.org/pubs.rsc.org#cit8) using K-means clustering to exclude similar families of materials from the training set to measure the extrapolatory power of an ML algorithm (its ability to predict the performance of materials with chemistries qualitatively different from the training set). The value of such an approach can be seen in the case of predicting new classes of superconductors. One may choose to remove cuprate superconductors from the training set, and if an ML model can then successfully predict the existence of cuprate superconductors without prior knowledge of them, we can conclude that model is likely to perform better at predicting new classes of superconductors than a model which could not predict the existence of cuprate superconductors. LOCO-CV provides an algorithmic framework to measure the performance of models on predicting new classes of materials by defining these classes as clusters found by the K-means clustering algorithm. Application and implementation of this algorithm is discussed further in Section 2.2.1.\n\nWhile differences in cluster sizes in this domain are expected, it has been observed that clusters found with K-means can differ in size by orders of magnitude, [9](https://pubs.rsc.org/pubs.rsc.org#cit9) which can pose a practical challenge to adoption of this method. With such differences in cluster size, LOCO-CV measurements can represent the performance of an algorithm on a small training set rather than the performance of an algorithm in extrapolation. As representation plays a role in clustering, it is pertinent to investigate the issues of representation and clustering together, even though the representation used in clustering does not need to be the same as that used to train the model ( [F...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c"
    },
    {
      "title": "Bayesian leave-one-out cross-validation for large data",
      "text": "Bayesian leave-one-out cross-validation for large data\n[![[International Conference on Machine Learning Logo]](https://proceedings.mlr.press/v97/assets/images/logo-pmlr.svg)](https://proceedings.mlr.press/)Proceedings of Machine Learning Research\n[[edit](https://github.com/mlresearch/v97/edit/gh-pages/_posts/2019-05-24-magnusson19a.md)]\n# Bayesian leave-one-out cross-validation for large data\nM\u00e5ns Magnusson,Michael Andersen,Johan Jonasson,Aki Vehtari\n*Proceedings of the 36th International Conference on Machine Learning*,PMLR 97:4244-4253,2019.\n#### Abstract\nModel inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.\n#### Cite this Paper\nBibTeX\n`@InProceedings{pmlr-v97-magnusson19a,\ntitle = {{B}ayesian leave-one-out cross-validation for large data},\nauthor = {Magnusson, M{\\\\aa}ns and Andersen, Michael and Jonasson, Johan and Vehtari, Aki},\nbooktitle = {Proceedings of the 36th International Conference on Machine Learning},\npages = {4244--4253},\nyear = {2019},\neditor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},\nvolume = {97},\nseries = {Proceedings of Machine Learning Research},\nmonth = {09--15 Jun},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v97/magnusson19a/magnusson19a.pdf},\nurl = {https://proceedings.mlr.press/v97/magnusson19a.html},\nabstract = {Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.}\n}`\nCopy to ClipboardDownload\nEndnote\n`%0 Conference Paper\n%T Bayesian leave-one-out cross-validation for large data\n%A M\u00e5ns Magnusson\n%A Michael Andersen\n%A Johan Jonasson\n%A Aki Vehtari\n%B Proceedings of the 36th International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2019\n%E Kamalika Chaudhuri\n%E Ruslan Salakhutdinov\t%F pmlr-v97-magnusson19a\n%I PMLR\n%P 4244--4253\n%U https://proceedings.mlr.press/v97/magnusson19a.html\n%V 97\n%X Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.`\nCopy to ClipboardDownload\nAPA\n`Magnusson, M., Andersen, M., Jonasson, J. & Vehtari, A.. (2019). Bayesian leave-one-out cross-validation for large data.*Proceedings of the 36th International Conference on Machine Learning*, in*Proceedings of Machine Learning Research*97:4244-4253 Available from https://proceedings.mlr.press/v97/magnusson19a.html.`\nCopy to ClipboardDownload\n#### Related Material\n* [Download PDF](http://proceedings.mlr.press/v97/magnusson19a/magnusson19a.pdf)\n* [Supplementary PDF](http://proceedings.mlr.press/v97/magnusson19a/magnusson19a-supp.pdf)\n* [Code](https://github.com/MansMeg/loo)",
      "url": "https://proceedings.mlr.press/v97/magnusson19a.html"
    },
    {
      "title": "",
      "text": "Gary Tom \nDepartment of Chemistry\nChemical Physics Theory Group\nUniversity of Toronto\nTorontoONCanada\n\nDepartment of Computer Science\nUniversity of Toronto\nTorontoONCanada\n\nRiley J Hickman \nDepartment of Chemistry\nChemical Physics Theory Group\nUniversity of Toronto\nTorontoONCanada\n\nDepartment of Computer Science\nUniversity of Toronto\nTorontoONCanada\n\nVector Institute for Artificial Intelligence\nTorontoONCanada\n\nAniket Zinzuwadia \nHarvard Medical School\nHarvard University\nBostonMAUSA\n\nAfshan Mohajeri \nDepartment of Chemistry\nShiraz University\nShirazIran\n\nBenjamin Sanchez-Lengeling \nGoogle Research\nBrain Team\n\nAl\u00e1n Aspuru-Guzik \nDepartment of Chemistry\nChemical Physics Theory Group\nUniversity of Toronto\nTorontoONCanada\n\nDepartment of Computer Science\nUniversity of Toronto\nTorontoONCanada\n\nVector Institute for Artificial Intelligence\nTorontoONCanada\n\nDepartment of Chemical Engineering & Applied Chemistry\nUniversity of Toronto\nTorontoONCanada\n\nDepartment of Materials Science & Engineering\nUniversity of Toronto\nTorontoONCanada\n\nLebovic Fellow\nCanadian Institute for Advanced Research\nTorontoONCanada\n\nCalibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS\n(Dated: December 7, 2022)\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (< 2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (binary, regression) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution data via ablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n\nI. INTRODUCTION\n\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure-activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making. 1 Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general, 2 enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications. 3 When compared to traditional ab initio techniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\n\nTo date, many studies consider molecular property prediction tasks where training data is plentiful. 4,5 In * alan@aspuru.com real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (\u2264 2000 data points) are available due to the expense associated with generating accurate property measurements (monetary, resource, labour, or ethical limitations). Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\n\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on 1) the generalizability, the ability of a model to predict accurately on new chemical data, and 2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions.\n\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization",
      "url": "https://export.arxiv.org/pdf/2212.01574v2.pdf"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    },
    {
      "title": "Step Forward Cross Validation for Bioactivity Prediction: Out of Distribution Validation in Drug Discovery",
      "text": "<div><article><section></section><section><section><h2>Abstract</h2>\n<p>Recent advances in machine learning methods for materials science have significantly enhanced accurate predictions of the properties of novel materials. Here, we explore whether these advances can be adapted to drug discovery by addressing the problem of prospective validation - the assessment of the performance of a method on out-of-distribution data. First, we tested whether k-fold n-step forward cross-validation could improve the accuracy of out-of-distribution small molecule bioactivity predictions. We found that it is more helpful than conventional random split cross-validation in describing the accuracy of a model in real-world drug discovery settings. We also analyzed discovery yield and novelty error, finding that these two metrics provide an understanding of the applicability domain of models and an assessment of their ability to predict molecules with desirable bioactivity compared to other small molecules. Based on these results, we recommend incorporating a k-fold n-step forward cross-validation and these metrics when building state-of-the-art models for bioactivity prediction in drug discovery.</p></section><section><h2>1. Introduction</h2>\n<p>Recently, many advancements have been made in developing computational methods for predicting properties in materials science. Suitable validation methods have also been introduced to estimate the performance of these predictive models.<sup><a href=\"#R1\">1</a>\u2013<a href=\"#R3\">3</a></sup> Here, we investigated whether these validation methods can be translated into the field of drug discovery. Here, we address the problem of prospective validation. Since predictive models are trained and validated on the experimentally measured activity of libraries of compounds, real-world use in drug discovery requires strong performance on out-of-distribution data.<sup><a href=\"#R4\">4</a></sup> This is because the goal is often to accurately predict the properties of compounds that have not been synthesized yet. Inadequate prospective validation is a common issue in the drug discovery literature, often creating a mismatch between published studies and real-world use.<sup><a href=\"#R5\">5</a>,<a href=\"#R6\">6</a></sup> This problem is less severe in domains such as materials science, where the underlying physical principles are often known<sup><a href=\"#R3\">3</a>,<a href=\"#R7\">7</a></sup>, and protein folding, where evolution led to a lower-dimensional underlying space of possible solutions<sup><a href=\"#R8\">8</a></sup>. However, this problem is significant in drug discovery because the chemical space is vast (more than 10^60 small molecules) and only explored to a limited extent, making it challenging to extrapolate to novel chemical series.<sup><a href=\"#R5\">5</a></sup></p>\n<p>Benchmarking state-of-the-art models is more reliable for real-world decision-making when predicting compounds different from those in the training data space. However, most studies use cross-validation (CV) to evaluate models by randomly splitting the datasets for training versus testing.<sup><a href=\"#R9\">9</a></sup> This approach typically suffers from a limited applicability domain because test compounds are often similar to compounds in the training set. To mitigate this problem, splitting datasets by chemical scaffold or time-split has been proposed.<sup><a href=\"#R10\">10</a>,<a href=\"#R11\">11</a></sup> Even though these splits could be repeated for multiple external test sets (for example, repeated nested cross-validation), studies usually lack a detailed analysis of how variations in the drug discovery landscape and chemical space influence outcomes by differentiating between compounds unlikely to be drug-like and those that have desirable bioactivity and physicochemical properties.</p>\n<p>To overcome these problems, one can take inspiration from machine learning (ML) studies for materials discovery, where validation and evaluation strategies have been developed for effective prospective discovery, i.e., identifying materials whose properties lie outside the range of training data.<sup><a href=\"#R3\">3</a>,<a href=\"#R12\">12</a></sup> This trend makes sense because, in materials discovery, the goal is often to discover materials with a higher or lower property of interest (e.g., conductivity, band gap, etc.) than already known materials.<sup><a href=\"#R2\">2</a></sup> In one aspect, drug discovery is similar, as models are trained on data from previously known small molecules and then used to predict the bioactivity of compounds optimized to have desirable properties.</p>\n<p>Learning from these developments, we propose implementing a validation method and two metrics commonly used in prospective validation from materials science to the search for small molecules in drug discovery: (a) k-fold n-step forward cross-validation<sup><a href=\"#R12\">12</a></sup>, (b) <em>novelty error,</em> and (c) <em>discovery yield</em><sup><a href=\"#R2\">2</a></sup>.</p>\n<p>During drug discovery, several properties of a compound are optimized simultaneously. One of the goals is to decrease logP, the logarithm of the partition coefficient (P) of a compound between n-octanol and water, a standard measure of hydrophobicity.<sup><a href=\"#R13\">13</a>,<a href=\"#R14\">14</a></sup> Moderate logP values (typically between 1 and 3) are preferred in drug candidates to balance lipophilicity and hydrophilicity, enhancing oral bioavailability through good lipid membrane permeability and adequate aqueous solubility. A moderate logP value also ensures proper drug distribution, avoiding excessive accumulation in fatty tissues or insufficient penetration through cell membranes.<sup><a href=\"#R15\">15</a></sup> Therefore, we implemented a sorted k-fold n-step forward cross-validation (SFCV) to validate models, where the training and test datasets are selected based on continuous blocks of decreasing logP. When implementing SFCV, it is essential to ensure that the folds in the later iterations represent the desired logP values, which should be moderate (between 1 and 3). One could then assess whether a model fails to accurately predict compounds with desired bioactivity compared to other small molecules using discovery yield. Novelty error shows whether models can generalize on new, unseen data that differ significantly from the data on which the model was trained.</p>\n<p>This is similar to using the applicability domain<sup><a href=\"#R4\">4</a></sup> and distance to model measures<sup><a href=\"#R16\">16</a></sup>. Overall, we present these validation and evaluation metrics to the specific needs of toxicity and protein target prediction for small molecules.<sup><a href=\"#R17\">17</a></sup></p></section><section><h2>2. Methods</h2>\n<section><h3>2.1. Dataset</h3>\n<p>Models for predicting compound bioactivity require training datasets of activity readouts for many compounds. An activity readout is often expressed as an IC50 value, the concentration at which a particular biological response is reduced to half (50%) of the original signal. While several datasets have binary readouts (active/inactive) for compounds towards given protein targets, these datasets are often noisy or employ arbitrary thresholds for binarising activity. Recently, it was demonstrated that combining data from different assay measurements is a significant noise source for such datasets.<sup><a href=\"#R18\">18</a></sup> Therefore, we restricted this study to having clean and single measurement type data, i.e., IC50 values. Although the actual safety and potency of a compound depends on the dose and Cmax value (i.e., the maximum concentration in plasma in the organism) and is not inherent to the IC50 of protein binding in a cell system, this study does not consider Cmax due to insufficient data in the public domain.<sup><a href=\"#R19\">19</a>,<a href=\"#R20\">20</a></sup> Following previous studies, we selected the three relevant protein targets: h...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11245006"
    },
    {
      "title": "Beware of q2!",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS1093326301001231)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S1093326301001231/purchase)\n- [Patient Access](https://www.elsevier.com/open-science/science-and-society/access-for-healthcare-and-patients)\n- Other access options\n\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-snippets)\n- [References (46)](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-references)\n- [Cited by (3546)](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling)\n\n## [Journal of Molecular Graphics and Modelling](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling)\n\n[Volume 20, Issue 4](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling/vol/20/issue/4), January 2002, Pages 269-276\n\n[![Journal of Molecular Graphics and Modelling](https://ars.els-cdn.com/content/image/1-s2.0-S1093326324X00029-cov150h.gif)](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling/vol/20/issue/4)\n\n# Beware of _q_ 2!\n\nAuthor links open overlay panelAlexanderGolbraikh, AlexanderTropsha\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/S1093-3263(01)00123-1](https://doi.org/10.1016/S1093-3263(01)00123-1) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S1093326301001231&orderBeanReset=true)\n\n## Abstract\n\nValidation is a crucial aspect of any quantitative structure\u2013activity relationship (QSAR) modeling. This paper examines one of the most popular validation criteria, leave-one-out cross-validated _R_ 2 (LOO _q_ 2). Often, a high value of this statistical characteristic ( _q_ 2>0.5) is considered as a proof of the high predictive ability of the model. In this paper, we show that this assumption is generally incorrect. In the case of 3D QSAR, the lack of the correlation between the high LOO _q_ 2 and the high predictive ability of a QSAR model has been established earlier \\[Pharm. Acta Helv. 70 (1995) 149; J. Chemomet. 10 (1996) 95; J. Med. Chem. 41 (1998) 2553\\]. In this paper, we use two-dimensional (2D) molecular descriptors and _k_ nearest neighbors ( _k_ NN) QSAR method for the analysis of several datasets. No correlation between the values of _q_ 2 for the training set and predictive ability for the test set was found for any of the datasets. Thus, the high value of LOO _q_ 2 appears to be the necessary but not the [sufficient condition](https://www.sciencedirect.com/topics/computer-science/sufficient-condition) for the model to have a high [predictive power](https://www.sciencedirect.com/topics/computer-science/predictive-power). We argue that this is the general property of QSAR models developed using LOO cross-validation. We emphasize that the external validation is the only way to establish a reliable QSAR model. We formulate a set of criteria for evaluation of predictive ability of QSAR models.\n\n## Introduction\n\nRapid development of combinatorial chemistry and high throughput screening methods in recent years has significantly increased a bulk of experimental structure\u2013activity relationship (SAR) datasets. These developments have emphasized a need for reliable analytical methods for biological SAR data examination such as quantitative SAR (QSAR). QSAR has been traditionally perceived as a means of establishing correlations between trends in chemical structure modifications and respective changes of biological activity \\[1\\]. However, in many cases of chemical library design, the number of compounds that could be practically synthesized and tested is much smaller than the total size of exhaustive virtual chemical libraries. There is a need for developing virtual library screening tools, and QSAR modeling can be adapted to the task of targeted library design \\[2\\], \\[3\\], \\[4\\]. Of course, any QSAR modeling should ultimately lead to statistically robust models capable of making accurate and reliable predictions of biological activities of compounds. However, the application of QSAR models for virtual screening places a special emphasis on statistical significance and predictive ability of these models as their most crucial characteristics. This paper examines the validity of one of the most popular criteria of QSAR model predictive ability, leave-one-out cross-validated _R_ 2 (LOO _q_ 2).\n\nThe process of QSAR model development can be generally divided into three stages: data preparation, data analysis, and model validation. The first stage includes selection of a molecular dataset for QSAR studies, calculation of molecular descriptors, and selection of a QSAR (statistical analysis and correlation) method. These steps represent a standard practice of any QSAR modeling, and their specific details are generally determined by the researchers\u2019 interests and software availability.\n\nThe second part of QSAR model development consists of an application of statistical approaches for QSAR model development. Many different algorithms and computer software are available for this purpose. Most are based on linear (multiple linear) regression with variable selection \\[5\\], partial least squares (PLS) \\[6\\], etc.) as well as non-linear (genetic algorithms \\[7\\], artificial neural networks \\[8\\], etc.) methods. In all approaches, descriptors serve as independent variables, and biological activities as dependent variables.\n\nThe last and as we emphasize in this paper, most important part of QSAR model development is the model _validation_. Most of the QSAR modeling methods implement the leave-one-out (or leave-some-out) cross-validation procedure. The outcome from the cross-validation procedure is cross-validated _R_ 2 ( _q_ 2), which is used as a criterion of both robustness and predictive ability of the model. Many authors consider high _q_ 2 (for instance, _q_ 2>0.5) as an indicator or even as the ultimate proof that the model is highly predictive. A widely used approach to establish the model robustness is so-called _y_-randomization (randomization of response, e.g. biological activities) \\[9\\]. It consists of repeating the calculation procedure with randomized activities and subsequent probability assessment of the resultant statistics. Often, it is used along with cross-validation. Sometimes, models are tested for their ability to predict accurately the activity of one or two compounds that were not used in model development (see, for instance \\[10\\], \\[11\\]). However, it is still common not to test QSAR models (characterized by a reasonably high LOO _q_ 2) for their ability to predict accurately biological activities of compounds from an external test dataset, i.e. those compounds, which were not used for the model development.\n\nAlthough, the low value of _q_ 2 for the training set can indeed serve as an indicator of a low predictive ability of a model, the opposite is not necessarily true. Indeed, the high _q_ 2 does not imply automatically a high predictive ability of the model. In order to both develop the model and validate it, one ...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231"
    }
  ]
}