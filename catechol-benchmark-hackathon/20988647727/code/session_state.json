{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T09:17:13.972797",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Catechol Reaction Yield Prediction - Techniques Guide\n\n## Problem Overview\nThis is a chemistry reaction yield prediction problem. The goal is to predict three yield outputs (SM, Product 2, Product 3) based on solvent conditions, temperature, and residence time. The evaluation uses a custom cross-validation procedure with two tasks:\n- Task 0: Single solvent data (leave-one-solvent-out CV, 24 folds)\n- Task 1: Full data with solvent mixtures (leave-one-ramp-out CV, 13 folds)\n\n**Target score to beat: 0.017270** (lower is better)\n\n## Data Understanding\n**Reference notebook:** `exploration/eda.ipynb`\n- Full data: 1227 samples, 19 columns\n- Single solvent data: 656 samples, 13 columns\n- 24 unique solvents, 13 unique solvent ramps\n- Targets are yields in range [0, 1] (some slightly exceed 1.0)\n- Temperature range: 175-225\u00b0C\n- Residence Time range: 2-15 minutes\n- SolventB% range: 0-1\n\n## Key Techniques from Top Solutions\n\n### 1. Physics-Informed Feature Engineering (CRITICAL)\n**Arrhenius Kinetics Features** - The most important technique from the top solution (0.09831 score):\n- Convert temperature to Kelvin: `temp_k = temp_c + 273.15`\n- Inverse temperature: `inv_temp = 1000.0 / temp_k`\n- Log time: `log_time = np.log(time + 1e-6)`\n- Interaction term: `interaction = inv_temp * log_time`\n\nThis is based on the Arrhenius equation for reaction kinetics: k = A * exp(-Ea/RT)\n\n### 2. Chemical Symmetry and Test Time Augmentation (TTA)\nFor mixed solvents, a mixture of \"Solvent A + Solvent B\" is physically identical to \"Solvent B + Solvent A\". \n**TTA Strategy:**\n- During inference for mixed solvents, predict twice:\n  - Prediction 1: Input as (A, B)\n  - Prediction 2: Input as (B, A) with flipped concentration\n  - Final = (Pred1 + Pred2) / 2\n- Also train on both symmetries (data augmentation)\n\n### 3. Solvent Featurization\nAvailable lookup tables:\n- **spange_descriptors** (13 features): Best for linear mixing, most commonly used\n- **acs_pca_descriptors** (5 features): PCA-based from ACS Green Chemistry\n- **drfps_catechol** (2048 features): Differential reaction fingerprints\n- **fragprints** (2133 features): Fragment + fingerprint concatenation\n\n**Linear mixing for solvent mixtures:**\n```\nmixed_features = A_features * (1 - pct) + B_features * pct\n```\n\n**Feature Combination Strategy:**\n- Try concatenating multiple featurizations (e.g., spange + acs_pca)\n- Use PCA to reduce dimensionality if using high-dim features like drfps/fragprints\n\n### 4. Additional Numeric Feature Engineering\n- Polynomial features: rt\u00b2, temp\u00b2, rt*temp\n- Log transforms: log1p(rt), log1p(temp)\n- Reaction energy proxy: Temperature * Residence Time\n- Concentration-temperature interaction: SolventB% * Temperature\n- Inverse features: 1/rt, 1/temp (for Arrhenius-like relationships)\n\n### 5. Model Architecture\n**MLP (Most successful):**\n- BatchNorm at input (critical for normalization)\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Output: 3 neurons with Sigmoid activation (for bounded outputs)\n- Loss: HuberLoss (robust to outliers) or MSELoss\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Gradient clipping: max_norm=1.0\n- Epochs: 250-300\n\n**Alternative Architectures to Try:**\n- Deeper networks: [256, 128, 64, 32]\n- Wider networks: [256, 256, 128]\n- Residual connections for deeper networks\n- Multi-task learning with shared representations and task-specific heads\n\n**Model Bagging:**\n- Train 5-7 models with different seeds\n- Average predictions for robustness\n- Consider weighted averaging based on validation performance\n\n### 6. Advanced Techniques for Small Datasets\n\n**Probabilistic Models:**\n- Monte Carlo Dropout: Keep dropout active during inference, average multiple predictions\n- Bayesian Neural Networks: Provide uncertainty estimates\n- Deep Kernel Learning: Combine NN feature extraction with GP uncertainty\n\n**Regularization (Critical for small data):**\n- Strong dropout (0.2-0.3)\n- L2 weight decay (1e-4 to 1e-5)\n- Early stopping with patience\n- Data augmentation (symmetry flipping for mixtures)\n\n**Feature Scaling:**\n- StandardScaler on all numeric features\n- Fit scaler on training data only, transform test data\n\n### 7. Post-Processing\n- Clip predictions to [0, 1]\n- Optional: Normalize rows so yields sum to 1 (chemical constraint - SM + P2 + P3 \u2248 1)\n- Consider softmax-like normalization for multi-output\n\n### 8. Validation Strategy\nThe competition uses a specific CV structure:\n- Single solvent: Leave-one-solvent-out (24 folds)\n- Full data: Leave-one-ramp-out (13 folds)\n- Must use the exact split generators from utils.py\n\n**Best Practices:**\n- Nest hyperparameter tuning inside each fold to avoid leakage\n- Track per-fold performance to identify difficult solvents/ramps\n- Use consistent random seeds for reproducibility\n\n### 9. Ensemble Strategies\n**Seed Ensemble:**\n- Train same model with 5-7 different random seeds\n- Average predictions\n\n**Feature Ensemble:**\n- Train separate models on different featurizations\n- Combine predictions (average or weighted)\n\n**Model Ensemble:**\n- Combine MLP with gradient boosting (XGBoost/LightGBM)\n- Stack models with a meta-learner\n\n**Stacking:**\n- Use out-of-fold predictions as features for a second-level model\n- Can significantly improve performance\n\n### 10. Alternative Approaches\n\n**XGBoost/LightGBM:**\n- Can work but typically underperforms MLP for this problem\n- Use MultiOutputRegressor wrapper\n- Enable categorical features for solvent names\n- Try with Arrhenius features\n\n**Gaussian Processes:**\n- Could leverage uncertainty quantification\n- Deep Kernel Learning combines NN feature learning with GP\n- May be computationally expensive but good for small data\n\n**Transformer-based Models:**\n- If using reaction SMILES, consider BERT-based models\n- Data augmentation via SMILES randomization can help\n\n### 11. Hyperparameter Tuning Priority\n1. Learning rate (1e-4 to 1e-3)\n2. Hidden layer sizes\n3. Dropout rate (0.1 to 0.3)\n4. Weight decay (1e-5 to 1e-4)\n5. Batch size (16, 32, 64)\n6. Number of epochs with early stopping\n\n### 12. Multi-Output Regression Considerations\n- Multi-task neural networks with shared representations outperform single-task models\n- The three outputs (SM, Product 2, Product 3) are chemically related - exploit this\n- Consider correlation between outputs when designing loss functions\n- Distributional regression (predicting distributions instead of point estimates) can improve robustness\n\n## Implementation Priority (Recommended Order)\n1. **Baseline:** Basic MLP + spange_descriptors + StandardScaler\n2. **Physics:** Add Arrhenius kinetics features (1/T, ln(t), interaction)\n3. **Symmetry:** Implement TTA for chemical symmetry (mixed solvents)\n4. **Bagging:** Add model bagging (5-7 seeds)\n5. **Features:** Try combining multiple featurizations\n6. **Architecture:** Experiment with deeper/wider networks\n7. **Ensemble:** Combine different model types\n8. **Advanced:** Try probabilistic models or transformers\n\n## Code Structure Notes\n- The submission must follow the template structure with specific last 3 cells\n- Only the model definition line can be changed\n- Model must have train_model(X_train, Y_train) and predict(X) methods\n- Use torch.set_default_dtype(torch.double) for numerical stability\n\n## Key Insights from Top Kernels\n1. Arrhenius kinetics features are the single most impactful improvement\n2. Chemical symmetry TTA provides consistent gains for mixed solvents\n3. Spange descriptors work best for linear mixing\n4. Model bagging with 5-7 seeds reduces variance significantly\n5. HuberLoss is more robust than MSELoss for this data\n6. BatchNorm at input is critical for stable training\n\n## Potential Breakthrough Approaches\nTo beat the target score of 0.017270 (which is very low), consider:\n1. **Ensemble of ensembles:** Combine multiple bagged models with different architectures\n2. **Feature engineering depth:** Explore more physics-based features beyond Arrhenius\n3. **Per-solvent models:** Train specialized models for different solvent families\n4. **Output correlation modeling:** Explicitly model the correlation between SM, P2, P3\n5. **Uncertainty-weighted predictions:** Use uncertainty estimates to weight predictions\n6. **Transfer learning:** Pre-train on related chemistry datasets if available\n",
      "agent": "explorer",
      "timestamp": "2026-01-14T09:23:37.700181"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics feature engineering: Use 1/Temperature (in Kelvin), ln(Time), and their interaction as features. This is physics-informed and improves predictions.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Test Time Augmentation (TTA) for chemical symmetry: For mixed solvents, predict twice - once with (A,B) and once with (B,A) flipped - then average. This respects physical symmetry of mixtures.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors provide best base for linear mixing of solvents. Use linear interpolation: A*(1-pct) + B*pct for mixed solvent features.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model bagging with 7 models improves robustness. Use HuberLoss for robustness to outliers. MLP architecture: BatchNorm -> Linear(128) -> ReLU -> Dropout(0.2) repeated, ending with Sigmoid.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Numeric feature engineering: rt, temp, rt^2, temp^2, log1p(rt), log1p(temp), rt*temp. Use StandardScaler on all features.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "CV structure: Task 0 = single solvent (leave-one-solvent-out), Task 1 = full data with mixtures (leave-one-ramp-out). Target columns are Product 2, Product 3, SM (yields 0-1).",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "XGBoost approach: Feature engineering with Reaction_Energy = Temperature * Residence Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip to [0,1] and normalize rows to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Available solvent featurizations: spange_descriptors (best for linear mixing), acs_pca_descriptors, drfps_catechol, fragprints. Spange descriptors are most commonly used in top solutions.",
      "source": "../research/description.md",
      "agent": "explorer"
    },
    {
      "finding": "Deep Kernel Learning (DKL) combines neural network feature learning with Gaussian Process uncertainty quantification - good for reaction yield prediction with uncertainty estimates.",
      "source": "WebSearch: ML techniques for chemical reaction yields",
      "agent": "explorer"
    },
    {
      "finding": "Transformer/BERT models on reaction SMILES achieve state-of-the-art for yield prediction. Data augmentation (molecule order permutations, SMILES randomizations) improves results.",
      "source": "WebSearch: ML techniques for chemical reaction yields",
      "agent": "explorer"
    },
    {
      "finding": "For small chemistry datasets (<2000 samples): Use probabilistic models (Bayesian NN, MC Dropout) for uncertainty quantification. Strong regularization (dropout, L2 weight decay) and data augmentation (SMILES randomization) are critical.",
      "source": "WebSearch: NN predictions for small chemistry datasets",
      "agent": "explorer"
    },
    {
      "finding": "Leave-one-cluster-out CV (LOCO-CV) is recommended for measuring extrapolatory power. Nest hyperparameter tuning inside each LOO fold to avoid information leakage.",
      "source": "WebSearch: NN predictions for small chemistry datasets",
      "agent": "explorer"
    },
    {
      "finding": "Multi-task neural networks with shared representations and task-specific heads outperform single-task models for QSAR. Distributional Mixture of Experts (DMoE) predicts probability distributions instead of point estimates for more robust performance.",
      "source": "WebSearch: multi-output regression neural networks chemistry",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning techniques for predicting chemical reaction yields?",
      "finding": "Predicting the yield of a chemical reaction is now treated as a supervised regression problem that can be tackled with a variety of modern machine\u2011learning models.  The most successful approaches combine rich, text\u2011based representations of the reaction (e.g., reaction SMILES) with deep\u2011learning architectures originally developed for natural\u2011language processing.  Encoder\u2011transformer models that treat the whole reaction string as a \u201csentence\u201d and attach a regression head have shown \u201coutstanding\u201d performance on high\u2011throughput experiment datasets, often surpassing traditional fingerprint\u2011based methods that rely on one\u2011hot encodings or concatenated molecular descriptors\u202f([research.ibm.com](https://research.ibm.com/publications/prediction-of-chemical-reaction-yields-using-deep-learning))\u202f([rxn4chemistry.github.io](https://rxn4chemistry.github.io/rxn_yields)).  Variants of the Molecular Transformer, fine\u2011tuned on yield data, are now a standard baseline for yield prediction.\n\nWhen only a few ",
      "agent": "explorer"
    },
    {
      "query": "How to improve neural network predictions for small chemistry datasets with leave-one-out cross validation?",
      "finding": "Improving neural\u2011network predictions on very small chemistry datasets while using leave\u2011one\u2011out (LOO) cross\u2011validation can be done by combining careful data handling, probabilistic modeling, and rigorous validation\u202f\u2013\u202fall of which are highlighted in recent cheminformatics studies ([Thomas\u202fet\u202fal.,\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10);\u202f[Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b);\u202f[Rosseinsky\u202fet\u202fal.,\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c);\u202f[Vehtari\u202fet\u202fal.,\u202f2019](https://proceedings.mlr.press/v97/magnusson19a.html)).\n\n**Step\u2011by\u2011step workflow**\n\n1. **Select robust molecular representations**  \n   - Encode each molecule with several complementary descriptors (e.g., circular fingerprints, graph\u2011based embeddings, physicochemical descriptors) and test them separately; diverse representations help compensate for the limited number of samples ([Tom\u202fet\u202fal.,\u202f2022/2023](https://pubs.rsc.org",
      "agent": "explorer"
    },
    {
      "query": "What are the best techniques for multi-output regression with neural networks for chemistry applications?",
      "finding": "Multi\u2011output regression in chemistry is most often tackled with **multi\u2011task neural architectures** that share a common representation of the molecule while learning separate heads for each property.  Early work showed that such networks can outperform single\u2011task models on QSAR datasets by jointly fitting several assays and using regularisation tricks from deep learning\u202f([arxiv.org](https://arxiv.org/abs/1406.1231)).  A modern, highly effective variant replaces fully\u2011connected layers with **graph convolutional networks (GCNs)** that operate directly on molecular graphs; a shared GCN encoder followed by multiple task\u2011specific read\u2011outs has been shown to improve prediction of ADMET endpoints such as log\u202fD, solubility and melting point\u202f([mdpi-res.com](https://mdpi-res.com/d_attachment/molecules/molecules-25-00044/article_deploy/molecules-25-00044.pdf)).  When experimental data are scarce, **multi\u2011fidelity learning** can be added: a low\u2011fidelity quantum\u2011chemical dataset (e.g., COSMO\u2011RS) i",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 0
}