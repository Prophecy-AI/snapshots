{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-13T10:07:47.401706",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Seed Prompt: Chemical Reaction Yield Prediction\n\n## Problem Overview\nThis is a regression task predicting chemical reaction yields (SM, Product 2, Product 3) for the allyl-substituted catechol rearrangement under different solvent and process conditions. The challenge involves leave-one-out cross-validation where entire solvents or solvent ramps are held out.\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: data shapes, target distributions, input ranges, solvent lookup tables\n\n**Key data characteristics:**\n- Single solvent data: 656 samples, 24 unique solvents (24-fold LOO CV)\n- Full mixture data: 1227 samples, 13 unique solvent ramps (13-fold LOO CV)\n- Targets are yields in [0,1] range but don't sum to 1 (mean ~0.8)\n- Inputs: Temperature (175-225\u00b0C), Residence Time (2-15 min), SolventB% (0-1)\n- Multiple solvent featurizations available: spange_descriptors (13 features), drfps (2048), fragprints (2133), acs_pca (5)\n\n## MANDATORY SUBMISSION STRUCTURE\n**CRITICAL: The submission must follow the template structure exactly:**\n- The last three cells must remain unchanged except for the model definition line\n- Only `model = MLPModel()` can be replaced with a new model definition\n- The model must implement: `train_model(X_train, y_train)` and `predict(X_test)` methods\n- For single solvent: `model = YourModel(data='single')`\n- For full data: `model = YourModel(data='full')`\n\n## Physics-Informed Feature Engineering (CRITICAL)\n\n### Arrhenius Kinetics Features\nBased on reaction kinetics principles, transform raw inputs:\n```python\ntemp_k = Temperature + 273.15  # Convert to Kelvin\ninv_temp = 1000.0 / temp_k     # Inverse temperature (Arrhenius)\nlog_time = np.log(Residence_Time + 1e-6)  # Log time\ninteraction = inv_temp * log_time  # Interaction term\n```\nThese features capture the exponential relationship between temperature and reaction rate (Arrhenius equation: k = A * exp(-Ea/RT)).\n\n### Additional Feature Engineering\n- `Reaction_Energy = Temperature * Residence_Time`\n- `B_Conc_Temp = SolventB% * Temperature`\n- Consider polynomial features of temperature and time\n\n## Chemical Symmetry (CRITICAL for Mixed Solvents)\nFor solvent mixtures, A+B is physically identical to B+A. Exploit this:\n\n1. **Data Augmentation**: Train on both (A,B) and (B,A) orderings\n2. **Test Time Augmentation (TTA)**: Predict both orderings and average\n   - Prediction 1: Input as (A, B)\n   - Prediction 2: Input as (B, A) with flipped percentages\n   - Final = (Pred1 + Pred2) / 2\n\nThis mathematically guarantees the model respects physical symmetry and reduces variance.\n\n## Model Architectures\n\n### Neural Network (MLP) - Recommended\nBest performing approach based on public kernels (score ~0.098):\n- Architecture: BatchNorm \u2192 Linear \u2192 BatchNorm \u2192 ReLU \u2192 Dropout(0.2) \u2192 repeat\n- Hidden layers: [128, 128, 64]\n- Output: Sigmoid activation (constrains to [0,1])\n- Loss: HuberLoss (robust to outliers) or MSELoss\n- Optimizer: Adam with lr=5e-4, weight_decay=1e-5\n- Scheduler: ReduceLROnPlateau (factor=0.5, patience=20)\n- Epochs: 300, batch_size=32\n- Gradient clipping: max_norm=1.0\n\n### Ensemble/Bagging (Recommended for Variance Reduction)\n- Use 5-7 bagged models for variance reduction\n- Average predictions across all models\n- Each model trained with different random initialization\n- Critical for small dataset with LOO-CV\n\n### LightGBM Alternative\n- 3 separate regressors (one per target)\n- Key hyperparameters: learning_rate=0.03, max_depth=6\n- Early stopping with 100 rounds patience\n- 12% validation split for early stopping\n\n### Gaussian Process Regression (Consider for Small Data)\n- Well-suited for small datasets with uncertainty quantification\n- Use RBF kernel with automatic relevance determination\n- Can provide calibrated uncertainty estimates\n\n## Solvent Featurization\n\n### Spange Descriptors (Recommended)\n- 13 physical/chemical properties: dielectric constant, ET(30), alpha, beta, pi*, SA, SB, SP, SdP, N, etc.\n- Best for linear mixing of solvent features\n- Interpretable and physics-based\n\n### For Mixed Solvents\nWeighted average based on mixture percentage:\n```python\nX_feat = X_A_feat * (1 - SolventB%) + X_B_feat * SolventB%\n```\n\n### Alternative Featurizations\n- DRFPS (2048 features): Differential reaction fingerprints\n- Fragprints (2133 features): Fragment + fingerprint concatenation\n- Consider combining multiple featurizations\n\n## Post-Processing\n- Clip predictions to [0, 1] range\n- Optional: Normalize rows so sum equals 1 (chemical constraint, but note targets don't naturally sum to 1)\n\n## Validation Strategy\n- Single solvent: Leave-one-solvent-out (24 folds)\n- Full data: Leave-one-ramp-out (13 folds)\n- **Same hyperparameters must be used across ALL folds (no per-fold tuning)**\n- Different hyperparameters allowed for different tasks (single vs full) or objectives (SM vs Products)\n\n## Key Techniques Summary (Priority Order)\n1. **Physics-informed features**: Arrhenius kinetics (1/T, ln(t), interaction)\n2. **Symmetry exploitation**: TTA for mixed solvents (data aug + inference aug)\n3. **Ensemble**: Bagged MLPs (5-7 models) for variance reduction\n4. **Robust training**: HuberLoss, BatchNorm, Dropout, gradient clipping\n5. **Proper featurization**: Spange descriptors with weighted mixing\n6. **Regularization**: Weight decay, dropout to prevent overfitting on small data\n\n## Advanced Techniques to Consider\n- **Multi-task learning**: Predict all 3 targets jointly (shared representation)\n- **Transfer learning**: Pre-train on full data, fine-tune for single solvent\n- **Uncertainty quantification**: Ensemble variance or MC Dropout\n- **Feature selection**: Remove uninformative solvent descriptors\n\n## Target Score\nBeat **0.017270** (lower is better). Current best public kernel achieves ~0.098.\n\n## Implementation Notes\n- Use torch.set_default_dtype(torch.double) for numerical stability\n- Ensure reproducibility with random seeds where possible\n- Monitor for overfitting given small dataset size\n- The evaluation metric appears to be MSE-based\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T10:14:39.156003"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius kinetics features: Use 1/Temperature (in Kelvin) and ln(Time) as physics-informed features. Also include interaction term (inv_temp * log_time). This captures reaction kinetics.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Chemical symmetry TTA: For mixed solvents, A+B is physically identical to B+A. Use Test Time Augmentation by predicting both ways and averaging. Also train on both symmetries (data augmentation).",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model ensemble: Use 7 bagged MLP models with BatchNorm, ReLU, Dropout(0.2), Sigmoid output. HuberLoss is robust to outliers. LR scheduler with ReduceLROnPlateau. 300 epochs, batch_size=32, lr=5e-4.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "LightGBM approach: Use 3 separate LightGBM regressors (one per target). Key hyperparameters: learning_rate=0.03, max_depth=6, with early stopping (100 rounds patience). Use 12% validation split for early stopping.",
      "source": "../research/kernels/tobimichigan_catechol-pipeline-with-lightgbm/catechol-pipeline-with-lightgbm.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature engineering ideas: Reaction_Energy = Temperature * Residence_Time, B_Conc_Temp = SolventB% * Temperature. Post-processing: clip outputs to [0,1] and normalize rows so sum equals 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Spange descriptors provide best base for linear mixing of solvent features. Available featurizations: drfps_catechol, fragprints, smiles, acs_pca_descriptors, spange_descriptors.",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data sizes: Full data has 1227 samples with 13 unique solvent ramps. Single solvent data has 656 samples with 24 unique solvents. CV is leave-one-solvent-out (24 folds) for single and leave-one-ramp-out (13 folds) for full.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Targets (SM, Product 2, Product 3) are yields in [0,1] range. They don't sum to 1 (mean ~0.8), so there's unaccounted product. Some values slightly exceed 1 (max 1.08).",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Input ranges: Temperature 175-225\u00b0C, Residence Time 2-15 min, SolventB% 0-1. Spange descriptors have 13 features, DRFPS has 2048 features, fragprints has 2133 features.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Clean MLP implementation: BatchNorm at input, then [128,128,64] hidden layers with BatchNorm+ReLU+Dropout(0.2) after each. Sigmoid output. MSELoss, Adam lr=5e-4, weight_decay=1e-5, ReduceLROnPlateau scheduler, gradient clipping max_norm=1.0, 300 epochs, batch_size=32.",
      "source": "../research/kernels/omarafik_system-malfunction-v1/system-malfunction-v1.ipynb",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields?",
      "finding": "Predicting the yield of a chemical reaction is now most reliably tackled with data\u2011driven models that combine rich reaction representations and deep learning architectures.  The current \u201cbest\u201d approaches centre on **transformer\u2011based sequence models** that treat the reaction SMILES string as a language\u2011like input and attach a regression head to output a continuous yield value.  Schwaller\u202fet\u202fal. demonstrated that an encoder\u2011only transformer trained on high\u2011throughput experiment (HTE) data achieves state\u2011of\u2011the\u2011art performance, outperforming traditional one\u2011hot or fingerprint concatenations\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/60c750f2ee301c70b1c7a973)).  A closely related implementation is provided by the open\u2011source RXN\u2011yields platform, which uses the same transformer encoder on reaction SMILES and reports \u201coutstanding prediction performance\u201d on Buchwald\u2011Hartwig and Suzuki\u2011Miyaura HTE datasets\u202f([rxn4chemistry.github.io](https://rxn4chemistry.github.io/rxn_yi",
      "agent": "explorer"
    },
    {
      "query": "What are the best approaches for small dataset regression with leave-one-out cross-validation in chemistry?",
      "finding": "Small\u2011dataset regression in chemistry is challenging because the limited number of experiments amplifies the risk of over\u2011fitting and makes model validation critical.  In this regime the choice of representation (e.g., physicochemical or learned descriptors) and the algorithm must be coupled with a validation scheme that can reliably estimate predictive performance without sacrificing too much data to a hold\u2011out set.  Recent reviews emphasize moving beyond simple linear regressors to more flexible learners that can capture non\u2011linear structure\u2013property relationships while remaining robust to sparsity\u202f([Chem. Sci.\u202f2025](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k)).\n\nAmong the most effective approaches are (i) kernel\u2011based or random\u2011projection methods that create low\u2011dimensional, information\u2011rich embeddings and work well with ridge or Gaussian\u2011process regression\u202f([Digital Discovery\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)); (ii) probabilis",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}