{
  "query": "What are the best approaches for small dataset regression with leave-one-out cross-validation in chemistry?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Small\u2011dataset regression in chemistry is challenging because the limited number of experiments amplifies the risk of over\u2011fitting and makes model validation critical.  In this regime the choice of representation (e.g., physicochemical or learned descriptors) and the algorithm must be coupled with a validation scheme that can reliably estimate predictive performance without sacrificing too much data to a hold\u2011out set.  Recent reviews emphasize moving beyond simple linear regressors to more flexible learners that can capture non\u2011linear structure\u2013property relationships while remaining robust to sparsity\u202f([Chem. Sci.\u202f2025](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k)).\n\nAmong the most effective approaches are (i) kernel\u2011based or random\u2011projection methods that create low\u2011dimensional, information\u2011rich embeddings and work well with ridge or Gaussian\u2011process regression\u202f([Digital Discovery\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)); (ii) probabilistic models such as the DIONYSUS framework, which explicitly calibrate uncertainty and have been shown to generalize better on low\u2011data chemical sets\u202f([Digital Discovery\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)); and (iii) instance\u2011based learners like k\u2011nearest\u2011neighbour (k\u2011NN) regression, for which fast LOOCV algorithms now make exhaustive leave\u2011one\u2011out evaluation tractable even for modest datasets\u202f([arXiv\u202f2024](https://arxiv.org/abs/2405.04919)).  Tree\u2011based ensembles (e.g., random forests or gradient boosting) and regularised linear models (ridge, LASSO) also remain popular because they are less sensitive to descriptor collinearity and can be tuned with Bayesian optimisation in low\u2011data settings\u202f([Chem. Sci.\u202f2025](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k)).\n\nValidation should not rely solely on the traditional LOO\u202f\\(q^2\\) statistic, which can be misleading for small, biased data sets\u202f([J. Cheminformatics\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10);\u202f[Beware of\u202f\\(q^2\\)\u202f2002](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231)).  A more reliable workflow combines (a) LOOCV for hyper\u2011parameter selection (using the fast k\u2011NN LOOCV or analytic formulas for kernel methods), (b) leave\u2011one\u2011cluster\u2011out CV when the data contain natural groupings (e.g., scaffold families)\u202f([Digital Discovery\u202f2022](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c)), and (c) an external test set or repeated double\u2011cross\u2011validation to assess true generalisation\u202f([J. Chemometrics\u202f2009](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.1225)).  Together with careful descriptor selection, uncertainty quantification from probabilistic models, and, where possible, modest data augmentation (e.g., virtual screening or computational chemistry calculations), these strategies constitute the current best practice for regression on small chemical datasets using leave\u2011one\u2011out cross\u2011validation.",
      "url": ""
    },
    {
      "title": "Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2022/dd/d2dd00039c)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00046f)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00056c)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D2DD00039C](https://doi.org/10.1039/D2DD00039C)\n(Paper)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2022, **1**, 763-778\n\n# Random projections and kernelised leave one cluster out cross validation: universal baselines and evaluation tools for supervised machine learning of material properties [\u2020](https://pubs.rsc.org/pubs.rsc.org\\#fn1)\n\nSamantha\nDurdy\n\\*ab,\nMichael W.\nGaultois\nbc,\nVladimir V.\nGusev\nbc,\nDanushka\nBollegala\nab and Matthew J.\nRosseinsky\nbcaDepartment of Computer Science, University of Liverpool, Ashton Street, Liverpool, L69 3BX, UK. E-mail: [samantha.durdy@liverpool.ac.uk](mailto:samantha.durdy@liverpool.ac.uk)bLeverhulme Research Centre for Functional Materials Design, University of Liverpool, 51 Oxford Street, Liverpool, L7 3NY, UKcDepartment of Chemistry, University of Liverpool, Crown St, Liverpool, L69 7ZD, UK\n\nReceived\n9th May 2022\n, Accepted 31st August 2022\n\nFirst published on 2nd September 2022\n\n## Abstract\n\nWith machine learning being a popular topic in current computational materials science literature, creating representations for compounds has become common place. These representations are rarely compared, as evaluating their performance \u2013 and the performance of the algorithms that they are used with \u2013 is non-trivial. With many materials datasets containing bias and skew caused by the research process, leave one cluster out cross validation (LOCO-CV) has been introduced as a way of measuring the performance of an algorithm in predicting previously unseen groups of materials. This raises the question of the impact, and control, of the range of cluster sizes on the LOCO-CV measurement outcomes. We present a thorough comparison between composition-based representations, and investigate how kernel approximation functions can be used to better separate data to enhance LOCO-CV applications. We find that domain knowledge does not improve machine learning performance in most tasks tested, with band gap prediction being the notable exception. We also find that the radial basis function improves the linear separability of chemical datasets in all 10 datasets tested and provides a framework for the application of this function in the LOCO-CV process to improve the outcome of LOCO-CV measurements regardless of machine learning algorithm, choice of metric, and choice of compound representation. We recommend kernelised LOCO-CV as a training paradigm for those looking to measure the extrapolatory power of an algorithm on materials data.\n\n## 1 Introduction\n\nRecent advances in materials science have seen a plethora of research into application of machine learning (ML) algorithms. Much of this research has focused on supervised ML methods, such as random forests (RFs) and neural networks. More recently, authors have laid out the best practices to help unify and progress this field. [1\u20134](https://pubs.rsc.org/pubs.rsc.org#cit1)\n\nData representation can play a large role in the performance of ML algorithms; however, optimum choice of representation is not always apparent. In materials science it is often difficult to choose an appropriate representation due to variability in the ML task and in the nature of the chemistry, composition and structures of the materials studied. Additionally, some properties of a material, such its crystal structure in the case of crystalline materials, may not be known until its synthesis. Accordingly, many studies derive representations from either the ratios of elements in the chemical composition, or from domain knowledge-based properties (referred to as features) of these elements, or both, in a process called \u201cfeaturisation\u201d.\n\nGiven the ubiquity of featurisation methods such as those presented here in materials applications, it is important to evaluate the statistical advantage of specific feature sets. [5](https://pubs.rsc.org/pubs.rsc.org#cit5) Section 2.1 overviews different featurisation techniques and how their effectiveness has been previously reported. We expand on this evaluation in Section 3.1, in which seven representations are investigated across five case studies from the literature to explore how these representations perform in published ML tasks. These cases thus represent practical applications, rather than constructed tasks. Each of these representations is also compared to a random projection of equal size to establish the performance benefit of domain knowledge over random noise.\n\nEvaluating the generalisability of ML models is a known challenge across data science, and is of particular concern in materials science, where data sets are of limited size compared with other application areas for ML, and often biased towards historically interesting materials or those closely related to known high-performance materials for certain performance metrics. Typically, models are evaluated on test sets separate from their training data, through a consistent train:test split or N-fold cross validation. However, this does not consider skew in a dataset. In chemical datasets, families of promising materials are often explored more thoroughly than the domain as a whole, which introduces bias and reduces the generalisability of ML models because the data they are trained and tested on are not sampled in a way representative of the domain of target chemistries to be screened with these models. Investigations into how such skew can affect ML models has seen that this skew can result in overfitting [6](https://pubs.rsc.org/pubs.rsc.org#cit6) and that more skewed datasets require more data points in order to train models to achieve similar predictive performance when compared to models trained on less skewed datasets. [7](https://pubs.rsc.org/pubs.rsc.org#cit7)\n\nLeave one cluster out cross validation (LOCO-CV) was suggested to combat this, [8](https://pubs.rsc.org/pubs.rsc.org#cit8) using K-means clustering to exclude similar families of materials from the training set to measure the extrapolatory power of an ML algorithm (its ability to predict the performance of materials with chemistries qualitatively different from the training set). The value of such an approach can be seen in the case of predicting new classes of superconductors. One may choose to remove cuprate superconductors from the training set, and if an ML model can then successfully predict the existence of cuprate superconductors without prior knowledge of them, we can conclude that model is likely to perform better at predicting new classes of superconductors than a model which could not predict the existence of cuprate superconductors. LOCO-CV provides an algorithmic framework to measure the performance of models on predicting new classes of materials by defining these classes as clusters found by the K-means clustering algorithm. Application and implementation of this algorithm is discussed further in Section 2.2.1.\n\nWhile differences in cluster sizes in this domain are expected, it has been observed that clusters found with K-means can differ in size by orders of magnitude, [9](https://pubs.rsc.org/pubs.rsc.org#cit9) which can pose a practical challenge to adoption of this method. With such differences in cluster size, LOCO-CV measurements can represent the performance of an algorithm on a small training set rather than the performance of an algorithm in extrapolation. As representation plays a role in clustering, it is pertinent to investigate the issues of representation and clustering together, even though the representation used in clustering does not need to be the same as that used to train the model ( [F...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2022/dd/d2dd00039c"
    },
    {
      "title": "Machine learning workflows beyond linear models in low-data regimes",
      "text": "[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/sc/d5sc00996k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc01701g)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc01073j)\n\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k)\n\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png) Open Access Article\n\n![](https://pubs.rsc.org/content/newimages/CCBY-NC.svg)\nThis Open Access Article is licensed under a [Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\n\nDOI:\u00a0[10.1039/D5SC00996K](https://doi.org/10.1039/D5SC00996K)\n(Edge Article)\n[Chem. Sci.](https://doi.org/10.1039/2041-6539/2010), 2025, **16**, 8555-8560\n\n# Machine learning workflows beyond linear models in low-data regimes [\u2020](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k\\#fn1)\n\nDavid\nDalmau\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-2506-6546)a,\nMatthew S.\nSigman\nb and Juan V.\nAlegre-Requena\n[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-0769-7168)\\*a\n\naDepartamento de Qu\u00edmica Inorg\u00e1nica, Instituto de S\u00edntesis Qu\u00edmica y Cat\u00e1lisis Homog\u00e9nea (ISQCH), CSIC-Universidad de Zaragoza, C/Pedro Cerbuna 12, 50009 Zaragoza, Spain. E-mail: [jv.alegre@csic.es](mailto:jv.alegre@csic.es)\n\nbDepartment of Chemistry, University of Utah, 315 South 1400 East, Salt Lake City, Utah 84112, USA\n\nReceived\n7th February 2025\n, Accepted 10th April 2025\n\nFirst published on 15th April 2025\n\n* * *\n\n## Abstract\n\nData-driven methodologies are transforming chemical research by providing chemists with digital tools that accelerate discovery and promote sustainability. In this context, non-linear machine learning algorithms are among the most disruptive technologies in the field and have proven effective for handling large datasets. However, in data-limited scenarios, linear regression has traditionally prevailed due to its simplicity and robustness, while non-linear models have been met with skepticism over concerns related to interpretability and overfitting. In this study, we introduce ready-to-use, automated workflows designed to overcome these challenges. These frameworks mitigate overfitting through Bayesian hyperparameter optimization by incorporating an objective function that accounts for overfitting in both interpolation and extrapolation. Benchmarking on eight diverse chemical datasets, ranging from 18 to 44 data points, demonstrates that when properly tuned and regularized, non-linear models can perform on par with or outperform linear regression. Furthermore, interpretability assessments and de novo predictions reveal that non-linear models capture underlying chemical relationships similarly to their linear counterparts. Ultimately, the automated non-linear workflows presented have the potential to become valuable tools in a chemist's toolbox for studying problems in low-data regimes alongside traditional linear models.\n\n* * *\n\n## Introduction\n\nData-driven approaches have become increasingly popular due to their ability to save time, effort, and resources, all while promoting sustainability through digitalization. [1](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit1) In the field of chemistry, machine learning (ML) has significantly impacted the exploration of chemical spaces and the prediction of molecular properties and reaction outcomes. [2](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit2) These advancements have led to substantial progress in various areas, [3](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit3) including drug discovery, [4,5](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit4) materials science, [6,7](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit6) chemical synthesis, [8\u201311](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit8) and catalyst development. [12,13](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit12)\n\nHowever, modeling small datasets in chemical research presents inherent challenges. Such datasets are particularly susceptible to underfitting, where models fail to capture underlying relationships, and overfitting, where models overly adapt to data by capturing noise or irrelevant patterns. [14](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit14) These issues stem from the limited number of data points, the complexity of algorithms relative to dataset size, and the presence of noise, all of which hinder a model's ability to generalize effectively. [15](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit15)\n\nMultivariate linear regression (MVL) is arguably the most used method in low-data scenarios due to its simplicity, robustness, and consistent performance with small datasets. [16](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit16) MVL models often present a bias-variance tradeoff that helps mitigate overfitting while providing intuitive interpretability. [14](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit14) Although more advanced ML algorithms like random forests (RF), gradient boosting (GB), and neural networks (NN) can achieve higher predictive accuracy, [17,18](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit17) their effectiveness in low-data scenarios is often limited by their sensitivity to overfitting and difficult interpretation. [19](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit19) These models also require careful hyperparameter tuning and regularization techniques to generalize effectively. [20\u201322](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit20)\n\nTo fully harness the capabilities of non-linear ML algorithms in low-data scenarios, it is essential to address these challenges. To this end, we have developed a fully automated workflow integrated into the ROBERT software. The approach is specifically designed to mitigate overfitting, reduce human intervention, eliminate biases in model selection, and enhance the interpretability of complex models. Our goal is to demonstrate that, even in low-data regimes, non-linear algorithms can be as effective as MVL when properly tuned and regularized. This new workflow not only broadens the scope of ML applications in chemistry but also aims to incorporate non-linear algorithms as part of the chemists' toolbox for studying low-data scenarios ( [Fig. 1](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#imgfig1)).\n\n|     |     |     |\n| --- | --- | --- |\n| [![image file: d5sc00996k-f1.tif](https://pubs.rsc.org/image/article/2025/SC/d5sc00996k/d5sc00996k-f1.gif)](https://pubs.rsc.org/image/article/2025/SC/d5sc00996k/d5sc00996k-f1_hi-res.gif) |\n|  | **Fig. 1** Traditional conceptions of linear and non-linear regression models for low-data regimes. |  |\n\n## Discussion\n\n### Adapting non-linear ML workflows for small datasets\n\nRecently, we developed ROBERT, a program that enables users to develop ML models automatically from a CSV database by performing data curation, hyperparameter optimization, model selection, and evaluation. It generates a comprehensive PDF report that includes key information such as performance metrics, cross-validation results, feature importance, and outlier detection, along with detailed guidelines to ensure reproducibility and transparency.\n\nIn line with previous studies, [23](https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k#cit23) we observed that the most limiting factor in applying non-linear models to low-data regimes is overfitt...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/sc/d5sc00996k"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[2405.04919] Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2405.04919\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2405.04919**(stat)\n[Submitted on 8 May 2024 ([v1](https://arxiv.org/abs/2405.04919v1)), last revised 4 Dec 2024 (this version, v2)]\n# Title:Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression\nAuthors:[Motonobu Kanagawa](https://arxiv.org/search/stat?searchtype=author&amp;query=Kanagawa,+M)\nView a PDF of the paper titled Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression, by Motonobu Kanagawa\n[View PDF](https://arxiv.org/pdf/2405.04919)[HTML (experimental)](https://arxiv.org/html/2405.04919v2)> > Abstract:\n> We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method. Comments:|To appear in Transactions of Machine Learning Research (TMLR)|\nSubjects:|Machine Learning (stat.ML); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)|\nCite as:|[arXiv:2405.04919](https://arxiv.org/abs/2405.04919)[stat.ML]|\n|(or[arXiv:2405.04919v2](https://arxiv.org/abs/2405.04919v2)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2405.04919](https://doi.org/10.48550/arXiv.2405.04919)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Motonobu Kanagawa [[view email](https://arxiv.org/show-email/694b27f9/2405.04919)]\n**[[v1]](https://arxiv.org/abs/2405.04919v1)**Wed, 8 May 2024 09:41:25 UTC (159 KB)\n**[v2]**Wed, 4 Dec 2024 17:18:05 UTC (289 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression, by Motonobu Kanagawa\n* [View PDF](https://arxiv.org/pdf/2405.04919)\n* [HTML (experimental)](https://arxiv.org/html/2405.04919v2)\n* [TeX Source](https://arxiv.org/src/2405.04919)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.04919&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.04919&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2024-05](https://arxiv.org/list/stat.ML/2024-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2405.04919?context=cs)\n[cs.DS](https://arxiv.org/abs/2405.04919?context=cs.DS)\n[cs.LG](https://arxiv.org/abs/2405.04919?context=cs.LG)\n[stat](https://arxiv.org/abs/2405.04919?context=stat)\n[stat.CO](https://arxiv.org/abs/2405.04919?context=stat.CO)\n[stat.ME](https://arxiv.org/abs/2405.04919?context=stat.ME)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.04919)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.04919)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.04919)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2405.04919&amp;description=Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2405.04919&amp;title=Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.04919)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2405.04919"
    },
    {
      "title": "Beware of q2!",
      "text": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS1093326301001231)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S1093326301001231/purchase)\n- [Patient Access](https://www.elsevier.com/open-science/science-and-society/access-for-healthcare-and-patients)\n- Other access options\n\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-snippets)\n- [References (46)](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-references)\n- [Cited by (3546)](https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling)\n\n## [Journal of Molecular Graphics and Modelling](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling)\n\n[Volume 20, Issue 4](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling/vol/20/issue/4), January 2002, Pages 269-276\n\n[![Journal of Molecular Graphics and Modelling](https://ars.els-cdn.com/content/image/1-s2.0-S1093326324X00029-cov150h.gif)](https://www.sciencedirect.com/journal/journal-of-molecular-graphics-and-modelling/vol/20/issue/4)\n\n# Beware of _q_ 2!\n\nAuthor links open overlay panelAlexanderGolbraikh, AlexanderTropsha\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/S1093-3263(01)00123-1](https://doi.org/10.1016/S1093-3263(01)00123-1) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S1093326301001231&orderBeanReset=true)\n\n## Abstract\n\nValidation is a crucial aspect of any quantitative structure\u2013activity relationship (QSAR) modeling. This paper examines one of the most popular validation criteria, leave-one-out cross-validated _R_ 2 (LOO _q_ 2). Often, a high value of this statistical characteristic ( _q_ 2>0.5) is considered as a proof of the high predictive ability of the model. In this paper, we show that this assumption is generally incorrect. In the case of 3D QSAR, the lack of the correlation between the high LOO _q_ 2 and the high predictive ability of a QSAR model has been established earlier \\[Pharm. Acta Helv. 70 (1995) 149; J. Chemomet. 10 (1996) 95; J. Med. Chem. 41 (1998) 2553\\]. In this paper, we use two-dimensional (2D) molecular descriptors and _k_ nearest neighbors ( _k_ NN) QSAR method for the analysis of several datasets. No correlation between the values of _q_ 2 for the training set and predictive ability for the test set was found for any of the datasets. Thus, the high value of LOO _q_ 2 appears to be the necessary but not the [sufficient condition](https://www.sciencedirect.com/topics/computer-science/sufficient-condition) for the model to have a high [predictive power](https://www.sciencedirect.com/topics/computer-science/predictive-power). We argue that this is the general property of QSAR models developed using LOO cross-validation. We emphasize that the external validation is the only way to establish a reliable QSAR model. We formulate a set of criteria for evaluation of predictive ability of QSAR models.\n\n## Introduction\n\nRapid development of combinatorial chemistry and high throughput screening methods in recent years has significantly increased a bulk of experimental structure\u2013activity relationship (SAR) datasets. These developments have emphasized a need for reliable analytical methods for biological SAR data examination such as quantitative SAR (QSAR). QSAR has been traditionally perceived as a means of establishing correlations between trends in chemical structure modifications and respective changes of biological activity \\[1\\]. However, in many cases of chemical library design, the number of compounds that could be practically synthesized and tested is much smaller than the total size of exhaustive virtual chemical libraries. There is a need for developing virtual library screening tools, and QSAR modeling can be adapted to the task of targeted library design \\[2\\], \\[3\\], \\[4\\]. Of course, any QSAR modeling should ultimately lead to statistically robust models capable of making accurate and reliable predictions of biological activities of compounds. However, the application of QSAR models for virtual screening places a special emphasis on statistical significance and predictive ability of these models as their most crucial characteristics. This paper examines the validity of one of the most popular criteria of QSAR model predictive ability, leave-one-out cross-validated _R_ 2 (LOO _q_ 2).\n\nThe process of QSAR model development can be generally divided into three stages: data preparation, data analysis, and model validation. The first stage includes selection of a molecular dataset for QSAR studies, calculation of molecular descriptors, and selection of a QSAR (statistical analysis and correlation) method. These steps represent a standard practice of any QSAR modeling, and their specific details are generally determined by the researchers\u2019 interests and software availability.\n\nThe second part of QSAR model development consists of an application of statistical approaches for QSAR model development. Many different algorithms and computer software are available for this purpose. Most are based on linear (multiple linear) regression with variable selection \\[5\\], partial least squares (PLS) \\[6\\], etc.) as well as non-linear (genetic algorithms \\[7\\], artificial neural networks \\[8\\], etc.) methods. In all approaches, descriptors serve as independent variables, and biological activities as dependent variables.\n\nThe last and as we emphasize in this paper, most important part of QSAR model development is the model _validation_. Most of the QSAR modeling methods implement the leave-one-out (or leave-some-out) cross-validation procedure. The outcome from the cross-validation procedure is cross-validated _R_ 2 ( _q_ 2), which is used as a criterion of both robustness and predictive ability of the model. Many authors consider high _q_ 2 (for instance, _q_ 2>0.5) as an indicator or even as the ultimate proof that the model is highly predictive. A widely used approach to establish the model robustness is so-called _y_-randomization (randomization of response, e.g. biological activities) \\[9\\]. It consists of repeating the calculation procedure with randomized activities and subsequent probability assessment of the resultant statistics. Often, it is used along with cross-validation. Sometimes, models are tested for their ability to predict accurately the activity of one or two compounds that were not used in model development (see, for instance \\[10\\], \\[11\\]). However, it is still common not to test QSAR models (characterized by a reasonably high LOO _q_ 2) for their ability to predict accurately biological activities of compounds from an external test dataset, i.e. those compounds, which were not used for the model development.\n\nAlthough, the low value of _q_ 2 for the training set can indeed serve as an indicator of a low predictive ability of a model, the opposite is not necessarily true. Indeed, the high _q_ 2 does not imply automatically a high predictive ability of the model. In order to both develop the model and validate it, one ...",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1093326301001231"
    },
    {
      "title": "Cross-validation pitfalls when selecting and assessing regression and classification models",
      "text": "Cross-validation pitfalls when selecting and assessing regression and classification models | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/1758-2946-6-10?)\n# Cross-validation pitfalls when selecting and assessing regression and classification models\n* Methodology\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:29 March 2014\n* Volume\u00a06, article\u00a0number10, (2014)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nCross-validation pitfalls when selecting and assessing regression and classification models\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n* [Damjan Krstajic](#auth-Damjan-Krstajic-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3),\n* [Ljubomir J Buturovic](#auth-Ljubomir_J-Buturovic-Aff3)[3](#Aff3),\n* [David E Leahy](#auth-David_E-Leahy-Aff4)[4](#Aff4)&amp;\n* \u2026* [Simon Thomas](#auth-Simon-Thomas-Aff5)[5](#Aff5)Show authors\n* 150kAccesses\n* 902Citations\n* 42Altmetric\n* 1Mention\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10/metrics)\n## Abstract\n### Background\nWe address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.\n### Methods\nWe describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case.\n### Results\nWe show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models.\n### Conclusions\nWe demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-024-06630-y/MediaObjects/10994_2024_6630_Fig1_HTML.png)\n### [Reducing cross-validation variance through seed blocking in hyperparameter tuning](https://link.springer.com/10.1007/s10994-024-06630-y?fromPaywallRec=false)\nArticle17 February 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-31041-7?as&#x3D;webp)\n### [Enhancement of Cross Validation Using Hybrid Visual and Analytical Means with Shannon Function](https://link.springer.com/10.1007/978-3-030-31041-7_29?fromPaywallRec=false)\nChapter\u00a9 2020\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42979-022-01051-x/MediaObjects/42979_2022_1051_Figa_HTML.png)\n### [An Efficient Ridge Regression Algorithm with Parameter Estimation for Data Analysis in Machine Learning](https://link.springer.com/10.1007/s42979-022-01051-x?fromPaywallRec=false)\nArticle23 February 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Compound Screening](https://jcheminf.biomedcentral.com/subjects/compound-screening)\n* [Linear Models and Regression](https://jcheminf.biomedcentral.com/subjects/linear-models-and-regression)\n* [Learning algorithms](https://jcheminf.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Molecular Target Validation](https://jcheminf.biomedcentral.com/subjects/molecular-target-validation)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Background\nAllen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)], Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] and Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)], independently introduced cross-validation as a way of estimating parameters for predictive models in order to improve predictions. Allen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)] proposed the PRESS (Prediction Sum of Squares) criteria, equivalent to leave-one-out cross-validation, for problems with selection of predictors and suggested it for general use. Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] suggested the use of leave-one-out cross-validation for estimating model parameters and for assessing their predictive error. It is important to note that Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] was the first to clearly differentiate between the use of cross-validation to select the model (\u201ccross-validatory choice\u201d) and to assess the model (\u201ccross-validatory assessment\u201d). Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)] introduced the Predictive Sample Reuse Method, a method equivalent to V-fold cross-validation, arguing that it improves predictive performance of the cross-validatory choice, at a cost of introducing pseudo-randomness in the process. Since then, cross-validation, with its different varieties, has been investigated extensively and, due to its universality, gained popularity in statistical modelling.\nIn an ideal situation we would have enough data to train and validate our models (training samples) and have separate data for assessing the quality of our model (test samples). Both training and test samples would need to be sufficiently large and diverse in order to be represenatitive. However such data rich situations are rare in life sciences, including QSAR. A major problem with selection and assessment of models is that we usually only have information from the training samples, and it is therefore not feasible to calculate a test error. However, even though we cannot calculate the test error, it is possible to estimate the expected test error using training samples. It can be shown that the expected test error is the...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10"
    },
    {
      "title": "Repeated double cross validation",
      "text": "Repeated double cross validation - Filzmoser - 2009 - Journal of Chemometrics - Wiley Online Library\n\nOpens in a new windowOpens an external websiteOpens an external website in a new window\n\nClose this dialog\n\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link: [Privacy Policy](https://www.wiley.com/privacy)\n\nClose Cookie Preferences\n\n[Journal of Chemometrics](https://analyticalsciencejournals.onlinelibrary.wiley.com/journal/1099128x)\n\n[Volume 23, Issue 4](https://analyticalsciencejournals.onlinelibrary.wiley.com/toc/1099128x/2009/23/4) p. 160-171 [![Journal of Chemometrics](https://analyticalsciencejournals.onlinelibrary.wiley.com/pb-assets/journal-banners/1099128x-1525769838240.jpg)](https://analyticalsciencejournals.onlinelibrary.wiley.com/journal/1099128x)\n\nResearch Article\n\n# Repeated double cross validation\n\n[Peter Filzmoser](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Filzmoser/Peter),\n\nPeter Filzmoser\n\nInstitute of Statistics and Probability Theory, Vienna University of Technology, Wiedner Hauptstrasse 8-10, A-1040 Vienna, Austria\n\n[Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Filzmoser/Peter)\n[Bettina Liebmann](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Liebmann/Bettina),\n\nBettina Liebmann\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria\n\n[Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Liebmann/Bettina)\n[Kurt Varmuza](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Varmuza/Kurt),\n\nCorresponding Author\n\nKurt Varmuza\n\n- [kvarmuza@email.tuwien.ac.at](mailto:kvarmuza@email.tuwien.ac.at)\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria. [Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Varmuza/Kurt)\n\n[Peter Filzmoser](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Filzmoser/Peter),\n\nPeter Filzmoser\n\nInstitute of Statistics and Probability Theory, Vienna University of Technology, Wiedner Hauptstrasse 8-10, A-1040 Vienna, Austria\n\n[Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Filzmoser/Peter)\n[Bettina Liebmann](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Liebmann/Bettina),\n\nBettina Liebmann\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria\n\n[Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Liebmann/Bettina)\n[Kurt Varmuza](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Varmuza/Kurt),\n\nCorresponding Author\n\nKurt Varmuza\n\n- [kvarmuza@email.tuwien.ac.at](mailto:kvarmuza@email.tuwien.ac.at)\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria\n\nLaboratory for Chemometrics, Institute of Chemical Engineering, Vienna University of Technology, Getreidemarkt 9/166, A-1060 Vienna, Austria. [Search for more papers by this author](https://analyticalsciencejournals.onlinelibrary.wiley.com/authored-by/Varmuza/Kurt)\n\nFirst published: 11 February 2009\n\n[https://doi.org/10.1002/cem.1225](https://doi.org/10.1002/cem.1225)\n\nCitations: [362](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.1225#citedby-section)\n\n### Abstract\n\nRepeated double cross validation (rdCV) is a strategy for (a) optimizing the complexity of regression models and (b) for a realistic estimation of prediction errors when the model is applied to new cases (that are within the population of the data used). This strategy is suited for small data sets and is a complementary method to bootstrap methods. rdCV is a formal, partly new combination of known procedures and methods, and has been implemented in a function for the programming environment R, providing several types of plots for model evaluation. The current version of the software is dedicated to regression models obtained by partial least-squares (PLS). The applied methods for repeated splits of the data into test sets and calibration sets, as well as for estimation of the optimum number of PLS components, are described. The relevance of some parameters (number of segments in CV, number of repetitions) is investigated. rdCV is applied to two data sets from chemistry: (1) determination of glucose concentrations from near infrared (NIR) data in mash samples from bioethanol production; (2) modeling the gas chromatographic retention indices of polycyclic aromatic compounds from molecular descriptors. Models using all original variables and models using a small subset of the variables, selected by a genetic algorithm (GA), are compared by rdCV. Copyright \u00a9 2009 John Wiley & Sons, Ltd.\n\n## REFERENCES\n\n- 1Martens H,\nNaes T.\nMultivariate Calibration.\nWiley: Chichester, UK;\n1989.\n[Web of Science\u00ae](https://analyticalsciencejournals.onlinelibrary.wiley.com/servlet/linkout?suffix=null&dbid=128&doi=10.1002%2Fcem.1225&key=A1989AA23500022&getFTLinkType=true&doiForPubOfPage=10.1002%2Fcem.1225&refDoi=e_1_2_1_2_2%3AISI&linkType=ISI&linkSource=FULL_TEXT&linkLocation=Reference)[Google Scholar](https://analyticalsciencejournals.onlinelibrary.wiley.com/action/getFTRLinkout?url=http%3A%2F%2Fscholar.google.com%2Fscholar_lookup%3Fhl%3Den%26publication_year%3D1989%26author%3DH%2BMartens%26author%3DT%2BNaes%26title%3DMultivariate%2BCalibration&doi=10.1002%2Fcem.1225&linkType=gs&linkLocation=Reference&linkSource=FULL_TEXT)\n\n- 2Varmuza K,\nFilzmoser P.\nIntroduction to Multivariate Statistical Analysis in Chemometrics.\nFrancis & Taylor, CRC Press: Boca Raton, FL, USA;\n2009.\n10.1201/9781420059496[Google Scholar](https://analyticalsciencejournals.onlinelibrary.wiley.com/action/getFTRLinkout?url=http%3A%2F%2Fscholar.google.com%2Fscholar_lookup%3Fhl%3Den%26publication_year%3D2009%26author%3DK%2BVarmuza%26author%3DP%2BFilzmoser%26title%3DIntroduction%2Bto%2BMultivariate%2BStatistical%2BAnalysis%2Bin%2BChemometrics&doi=10.1002%2Fcem.1225&doiOfLink=10.1201%2F9781420059496&linkType=gs&linkLocation=Reference&linkSource=FULL_TEXT)\n\n- 3Wold S,\nRuhe A,\nWold H,\nDunn WJI.\nThe collinearity problem in linear regression. The partial least squares approach to generalized inverses.\n_SIAM J. Sci. Stat. Comput._ 1984;\n5:\n735\u2013743.\n10.1137/0905052[Web of Science\u00ae](https://analyticalsciencejournals.onlinelibrary.wiley.com/servlet/linkout?suffix=null&dbid=128&doi=10.1002%2Fcem.1225&key=A1984TG34100017&getFTLinkType=true&doiForPubOfPage=10.1002%2Fcem.1225&refDoi=10.1137%2F0905052&linkType=ISI&linkSource=FULL_TEXT&linkLocation=Reference)[Google Scholar](https://analyticalsciencejournals.onlinelibrary.wiley.com/action/getFTRLinkout?url=http%3A%2F%2Fscholar.google.com%2Fscholar_lookup%3Fhl%3Den%26volume%3D5%26publication_year%3D1984%26pages%3D735-743%26journal%3DSIAM%2BJ.%2BSci.%2BStat.%2BComput.%26author%3DS%2BWold%26author%3DA%2BRuhe%26author%3DH%2BWold%26author%3DWJI%2BDunn%26title%3DThe%2Bcollinearity%2Bproblem%2Bin%2Blinear%2Bregression.%2BThe%2Bpartial%2Bleast%2Bsquares%2Bapproach%2Bto%2Bgeneralized%2Binverses&doi=10.1002%2Fcem.1225&doiOfLink=10.1137%2F0905052&linkType=gs&linkLocation=Reference&linkSource=FULL_TEXT)\n\n- 4Vandeginste BGM,\nMassart DL,\nBuydens LCM,\nDe Jong S,\nSmeyers-Verbeke J.\nHandbook of Chemometrics and Qualimetrics: Part B.\nElsevier: Amsterdam, The Netherlands;\n1...",
      "url": "https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/cem.1225"
    },
    {
      "title": "GREYC Chemistry dataset",
      "text": "GREYC's Chemistry dataset\n\n![ensicaen](https://brunl01.users.greyc.fr/IMAGES/logo_ensicaen.jpg)[![greyc](https://brunl01.users.greyc.fr/IMAGES/LogoGreyc.png)](https://www.greyc.fr/)![LCMT](https://brunl01.users.greyc.fr/CHEMISTRY/logo_lcmt.png)\n\nThis page\ncontains several archive or links towards various chemical databases\nof molecules. Each database concerns a specific problem (either\nclassification or Regression).\n\n| Dataset | \\# dataset | mean size | mean degree | min size | max size | Stereoisomerism | Problem type |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| [PAH](https://brunl01.users.greyc.fr/CHEMISTRY/#PAH) | 94 | 20.7 | 2.4 | 10 | 28 | No | Classif. |\n| [MAO](https://brunl01.users.greyc.fr/CHEMISTRY/#MAO) | 68 | 18.4 | 2.1 | 11 | 27 | No | Classif. |\n| [PTC](https://brunl01.users.greyc.fr/CHEMISTRY/#PTC) | 416 | 14.4 | 2.1 | 2 | 64 | No | Classif. |\n| [AIDS](https://brunl01.users.greyc.fr/CHEMISTRY/#AIDS) | 2000 | 15.7 | 2.1 | 2 | 95 | No | Classif. |\n| [Alkane](https://brunl01.users.greyc.fr/CHEMISTRY/#Alkane) | 150 | 8.9 | 1.8 | 1 | 10 | No | Regression |\n| [Acyclic](https://brunl01.users.greyc.fr/CHEMISTRY/#Acyclic) | 185 | 8.2 | 1.8 | 3 | 11 | No | Regression |\n| [Chiral Acyclic](https://brunl01.users.greyc.fr/CHEMISTRY/#Stereo) | 35 | 21.29 | 1.98 | 14 | 32 | Yes | Regression |\n| [Vitamin D](https://brunl01.users.greyc.fr/CHEMISTRY/#VitaminD) | 69 | 76.91 | 2.05 | 68 | 88 | Yes | Regression |\n| [ACE](https://brunl01.users.greyc.fr/CHEMISTRY/#ACE) | 32 | 52 | 2.04 | 52 | 52 | Yes | Classification |\n| [Steroid](https://brunl01.users.greyc.fr/CHEMISTRY/#Steroid) | 64 | 75.11 | 2.08 | 57 | 94 | Yes | Regression |\n| [Monoterpens](https://brunl01.users.greyc.fr/CHEMISTRY/#Mono) | 382 | 10 |  |  |  | No | Classification |\n\nDescription of the different chemical datasets provided by GREYC and LCMT laboratories.\n\n## Achiral Molecules\n\n- [Monoamine Oxydase (MAO) dataset](https://brunl01.users.greyc.fr/CHEMISTRY/mao.tgz)\n  (classification problem). The class are specified in the file\n  dataset.ds. This dataset is composed of 68 molecules divided\n  into two classes: 38 molecules inhibit the monoamine oxidase\n  (antidepressant drugs) and 30 do not.\n  Results bellow are obtained for each method using a leave one out\n  procedure with a two-class SVM. This classification scheme is made\n  for each of the 68 molecules of the dataset.\n\n\n\n  |  | Method | Classification accuracy |\n  | --- | --- | --- |\n  | (1) | Suard et al. (2002) | 80% (55/68) |\n  | (2) | Vishwanathan et al. (2010) | 82% (56/68) |\n  | (3) | Neuhaus and Bunke (2007) | 90% (61/68) |\n  | (4) | Riesen et al. (2007) | 91% (62/68) |\n  | (5) | Normalized standard Graph Laplacian kernel | 90% (61/68) |\n  | (6) | Normalized fast Graph Laplacian kernel | 90% (61/68) |\n  | (7) | Mah\u00e9 and Vert (2008) | 96% (65/68) |\n  | (8) | Ga\u00fcz\u00e8re et al. (2012) | 94% (64/68) |\n\n  Classification accuracy on the monoamine oxidase (MAO) dataset.\n  Method\n\n- [A database of acyclic molecules with hetero atoms](https://brunl01.users.greyc.fr/CHEMISTRY/Acyclic.tar.gz) (acyclic ethers, peroxides, acetals and their sulfur analogues). The dataset is concerned with the determination of the boiling point using regression (see dataset.ds)\n  In both results presented bellow GLK stands for graph Laplacian\n  kernel.\n\n\n\n  | |  | Method | Average error (\u00baC) | RMSE (\u00baC) |\n  | (1) | Cherqaoui et al. (1994a) | 3.01 | 5.102 |\n  | (2) | Neuhaus and Bunke (2007) | 6.84 | 9.04 |\n  | (3) | Riesen et al. (2007) | 7.39 | 9.94 |\n  | (4) | GLK (diffusion process) | 7.27 | 9.84 |\n  | (5) | Suard et al. (2002) | 7.28 | 12.37 |\n  | (6) | Vishwanathan et al. (2010) | 14.64 | 18.95 |\n  | (7) | Mah\u00e9 and Vert (2008) | 5.53 | 9.50 |\n  | (8) | Ga\u00fcz\u00e8re et al. (2012) | 4.90 | 7.80 |\n  | (9) | Ga\u00fcz\u00e8re et al. with forward selection | 3.23 | 4.36 |\n  | (10) | Ga\u00fcz\u00e8re et al. with backward elimination | 2.63 | 3.70 |\n\n  Boiling point prediction on acyclic molecule dataset using a leave one out procedure. |  | |  | Method | Average error (\u00baC) | RMSE (\u00baC) |\n  | (1) |  |  |  |\n  | (2) | Neuhaus and Bunke (2007) | 7.78 | 10.27 |\n  | (3) | Riesen et al. (2007) | 7.66 | 10.19 |\n  | (4) | GLK (Diffusion Process) | 8.58 | 11.99 |\n  | (5) | Suard et al. (2002) | 7.22 | 12.24 |\n  | (6) | Vishwanathan et al. (2010) | 14.57 | 18.72 |\n  | (7) | Mah\u00e9 and Vert (2008) | 6.42 | 11.02 |\n  | (8) | Ga\u00fcz\u00e8re et al. (2012) | 5.27 | 8.10 |\n  | (9) | Ga\u00fcz\u00e8re et al. with forward selection | 4.89 | 7.05 |\n  | (10) | Ga\u00fcz\u00e8re et al. with backward elimination | 4.87 | 6.75 |\n\n  Boiling point prediction on acyclic molecule dataset using 90% of the dataset as train\n  set and remaining 10% as test set. |\n\n- [The Predictive Toxicology\\\n  Challenge (PTC) dataset](https://brunl01.users.greyc.fr/CHEMISTRY/ptc.tgz) (classification problem). Please see [this page](http://www.predictive-toxicology.org/ptc/) for a full documentation.\n- [Alkane\\\n  database](https://brunl01.users.greyc.fr/CHEMISTRY/alkane_dataset.tar.gz)(regression problem) This is a purely structural database\n  (only carbons) with a problem of regression (boiling points data in\n  dataset.ds).\n  Results displayed bellow have been obtained using several\n  test sets composed of 10% of the database, the remaining 90% being\n  used as training set. For each test molecules composing the test and\n  training sets have been choosen randomly.\n\n\n\n  |  | Method | Average error (\u00baC) | RMSE (\u00baC) |\n  | (1) | Cherqaoui and Villemin (1994) | 3.11 | 3.70 |\n  | (2) | Neuhaus and Bunke (2007) | 5.42 | 10.01 |\n  | (3) | Riesen et al. (2007) | 5.27 | 7.10 |\n  | (4) | Suard et al. (2002) | 4.66 | 6.21 |\n  | (5) | Vishwanathan et al. (2010) | 10.61 | 16.28 |\n  | (6) | Graph Laplacian kernel | 10.79 | 16.45 |\n  | (7) | Mah\u00e9 and Vert (2008) | 2.41 | 3.48 |\n  | (8) | Ga\u00fcz\u00e8re et al. (2012) | 1.41 | 1.92 |\n\n  Boiling point prediction on alkane dataset. RMSE stands for root mean squared error.\n\n- [HIV database](https://iapr-tc15.greyc.fr/IAM/AIDS.zip)(classification problem) contains 42,687 compounds evaluated for evidence of anti-HIV activity by DTP AIDS Antiviral Screen of the National Cancer Institute. 422 molecules are confirmed active, 1081 moderately active and 41,184 inactive.\n- [Polyciclic Aromatic Hydrocarbons\\\n  dataset (PAH)](https://brunl01.users.greyc.fr/CHEMISTRY/PAH.tar.gz) This dataset is composed cyclic unlabeled graphs. All\n  atoms are carbons, all bounds are aromatics. Note that few acyclic\n  bounds connect some atoms to cycles. This is a classification problem (cancerous or not cancerous molecules)\n\n\n  |  | Method | Classification accuracy |\n  | (1) | [Treelet Kernel\\[5\\]](http://hal.archives-ouvertes.fr/hal-00773283) | 71.3 |\n  | (2) | Graph Edit distance\\[8\\] | 72 |\n  | (3) | Cycles \\[16\\] | 63 |\n  | (4) | [Relevant Cycle graph \\[6\\]](http://hal.archives-ouvertes.fr/hal-00768652) | 77.7 |\n  | (5) | [Relevant Cycle Hypergraph \\[7\\]](http://hal.archives-ouvertes.fr/hal-00829227) | 76.3 |\n  | (6) | [Augmented Cycle \\[8\\]](https://hal.archives-ouvertes.fr/hal-01066389) | 80.7 |\n\n  Classification of cyclic molecules solely composed of carbons using a 10-fold cross classification\n- [Monoterpenoide\\\n  dataset](https://brunl01.users.greyc.fr/CHEMISTRY/monoterpenoides.tar.gz) This dataset is composed different classes of [monoterpenoides](https://en.wikipedia.org/wiki/Monoterpene) molecules encoded by labeled graphs. This is a classification problem\n  with 10 classes (Molecules are grouped according to the existence of a same\n  precursor). This dataset is thus well suited for k-mean like\n  algorithms.\n- You may also be interested by the [datasets](http://cdb.ics.uci.edu/cgibin/LearningDatasetsWeb.py) of the University of California, Irvine\n\n## Chiral Molecules\n\n- [Optical rotation of chiral\\\n  molecules](https://brunl01.users.greyc.fr/CHEMISTRY/DatasetAcyclicChiral.tar).\n  A dataset of acyclic molecules with chiral atoms. Molecules are\n  provide with their optical rotation for a regression problem. Each\n  molecules is composed of one or two ...",
      "url": "https://brunl01.users.greyc.fr/CHEMISTRY"
    }
  ]
}