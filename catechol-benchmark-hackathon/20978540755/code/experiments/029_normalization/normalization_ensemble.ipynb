{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9855e5cb",
   "metadata": {},
   "source": [
    "# Post-Processing Normalization (SM+P2+P3=1)\n",
    "\n",
    "**Problem**: The CV-LB gap is ~10x. The intercept (0.0533) is 3x higher than target (0.01727).\n",
    "\n",
    "**Key Insight**: Other competitors use post-processing normalization (SM+P2+P3=1) but we haven't tried it!\n",
    "\n",
    "**Hypothesis**: Enforcing mass balance constraint may improve generalization.\n",
    "\n",
    "**Implementation**:\n",
    "- Use best model from exp_026 (weighted loss [1,1,2])\n",
    "- Add post-processing normalization: preds = preds / preds.sum(axis=1)\n",
    "- This enforces SM + P2 + P3 = 1 (mass balance)\n",
    "\n",
    "**Baseline**: exp_026 CV 0.008465, LB 0.0887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038f8ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:16.872877Z",
     "iopub.status.busy": "2026-01-14T09:03:16.872608Z",
     "iopub.status.idle": "2026-01-14T09:03:19.430323Z",
     "shell.execute_reply": "2026-01-14T09:03:19.429726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightgbm as lgb\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a046ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.432476Z",
     "iopub.status.busy": "2026-01-14T09:03:19.432172Z",
     "iopub.status.idle": "2026-01-14T09:03:19.439187Z",
     "shell.execute_reply": "2026-01-14T09:03:19.438660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "INPUT_LABELS_NUMERIC = [\"Residence Time\", \"Temperature\"]\n",
    "INPUT_LABELS_SINGLE_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT NAME\"]\n",
    "INPUT_LABELS_FULL_SOLVENT = [\"Residence Time\", \"Temperature\", \"SOLVENT A NAME\", \"SOLVENT B NAME\", \"SolventB%\"]\n",
    "\n",
    "def load_data(name=\"full\"):\n",
    "    if name == \"full\":\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "        X = df[INPUT_LABELS_FULL_SOLVENT]\n",
    "    else:\n",
    "        df = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "        X = df[INPUT_LABELS_SINGLE_SOLVENT]\n",
    "    Y = df[[\"Product 2\", \"Product 3\", \"SM\"]]\n",
    "    return X, Y\n",
    "\n",
    "def generate_leave_one_out_splits(X, Y):\n",
    "    for solvent in sorted(X[\"SOLVENT NAME\"].unique()):\n",
    "        mask = X[\"SOLVENT NAME\"] != solvent\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "def generate_leave_one_ramp_out_splits(X, Y):\n",
    "    ramps = X[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "    for _, row in ramps.iterrows():\n",
    "        mask = ~((X[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"]))\n",
    "        yield (X[mask], Y[mask]), (X[~mask], Y[~mask])\n",
    "\n",
    "print('Data loading functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9a5485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.440988Z",
     "iopub.status.busy": "2026-01-14T09:03:19.440814Z",
     "iopub.status.idle": "2026-01-14T09:03:19.492623Z",
     "shell.execute_reply": "2026-01-14T09:03:19.492137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spange: (26, 13), DRFP filtered: (24, 122), ACS PCA: (24, 5)\n",
      "Total features: 5 (kinetic) + 13 (Spange) + 122 (DRFP) + 5 (ACS PCA) = 145\n"
     ]
    }
   ],
   "source": [
    "# Load feature lookups\n",
    "SPANGE_DF = pd.read_csv(f'{DATA_PATH}/spange_descriptors_lookup.csv', index_col=0)\n",
    "DRFP_DF = pd.read_csv(f'{DATA_PATH}/drfps_catechol_lookup.csv', index_col=0)\n",
    "ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)\n",
    "\n",
    "# Filter DRFP to high-variance columns\n",
    "drfp_variance = DRFP_DF.var()\n",
    "nonzero_variance_cols = drfp_variance[drfp_variance > 0].index.tolist()\n",
    "DRFP_FILTERED = DRFP_DF[nonzero_variance_cols]\n",
    "\n",
    "print(f'Spange: {SPANGE_DF.shape}, DRFP filtered: {DRFP_FILTERED.shape}, ACS PCA: {ACS_PCA_DF.shape}')\n",
    "print(f'Total features: 5 (kinetic) + {SPANGE_DF.shape[1]} (Spange) + {DRFP_FILTERED.shape[1]} (DRFP) + {ACS_PCA_DF.shape[1]} (ACS PCA) = {5 + SPANGE_DF.shape[1] + DRFP_FILTERED.shape[1] + ACS_PCA_DF.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4d3c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.494495Z",
     "iopub.status.busy": "2026-01-14T09:03:19.494321Z",
     "iopub.status.idle": "2026-01-14T09:03:19.502815Z",
     "shell.execute_reply": "2026-01-14T09:03:19.502347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension: 145\n"
     ]
    }
   ],
   "source": [
    "# Combined Featurizer\n",
    "class ACSPCAFeaturizer:\n",
    "    def __init__(self, mixed=False):\n",
    "        self.mixed = mixed\n",
    "        self.spange_df = SPANGE_DF\n",
    "        self.drfp_df = DRFP_FILTERED\n",
    "        self.acs_pca_df = ACS_PCA_DF\n",
    "        self.feats_dim = 2 + 3 + self.spange_df.shape[1] + self.drfp_df.shape[1] + self.acs_pca_df.shape[1]\n",
    "\n",
    "    def featurize(self, X, flip=False):\n",
    "        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)\n",
    "        temp_c = X_vals[:, 1:2]\n",
    "        time_m = X_vals[:, 0:1]\n",
    "        temp_k = temp_c + 273.15\n",
    "        inv_temp = 1000.0 / temp_k\n",
    "        log_time = np.log(time_m + 1e-6)\n",
    "        interaction = inv_temp * log_time\n",
    "        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])\n",
    "        \n",
    "        if self.mixed:\n",
    "            A_spange = self.spange_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_spange = self.spange_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_drfp = self.drfp_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_drfp = self.drfp_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            A_acs = self.acs_pca_df.loc[X[\"SOLVENT A NAME\"]].values\n",
    "            B_acs = self.acs_pca_df.loc[X[\"SOLVENT B NAME\"]].values\n",
    "            pct = X[\"SolventB%\"].values.reshape(-1, 1)\n",
    "            if flip:\n",
    "                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)\n",
    "                X_drfp = B_drfp * (1 - (1-pct)) + A_drfp * (1-pct)\n",
    "                X_acs = B_acs * (1 - (1-pct)) + A_acs * (1-pct)\n",
    "            else:\n",
    "                X_spange = A_spange * (1 - pct) + B_spange * pct\n",
    "                X_drfp = A_drfp * (1 - pct) + B_drfp * pct\n",
    "                X_acs = A_acs * (1 - pct) + B_acs * pct\n",
    "        else:\n",
    "            X_spange = self.spange_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_drfp = self.drfp_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "            X_acs = self.acs_pca_df.loc[X[\"SOLVENT NAME\"]].values\n",
    "        \n",
    "        return np.hstack([X_kinetic, X_spange, X_drfp, X_acs])\n",
    "    \n",
    "    def featurize_torch(self, X, flip=False):\n",
    "        return torch.tensor(self.featurize(X, flip))\n",
    "\n",
    "print(f'Feature dimension: {ACSPCAFeaturizer().feats_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb495c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.504731Z",
     "iopub.status.busy": "2026-01-14T09:03:19.504242Z",
     "iopub.status.idle": "2026-01-14T09:03:19.509768Z",
     "shell.execute_reply": "2026-01-14T09:03:19.509277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedHuberLoss defined\n"
     ]
    }
   ],
   "source": [
    "# Weighted Huber Loss\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, weights=[1.0, 1.0, 2.0]):\n",
    "        super().__init__()\n",
    "        self.weights = torch.tensor(weights, dtype=torch.double)\n",
    "        self.huber = nn.HuberLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        huber_loss = self.huber(pred, target)\n",
    "        weighted_loss = huber_loss * self.weights.to(pred.device)\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "print('WeightedHuberLoss defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8775aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.511197Z",
     "iopub.status.busy": "2026-01-14T09:03:19.511040Z",
     "iopub.status.idle": "2026-01-14T09:03:19.517134Z",
     "shell.execute_reply": "2026-01-14T09:03:19.516654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModelInternal defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Model\n",
    "class MLPModelInternal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=3, dropout=0.05):\n",
    "        super(MLPModelInternal, self).__init__()\n",
    "        layers = [nn.BatchNorm1d(input_dim)]\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, h_dim), nn.BatchNorm1d(h_dim), nn.ReLU(), nn.Dropout(dropout)])\n",
    "            prev_dim = h_dim\n",
    "        layers.extend([nn.Linear(prev_dim, output_dim), nn.Sigmoid()])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print('MLPModelInternal defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ba6a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.519087Z",
     "iopub.status.busy": "2026-01-14T09:03:19.518592Z",
     "iopub.status.idle": "2026-01-14T09:03:19.529420Z",
     "shell.execute_reply": "2026-01-14T09:03:19.528931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedMLPEnsemble defined\n"
     ]
    }
   ],
   "source": [
    "# MLP Ensemble with Weighted Loss\n",
    "class WeightedMLPEnsemble:\n",
    "    def __init__(self, hidden_dims=[32, 16], n_models=5, data='single', loss_weights=[1.0, 1.0, 2.0]):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_models = n_models\n",
    "        self.data_type = data\n",
    "        self.loss_weights = loss_weights\n",
    "        self.featurizer = ACSPCAFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train, epochs=200, batch_size=32, lr=5e-4):\n",
    "        X_std = self.featurizer.featurize_torch(X_train, flip=False)\n",
    "        y_vals = torch.tensor(y_train.values)\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_train, flip=True)\n",
    "            X_all = torch.cat([X_std, X_flip], dim=0)\n",
    "            y_all = torch.cat([y_vals, y_vals], dim=0)\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "            \n",
    "        input_dim = X_all.shape[1]\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            torch.manual_seed(42 + i * 13)\n",
    "            np.random.seed(42 + i * 13)\n",
    "            \n",
    "            model = MLPModelInternal(input_dim, self.hidden_dims).to(device).double()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "            criterion = WeightedHuberLoss(weights=self.loss_weights)\n",
    "            \n",
    "            dataset = TensorDataset(X_all.to(device), y_all.to(device))\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0.0\n",
    "                for batch_X, batch_y in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(batch_X)\n",
    "                    loss = criterion(pred, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                scheduler.step(epoch_loss / len(loader))\n",
    "            \n",
    "            model.eval()\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_feat = self.featurizer.featurize_torch(X_test, flip=False).to(device)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize_torch(X_test, flip=True).to(device)\n",
    "        \n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                pred = model(X_feat)\n",
    "                if self.data_type == 'full':\n",
    "                    pred_flip = model(X_flip)\n",
    "                    pred = (pred + pred_flip) / 2\n",
    "                all_preds.append(pred)\n",
    "        \n",
    "        return torch.stack(all_preds).mean(dim=0).cpu()\n",
    "\n",
    "print('WeightedMLPEnsemble defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8466ba62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.531100Z",
     "iopub.status.busy": "2026-01-14T09:03:19.530631Z",
     "iopub.status.idle": "2026-01-14T09:03:19.538034Z",
     "shell.execute_reply": "2026-01-14T09:03:19.537540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMWrapper defined\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Wrapper\n",
    "class LGBMWrapper:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.featurizer = ACSPCAFeaturizer(mixed=(data=='full'))\n",
    "        self.models = []\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        X_std = self.featurizer.featurize(X_train, flip=False)\n",
    "        y_vals = y_train.values\n",
    "        \n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_train, flip=True)\n",
    "            X_all = np.vstack([X_std, X_flip])\n",
    "            y_all = np.vstack([y_vals, y_vals])\n",
    "        else:\n",
    "            X_all, y_all = X_std, y_vals\n",
    "        \n",
    "        self.models = []\n",
    "        params = {'objective': 'regression', 'metric': 'mse', 'boosting_type': 'gbdt',\n",
    "                  'num_leaves': 31, 'learning_rate': 0.05, 'feature_fraction': 0.9,\n",
    "                  'bagging_fraction': 0.8, 'bagging_freq': 5, 'verbose': -1, 'seed': 42}\n",
    "        \n",
    "        for i in range(3):\n",
    "            train_data = lgb.Dataset(X_all, label=y_all[:, i])\n",
    "            model = lgb.train(params, train_data, num_boost_round=100)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_feat = self.featurizer.featurize(X_test, flip=False)\n",
    "        if self.data_type == 'full':\n",
    "            X_flip = self.featurizer.featurize(X_test, flip=True)\n",
    "        \n",
    "        preds = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            pred = model.predict(X_feat)\n",
    "            if self.data_type == 'full':\n",
    "                pred_flip = model.predict(X_flip)\n",
    "                pred = (pred + pred_flip) / 2\n",
    "            preds.append(pred)\n",
    "        \n",
    "        return torch.tensor(np.column_stack(preds))\n",
    "\n",
    "print('LGBMWrapper defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd27477b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:19.539706Z",
     "iopub.status.busy": "2026-01-14T09:03:19.539255Z",
     "iopub.status.idle": "2026-01-14T09:03:19.545618Z",
     "shell.execute_reply": "2026-01-14T09:03:19.545127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormalizationEnsemble defined with POST-PROCESSING NORMALIZATION (SM+P2+P3=1)\n"
     ]
    }
   ],
   "source": [
    "# Normalization Ensemble: MLP + LightGBM with POST-PROCESSING NORMALIZATION\n",
    "class NormalizationEnsemble:\n",
    "    def __init__(self, data='single'):\n",
    "        self.data_type = data\n",
    "        self.mlp = WeightedMLPEnsemble(hidden_dims=[32, 16], n_models=5, data=data, loss_weights=[1.0, 1.0, 2.0])\n",
    "        self.lgbm = LGBMWrapper(data=data)\n",
    "        self.mlp_weight = 0.6\n",
    "        self.lgbm_weight = 0.4\n",
    "\n",
    "    def train_model(self, X_train, y_train):\n",
    "        self.mlp.train_model(X_train, y_train)\n",
    "        self.lgbm.train_model(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        mlp_pred = self.mlp.predict(X_test)\n",
    "        lgbm_pred = self.lgbm.predict(X_test)\n",
    "        \n",
    "        # Weighted average\n",
    "        combined = self.mlp_weight * mlp_pred + self.lgbm_weight * lgbm_pred\n",
    "        \n",
    "        # Clip to [0, 1]\n",
    "        combined = torch.clamp(combined, 0, 1)\n",
    "        \n",
    "        # POST-PROCESSING NORMALIZATION: SM + P2 + P3 = 1\n",
    "        # preds is [P2, P3, SM] - normalize so they sum to 1\n",
    "        row_sums = combined.sum(dim=1, keepdim=True)\n",
    "        row_sums = torch.clamp(row_sums, min=1e-6)  # Avoid division by zero\n",
    "        normalized = combined / row_sums\n",
    "        \n",
    "        return normalized\n",
    "\n",
    "print('NormalizationEnsemble defined with POST-PROCESSING NORMALIZATION (SM+P2+P3=1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26379c31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:03:34.026013Z",
     "iopub.status.busy": "2026-01-14T09:03:34.025437Z",
     "iopub.status.idle": "2026-01-14T09:23:05.563705Z",
     "shell.execute_reply": "2026-01-14T09:23:05.563114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:49, 49.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [01:38, 49.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [02:25, 48.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [03:12, 47.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [04:01, 48.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [04:50, 48.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [05:39, 48.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [06:28, 48.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [07:17, 48.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [08:06, 48.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [08:55, 48.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [09:44, 48.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [10:33, 48.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [11:21, 48.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [12:10, 48.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [12:59, 48.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [13:50, 49.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [14:39, 49.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [15:27, 48.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [16:16, 48.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [17:05, 48.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [17:54, 48.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [18:42, 48.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [19:31, 48.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [19:31, 48.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "import tqdm\n",
    "\n",
    "X, Y = load_data(\"single_solvent\")\n",
    "\n",
    "split_generator = generate_leave_one_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = NormalizationEnsemble(data='single')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 0,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_single_solvent = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf09461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T09:23:19.794408Z",
     "iopub.status.busy": "2026-01-14T09:23:19.793668Z",
     "iopub.status.idle": "2026-01-14T10:02:05.023351Z",
     "shell.execute_reply": "2026-01-14T10:02:05.022795Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [02:49, 169.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [05:36, 168.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [08:29, 170.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [11:17, 169.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [14:05, 168.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [16:53, 168.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [19:41, 168.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [22:33, 169.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [25:24, 169.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [28:30, 174.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [31:54, 183.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [35:19, 190.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [38:45, 195.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [38:45, 178.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "X, Y = load_data(\"full\")\n",
    "\n",
    "split_generator = generate_leave_one_ramp_out_splits(X, Y)\n",
    "all_predictions = []\n",
    "\n",
    "for fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n",
    "    (train_X, train_Y), (test_X, test_Y) = split\n",
    "\n",
    "    model = NormalizationEnsemble(data='full')  # CHANGE THIS LINE ONLY\n",
    "    model.train_model(train_X, train_Y)\n",
    "\n",
    "    predictions = model.predict(test_X)  # Shape: [N, 3]\n",
    "\n",
    "    # Move to CPU and convert to numpy\n",
    "    predictions_np = predictions.detach().cpu().numpy()\n",
    "\n",
    "    # Add metadata and flatten to long format\n",
    "    for row_idx, row in enumerate(predictions_np):\n",
    "        all_predictions.append({\n",
    "            \"task\": 1,\n",
    "            \"fold\": fold_idx,\n",
    "            \"row\": row_idx,\n",
    "            \"target_1\": row[0],\n",
    "            \"target_2\": row[1],\n",
    "            \"target_3\": row[2]\n",
    "        })\n",
    "\n",
    "# Save final submission\n",
    "submission_full_data = pd.DataFrame(all_predictions)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n",
    "########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126f811d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T10:02:22.615216Z",
     "iopub.status.busy": "2026-01-14T10:02:22.614467Z",
     "iopub.status.idle": "2026-01-14T10:02:22.635478Z",
     "shell.execute_reply": "2026-01-14T10:02:22.634855Z"
    }
   },
   "outputs": [],
   "source": [
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n",
    "\n",
    "submission = pd.concat([submission_single_solvent, submission_full_data])\n",
    "submission = submission.reset_index()\n",
    "submission.index.name = \"id\"\n",
    "submission.to_csv(\"/home/submission/submission.csv\", index=True)\n",
    "\n",
    "########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n",
    "########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f8b829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T10:03:03.132401Z",
     "iopub.status.busy": "2026-01-14T10:03:03.131896Z",
     "iopub.status.idle": "2026-01-14T10:03:03.173142Z",
     "shell.execute_reply": "2026-01-14T10:03:03.172524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NORMALIZATION CHECK ===\n",
      "Single Solvent: pred sums range [1.0000, 1.0000], mean=1.0000\n",
      "Full Data: pred sums range [1.0000, 1.0000], mean=1.0000\n",
      "\n",
      "=== CV SCORE VERIFICATION ===\n",
      "Single Solvent MSE: 0.016141 (n=656)\n",
      "Full Data MSE: 0.016201 (n=1227)\n",
      "Overall MSE: 0.016180\n",
      "\n",
      "exp_026 baseline (no normalization): CV 0.008465, LB 0.0887\n",
      "This (with normalization): CV 0.016180\n",
      "\n",
      "✗ WORSE: 91.14% worse than exp_026\n"
     ]
    }
   ],
   "source": [
    "# Calculate CV score (for verification only - NOT part of submission)\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "# Get actuals in same order as predictions\n",
    "actuals_single = []\n",
    "for solvent in sorted(X_single[\"SOLVENT NAME\"].unique()):\n",
    "    mask = X_single[\"SOLVENT NAME\"] == solvent\n",
    "    actuals_single.append(Y_single[mask].values)\n",
    "actuals_single = np.vstack(actuals_single)\n",
    "\n",
    "actuals_full = []\n",
    "ramps = X_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates()\n",
    "for _, row in ramps.iterrows():\n",
    "    mask = (X_full[\"SOLVENT A NAME\"] == row[\"SOLVENT A NAME\"]) & (X_full[\"SOLVENT B NAME\"] == row[\"SOLVENT B NAME\"])\n",
    "    actuals_full.append(Y_full[mask].values)\n",
    "actuals_full = np.vstack(actuals_full)\n",
    "\n",
    "# Get predictions\n",
    "preds_single = submission_single_solvent[['target_1', 'target_2', 'target_3']].values\n",
    "preds_full = submission_full_data[['target_1', 'target_2', 'target_3']].values\n",
    "\n",
    "# Calculate MSE\n",
    "mse_single = np.mean((actuals_single - preds_single) ** 2)\n",
    "mse_full = np.mean((actuals_full - preds_full) ** 2)\n",
    "n_single = len(actuals_single)\n",
    "n_full = len(actuals_full)\n",
    "overall_mse = (mse_single * n_single + mse_full * n_full) / (n_single + n_full)\n",
    "\n",
    "# Check normalization constraint\n",
    "pred_sums_single = preds_single.sum(axis=1)\n",
    "pred_sums_full = preds_full.sum(axis=1)\n",
    "print(f'\\n=== NORMALIZATION CHECK ===')\n",
    "print(f'Single Solvent: pred sums range [{pred_sums_single.min():.4f}, {pred_sums_single.max():.4f}], mean={pred_sums_single.mean():.4f}')\n",
    "print(f'Full Data: pred sums range [{pred_sums_full.min():.4f}, {pred_sums_full.max():.4f}], mean={pred_sums_full.mean():.4f}')\n",
    "\n",
    "print(f'\\n=== CV SCORE VERIFICATION ===')\n",
    "print(f'Single Solvent MSE: {mse_single:.6f} (n={n_single})')\n",
    "print(f'Full Data MSE: {mse_full:.6f} (n={n_full})')\n",
    "print(f'Overall MSE: {overall_mse:.6f}')\n",
    "print(f'\\nexp_026 baseline (no normalization): CV 0.008465, LB 0.0887')\n",
    "print(f'This (with normalization): CV {overall_mse:.6f}')\n",
    "\n",
    "if overall_mse < 0.008465:\n",
    "    improvement = (0.008465 - overall_mse) / 0.008465 * 100\n",
    "    print(f'\\n✓ IMPROVEMENT: {improvement:.2f}% better than exp_026!')\n",
    "else:\n",
    "    degradation = (overall_mse - 0.008465) / 0.008465 * 100\n",
    "    print(f'\\n✗ WORSE: {degradation:.2f}% worse than exp_026')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a83742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-14T10:04:07.555001Z",
     "iopub.status.busy": "2026-01-14T10:04:07.554320Z",
     "iopub.status.idle": "2026-01-14T10:04:07.573777Z",
     "shell.execute_reply": "2026-01-14T10:04:07.573063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ACTUAL TARGET SUMS ===\n",
      "Single Solvent: range [0.0288, 1.0000], mean=0.7955, std=0.1942\n",
      "Full Data: range [0.0112, 1.1233], mean=0.8035, std=0.2091\n",
      "\\nThe actual targets do NOT sum to exactly 1.0!\n"
     ]
    }
   ],
   "source": [
    "# Check actual target sums in the data\n",
    "X_single, Y_single = load_data(\"single_solvent\")\n",
    "X_full, Y_full = load_data(\"full\")\n",
    "\n",
    "actual_sums_single = Y_single.values.sum(axis=1)\n",
    "actual_sums_full = Y_full.values.sum(axis=1)\n",
    "\n",
    "print(f'=== ACTUAL TARGET SUMS ===')\n",
    "print(f'Single Solvent: range [{actual_sums_single.min():.4f}, {actual_sums_single.max():.4f}], mean={actual_sums_single.mean():.4f}, std={actual_sums_single.std():.4f}')\n",
    "print(f'Full Data: range [{actual_sums_full.min():.4f}, {actual_sums_full.max():.4f}], mean={actual_sums_full.mean():.4f}, std={actual_sums_full.std():.4f}')\n",
    "print(f'\\\\nThe actual targets do NOT sum to exactly 1.0!')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
