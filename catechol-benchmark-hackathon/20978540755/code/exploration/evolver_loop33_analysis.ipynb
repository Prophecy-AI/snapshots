{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b9be8d",
   "metadata": {},
   "source": [
    "# Loop 33 Analysis: Understanding the CV-LB Gap\n",
    "\n",
    "**Current State:**\n",
    "- Best CV: 0.008194 (exp_032 - GP 0.15 + MLP 0.55 + LGBM 0.3)\n",
    "- Best LB: 0.0877 (exp_030)\n",
    "- Target: 0.01670\n",
    "- Gap to target: 5.25x\n",
    "- Submissions remaining: 2\n",
    "\n",
    "**Key Question:** Why is there such a large CV-LB gap and how can we close it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df)\n",
    "print(f'\\nCV-LB Ratio: {df[\"lb\"].mean() / df[\"cv\"].mean():.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression to understand CV-LB relationship\n",
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'Linear fit: LB = {slope:.2f} * CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'\\nIntercept: {intercept:.4f}')\n",
    "print(f'Target LB: 0.01670')\n",
    "print(f'\\nTo reach target LB = 0.01670:')\n",
    "print(f'  Required CV = (0.01670 - {intercept:.4f}) / {slope:.2f} = {(0.01670 - intercept) / slope:.6f}')\n",
    "\n",
    "if (0.01670 - intercept) / slope < 0:\n",
    "    print('\\n⚠️ IMPOSSIBLE with current CV-LB relationship!')\n",
    "    print('The intercept alone (0.0527) is 3.15x higher than target (0.01670)')\n",
    "else:\n",
    "    print(f'\\n✓ Achievable with CV = {(0.01670 - intercept) / slope:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55122d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV vs LB\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['cv'], df['lb'], s=100, alpha=0.7, label='Submissions')\n",
    "\n",
    "# Fit line\n",
    "cv_range = np.linspace(0, 0.015, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "plt.plot(cv_range, lb_pred, 'r--', label=f'Fit: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Target line\n",
    "plt.axhline(y=0.01670, color='g', linestyle=':', label='Target LB = 0.01670')\n",
    "\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('LB Score')\n",
    "plt.title('CV vs LB Relationship')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('/home/code/exploration/cv_lb_relationship.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey Insight: The linear fit has an intercept of ~0.053, which is 3.15x higher than target.')\n",
    "print('This means even with CV=0, predicted LB would be 0.053 - far from target 0.01670.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdeb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what the top kernels are doing differently\n",
    "print('=== Analysis of Top Kernels ===')\n",
    "print()\n",
    "print('1. \"mixall\" kernel (8 votes):')\n",
    "print('   - Uses GroupKFold (5-fold) instead of Leave-One-Out CV')\n",
    "print('   - Ensemble: MLP + XGBoost + RandomForest + LightGBM')\n",
    "print('   - Uses Optuna for hyperparameter optimization')\n",
    "print('   - Key: Different CV scheme may have different CV-LB relationship!')\n",
    "print()\n",
    "print('2. \"System Malfunction V1\" kernel (29 votes):')\n",
    "print('   - Simple MLP with Spange descriptors')\n",
    "print('   - Standard Leave-One-Out CV')\n",
    "print('   - Uses MSELoss instead of HuberLoss')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to understand the problem better\n",
    "DATA_PATH = '/home/data'\n",
    "\n",
    "df_single = pd.read_csv(f'{DATA_PATH}/catechol_single_solvent_yields.csv')\n",
    "df_full = pd.read_csv(f'{DATA_PATH}/catechol_full_data_yields.csv')\n",
    "\n",
    "print('=== Data Statistics ===')\n",
    "print(f'Single Solvent: {len(df_single)} samples, {df_single[\"SOLVENT NAME\"].nunique()} solvents')\n",
    "print(f'Full Data: {len(df_full)} samples, {df_full[[\"SOLVENT A NAME\", \"SOLVENT B NAME\"]].drop_duplicates().shape[0]} ramps')\n",
    "print()\n",
    "print('Target distributions:')\n",
    "for col in ['Product 2', 'Product 3', 'SM']:\n",
    "    print(f'  {col}: mean={df_single[col].mean():.4f}, std={df_single[col].std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-solvent variance\n",
    "print('=== Per-Solvent Analysis ===')\n",
    "solvent_stats = df_single.groupby('SOLVENT NAME').agg({\n",
    "    'Product 2': ['mean', 'std'],\n",
    "    'Product 3': ['mean', 'std'],\n",
    "    'SM': ['mean', 'std']\n",
    "})\n",
    "print(solvent_stats.head(10))\n",
    "print()\n",
    "print('Solvents with highest variance (hardest to predict):')\n",
    "solvent_stats['total_std'] = solvent_stats[('Product 2', 'std')] + solvent_stats[('Product 3', 'std')] + solvent_stats[('SM', 'std')]\n",
    "print(solvent_stats.nlargest(5, 'total_std')[['total_std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: The CV-LB gap suggests our model is overfitting to the CV scheme\n",
    "# The leave-one-solvent-out CV may not reflect the true test distribution\n",
    "\n",
    "print('=== Key Insights ===')\n",
    "print()\n",
    "print('1. CV-LB Gap Analysis:')\n",
    "print('   - Linear fit: LB = 4.27 * CV + 0.0527')\n",
    "print('   - Intercept (0.0527) is 3.15x higher than target (0.01670)')\n",
    "print('   - This suggests a fundamental mismatch between CV and test distribution')\n",
    "print()\n",
    "print('2. Possible Causes:')\n",
    "print('   - Leave-one-solvent-out CV is too optimistic')\n",
    "print('   - The test set may have different solvents or conditions')\n",
    "print('   - Our features may not capture the true solvent similarity')\n",
    "print()\n",
    "print('3. Potential Solutions:')\n",
    "print('   - Try different CV schemes (GroupKFold like \"mixall\" kernel)')\n",
    "print('   - Use simpler models that generalize better')\n",
    "print('   - Focus on features that capture solvent similarity')\n",
    "print('   - Try domain adaptation techniques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we've tried vs what we haven't\n",
    "print('=== Experiment Summary ===')\n",
    "print()\n",
    "print('TRIED (33 experiments):')\n",
    "print('  ✓ MLP with various architectures')\n",
    "print('  ✓ LightGBM, XGBoost, CatBoost')\n",
    "print('  ✓ Gaussian Process')\n",
    "print('  ✓ Ridge Regression')\n",
    "print('  ✓ Kernel Ridge Regression')\n",
    "print('  ✓ Various feature sets (Spange, DRFP, ACS PCA)')\n",
    "print('  ✓ Ensemble methods (GP + MLP + LGBM)')\n",
    "print('  ✓ Different loss functions (MSE, Huber, weighted)')\n",
    "print('  ✓ Data augmentation (TTA for mixtures)')\n",
    "print()\n",
    "print('NOT TRIED:')\n",
    "print('  ✗ Different CV scheme (GroupKFold instead of LOO)')\n",
    "print('  ✗ Training on combined single + full data')\n",
    "print('  ✗ Transfer learning from single to full')\n",
    "print('  ✗ Uncertainty-based sample weighting')\n",
    "print('  ✗ Post-hoc calibration of predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print('=== STRATEGIC RECOMMENDATION ===')\n",
    "print()\n",
    "print('With only 2 submissions remaining and a 5.25x gap to target,')\n",
    "print('we need to focus on approaches that could fundamentally change')\n",
    "print('the CV-LB relationship, not just improve CV.')\n",
    "print()\n",
    "print('PRIORITY 1: Submit exp_032 (best CV 0.008194)')\n",
    "print('  - Even though predicted LB is ~0.0878, we need to verify')\n",
    "print('  - The 1.26% CV improvement might translate differently on LB')\n",
    "print('  - This gives us a data point for the new ensemble weights')\n",
    "print()\n",
    "print('PRIORITY 2: Try a fundamentally different approach')\n",
    "print('  - GroupKFold CV (like \"mixall\" kernel) might have different CV-LB')\n",
    "print('  - Or try training on combined single + full data')\n",
    "print('  - Or try a much simpler model (pure Ridge on Spange features)')\n",
    "print()\n",
    "print('The target IS reachable - we just need to find the right approach.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
