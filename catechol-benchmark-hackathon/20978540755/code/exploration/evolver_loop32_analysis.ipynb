{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d01dc1",
   "metadata": {},
   "source": [
    "# Loop 32 Strategic Analysis\n",
    "\n",
    "## Key Findings from exp_031\n",
    "- Higher GP weight (0.4 vs 0.2) made things WORSE (10.61% worse CV)\n",
    "- GP is complementary but not as accurate as MLP/LGBM\n",
    "- The optimal GP weight is around 0.2, not higher\n",
    "\n",
    "## Current State\n",
    "- Best CV: 0.008298 (exp_030)\n",
    "- Best LB: 0.0877 (exp_030)\n",
    "- Target: 0.01670\n",
    "- Gap: 5.25x\n",
    "- Submissions remaining: 2\n",
    "\n",
    "## Key Insight from \"mixall\" Kernel\n",
    "The \"mixall\" kernel uses **GroupKFold (5 splits)** instead of Leave-One-Out!\n",
    "This is a fundamentally different CV scheme that might have different CV-LB relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CV-LB relationship\n",
    "from scipy import stats\n",
    "\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "\n",
    "print(f'\\n=== CV-LB Relationship ===')\n",
    "print(f'Linear fit: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "print(f'R² = {r_value**2:.4f}')\n",
    "print(f'Intercept = {intercept:.4f}')\n",
    "print(f'Target = 0.01670')\n",
    "print(f'\\nIntercept ({intercept:.4f}) > Target (0.01670): {intercept > 0.01670}')\n",
    "\n",
    "# What CV would we need to hit target?\n",
    "required_cv = (0.01670 - intercept) / slope\n",
    "print(f'\\nTo hit target 0.01670:')\n",
    "print(f'Required CV = (0.01670 - {intercept:.4f}) / {slope:.2f} = {required_cv:.6f}')\n",
    "if required_cv < 0:\n",
    "    print('IMPOSSIBLE: Required CV is negative!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot submissions\n",
    "ax.scatter(cv, lb, s=100, c='blue', label='Submissions')\n",
    "\n",
    "# Plot linear fit\n",
    "cv_range = np.linspace(0, max(cv)*1.2, 100)\n",
    "lb_pred = slope * cv_range + intercept\n",
    "ax.plot(cv_range, lb_pred, 'r--', label=f'Fit: LB = {slope:.2f}*CV + {intercept:.4f}')\n",
    "\n",
    "# Plot target\n",
    "ax.axhline(y=0.01670, color='green', linestyle=':', linewidth=2, label='Target (0.01670)')\n",
    "\n",
    "# Plot intercept\n",
    "ax.axhline(y=intercept, color='orange', linestyle='--', alpha=0.5, label=f'Intercept ({intercept:.4f})')\n",
    "\n",
    "ax.set_xlabel('CV Score')\n",
    "ax.set_ylabel('LB Score')\n",
    "ax.set_title('CV vs LB Relationship - The Intercept Problem')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/code/exploration/loop32_cv_lb.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('\\nKey insight: The intercept (0.0524) is 3x larger than the target (0.01670).')\n",
    "print('Even with CV=0, the predicted LB would be 0.0524 > target.')\n",
    "print('We need to fundamentally change the CV-LB relationship, not just improve CV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403280f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What approaches might change the CV-LB relationship?\n",
    "print('\\n=== Approaches to Change CV-LB Relationship ===')\n",
    "print()\n",
    "print('1. DIFFERENT CV SCHEME (from \"mixall\" kernel):')\n",
    "print('   - GroupKFold (5 splits) instead of Leave-One-Out')\n",
    "print('   - This changes how we evaluate, not the model')\n",
    "print('   - May give more realistic CV estimates')\n",
    "print('   - The LB evaluation still uses LOO, so this tests if different CV correlates better')\n",
    "print()\n",
    "print('2. SIMPLER MODEL:')\n",
    "print('   - Our CV (0.0083) is already 2x better than target LB (0.01670)')\n",
    "print('   - The problem is generalization, not model quality')\n",
    "print('   - Simpler models may generalize better')\n",
    "print('   - Try: Ridge regression, simple MLP [32, 16]')\n",
    "print()\n",
    "print('3. DIFFERENT FEATURES:')\n",
    "print('   - Current: 145 features (Spange + DRFP + ACS PCA + kinetics)')\n",
    "print('   - Try: Only Spange (13) + kinetics (5) = 18 features')\n",
    "print('   - Fewer features = less overfitting')\n",
    "print()\n",
    "print('4. PURE GP MODEL:')\n",
    "print('   - GP has different inductive biases than NN')\n",
    "print('   - May have fundamentally different CV-LB relationship')\n",
    "print('   - exp_030 showed GP helps, but was only 0.2 weight')\n",
    "print('   - Try: Pure GP (no MLP/LGBM)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what we've tried and what's left\n",
    "print('\\n=== What We\\'ve Tried ===')\n",
    "print()\n",
    "print('MODELS:')\n",
    "print('✓ MLP (various architectures)')\n",
    "print('✓ LightGBM')\n",
    "print('✓ XGBoost')\n",
    "print('✓ Random Forest')\n",
    "print('✓ Ridge Regression')\n",
    "print('✓ Gaussian Process')\n",
    "print('✓ Ensembles (MLP+LGBM, GP+MLP+LGBM)')\n",
    "print()\n",
    "print('FEATURES:')\n",
    "print('✓ Spange descriptors (13)')\n",
    "print('✓ DRFP (122 high-variance)')\n",
    "print('✓ ACS PCA (5)')\n",
    "print('✓ Arrhenius kinetics (5)')\n",
    "print('✓ Combined (145 total)')\n",
    "print('✓ Simple (18 = Spange + kinetics)')\n",
    "print()\n",
    "print('TECHNIQUES:')\n",
    "print('✓ Data augmentation (flip A/B for mixtures)')\n",
    "print('✓ Test Time Augmentation (TTA)')\n",
    "print('✓ Weighted loss (higher weight for SM)')\n",
    "print('✓ Ensemble weighting optimization')\n",
    "print()\n",
    "print('NOT YET TRIED:')\n",
    "print('✗ GroupKFold CV scheme (from \"mixall\" kernel)')\n",
    "print('✗ Pure GP model (without MLP/LGBM)')\n",
    "print('✗ Kernel Ridge Regression with chemical kernels')\n",
    "print('✗ Multi-output GP with correlations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eec398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final recommendation\n",
    "print('\\n=== STRATEGIC RECOMMENDATION ===')\n",
    "print()\n",
    "print('With only 2 submissions remaining, we need to be strategic.')\n",
    "print()\n",
    "print('OPTION A: Submit exp_030 (already done) - BEST LB 0.0877')\n",
    "print('  - Already submitted, achieved best LB')\n",
    "print('  - No further action needed')\n",
    "print()\n",
    "print('OPTION B: Try Pure GP Model')\n",
    "print('  - GP has different mathematical framework')\n",
    "print('  - May have different CV-LB relationship')\n",
    "print('  - If CV is worse but LB is better, that\\'s valuable')\n",
    "print()\n",
    "print('OPTION C: Try Ridge Regression with Spange-only features')\n",
    "print('  - Simplest possible model')\n",
    "print('  - May generalize better')\n",
    "print('  - Fast to test')\n",
    "print()\n",
    "print('OPTION D: Try GroupKFold CV scheme')\n",
    "print('  - Different CV scheme from \"mixall\" kernel')\n",
    "print('  - May give more realistic estimates')\n",
    "print('  - But LB evaluation still uses LOO')\n",
    "print()\n",
    "print('RECOMMENDED: Try OPTION B (Pure GP) or OPTION C (Ridge with simple features)')\n",
    "print('These are fundamentally different approaches that might change the CV-LB relationship.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ea19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight from analysis\n",
    "print('\\n=== KEY INSIGHT ===')\n",
    "print()\n",
    "print('The \"mixall\" kernel achieves good CV/LB in only 2 minutes using:')\n",
    "print('1. GroupKFold (5 splits) instead of Leave-One-Out')\n",
    "print('2. Ensemble of MLP + XGBoost + Random Forest + LightGBM')\n",
    "print('3. Spange descriptors only (no DRFP)')\n",
    "print('4. Optuna hyperparameter optimization')\n",
    "print()\n",
    "print('The key difference is the CV scheme!')\n",
    "print('GroupKFold with 5 splits means:')\n",
    "print('- Each fold has ~5 solvents in test set (vs 1 in LOO)')\n",
    "print('- More data in test set = more stable CV estimate')\n",
    "print('- May correlate better with LB')\n",
    "print()\n",
    "print('However, the LB evaluation still uses LOO.')\n",
    "print('So the question is: does GroupKFold CV correlate better with LOO LB?')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
