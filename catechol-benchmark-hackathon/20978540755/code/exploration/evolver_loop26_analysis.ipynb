{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641934e9",
   "metadata": {},
   "source": [
    "# Loop 26 Analysis: Per-Target Failure and Next Steps\n",
    "\n",
    "## Key Findings from exp_025\n",
    "\n",
    "The per-target model experiment FAILED (CV 0.009068 vs baseline 0.008689, 4.36% worse).\n",
    "\n",
    "**Per-Target MSE Breakdown:**\n",
    "- Product 2 MSE: 0.005917 (IMPROVED)\n",
    "- Product 3 MSE: 0.007797 (IMPROVED)\n",
    "- SM MSE: 0.014034 (MUCH WORSE - this is the culprit!)\n",
    "\n",
    "**Key Insight:** The SM model with larger architecture [64,32] is OVERFITTING. The joint model provides multi-task regularization that helps SM prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0721806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print('Submission History:')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Linear fit\n",
    "from scipy import stats\n",
    "slope, intercept, r, p, se = stats.linregress(df['cv'], df['lb'])\n",
    "print(f'\\nCV-LB Relationship: LB = {slope:.2f}*CV + {intercept:.4f} (R²={r**2:.3f})')\n",
    "print(f'Target: 0.01727')\n",
    "print(f'Predicted CV for target LB: {(0.01727 - intercept) / slope:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691221b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what approaches have been tried\n",
    "experiments = [\n",
    "    {'id': 'exp_000', 'cv': 0.0111, 'approach': 'MLP baseline'},\n",
    "    {'id': 'exp_001', 'cv': 0.0123, 'approach': 'LightGBM baseline'},\n",
    "    {'id': 'exp_003', 'cv': 0.0105, 'approach': 'Combined Spange+DRFP'},\n",
    "    {'id': 'exp_005', 'cv': 0.0104, 'approach': 'Large ensemble (15 models)'},\n",
    "    {'id': 'exp_006', 'cv': 0.0097, 'approach': 'Simpler [64,32]'},\n",
    "    {'id': 'exp_007', 'cv': 0.0093, 'approach': 'Even simpler [32,16]'},\n",
    "    {'id': 'exp_009', 'cv': 0.0092, 'approach': 'Ridge regression'},\n",
    "    {'id': 'exp_012', 'cv': 0.0090, 'approach': 'Simple ensemble MLP+LGBM'},\n",
    "    {'id': 'exp_024', 'cv': 0.0087, 'approach': 'ACS PCA features'},\n",
    "    {'id': 'exp_025', 'cv': 0.0091, 'approach': 'Per-target models (WORSE)'},\n",
    "]\n",
    "\n",
    "print('Experiment Trajectory:')\n",
    "for e in experiments:\n",
    "    print(f\"{e['id']}: CV {e['cv']:.4f} - {e['approach']}\")\n",
    "\n",
    "print('\\n=== WHAT WORKED ===')\n",
    "print('1. Simpler architectures [32,16] > [256,128,64]')\n",
    "print('2. MLP + LightGBM ensemble')\n",
    "print('3. ACS PCA features (additional 5 features)')\n",
    "print('4. Arrhenius kinetics features')\n",
    "print('5. TTA for mixtures')\n",
    "\n",
    "print('\\n=== WHAT FAILED ===')\n",
    "print('1. Per-target models (SM overfits with separate model)')\n",
    "print('2. Deep residual networks')\n",
    "print('3. Attention mechanisms')\n",
    "print('4. Fragprints instead of DRFP')\n",
    "print('5. Very large ensembles alone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb581ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the per-target failure more deeply\n",
    "print('=== PER-TARGET ANALYSIS ===')\n",
    "print('\\nexp_024 (joint model) vs exp_025 (per-target):')\n",
    "print('\\nTarget    | exp_024 (joint) | exp_025 (per-target) | Change')\n",
    "print('-' * 60)\n",
    "\n",
    "# exp_024 per-target breakdown (estimated from overall CV)\n",
    "# Overall CV 0.008689, assuming similar distribution\n",
    "exp024_overall = 0.008689\n",
    "exp025_p2 = 0.005917\n",
    "exp025_p3 = 0.007797\n",
    "exp025_sm = 0.014034\n",
    "exp025_overall = 0.009068\n",
    "\n",
    "print(f'Product 2 | ~0.006 (est)    | {exp025_p2:.6f}       | Improved')\n",
    "print(f'Product 3 | ~0.008 (est)    | {exp025_p3:.6f}       | Improved')\n",
    "print(f'SM        | ~0.012 (est)    | {exp025_sm:.6f}       | WORSE!')\n",
    "print(f'Overall   | {exp024_overall:.6f}       | {exp025_overall:.6f}       | 4.36% worse')\n",
    "\n",
    "print('\\n=== KEY INSIGHT ===')\n",
    "print('The joint model provides MULTI-TASK REGULARIZATION:')\n",
    "print('- SM benefits from shared representation with Products')\n",
    "print('- Separating SM removes this regularization')\n",
    "print('- The [64,32] architecture for SM alone OVERFITS')\n",
    "\n",
    "print('\\n=== NEXT STEPS ===')\n",
    "print('1. Try LOSS WEIGHTING instead of separate models')\n",
    "print('   - Keep joint model, but weight SM loss higher')\n",
    "print('   - loss = 2.0*SM_loss + 1.0*P2_loss + 1.0*P3_loss')\n",
    "print('2. Try CONSISTENCY REGULARIZATION')\n",
    "print('   - Add constraint: SM + P2 + P3 ≈ 1 (mass balance)')\n",
    "print('3. Try 4-MODEL ENSEMBLE')\n",
    "print('   - Add XGBoost and RandomForest for diversity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What unexplored approaches remain?\n",
    "print('=== UNEXPLORED APPROACHES ===')\n",
    "\n",
    "approaches = [\n",
    "    ('Loss weighting for SM', 'HIGH', 'Weight SM loss 2x in joint model'),\n",
    "    ('Consistency regularization', 'MEDIUM', 'Add SM + P2 + P3 ≈ 1 constraint'),\n",
    "    ('4-model ensemble', 'MEDIUM', 'Add XGBoost + RandomForest'),\n",
    "    ('Stacking meta-learner', 'MEDIUM', 'Learn optimal combination weights'),\n",
    "    ('Non-linear mixture encoding', 'LOW', 'Add A*B*pct*(1-pct) interaction'),\n",
    "    ('Larger MLP ensemble (10+)', 'LOW', 'More models for variance reduction'),\n",
    "    ('Different optimizer (AdamW)', 'LOW', 'May help with regularization'),\n",
    "    ('Cosine annealing LR', 'LOW', 'Better learning rate schedule'),\n",
    "]\n",
    "\n",
    "print('\\nPriority | Approach | Description')\n",
    "print('-' * 70)\n",
    "for approach, priority, desc in approaches:\n",
    "    print(f'{priority:8} | {approach:30} | {desc}')\n",
    "\n",
    "print('\\n=== RECOMMENDED NEXT EXPERIMENT ===')\n",
    "print('exp_026: Loss-Weighted Joint Model')\n",
    "print('- Keep joint [32,16] MLP + LightGBM ensemble')\n",
    "print('- Weight SM loss 2x higher in training')\n",
    "print('- Preserves multi-task regularization')\n",
    "print('- Focuses optimization on hardest target (SM)')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
