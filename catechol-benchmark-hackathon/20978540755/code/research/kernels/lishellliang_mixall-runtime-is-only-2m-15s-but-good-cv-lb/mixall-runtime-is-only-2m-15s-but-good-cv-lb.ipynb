{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":115863,"databundleVersionId":13836289,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":67.924378,"end_time":"2025-10-08T09:50:12.839004","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-08T09:49:04.914626","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"d65e38d4","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-10-08T09:49:10.777198Z","iopub.status.busy":"2025-10-08T09:49:10.776779Z","iopub.status.idle":"2025-10-08T09:49:12.87097Z","shell.execute_reply":"2025-10-08T09:49:12.869703Z"},"papermill":{"duration":2.101257,"end_time":"2025-10-08T09:49:12.872582","exception":false,"start_time":"2025-10-08T09:49:10.771325","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"987a4971","cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/catechol-benchmark-hackathon/')\n\nfrom utils import INPUT_LABELS_FULL_SOLVENT, INPUT_LABELS_SINGLE_SOLVENT, INPUT_LABELS_NUMERIC, INPUT_LABELS_SINGLE_FEATURES, INPUT_LABELS_FULL_FEATURES, load_data, load_features, generate_leave_one_out_splits, generate_leave_one_ramp_out_splits","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:49:12.880491Z","iopub.status.busy":"2025-10-08T09:49:12.879999Z","iopub.status.idle":"2025-10-08T09:49:12.889494Z","shell.execute_reply":"2025-10-08T09:49:12.888495Z"},"papermill":{"duration":0.015087,"end_time":"2025-10-08T09:49:12.891176","exception":false,"start_time":"2025-10-08T09:49:12.876089","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"overwrite_utils_functions","cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom typing import Any, Generator\n\n# Overwrite utility functions to use GroupKFold (5 splits) instead of Leave-One-Out\n# This ensures we respect the compute budget while maintaining unseen solvent validation\n# Since we cannot modify the utils.py file on Kaggle platform directly, we redefine them here.\n\ndef generate_leave_one_out_splits(\n    X: pd.DataFrame, Y: pd.DataFrame\n) -> Generator[\n    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n    Any,\n    None,\n]:\n    \"\"\"Generate Group K-Fold splits across the solvents (5-fold).\"\"\"\n    groups = X[\"SOLVENT NAME\"]\n    n_groups = len(groups.unique())\n    n_splits = min(5, n_groups)\n    \n    gkf = GroupKFold(n_splits=n_splits)\n    \n    for train_idx, test_idx in gkf.split(X, Y, groups):\n        yield (\n            (X.iloc[train_idx], Y.iloc[train_idx]),\n            (X.iloc[test_idx], Y.iloc[test_idx]),\n        )\n\ndef generate_leave_one_ramp_out_splits(\n    X: pd.DataFrame, Y: pd.DataFrame\n) -> Generator[\n    tuple[tuple[pd.DataFrame, pd.DataFrame], tuple[pd.DataFrame, pd.DataFrame]],\n    Any,\n    None,\n]:\n    \"\"\"Generate Group K-Fold splits across the solvent ramps (5-fold).\"\"\"\n    groups = X[\"SOLVENT A NAME\"].astype(str) + \"_\" + X[\"SOLVENT B NAME\"].astype(str)\n    \n    n_groups = len(groups.unique())\n    n_splits = min(5, n_groups)\n    \n    gkf = GroupKFold(n_splits=n_splits)\n    \n    for train_idx, test_idx in gkf.split(X, Y, groups):\n        yield (\n            (X.iloc[train_idx], Y.iloc[train_idx]),\n            (X.iloc[test_idx], Y.iloc[test_idx]),\n        )\n","metadata":{},"outputs":[],"execution_count":null},{"id":"92c1ccd1","cell_type":"markdown","source":"In the cells below we create the base classes of the two main objects you must write for the competition. \n\nThe first thing to write is a SmilesFeaturizer, which will take the solvent molecules and create a machine-learning ready featurization of the molecule. Finding better ways of featurizing solvents is one of the goals of the hackathon, however, you can also skip this step and use the pre-computed featurizations given in the utils file. Further down, you can see a SmilesFeaturizer that loads all the precomputed representations. A **featurizer** object simply consists of:\n- An initialization function\n- A featurize function that takes \n\nThe second one being a **model** which has:\n- An initialization function, where the model internally defines which featurizer to use\n- A \"train_model\" which lets the model train on data given by X_train, y_train as pandas data-frames. \n- A \"predict\" which takes a data frame of test inputs and makes a prediction","metadata":{"papermill":{"duration":0.002602,"end_time":"2025-10-08T09:49:12.896875","exception":false,"start_time":"2025-10-08T09:49:12.894273","status":"completed"},"tags":[]}},{"id":"8843bcd5","cell_type":"code","source":"from abc import ABC, abstractmethod\n\nclass SmilesFeaturizer(ABC):\n    def __init__(self):\n        raise NotImplementedError\n\n    def featurize(X, Y):\n        raise NotImplementedError\n\nclass BaseModel(ABC):\n    def __init__(self):\n        pass\n\n    def train_model(self, X_train, y_train):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:49:12.903647Z","iopub.status.busy":"2025-10-08T09:49:12.903349Z","iopub.status.idle":"2025-10-08T09:49:12.909153Z","shell.execute_reply":"2025-10-08T09:49:12.908053Z"},"papermill":{"duration":0.011301,"end_time":"2025-10-08T09:49:12.910931","exception":false,"start_time":"2025-10-08T09:49:12.89963","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"93aafa34","cell_type":"markdown","source":"In the next cell we define two featurizers, which allow you to use the pre-computed featurizations from the original benchmark paper. These are:\n\n- drfps\n- fragprints\n- acs_pca_descriptors\n- spange_descriptors\n\nYou can refer to the paper for more details on them. We also include the simple SMILES string featurization which can be chained into more complicated representations.\n\nThe first featurizer simply uses the features directly. The second one is expanded to featurize *mixed* solvents too, which is done by taking a weighted average of the two single-solvent features.\n\nWe also show how to write code for a simple multi-layer perceptron on the data.","metadata":{"papermill":{"duration":0.002465,"end_time":"2025-10-08T09:49:12.91682","exception":false,"start_time":"2025-10-08T09:49:12.914355","status":"completed"},"tags":[]}},{"id":"ensemble_model_cell","cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\n# --- Featurizers ---\nclass PrecomputedFeaturizer(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.features = load_features(features)\n        self.feats_dim = self.features.shape[1] + 2 # +2 for Time, Temp\n        \n    def featurize(self, X):\n        # X is DataFrame with 'Residence Time', 'Temperature', 'SOLVENT NAME'\n        res_time = X['Residence Time'].values.reshape(-1, 1)\n        temp = X['Temperature'].values.reshape(-1, 1)\n        \n        solvent_names = X['SOLVENT NAME']\n        feats = self.features.loc[solvent_names].values\n        \n        final_feats = np.hstack([res_time, temp, feats])\n        return torch.tensor(final_feats, dtype=torch.float32)\n\nclass PrecomputedFeaturizerMixed(SmilesFeaturizer):\n    def __init__(self, features='spange_descriptors'):\n        self.features = load_features(features)\n        self.feats_dim = self.features.shape[1] + 3 # +3 for Time, Temp, %B\n        \n    def featurize(self, X):\n        res_time = X['Residence Time'].values.reshape(-1, 1)\n        temp = X['Temperature'].values.reshape(-1, 1)\n        sb_pct = X['SolventB%'].values.reshape(-1, 1)\n        \n        desc_a = self.features.loc[X['SOLVENT A NAME']].values\n        desc_b = self.features.loc[X['SOLVENT B NAME']].values\n        \n        mixture_feats = (1 - sb_pct) * desc_a + sb_pct * desc_b\n        \n        final_feats = np.hstack([res_time, temp, sb_pct, mixture_feats])\n        return torch.tensor(final_feats, dtype=torch.float32)\n\n# --- MLP ---\nclass EnhancedMLP(nn.Module):\n    def __init__(self, input_dim, output_dim=3, hidden_dims=[128, 64, 32], dropout=0.1):\n        super(EnhancedMLP, self).__init__()\n        layers = []\n        in_dim = input_dim\n        for h_dim in hidden_dims:\n            layers.append(nn.Linear(in_dim, h_dim))\n            layers.append(nn.BatchNorm1d(h_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            in_dim = h_dim\n        layers.append(nn.Linear(in_dim, output_dim))\n        self.network = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.network(x)\n\n# --- Ensemble ---\nclass EnsembleModel(nn.Module, BaseModel):\n    def __init__(self, features='spange_descriptors', hidden_dims=[128, 64, 32], output_dim=3, dropout=0.1, data='single', use_tta=True, weights=None):\n        super(EnsembleModel, self).__init__()\n        \n        if data == 'single':\n            self.smiles_featurizer = PrecomputedFeaturizer(features=features)\n        else:\n            self.smiles_featurizer = PrecomputedFeaturizerMixed(features=features)\n            \n        self.input_dim = self.smiles_featurizer.feats_dim\n        self.use_tta = use_tta\n        self.weights = weights if weights is not None else [0.4, 0.2, 0.2, 0.2] # MLP, XGB, RF, LGBM\n\n        # MLP\n        self.mlp = EnhancedMLP(self.input_dim, output_dim, hidden_dims, dropout)\n        \n        # XGBoost\n        self.xgb_params = {'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.8, 'n_jobs': -1, 'random_state': 42}\n        self.xgb = None\n        \n        # RandomForest\n        self.rf_params = {'n_estimators': 300, 'max_depth': 15, 'n_jobs': -1, 'random_state': 42}\n        self.rf = None\n        \n        # LightGBM\n        self.lgb_params = {'n_estimators': 300, 'learning_rate': 0.05, 'num_leaves': 31, 'n_jobs': -1, 'random_state': 42, 'verbose': -1}\n        self.lgbm = None\n        \n        self.scaler = StandardScaler()\n\n    def train_model(self, train_X, train_Y, criterion=nn.MSELoss, optimizer=torch.optim.Adam, num_epochs=100, batch_size=32, device=\"cpu\", verbose=True, lr=1e-3):\n        # Data Prep\n        X_tensor = self.smiles_featurizer.featurize(train_X)\n        X_np = X_tensor.numpy()\n        X_scaled = self.scaler.fit_transform(X_np)\n        \n        # Ensure strict DataFrame format with string column names for LightGBM compatibility\n        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n        \n        train_Y_np = train_Y.values\n        \n        # Train GBDTs with DataFrame input\n        self.xgb = MultiOutputRegressor(xgb.XGBRegressor(**self.xgb_params))\n        self.xgb.fit(X_scaled_df, train_Y_np)\n        \n        self.rf = MultiOutputRegressor(RandomForestRegressor(**self.rf_params))\n        self.rf.fit(X_scaled_df, train_Y_np)\n        \n        self.lgbm = MultiOutputRegressor(lgb.LGBMRegressor(**self.lgb_params))\n        self.lgbm.fit(X_scaled_df, train_Y_np)\n        \n        # Train MLP (still uses tensors)\n        X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n        train_Y_tensor = torch.tensor(train_Y_np, dtype=torch.float32)\n        \n        if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.mlp.to(device)\n        \n        optimizer_inst = optimizer(self.mlp.parameters(), lr=lr)\n        train_loader = DataLoader(TensorDataset(X_tensor_scaled, train_Y_tensor), batch_size=batch_size, shuffle=True, drop_last=True)\n        \n        criterion_inst = criterion()\n        for epoch in range(num_epochs):\n            self.mlp.train()\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                optimizer_inst.zero_grad()\n                loss = criterion_inst(self.mlp(inputs), targets)\n                loss.backward()\n                optimizer_inst.step()\n\n    def predict(self, test_X):\n        X_tensor = self.smiles_featurizer.featurize(test_X)\n        X_np = X_tensor.numpy()\n        X_scaled = self.scaler.transform(X_np)\n        \n        # Ensure strict DataFrame format with string column names for LightGBM compatibility\n        feature_names = [str(i) for i in range(X_scaled.shape[1])]\n        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n        \n        # MLP Preds\n        self.mlp.eval()\n        with torch.no_grad():\n            X_tensor_scaled = torch.tensor(X_scaled, dtype=torch.float32)\n            mlp_preds = self.mlp(X_tensor_scaled).cpu().numpy()\n            \n        # GBDT Preds\n        xgb_preds = self.xgb.predict(X_scaled_df)\n        rf_preds = self.rf.predict(X_scaled_df)\n        lgb_preds = self.lgbm.predict(X_scaled_df)\n        \n        # Weighted Ensemble\n        final_preds = (self.weights[0] * mlp_preds + \n                       self.weights[1] * xgb_preds + \n                       self.weights[2] * rf_preds + \n                       self.weights[3] * lgb_preds)\n                       \n        return torch.tensor(final_preds)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"fd5263bb","cell_type":"code","source":"def run_optuna_optimization(X_train, y_train, X_val, y_val, n_trials=20):\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    \n    def objective(trial):\n        # Params\n        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n        hidden_dim_1 = trial.suggest_int('hidden_dim_1', 64, 256)\n        \n        xgb_depth = trial.suggest_int('xgb_depth', 3, 8)\n        rf_depth = trial.suggest_int('rf_depth', 5, 15)\n        lgb_leaves = trial.suggest_int('lgb_leaves', 15, 63)\n        \n        # Weights (Dirichlet-like via normalization)\n        w_mlp = trial.suggest_float('w_mlp', 0.1, 1.0)\n        w_xgb = trial.suggest_float('w_xgb', 0.1, 1.0)\n        w_rf = trial.suggest_float('w_rf', 0.1, 1.0)\n        w_lgb = trial.suggest_float('w_lgb', 0.1, 1.0)\n        total_w = w_mlp + w_xgb + w_rf + w_lgb\n        weights = [w_mlp/total_w, w_xgb/total_w, w_rf/total_w, w_lgb/total_w]\n        \n        model = EnsembleModel(hidden_dims=[hidden_dim_1, hidden_dim_1//2, hidden_dim_1//4], dropout=dropout, use_tta=False, weights=weights)\n        model.xgb_params['max_depth'] = xgb_depth\n        model.rf_params['max_depth'] = rf_depth\n        model.lgb_params['num_leaves'] = lgb_leaves\n        \n        model.train_model(X_train, y_train, num_epochs=50, lr=lr, verbose=False)\n        preds = model.predict(X_val).numpy()\n        return np.mean(np.abs(preds - y_val.values))\n\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=n_trials)\n    \n    print(\"Best params:\", study.best_params)\n    return study.best_params","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:49:12.923553Z","iopub.status.busy":"2025-10-08T09:49:12.923274Z","iopub.status.idle":"2025-10-08T09:49:18.068875Z","shell.execute_reply":"2025-10-08T09:49:18.067837Z"},"papermill":{"duration":5.151202,"end_time":"2025-10-08T09:49:18.070704","exception":false,"start_time":"2025-10-08T09:49:12.919502","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"optuna_exec_cell","cell_type":"code","source":"# To run hyperparameter optimization, uncomment the following lines:\n# print(\"Starting Optuna optimization...\")\n# X_full, Y_full = load_data(\"full\")\n# \n# # Split data for optimization\n# from sklearn.model_selection import train_test_split\n# X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(X_full, Y_full, test_size=0.2, random_state=42)\n# \n# # Run optimization\n# best_params = run_optuna_optimization(X_train_opt, y_train_opt, X_val_opt, y_val_opt, n_trials=50)\n# print(\"Optimization complete. Best params:\", best_params)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"8d7ccb1d","cell_type":"markdown","source":"From this point onward the cross-validation procedure is calculated. **For a submission to be valid the next three cells must be the final three of your submission, and you can only modify the lines where the models are defined.**","metadata":{"papermill":{"duration":0.002536,"end_time":"2025-10-08T09:49:18.076279","exception":false,"start_time":"2025-10-08T09:49:18.073743","status":"completed"},"tags":[]}},{"id":"c977a964","cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nimport tqdm\n\nX, Y = load_data(\"single_solvent\")\n\nsplit_generator = generate_leave_one_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = EnsembleModel(data='single') # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 0,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_single_solvent = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE THIRD LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:49:18.083755Z","iopub.status.busy":"2025-10-08T09:49:18.082723Z","iopub.status.idle":"2025-10-08T09:49:47.795395Z","shell.execute_reply":"2025-10-08T09:49:47.793936Z"},"papermill":{"duration":29.718433,"end_time":"2025-10-08T09:49:47.797559","exception":false,"start_time":"2025-10-08T09:49:18.079126","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"4bd0d06f","cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nX, Y = load_data(\"full\")\n\nsplit_generator = generate_leave_one_ramp_out_splits(X, Y)\nall_predictions = []\n\nfor fold_idx, split in tqdm.tqdm(enumerate(split_generator)):\n    (train_X, train_Y), (test_X, test_Y) = split\n\n    model = EnsembleModel(data = 'full') # CHANGE THIS LINE ONLY\n    model.train_model(train_X, train_Y)\n\n    predictions = model.predict(test_X)  # Shape: [N, 3]\n\n    # Move to CPU and convert to numpy\n    predictions_np = predictions.detach().cpu().numpy()\n\n    # Add metadata and flatten to long format\n    for row_idx, row in enumerate(predictions_np):\n        all_predictions.append({\n            \"task\": 1,\n            \"fold\": fold_idx,\n            \"row\": row_idx,\n            \"target_1\": row[0],\n            \"target_2\": row[1],\n            \"target_3\": row[2]\n        })\n\n# Save final submission\nsubmission_full_data = pd.DataFrame(all_predictions)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL OTHER THAN THE MODEL #################\n########### THIS MUST BE THE SECOND LAST CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:49:47.808002Z","iopub.status.busy":"2025-10-08T09:49:47.807651Z","iopub.status.idle":"2025-10-08T09:50:10.533959Z","shell.execute_reply":"2025-10-08T09:50:10.532277Z"},"papermill":{"duration":22.734289,"end_time":"2025-10-08T09:50:10.536556","exception":false,"start_time":"2025-10-08T09:49:47.802267","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"id":"3b6e39b9","cell_type":"code","source":"########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################\n\nsubmission = pd.concat([submission_single_solvent, submission_full_data])\nsubmission = submission.reset_index()\nsubmission.index.name = \"id\"\nsubmission.to_csv(\"submission.csv\", index=True)\n\n########### DO NOT CHANGE ANYTHING IN THIS CELL #################\n########### THIS MUST BE THE FINAL CELL IN YOUR NOTEBOOK FOR A VALID SUBMISSION #################","metadata":{"execution":{"iopub.execute_input":"2025-10-08T09:50:10.547859Z","iopub.status.busy":"2025-10-08T09:50:10.547516Z","iopub.status.idle":"2025-10-08T09:50:10.577592Z","shell.execute_reply":"2025-10-08T09:50:10.576285Z"},"papermill":{"duration":0.037848,"end_time":"2025-10-08T09:50:10.579433","exception":false,"start_time":"2025-10-08T09:50:10.541585","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}