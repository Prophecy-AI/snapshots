## Current Status
- Best CV score: 0.008194 (exp_032/035/036 - GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 (exp_030)
- CV-LB relationship: LB = 4.30*CV + 0.0524 (RÂ²=0.97)
- **CRITICAL**: Intercept (0.0524) > Target (0.0347)
- Submissions remaining: 2

## Response to Evaluator
- Technical verdict was CONCERNS due to implementation bug in exp_037 (similarity weighting)
- Evaluator correctly identified that unnormalized features and wrong sigma caused the failure
- Evaluator's top priority: Try aggressive simplification OR fix similarity weighting
- I AGREE with the evaluator's assessment. The similarity weighting concept might work with proper implementation, but the more promising direction is aggressive simplification.

## Data Understanding
- Reference notebooks: `exploration/eda.ipynb`, `exploration/evolver_loop35_analysis.ipynb`
- Key patterns:
  - CV-LB gap is STRUCTURAL (intercept > target)
  - Even perfect CV (0) would give LB ~0.0524
  - We need to CHANGE the CV-LB relationship, not just improve CV
  - Simpler models (exp_006, exp_007) had better residuals than complex ones

## CRITICAL INSIGHT
The CV-LB relationship has an intercept of 0.0524, which is LARGER than the target (0.0347).
This means we CANNOT reach the target by improving CV alone. We need to fundamentally change
the CV-LB relationship by reducing overfitting to solvent-specific patterns.

## Recommended Approaches

### PRIORITY 1: Aggressive Simplification with Minimal Features
**Rationale**: The CV-LB gap suggests overfitting to solvent-specific patterns. By using minimal features, we force the model to learn more general patterns that transfer better to unseen solvents.

**Implementation**:
```python
# Use ONLY these features:
# 1. Arrhenius kinetics: 1/T, ln(t), interaction
# 2. Top 3 Spange features: dielectric constant, alpha, beta (most physically meaningful)
# Total: 6 features

class MinimalFeaturizer:
    def __init__(self, mixed=False):
        self.mixed = mixed
        # Only use 3 most important Spange features
        self.spange_cols = ['dielectric constant', 'alpha', 'beta']
        self.feats_dim = 5 + 3  # kinetics (5) + spange (3)
    
    def featurize(self, X, flip=False):
        X_vals = X[INPUT_LABELS_NUMERIC].values.astype(np.float64)
        temp_c = X_vals[:, 1:2]
        time_m = X_vals[:, 0:1]
        temp_k = temp_c + 273.15
        inv_temp = 1000.0 / temp_k
        log_time = np.log(time_m + 1e-6)
        interaction = inv_temp * log_time
        X_kinetic = np.hstack([X_vals, inv_temp, log_time, interaction])
        
        if self.mixed:
            A_spange = SPANGE_DF.loc[X["SOLVENT A NAME"]][self.spange_cols].values
            B_spange = SPANGE_DF.loc[X["SOLVENT B NAME"]][self.spange_cols].values
            pct = X["SolventB%"].values.reshape(-1, 1)
            if flip:
                X_spange = B_spange * (1 - (1-pct)) + A_spange * (1-pct)
            else:
                X_spange = A_spange * (1 - pct) + B_spange * pct
        else:
            X_spange = SPANGE_DF.loc[X["SOLVENT NAME"]][self.spange_cols].values
        
        return np.hstack([X_kinetic, X_spange])
```

**Model**: Simple MLP [16, 8] with high dropout (0.3) and weight decay (1e-3)
- OR Ridge Regression with alpha=1.0

### PRIORITY 2: Fixed Similarity Weighting (if Priority 1 fails)
**Rationale**: The evaluator correctly identified the implementation bug. With proper normalization and sigma tuning, similarity weighting might help.

**Implementation**:
```python
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist

def compute_solvent_similarity_fixed(train_solvents, test_solvent, sigma=None):
    # 1. Normalize Spange features
    scaler = StandardScaler()
    spange_normalized = scaler.fit_transform(SPANGE_DF.values)
    spange_normalized_df = pd.DataFrame(spange_normalized, index=SPANGE_DF.index, columns=SPANGE_DF.columns)
    
    # 2. Compute appropriate sigma (median distance)
    if sigma is None:
        distances = pdist(spange_normalized)
        sigma = np.median(distances)  # Should be ~1-2 for normalized features
    
    # 3. Get test solvent features
    test_idx = SPANGE_DF.index.get_loc(test_solvent)
    test_features = spange_normalized[test_idx]
    
    # 4. Compute similarity for each unique solvent
    unique_train_solvents = train_solvents.unique()
    solvent_similarities = {}
    for solvent in unique_train_solvents:
        train_idx = SPANGE_DF.index.get_loc(solvent)
        train_features = spange_normalized[train_idx]
        dist = np.sqrt(np.sum((train_features - test_features) ** 2))
        similarity = np.exp(-dist ** 2 / (2 * sigma ** 2))
        solvent_similarities[solvent] = similarity
    
    # 5. Map to all training samples
    weights = train_solvents.map(solvent_similarities).values
    
    # 6. Normalize to have mean 1
    weights = weights / weights.mean()
    
    return weights
```

### PRIORITY 3: Pure Ridge Regression (simplest possible)
**Rationale**: Maximum regularization, minimum overfitting. May sacrifice CV for better LB.

**Implementation**:
```python
from sklearn.linear_model import Ridge
from sklearn.multioutput import MultiOutputRegressor

class RidgeModel:
    def __init__(self, data='single', alpha=1.0):
        self.data_type = data
        self.alpha = alpha
        self.featurizer = MinimalFeaturizer(mixed=(data=='full'))
        self.model = None
        self.scaler = StandardScaler()
    
    def train_model(self, X_train, y_train, test_solvent=None):
        X_std = self.featurizer.featurize(X_train, flip=False)
        y_vals = y_train.values
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_train, flip=True)
            X_all = np.vstack([X_std, X_flip])
            y_all = np.vstack([y_vals, y_vals])
        else:
            X_all, y_all = X_std, y_vals
        
        X_scaled = self.scaler.fit_transform(X_all)
        self.model = MultiOutputRegressor(Ridge(alpha=self.alpha))
        self.model.fit(X_scaled, y_all)
    
    def predict(self, X_test):
        X_std = self.featurizer.featurize(X_test, flip=False)
        X_scaled = self.scaler.transform(X_std)
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_test, flip=True)
            X_flip_scaled = self.scaler.transform(X_flip)
            pred = (self.model.predict(X_scaled) + self.model.predict(X_flip_scaled)) / 2
        else:
            pred = self.model.predict(X_scaled)
        
        return torch.clamp(torch.tensor(pred), 0, 1)
```

## What NOT to Try
- Complex architectures (deep residual, attention) - increase overfitting
- More ensemble members - doesn't change CV-LB relationship
- DRFP features - increase overfitting to solvent-specific patterns
- Similarity weighting without fixing the implementation bug

## Validation Notes
- CV scheme: Leave-one-solvent-out (fixed by competition)
- CV-LB relationship: LB = 4.30*CV + 0.0524
- To reach target (0.0347), we need to REDUCE the intercept
- Simpler models may have worse CV but better LB

## Submission Strategy
With only 2 submissions remaining:
1. Try Priority 1 (aggressive simplification) - if CV is reasonable (< 0.015), submit
2. If Priority 1 fails, try Priority 2 (fixed similarity weighting)
3. Final submission: best model (either new experiment or exp_030)

## Key Insight
The target IS reachable. The problem is not model quality (our CV is excellent), but overfitting to solvent-specific patterns. By using minimal features and simple models, we can force the model to learn more general patterns that transfer better to unseen solvents.