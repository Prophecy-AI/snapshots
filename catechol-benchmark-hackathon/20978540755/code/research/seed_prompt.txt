## Current Status
- Best CV score: 0.0083 from exp_030 (GP+MLP+LGBM ensemble)
- Best LB score: 0.0877 from exp_030
- CV-LB gap: ~10.6x (LB = 4.30*CV + 0.0524, R²=0.97)
- Target: 0.01670
- Gap to target: 5.25x (425% worse than target)
- Submissions remaining: 2

## Response to Evaluator

**Technical verdict was TRUSTWORTHY.** The evaluator correctly verified exp_031 was well-executed.

**Evaluator's top priority was: Try Kernel Ridge Regression with Tanimoto kernel.** This is a valid suggestion, but I'm prioritizing approaches that might fundamentally change the CV-LB relationship.

**Key findings from exp_031:**
1. Higher GP weight (0.4 vs 0.2) made CV 10.61% WORSE
2. GP is complementary but not as accurate as MLP/LGBM
3. The optimal GP weight is around 0.2, not higher
4. Increasing GP weight does NOT improve generalization

**Critical insight from analysis:**
- The CV-LB relationship is: LB = 4.30*CV + 0.0524 (R²=0.97)
- Intercept (0.0524) > Target (0.01670)
- To hit target, we would need CV = -0.0083 (IMPOSSIBLE!)
- **We CANNOT reach target by improving CV alone**
- We need an approach that fundamentally changes the CV-LB relationship

## Data Understanding

**Reference notebooks:**
- `exploration/evolver_loop32_analysis.ipynb` - CV-LB relationship analysis
- `exploration/evolver_loop31_lb_feedback.ipynb` - LB feedback analysis
- `experiments/030_gp_ensemble/gp_ensemble.ipynb` - Best model implementation

**Key insight from "mixall" kernel:**
The "mixall" kernel uses **GroupKFold (5 splits)** instead of Leave-One-Out!
- This is a fundamentally different CV scheme
- May give more realistic CV estimates
- The LB evaluation still uses LOO, but different CV might correlate better

## Recommended Approaches

### PRIORITY 1: Pure GP Model (No MLP/LGBM)
**Why:** GP has fundamentally different mathematical framework than neural networks.
- GP is a Bayesian, non-parametric model
- May have different CV-LB relationship
- exp_030 showed GP helps (2.4% CV improvement, 1.1% LB improvement)
- But GP was only 0.2 weight - try pure GP

**Implementation:**
```python
class PureGPModel:
    def __init__(self, data='single'):
        self.data_type = data
        self.featurizer = SimpleFeaturizer(mixed=(data=='full'))  # 18 features
        self.models = []
        
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train)
        y_vals = y_train.values
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_train, flip=True)
            X_all = np.vstack([X_feat, X_flip])
            y_all = np.vstack([y_vals, y_vals])
        else:
            X_all, y_all = X_feat, y_vals
        
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_all)
        
        kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=0.1)
        
        for i in range(3):
            gp = GaussianProcessRegressor(
                kernel=kernel,
                n_restarts_optimizer=5,
                normalize_y=True,
                random_state=42
            )
            gp.fit(X_scaled, y_all[:, i])
            self.models.append(gp)
    
    def predict(self, X_test):
        X_feat = self.featurizer.featurize(X_test)
        X_scaled = self.scaler.transform(X_feat)
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_test, flip=True)
            X_flip_scaled = self.scaler.transform(X_flip)
        
        preds = []
        for i, model in enumerate(self.models):
            pred = model.predict(X_scaled)
            if self.data_type == 'full':
                pred_flip = model.predict(X_flip_scaled)
                pred = (pred + pred_flip) / 2
            preds.append(pred)
        
        return torch.clamp(torch.tensor(np.column_stack(preds)), 0, 1)
```

**Rationale:** If pure GP has a different CV-LB relationship, that's valuable information. Even if CV is worse, LB might be better.

### PRIORITY 2: Ridge Regression with Simple Features
**Why:** Simplest possible model may generalize better.
- Our CV (0.0083) is already 2x better than target LB (0.01670)
- The problem is generalization, not model quality
- Simpler models have fewer parameters to overfit

**Implementation:**
```python
from sklearn.linear_model import Ridge

class RidgeModel:
    def __init__(self, data='single', alpha=1.0):
        self.data_type = data
        self.featurizer = SimpleFeaturizer(mixed=(data=='full'))  # 18 features
        self.alpha = alpha
        
    def train_model(self, X_train, y_train):
        X_feat = self.featurizer.featurize(X_train)
        y_vals = y_train.values
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_train, flip=True)
            X_all = np.vstack([X_feat, X_flip])
            y_all = np.vstack([y_vals, y_vals])
        else:
            X_all, y_all = X_feat, y_vals
        
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_all)
        
        self.model = Ridge(alpha=self.alpha)
        self.model.fit(X_scaled, y_all)
    
    def predict(self, X_test):
        X_feat = self.featurizer.featurize(X_test)
        X_scaled = self.scaler.transform(X_feat)
        
        if self.data_type == 'full':
            X_flip = self.featurizer.featurize(X_test, flip=True)
            X_flip_scaled = self.scaler.transform(X_flip)
            pred = (self.model.predict(X_scaled) + self.model.predict(X_flip_scaled)) / 2
        else:
            pred = self.model.predict(X_scaled)
        
        return torch.clamp(torch.tensor(pred), 0, 1)
```

### PRIORITY 3: Ensemble with Different Weights
**Why:** exp_030 used GP(0.2) + MLP(0.5) + LGBM(0.3). Try different combinations.

**Options to try:**
- GP(0.3) + MLP(0.4) + LGBM(0.3) - Slightly higher GP
- GP(0.15) + MLP(0.55) + LGBM(0.3) - Lower GP, higher MLP
- GP(0.2) + MLP(0.4) + LGBM(0.4) - Equal MLP/LGBM

### PRIORITY 4: Multi-Output GP with Correlations
**Why:** Current GP treats each target independently. Multi-output GP captures correlations.
- SM + P2 + P3 ≈ 0.8 on average (targets are correlated)
- Multi-output GP can exploit this structure

## What NOT to Try

1. **Higher GP weight (0.4+)** - PROVEN WORSE (exp_031 was 10.61% worse)
2. **More complex models** - Already failed (exp_004 was 5x worse)
3. **More models in ensemble** - Diminishing returns
4. **Just improving CV** - The intercept (0.0524) > target (0.01670) means CV improvement alone won't work

## Validation Notes

**CV scheme:**
- Leave-one-solvent-out (24 folds for single, 13 folds for mixtures)
- This is the same as LB evaluation

**CV-LB calibration:**
- Linear fit: LB = 4.30*CV + 0.0524 (R²=0.97)
- Intercept (0.0524) > target (0.01670)
- Need to fundamentally change the CV-LB relationship

**Submission strategy:**
- 2 submissions remaining
- Only submit if we have a fundamentally different approach
- Focus on approaches that might change the CV-LB relationship

## Key Insight

**The problem is NOT model quality - our CV is already 2x better than the target LB.**

The problem is the CV-LB relationship. The intercept (0.0524) is 3x larger than the target (0.01670), meaning:
- Even with CV = 0, the predicted LB would be 0.0524
- We need an approach that fundamentally changes this relationship

**Potential approaches to change CV-LB relationship:**
1. Pure GP model (different mathematical framework)
2. Ridge regression (simplest possible model)
3. Different ensemble weights
4. Multi-output GP with correlations

**THE TARGET IS REACHABLE.** The top leaderboard score (0.01670) was achieved by someone. We need to find what they did differently. The key is NOT improving CV, but changing the CV-LB relationship.

## Experiment Order

1. **First:** Try Pure GP Model - fundamentally different approach
2. **Second:** Try Ridge Regression - simplest possible model
3. **Third:** Try different ensemble weights if time permits

Focus on approaches that might have a different CV-LB relationship, not just better CV.
