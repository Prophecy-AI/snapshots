## Current Status
- Best CV score: 0.0087 from exp_024 (ACS PCA Fixed Compliant)
- Best LB score: 0.0893 from exp_024 (NEW BEST!)
- Target: 0.01727 (lower is better)
- Gap to target: 5.17x (LB 0.0893 vs target 0.01727)
- CV-LB relationship: LB = 4.19*CV + 0.0537 (R²=0.955, updated with 9 data points)
- Intercept 95% CI: [0.0455, 0.0618] - still above target even at lower bound
- Submissions remaining: 4

## Response to Evaluator

**Technical verdict**: TRUSTWORTHY - The evaluator confirmed all fixes were correctly applied.

**Evaluator's top priority**: Submit exp_024. DONE - achieved LB 0.0893 (best yet!)

**Key concerns raised**:
1. **Per-target models unexplored**: Valid - competition explicitly allows different hyperparameters per target
2. **1% CV gap from exp_022**: Confirmed within normal variance - LB improved as expected

**My synthesis**: The submission validated that ACS PCA features help both CV and LB. The CV-LB relationship remains consistent (slope ~4.2, intercept ~0.054). The gap to target (5.17x) is large, but we have 4 submissions remaining and several unexplored approaches.

## Critical Analysis: The Gap Problem

The updated CV-LB linear fit shows:
- LB = 4.19*CV + 0.0537
- Even with CV=0, predicted LB = 0.0537 > target 0.01727
- The intercept 95% CI is [0.0455, 0.0618] - entirely above target

**BUT THIS DOESN'T MEAN THE TARGET IS UNREACHABLE:**

1. **Linear fit is based on only 9 data points** from similar approaches (all MLP/LGBM ensembles)
2. **A fundamentally different approach could have a different CV-LB relationship**
3. **The GNN benchmark achieved 0.0039** - proving much better scores are possible
4. **We haven't tried all tabular approaches** - per-target models, stacking, etc.

## Data Understanding

Reference notebooks:
- `experiments/024_acs_pca_fixed/acs_pca_fixed.ipynb`: Current best model (CV 0.0087, LB 0.0893)
- `exploration/evolver_loop25_lb_feedback.ipynb`: CV-LB analysis

Key findings:
1. **ACS PCA features**: 5 additional features improved both CV (3.5%) and LB (2.2%)
2. **Feature composition**: Spange (13) + DRFP (122) + Arrhenius (5) + ACS PCA (5) = 145 features
3. **Improvement trajectory**: CV improved 21.6%, LB improved 9.1% from baseline
4. **Target characteristics**: SM (mean 0.52) vs Products (mean 0.13) are very different

## Recommended Approaches

**PRIORITY 1: Per-Target Models (HIGH IMPACT)**

The competition explicitly allows "different hyper-parameters for different objectives". This is unexplored!

Target characteristics:
- SM: mean 0.52, std 0.36 (starting material remaining - higher values)
- Product 2: mean 0.13, std 0.14 (yield - lower values)
- Product 3: mean 0.13, std 0.14 (yield - lower values)
- Product 2 and Product 3 correlation: 0.923 (very high!)

Implementation approach:
```python
class PerTargetEnsemble:
    def __init__(self, data='single'):
        # Separate model for SM (different characteristics)
        self.sm_model = MLPEnsemble(hidden_dims=[64, 32], n_models=5, data=data)
        # Shared model for Products (highly correlated)
        self.product_model = MLPEnsemble(hidden_dims=[32, 16], n_models=5, data=data)
    
    def train_model(self, X_train, y_train):
        # Train SM model on SM target only
        self.sm_model.train_model(X_train, y_train[['SM']])
        # Train Product model on both products
        self.product_model.train_model(X_train, y_train[['Product 2', 'Product 3']])
    
    def predict(self, X_test):
        sm_pred = self.sm_model.predict(X_test)
        product_pred = self.product_model.predict(X_test)
        return torch.cat([product_pred, sm_pred], dim=1)  # [P2, P3, SM]
```

**PRIORITY 2: 4-Model Ensemble (MEDIUM IMPACT)**

Current ensemble: MLP + LightGBM (2 models)
Proposed: MLP + LightGBM + XGBoost + RandomForest (4 models)

More model diversity typically helps reduce variance and improve generalization.

```python
class FourModelEnsemble:
    def __init__(self, data='single'):
        self.mlp = MLPEnsemble(hidden_dims=[32, 16], n_models=5, data=data)
        self.lgbm = LGBMWrapper(data=data)
        self.xgb = XGBWrapper(data=data)  # NEW
        self.rf = RFWrapper(data=data)    # NEW
        # Weights: MLP 0.4, XGB 0.2, RF 0.2, LGBM 0.2
```

**PRIORITY 3: Stacking Meta-Learner (MEDIUM IMPACT)**

Instead of fixed weights, learn optimal combination from OOF predictions:

```python
# Get OOF predictions from each base model during CV
oof_mlp = []  # Shape: [n_samples, 3]
oof_lgbm = []
oof_xgb = []

# Stack features
X_meta = np.hstack([oof_mlp, oof_lgbm, oof_xgb])  # Shape: [n_samples, 9]

# Train meta-learner (Ridge regression for stability)
meta = Ridge(alpha=1.0)
meta.fit(X_meta, y_train)
```

**PRIORITY 4: Non-linear Mixture Encoding (LOW-MEDIUM IMPACT)**

Current: Linear interpolation `A*(1-pct) + B*pct`
Proposed: Add interaction term `A*B*pct*(1-pct)`

This captures non-linear solvent interactions that linear mixing misses.

**PRIORITY 5: Larger MLP Ensemble (LOW IMPACT)**

Current: 5 MLPs
Proposed: 10-15 MLPs for variance reduction

Previous experiments showed only 0.7% improvement from 5→15 models, but combined with other changes might help.

## What NOT to Try

1. **Attention mechanisms** - EXHAUSTED. exp_021 showed 159% worse.
2. **Fragprints instead of DRFP** - EXHAUSTED. exp_020 showed 8.28% worse.
3. **Deep residual networks** - EXHAUSTED. exp_004 showed 5x worse.
4. **MSELoss without scheduler** - CONFIRMED WORSE. exp_023 showed 4.2% degradation.
5. **Single-layer networks** - EXHAUSTED. [16] is too simple.
6. **Very large ensembles alone** - Only 0.7% improvement, not worth the time.

## Validation Notes

- Use leave-one-solvent-out CV for single solvents (24 folds)
- Use leave-one-ramp-out CV for mixtures (13 folds)
- Weighted average of single and full data MSE
- TTA for mixtures (average both orderings)
- CV-LB gap is ~10x - expect LB to be much higher than CV

## Template Compliance (CRITICAL)

The competition requires EXACT template structure:
- Last 3 cells must match template exactly
- Only allowed change: `model = MLPModel()` line can be replaced with new model definition
- Same hyperparameters across all folds (unless explainable rationale)
- Per-target models ARE allowed: "using a different model for alcohols vs esters is allowed"

## Key Insight

The target (0.01727) is 5.17x better than our best LB (0.0893). The linear CV-LB relationship suggests this is very challenging, BUT:

1. **The linear fit is based on only 9 data points** from similar approaches
2. **Per-target models are explicitly allowed** and unexplored
3. **The GNN benchmark achieved 0.0039** - proving much better scores are possible
4. **We have 4 submissions remaining** to test new approaches

**DO NOT GIVE UP. The target IS reachable. Try per-target models next.**

## Immediate Action Plan

1. **exp_025: Per-Target Models** - Separate models for SM vs Products
   - SM model: larger architecture (64, 32) for higher-variance target
   - Product model: shared model for highly correlated P2/P3
   - Expected: Different CV-LB relationship due to fundamentally different approach

2. **exp_026: 4-Model Ensemble** - Add XGBoost and RandomForest
   - More diversity in ensemble
   - May reduce variance and improve generalization

3. **exp_027: Stacking Meta-Learner** - Learn optimal combination weights
   - Use Ridge regression on OOF predictions
   - May find better weights than fixed 0.6/0.4

4. **Submit best candidate** - Validate new approach's CV-LB relationship