## Current Status
- Best CV score: 0.008465 (exp_026 - Weighted Loss Joint Model)
- Best LB score: 0.0887 (exp_026 - just submitted)
- CV-LB gap: ~10x ratio (LB = 4.22*CV + 0.0533, R²=0.96)
- Target: 0.01727
- Gap to target: 5.14x (0.0887 / 0.01727)
- Submissions remaining: 3

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The weighted loss implementation is sound.
- Evaluator's top priority: Submit exp_026 after fixing compliance (remove cell 13). DONE - submitted and got LB 0.0887.
- Key concerns raised: (1) Template compliance - cell 13 exists after final cell. (2) Prediction clipping needed.
- The submission was successful, confirming the approach works. The CV-LB relationship is now well-calibrated with 10 data points.

## Critical Analysis: The CV-LB Gap Problem

The linear fit LB = 4.22*CV + 0.0533 reveals a fundamental problem:
- Even with CV = 0, predicted LB = 0.0533 (3x higher than target 0.01727)
- The intercept represents a "floor" that our current approach cannot break through
- This suggests a DISTRIBUTION SHIFT between train and test that our models cannot handle

**The target IS reachable, but NOT by optimizing CV further.** We need to:
1. Reduce the CV-LB gap (improve generalization)
2. Find approaches that break the linear relationship

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop27_lb_feedback.ipynb` for CV-LB analysis
- Key patterns:
  - SM is the hardest target (MSE ~0.012 vs ~0.006 for products)
  - Multi-task learning provides regularization benefit
  - Weighted loss (2x SM) improved all targets by 2.58%
  - The CV-LB ratio is consistently ~10x across all 10 submissions

## Recommended Approaches (Priority Order)

### PRIORITY 1: Adversarial Validation to Identify Distribution Shift
**Rationale**: The 10x CV-LB gap suggests features that work well on train don't generalize to test. We need to identify which features are causing this.

**Implementation**:
1. Create a binary classifier to distinguish train vs test features
2. Use feature importance to identify shift-causing features
3. Remove or down-weight these features
4. This could dramatically reduce the CV-LB gap

### PRIORITY 2: Feature Selection for Generalization
**Rationale**: With 145 features, some may be overfitting to train distribution.

**Try**:
- Remove DRFP features (122 features) - they may not generalize
- Use only Spange (13) + ACS PCA (5) + Arrhenius (5) = 23 features
- Simpler feature set may generalize better

### PRIORITY 3: Stronger Regularization
**Rationale**: The 10x gap suggests overfitting to train distribution.

**Try**:
- Higher dropout (0.2-0.3 instead of 0.05)
- Stronger weight decay (1e-3 instead of 1e-5)
- Smaller hidden layers ([16,8] instead of [32,16])
- Fewer epochs (100 instead of 300)

### PRIORITY 4: Ensemble Diversity with Different Feature Sets
**Rationale**: Different feature sets may capture different aspects of generalization.

**Try**:
- Model 1: Spange only
- Model 2: ACS PCA only
- Model 3: Arrhenius kinetics only
- Ensemble: Average predictions from diverse models

### PRIORITY 5: Higher SM Weights
**Rationale**: SM is still the bottleneck. Try more aggressive weighting.

**Try**:
- Weights [1.0, 1.0, 3.0] or [1.0, 1.0, 4.0] for [P2, P3, SM]
- This may further improve SM prediction

## What NOT to Try
- More complex architectures (already tried [256,128,64], residual - all worse)
- More models in ensemble (15 models gave only 0.7% improvement)
- DRFP with PCA (CV 0.016948 - much worse)
- Per-target models (CV 0.009068 - worse than joint)

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvents (24 folds), leave-one-ramp-out for mixtures (13 folds)
- CV-LB relationship: LB = 4.22*CV + 0.0533 (R²=0.96)
- The intercept (0.0533) is the key problem - need to reduce it

## Key Insight
The target 0.01727 is BELOW the intercept of our CV-LB fit (0.0533). This means:
1. Our current approach has a "floor" of ~0.05 LB
2. To break through, we need approaches that REDUCE THE INTERCEPT
3. This requires better generalization, not just better CV

**Focus on GENERALIZATION, not CV optimization.**

## Template Compliance
- Last 3 cells must match template exactly
- Only model definition line can be changed
- Remove any verification cells after the final cell
