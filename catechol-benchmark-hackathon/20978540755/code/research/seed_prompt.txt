## Current Status
- Best CV score: 0.008689 from exp_024 (ACS PCA Fixed Compliant)
- Best LB score: 0.0893 from exp_024
- Target: 0.01727 (lower is better)
- Gap to target: 5.17x (LB 0.0893 vs target 0.01727)
- CV-LB relationship: LB = 4.19*CV + 0.0537 (R²=0.955)
- Submissions remaining: 4

## Response to Evaluator

**Technical verdict**: TRUSTWORTHY - The per-target experiment was correctly implemented.

**Evaluator's top priority**: Try loss-weighted joint model with SM emphasis. AGREED.

**Key concerns raised**:
1. **Per-target models failed**: SM MSE 0.014034 is 2x worse than Products. The larger [64,32] architecture for SM OVERFITS.
2. **Multi-task regularization is valuable**: The joint model provides implicit regularization that helps SM prediction.
3. **SM is the bottleneck**: Improving SM specifically could yield significant gains.

**My synthesis**: The evaluator correctly identified that per-target models hurt performance because they lose multi-task regularization. The solution is NOT separate models, but rather:
1. Keep joint model architecture
2. Weight SM loss higher to focus optimization on the hardest target
3. Add ensemble diversity (XGBoost, RandomForest) for variance reduction

## Data Understanding

Reference notebooks:
- `experiments/024_acs_pca_fixed/acs_pca_fixed.ipynb`: Current best model (CV 0.0087, LB 0.0893)
- `experiments/025_per_target/per_target_ensemble.ipynb`: Per-target failure analysis
- `exploration/evolver_loop26_analysis.ipynb`: Analysis of per-target failure

Key findings:
1. **Per-target MSE breakdown (exp_025)**:
   - Product 2 MSE: 0.005917 (IMPROVED with separate model)
   - Product 3 MSE: 0.007797 (IMPROVED with separate model)
   - SM MSE: 0.014034 (MUCH WORSE - overfitting!)
2. **Multi-task learning benefit**: Joint training provides regularization that helps SM
3. **SM characteristics**: mean 0.52, std 0.36 (highest variance target)
4. **Products characteristics**: mean ~0.13, std ~0.14, correlation 0.923

## Recommended Approaches

**PRIORITY 1: Loss-Weighted Joint Model (HIGH IMPACT)**

The per-target experiment showed that SM benefits from multi-task regularization. Instead of separate models, use weighted loss:

```python
class WeightedMSELoss(nn.Module):
    def __init__(self, weights=[1.0, 1.0, 2.0]):  # [P2, P3, SM]
        super().__init__()
        self.weights = torch.tensor(weights)
    
    def forward(self, pred, target):
        mse = (pred - target) ** 2
        weighted_mse = mse * self.weights.to(pred.device)
        return weighted_mse.mean()
```

This preserves multi-task regularization while focusing optimization on SM.

**PRIORITY 2: Uncertainty-Weighted Loss (HIGH IMPACT)**

Research (Kendall et al. CVPR 2018) shows that learning task-specific uncertainty weights automatically balances multi-task losses:

```python
class UncertaintyWeightedLoss(nn.Module):
    def __init__(self, n_tasks=3):
        super().__init__()
        # log(sigma^2) for numerical stability
        self.log_vars = nn.Parameter(torch.zeros(n_tasks))
    
    def forward(self, pred, target):
        mse = (pred - target) ** 2
        # L = 1/(2*sigma^2) * MSE + log(sigma)
        precision = torch.exp(-self.log_vars)
        loss = precision * mse + self.log_vars
        return loss.mean()
```

This automatically learns optimal weights based on task uncertainty.

**PRIORITY 3: 4-Model Ensemble (MEDIUM IMPACT)**

Add XGBoost and RandomForest for ensemble diversity:

```python
class FourModelEnsemble:
    def __init__(self, data='single'):
        self.mlp = MLPEnsemble(hidden_dims=[32, 16], n_models=5, data=data)
        self.lgbm = LGBMWrapper(data=data)
        self.xgb = XGBWrapper(data=data)  # NEW
        self.rf = RFWrapper(data=data)    # NEW
        # Weights: MLP 0.4, LGBM 0.2, XGB 0.2, RF 0.2
```

Research shows tree-based models (XGBoost, RF) have different inductive biases than neural networks, providing valuable ensemble diversity.

**PRIORITY 4: Stacking Meta-Learner (MEDIUM IMPACT)**

Instead of fixed weights, learn optimal combination from OOF predictions:

```python
# Get OOF predictions from each base model during CV
oof_mlp = []  # Shape: [n_samples, 3]
oof_lgbm = []
oof_xgb = []

# Stack features
X_meta = np.hstack([oof_mlp, oof_lgbm, oof_xgb])  # Shape: [n_samples, 9]

# Train meta-learner (Ridge regression for stability)
meta = Ridge(alpha=1.0)
meta.fit(X_meta, y_train)
```

**PRIORITY 5: Consistency Regularization (LOW-MEDIUM IMPACT)**

Add constraint that SM + P2 + P3 ≈ 1 (mass balance):

```python
def consistency_loss(pred):
    # pred: [batch, 3] = [P2, P3, SM]
    total = pred.sum(dim=1)
    return ((total - 1.0) ** 2).mean()

# Combined loss
loss = mse_loss + 0.1 * consistency_loss(pred)
```

## What NOT to Try

1. **Per-target models with separate architectures** - EXHAUSTED. exp_025 showed 4.36% worse.
2. **Larger SM architecture [64,32]** - EXHAUSTED. Overfits without multi-task regularization.
3. **Attention mechanisms** - EXHAUSTED. exp_021 showed 159% worse.
4. **Fragprints instead of DRFP** - EXHAUSTED. exp_020 showed 8.28% worse.
5. **Deep residual networks** - EXHAUSTED. exp_004 showed 5x worse.
6. **Very large ensembles alone** - Only 0.7% improvement, not worth the time.

## Validation Notes

- Use leave-one-solvent-out CV for single solvents (24 folds)
- Use leave-one-ramp-out CV for mixtures (13 folds)
- Weighted average of single and full data MSE
- TTA for mixtures (average both orderings)
- CV-LB gap is ~10x - expect LB to be much higher than CV

## Template Compliance (CRITICAL)

The competition requires EXACT template structure:
- Last 3 cells must match template exactly
- Only allowed change: `model = MLPModel()` line can be replaced with new model definition
- Same hyperparameters across all folds (unless explainable rationale)
- Loss weighting IS allowed: "using different hyper-parameters for different objectives is allowed"

## Key Insight from exp_025

The per-target experiment revealed a crucial insight: **multi-task learning provides implicit regularization that helps the hardest target (SM)**. Separating targets removes this benefit and causes SM to overfit.

The solution is NOT to separate models, but to:
1. Keep joint model (preserves multi-task regularization)
2. Weight SM loss higher (focuses optimization on hardest target)
3. Add ensemble diversity (reduces variance)

## Immediate Action Plan

1. **exp_026: Loss-Weighted Joint Model**
   - Keep joint [32,16] MLP + LightGBM ensemble
   - Weight SM loss 2x higher: `loss = MSE_P2 + MSE_P3 + 2*MSE_SM`
   - Expected: Better SM prediction while preserving multi-task regularization

2. **exp_027: Uncertainty-Weighted Loss**
   - Learn optimal task weights automatically
   - Based on Kendall et al. CVPR 2018
   - Expected: Automatic balancing of task difficulties

3. **exp_028: 4-Model Ensemble**
   - Add XGBoost and RandomForest
   - More diversity for variance reduction
   - Expected: Lower variance, better generalization

4. **Submit best candidate** - Validate new approach's CV-LB relationship

## THE TARGET IS REACHABLE

The per-target experiment was informative - it showed that multi-task learning is valuable. Use this insight to guide the next experiment. The target IS reachable with the right approach.