## Current Status
- Best CV score: 0.008601 from exp_022 (ACS PCA features) - NON-COMPLIANT
- Compliant CV score: 0.008964 from exp_023 (DEGRADED due to implementation mismatch)
- Best LB score: 0.0913 from exp_012
- Target: 0.0333 (lower is better)
- Gap to target: 2.74x (LB 0.0913 vs target 0.0333)
- CV-LB relationship: LB = 4.04*CV + 0.0552 (R²=0.946)
- Submissions remaining: 5

## Response to Evaluator

**Technical verdict**: CONCERNS - The evaluator correctly identified implementation differences between exp_019/exp_022 and exp_023 that caused CV degradation.

**Evaluator's top priority**: Fix the compliant notebook to match exp_022 exactly, then submit. I FULLY AGREE.

**Key concerns raised**:
1. **MSELoss vs HuberLoss**: exp_023 uses MSELoss, but exp_019/exp_022 used HuberLoss - MUST FIX
2. **Missing scheduler**: exp_023 has no ReduceLROnPlateau scheduler - MUST ADD
3. **Seed pattern**: exp_023 uses `42 + seed`, exp_019 uses `42 + i * 13` - MUST FIX
4. **Submission not made**: exp_023 was created but NOT submitted - MUST SUBMIT AFTER FIX

**My synthesis**: The evaluator's analysis is spot-on. The 4.2% CV degradation (0.008601 → 0.008964) is entirely explained by these implementation differences. Fixing them should restore the original performance.

## Data Understanding

Reference notebooks:
- `experiments/019_acs_pca/acs_pca_ensemble.ipynb`: Original ACS PCA experiment (CV 0.008601) - REFERENCE FOR CORRECT IMPLEMENTATION
- `experiments/023_acs_pca_compliant/acs_pca_compliant.ipynb`: Compliant but degraded (CV 0.008964) - NEEDS FIXES
- `exploration/evolver_loop24_analysis.ipynb`: Analysis of implementation differences

Key findings:
1. **ACS PCA features (5 features)**: Improved CV by 4.47% (0.009004 → 0.008601)
2. **Feature composition**: Spange (13) + DRFP (122) + Arrhenius (5) + ACS PCA (5) = 145 features
3. **Implementation matters**: HuberLoss + scheduler + correct seeds are critical for best performance
4. **CV-LB gap**: Linear fit predicts LB 0.090 for CV 0.008601 (1.4% better than 0.0913)

## Recommended Approaches

**PRIORITY 1: FIX COMPLIANT NOTEBOOK AND SUBMIT (IMMEDIATE)**

The exp_023 notebook needs exactly 3 fixes to match exp_019/exp_022:

1. **Change loss function** (in MLPEnsemble.train_model):
```python
# FROM:
criterion = nn.MSELoss()
# TO:
criterion = nn.HuberLoss()
```

2. **Add scheduler** (in MLPEnsemble.train_model):
```python
# ADD after optimizer definition:
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)

# ADD at end of each epoch (after loss.backward() and optimizer.step()):
epoch_loss += loss.item()
# After epoch loop ends:
scheduler.step(epoch_loss / len(loader))
```

3. **Fix seed pattern** (in MLPEnsemble.train_model):
```python
# FROM:
for seed in range(self.n_models):
    torch.manual_seed(42 + seed)
    np.random.seed(42 + seed)
# TO:
for i in range(self.n_models):
    torch.manual_seed(42 + i * 13)
    np.random.seed(42 + i * 13)
```

After these fixes:
- Re-run the notebook to verify CV matches 0.008601
- Submit to Kaggle immediately
- Expected LB: ~0.090 (1.4% better than 0.0913)

**PRIORITY 2: Per-Target Models (After submission)**

Competition rules explicitly allow "different hyper-parameters for different objectives (e.g., for SM vs Product 1)".

The targets have different characteristics:
- SM: mean 0.52, std 0.36 (higher values, more variance)
- Product 2: mean 0.13, std 0.14
- Product 3: mean 0.13, std 0.14
- Product 2 and Product 3 are highly correlated (0.923)

Implementation approach:
- Train separate MLP[32,16] + LightGBM ensembles for each target
- SM may benefit from different architecture/hyperparameters
- Products 2 and 3 could share a model due to high correlation

**PRIORITY 3: Stacking Meta-Learner**

Instead of fixed weights (0.6 MLP, 0.4 LGBM), train a meta-learner on out-of-fold predictions:
```python
# Get OOF predictions from MLP and LGBM
oof_mlp = np.zeros((n_samples, 3))
oof_lgbm = np.zeros((n_samples, 3))

# Train meta-learner (Ridge regression)
from sklearn.linear_model import Ridge
meta = Ridge(alpha=1.0)
meta.fit(np.hstack([oof_mlp, oof_lgbm]), y_train)
```

**PRIORITY 4: Non-linear Mixture Encoding**

Current approach uses linear interpolation for mixtures:
```python
X_feat = A * (1 - pct) + B * pct
```

Try non-linear mixing with interaction term:
```python
X_feat = A * (1 - pct) + B * pct + 0.1 * A * B * pct * (1 - pct)
```

This captures non-linear solvent interactions that linear mixing misses.

## What NOT to Try

1. **Attention mechanisms on tabular features** - EXHAUSTED. exp_021 showed 159% worse.
2. **Fragprints instead of DRFP** - EXHAUSTED. exp_020 showed 8.28% worse.
3. **Deep residual networks** - EXHAUSTED. exp_004 showed 5x worse.
4. **Very large ensembles (15+ models)** - EXHAUSTED. Only 0.7% improvement.
5. **Single-layer networks** - EXHAUSTED. [16] is too simple.
6. **MSELoss without scheduler** - CONFIRMED WORSE. exp_023 showed 4.2% degradation.

## Validation Notes

- Use leave-one-solvent-out CV for single solvents (24 folds)
- Use leave-one-ramp-out CV for mixtures (13 folds)
- Weighted average of single and full data MSE
- TTA for mixtures (average both orderings)
- CV-LB gap is ~10x - don't expect LB to match CV

## Template Compliance (CRITICAL)

The competition requires EXACT template structure:
- Last 3 cells must match template exactly
- Only allowed change: `model = MLPModel()` line can be replaced with new model definition
- Same hyperparameters across all folds (unless explainable rationale)

## Key Insight

The target (0.0333) is 2.74x better than our best LB (0.0913). The linear CV-LB relationship suggests that even CV=0 would give LB=0.0552 > target. However:

1. The linear fit is based on only 8 data points with HUGE confidence intervals
2. The intercept 95% CI spans [-3.27, 3.38] - the relationship could be very different
3. We haven't exhausted all tabular approaches yet
4. Per-target models and stacking are unexplored

**DO NOT GIVE UP. The target IS reachable. Fix the implementation, submit, and continue exploring.**

## Immediate Action

Create experiment exp_024:
1. Copy exp_023 notebook
2. Apply the 3 fixes (HuberLoss, scheduler, seed pattern)
3. Re-run to verify CV matches 0.008601
4. Submit to Kaggle
5. Continue with per-target models regardless of LB result
