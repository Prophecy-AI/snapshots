## Current Status
- Best CV score: 0.008465 (exp_026 - Weighted Loss Joint Model)
- Best LB score: 0.0887 (exp_026)
- CV-LB gap: ~10.5x ratio (LB = 4.22*CV + 0.0533, R²=0.96)
- Target: 0.01727
- Gap to target: 5.14x (0.0887 / 0.01727)
- Submissions remaining: 3

## Response to Evaluator
- Technical verdict was TRUSTWORTHY. The exp_027 simple features experiment was well-executed.
- Evaluator's top priority: DO NOT SUBMIT exp_027 (worse than exp_026). AGREED - exp_027 is 8.09% worse.
- Key concerns raised: (1) DRFP features ARE valuable - removing them hurt performance. (2) The CV-LB gap is STRUCTURAL, not caused by feature overfitting.
- Evaluator correctly identified that we need to focus on GENERALIZATION, not just CV optimization.

## Critical Analysis: The CV-LB Gap is Structural

The linear fit LB = 4.22*CV + 0.0533 has R²=0.96, meaning the relationship is very tight.
- Residuals are small (RMSE ~0.001)
- No clear pattern between model complexity and generalization
- The gap is NOT caused by specific model choices or features
- The gap is likely due to evaluation procedure differences or distribution shift

**Key Insight from exp_027**: Removing DRFP features (122 features) made CV 8.09% WORSE. This proves:
1. DRFP features capture important structural information
2. The CV-LB gap is NOT caused by DRFP feature overfitting
3. Simpler features do NOT generalize better for this problem

## Data Understanding
- Reference notebooks: See `exploration/evolver_loop28_analysis.ipynb` for residual analysis
- Key patterns:
  - SM is the hardest target (MSE ~0.012 vs ~0.006 for products)
  - Weighted loss (2x SM) improved all targets by 2.58%
  - The CV-LB ratio is consistently ~10x across all 10 submissions
  - Experiments with negative residuals (better generalization): exp_000, exp_003, exp_005, exp_024, exp_026
  - Experiments with positive residuals (worse generalization): exp_001, exp_009

## Recommended Approaches (Priority Order)

### PRIORITY 1: XGBoost/CatBoost Ensemble (NEW MODEL DIVERSITY)
**Rationale**: We only have MLP + LightGBM. XGBoost and CatBoost are different algorithms that may capture different patterns and generalize differently.

**Implementation**:
- Add XGBoost and CatBoost to the ensemble
- Use same features: Spange (13) + DRFP high-variance (122) + ACS PCA (5) + Arrhenius (5) = 145 features
- Ensemble weights: MLP (0.4) + LightGBM (0.2) + XGBoost (0.2) + CatBoost (0.2)
- Keep weighted loss [1,1,2] for MLP

**Expected outcome**: Different model types may have different generalization properties, potentially reducing the CV-LB gap.

### PRIORITY 2: Higher SM Weights [1,1,3] or [1,1,4]
**Rationale**: SM is still the bottleneck. Weighted loss [1,1,2] improved all targets by 2.58%. More aggressive weighting may help further.

**Implementation**:
- Try weights [1.0, 1.0, 3.0] for [P2, P3, SM]
- If that works, try [1.0, 1.0, 4.0]
- Keep same architecture and features

**Expected outcome**: Better SM predictions, which may improve overall score.

### PRIORITY 3: Stacking Meta-Learner
**Rationale**: Instead of fixed ensemble weights, train a meta-model to learn optimal combination.

**Implementation**:
- Train base models (MLP, LightGBM, XGBoost, CatBoost) on training folds
- Generate out-of-fold predictions
- Train a simple Ridge/Linear model on base predictions
- Use meta-model for final predictions

**Expected outcome**: Optimal combination of diverse models, potentially better generalization.

### PRIORITY 4: Consistency Constraint (SM + P2 + P3 ≈ 1)
**Rationale**: Physical constraint - starting material plus products should sum to ~1 (mass balance).

**Implementation**:
- Add soft constraint loss: λ * (SM + P2 + P3 - 1)²
- Or post-process predictions to normalize to sum=1
- This provides regularization based on domain knowledge

**Expected outcome**: More physically consistent predictions, potentially better generalization.

## What NOT to Try
- Simpler features (exp_027 proved DRFP is valuable)
- Deeper architectures (exp_004 failed with residual networks)
- More models in same ensemble (15 models gave only 0.7% improvement)
- Per-target models (exp_025 was worse than joint model)
- Ridge regression alone (exp_009 had positive residual)

## Validation Notes
- CV scheme: Leave-one-solvent-out for single solvents (24 folds), leave-one-ramp-out for mixtures (13 folds)
- CV-LB relationship: LB = 4.22*CV + 0.0533 (R²=0.96)
- The intercept (0.0533) is the key problem - need approaches that reduce it

## Key Insight
The target 0.01727 is BELOW the intercept of our CV-LB fit (0.0533). This means:
1. Our current approach has a "floor" of ~0.05 LB
2. To break through, we need approaches that REDUCE THE INTERCEPT
3. This requires better generalization, not just better CV
4. New model types (XGBoost, CatBoost) may have different generalization properties

**Focus on MODEL DIVERSITY and ENSEMBLE STRATEGIES, not just CV optimization.**

## Template Compliance
- Last 3 cells must match template exactly
- Only model definition line can be changed
- Remove any verification cells after the final cell
