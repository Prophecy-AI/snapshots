{
  "query": "How can I reduce the intercept in a linear relationship between cross-validation score and test score in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on guidance from statistics\u2011focused Q&A (Stats\u202fStackExchange) and standard cross\u2011validation practice, you can lower\u2014or eliminate\u2014the intercept in the linear fit that relates cross\u2011validation scores to test scores by first centering the data and only dropping the intercept when theory justifies it\u202f([stats.stackexchange.com](https://stats.stackexchange.com/questions/7948))\u202f([Medium](https://medium.com/swlh/why-do-we-need-an-intercept-in-regression-models-76485a98d03c)).\n\n**Steps**\n\n1. **Collect paired CV\u2011score / test\u2011score values** from your repeated cross\u2011validation runs (e.g., using `cross_val_score` in scikit\u2011learn)\u202f([scikit\u2011learn CV docs](https://scikit-learn.org/stable/modules/cross_validation.html)).  \n\n2. **Center each series**: subtract the mean CV score from every CV score and subtract the mean test score from every test score.  \n   ```python\n   cv_centered = cv_scores - cv_scores.mean()\n   test_centered = test_scores - test_scores.mean()\n   ```  \n   Centering forces the regression line through the origin, which directly reduces the intercept to (or near) zero.\n\n3. **Fit the linear relationship without an explicit intercept term** (i.e., force the line through the origin):  \n   ```python\n   from sklearn.linear_model import LinearRegression\n   model = LinearRegression(fit_intercept=False)\n   model.fit(cv_centered.reshape(-1,1), test_centered)\n   ```  \n   Only do this if you are confident that a zero CV score should correspond to a zero test score; otherwise keep the intercept\u202f([stats.stackexchange.com](https://stats.stackexchange.com/questions/7948),\u202f[Medium](https://medium.com/swlh/why-do-we-need-an-intercept-in-regression-models-76485a98d03c)).\n\n4. **Validate the effect** by recomputing the regression on the original (un\u2011centered) data and comparing intercept values. A substantial drop indicates the centering worked; if the intercept remains large, the relationship may truly require a non\u2011zero baseline.\n\n5. **(Optional) Apply regularization** such as ElasticNet to shrink all coefficients\u2014including any residual intercept\u2014while still using cross\u2011validation to select the regularization strength. This can further reduce over\u2011fitting and lower the intercept magnitude\u202f([scikit\u2011learn ElasticNet example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html)).\n\n6. **Interpret the result**: a near\u2011zero intercept means the CV score is an unbiased estimator of the test score after accounting for scaling; a significant intercept suggests systematic bias that must be modeled or corrected.\n\nBy centering the scores and, when appropriate, fitting without an intercept (or regularizing it), you directly reduce the intercept in the linear relationship between cross\u2011validation and test performance.",
      "url": ""
    },
    {
      "title": "Why Do We Need an Intercept in Regression Models? - Medium",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76485a98d03c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fwhy-do-we-need-an-intercept-in-regression-models-76485a98d03c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fswlh%2Fwhy-do-we-need-an-intercept-in-regression-models-76485a98d03c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**The Startup**](https://medium.com/swlh?source=post_page---publication_nav-f5af2b715248-76485a98d03c---------------------------------------)\n\n\u00b7\n\nGet smarter at building your thing. Follow to join The Startup\u2019s +8 million monthly readers & +772K followers.\n\n# Why Do We Need an Intercept in Regression Models?\n\n[Kaushik Jagini](https://medium.com/@kaushikjagini95?source=post_page---byline--76485a98d03c---------------------------------------)\n\n5 min read\n\n\u00b7\n\nOct 28, 2020\n\n--\n\n4\n\nListen\n\nShare\n\nLet\u2019s understand the intuition behind the role of an intercept in regression models.\n\nThe caveat for this article is that you are familiar with the simple equation of a straight line **y = mx + c**.\n\nHowever, to understand how it plays a role in regression problems, the familiarity with regression models such as linear regression is required.\n\nWell, there are way too many articles to help us understand what regression models are and how linear regression\u2019s been traditionally popular. However, if you want to understand the same at a granular level, there are very few. Furthermore, there\u2019s no one stop place to understand what\u2019s the role of an intercept in linear regression. Which is exactly my why for writing this.\n\nIn this post, I will start off with explaining how it plays a role geometrically and then move on to develop the intuition behind the role of an intercept. I will conclude by giving a few examples after which **you will be able to tell if intercept is required or not.**\n\nAlright, let\u2019s dive in.\n\nConsider the equation below:\n\nY = 3x. (A line with no intercept)\n\nFor the above equation, if:\n\nx = 0 then y =0;\n\nx=1 then y=3;\n\nx=2 then y =6;\n\nx=3 then y = 9 and so on\u2026\n\nLet\u2019s plot the above co-ordinates.\n\nThe key observation from the figure above is that the line passes through origin.\n\nIf you do not have the intercept, the line will **ALWAYS** pass through the origin.\n\nOkay, so what happens if the line always pass through origin??\n\nIf I ask you to join point A and B in the figure below, you can simply draw a straight line that can pass through them. ( On one condition \u2014 This line should pass through the origin)\n\nAgain, what If I ask you to draw a line that can pass through C and D in the figure below? ( Same condition \u2014 the line should pass through the origin)\n\nPress enter or click to view image in full size\n\nThere\u2019s no possibility. But, if you do not place that extra condition, then you can simply draw as shown below.\n\nPress enter or click to view image in full size\n\nSo, what did just happen here? If we put an extra condition that the line should pass through the origin, there are certain situations where we won\u2019t be able to draw a line to join two points. But, if we remove that condition, we can.\n\nThat condition is nothing but the intercept.\n\n**Can** pass through origin \u2014 A line **without** intercept.\n\n**Cannot** pass through origin \u2014 A line **with** intercept.\n\n> Having an intercept gives our model the freedom to capture ALL the linear patterns while a model with no intercept can capture only those patterns that pass through origin.\n\nOh, that\u2019s great! Now we understood how intercept plays a role in capturing the patterns.\n\nBut how do we interpret it? What can we infer from the linear model equation that the intercept value is 0.745 or 0.2 or -0.34 etc?\n\nDo we always need to have an intercept?\n\nWell, let\u2019s understand the above two with examples.\n\n**Example \u2014 1:**\n\nConsider you are working for any ed-tech startup. Since there is a lot of marketing campaign going on right now, you want to predict the number of course enrollment based on the marketing campaign. So the dependent variable is **course\\_enrollment** and the independent variable is **marketing**.\n\nThe equation here is,\n\ncourse\\_enrollment = (slope \\* marketing) + intercept. \\[of the form y=mx+c \\]\n\nIf the intercept = 0,\n\nY = (slope\\*marketing)\n\nNow, what if the marketing campaign was stopped i.e marketing = 0, will the course enrollment be totally 0? Nope, there will definitely be a minimum number of enrollments. **This, in other words indicate the trend**. What will be my sales if I stop marketing right now? This question can be answered only if we use intercept. Hence, we definitely need to have intercept in this case. Now you know what it means for an intercept with a value **0.745.** For this example, the intercept value of 0.745 indicate the trend when all other variables are 0.\n\n**Example \u2014 2:**\n\nIn this second example consider you are predicting the number of people who are cured from the coronavirus based on their age. Hence, the equation will be -\n\nCured = (Slope\\*age) + intercept\n\nIn the previous example we went ahead with the condition that intercept = 0 and then proved that it cannot be 0. Here we do vice versa.\n\nThe condition is, the intercept is not equal to 0.\n\nIn such cases, what if the input to the equation is such that the age = 0?\n\nSince age = 0 and intercept is not equal to 0, it does not make sense that with age 0, number of people cured is a non-zero.\n\nHence, we do not require intercept in such cases.\n\nThat\u2019s how crucial is the role of an intercept! I hope you got a better idea of it.\n\n> Note \u2014 If and only if you are confident that the output should be zero if all of the input variables are 0, only then can you get rid of using an intercept. Ideally, it\u2019s preferable to always and always use an intercept in the regression models.\n\n_Finally, feel free to add in any points I might have missed. Happy Learning!_\n\nIf you\u2019ve read until here, thank you. I\u2019m sure you and I have something in common.\n\nLet\u2019s connect on Linkedin:\n\nhttps://www.linkedin.com/in/kaushik-jagini-b76ba2139\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----76485a98d03c---------------------------------------)\n\n[Linear Regression](https://medium.com/tag/linear-regression?source=post_page-----76485a98d03c---------------------------------------)\n\n[Intercepts](https://medium.com/tag/intercepts?source=post_page-----76485a98d03c---------------------------------------)\n\n[Data Science](https://medium.com/tag/data-science?source=post_page-----76485a98d03c---------------------------------------)\n\n[Regression](https://medium.com/tag/regression?source=post_page-----76485a98d03c---------------------------------------)\n\n[**Published in The Startup**](https://medium.com/swlh?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\n[856K followers](https://medium.com/swlh/followers?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\n\u00b7 [Last published\u00a01 day ago](https://medium.com/swlh/i-relied-on-one-income-stream-as-a-creator-until-i-couldnt-3df7ff16aee5?source=post_page---post_publication_info--76485a98d03c---------------------------------------)\n\nGet smarter at building your thing. Follow to join Th...",
      "url": "https://medium.com/swlh/why-do-we-need-an-intercept-in-regression-models-76485a98d03c"
    },
    {
      "title": "Effect of model regularization on training and test error",
      "text": "[Skip to main content](https://scikit-learn.org/scikit-learn.org#main-content)\n\nBack to top`Ctrl` + `K`\n\n- [GitHub](https://github.com/scikit-learn/scikit-learn)\n\nChoose version\n\nNote\n\n[Go to the end](https://scikit-learn.org/scikit-learn.org#sphx-glr-download-auto-examples-model-selection-plot-train-error-vs-test-error-py)\nto download the full example code. or to run this example in your browser via JupyterLite or Binder\n\n# Effect of model regularization on training and test error [\\#](https://scikit-learn.org/scikit-learn.org\\#effect-of-model-regularization-on-training-and-test-error)\n\nIn this example, we evaluate the impact of the regularization parameter in a\nlinear model called [`ElasticNet`](https://scikit-learn.org/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet). To carry out this\nevaluation, we use a validation curve using\n[`ValidationCurveDisplay`](https://scikit-learn.org/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html#sklearn.model_selection.ValidationCurveDisplay). This curve shows the\ntraining and test scores of the model for different values of the regularization\nparameter.\n\nOnce we identify the optimal regularization parameter, we compare the true and\nestimated coefficients of the model to determine if the model is able to recover\nthe coefficients from the noisy input data.\n\n```\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n```\n\n## Generate sample data [\\#](https://scikit-learn.org/scikit-learn.org\\#generate-sample-data)\n\nWe generate a regression dataset that contains many features relative to the\nnumber of samples. However, only 10% of the features are informative. In this context,\nlinear models exposing L1 penalization are commonly used to recover a sparse\nset of coefficients.\n\n```\nfromsklearn.datasetsimport make_regression\nfromsklearn.model_selectionimport train_test_split\n\nn_samples_train, n_samples_test, n_features = 150, 300, 500\nX, y, true_coef = make_regression(\n    n_samples=n_samples_train + n_samples_test,\n    n_features=n_features,\n    n_informative=50,\n    shuffle=False,\n    noise=1.0,\n    coef=True,\n    random_state=42,\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\n)\n```\n\n## Model definition [\\#](https://scikit-learn.org/scikit-learn.org\\#model-definition)\n\nHere, we do not use a model that only exposes an L1 penalty. Instead, we use\nan [`ElasticNet`](https://scikit-learn.org/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) model that exposes both L1 and L2\npenalties.\n\nWe fix the `l1_ratio` parameter such that the solution found by the model is still\nsparse. Therefore, this type of model tries to find a sparse solution but at the same\ntime also tries to shrink all coefficients towards zero.\n\nIn addition, we force the coefficients of the model to be positive since we know that\n`make_regression` generates a response with a positive signal. So we use this\npre-knowledge to get a better model.\n\n```\nfromsklearn.linear_modelimport ElasticNet\n\nenet = ElasticNet(l1_ratio=0.9, positive=True, max_iter=10_000)\n```\n\n## Evaluate the impact of the regularization parameter [\\#](https://scikit-learn.org/scikit-learn.org\\#evaluate-the-impact-of-the-regularization-parameter)\n\nTo evaluate the impact of the regularization parameter, we use a validation\ncurve. This curve shows the training and test scores of the model for different\nvalues of the regularization parameter.\n\nThe regularization `alpha` is a parameter applied to the coefficients of the model:\nwhen it tends to zero, no regularization is applied and the model tries to fit the\ntraining data with the least amount of error. However, it leads to overfitting when\nfeatures are noisy. When `alpha` increases, the model coefficients are constrained,\nand thus the model cannot fit the training data as closely, avoiding overfitting.\nHowever, if too much regularization is applied, the model underfits the data and\nis not able to properly capture the signal.\n\nThe validation curve helps in finding a good trade-off between both extremes: the\nmodel is not regularized and thus flexible enough to fit the signal, but not too\nflexible to overfit. The [`ValidationCurveDisplay`](https://scikit-learn.org/modules/generated/sklearn.model_selection.ValidationCurveDisplay.html#sklearn.model_selection.ValidationCurveDisplay)\nallows us to display the training and validation scores across a range of alpha\nvalues.\n\n```\nimportnumpyasnp\n\nfromsklearn.model_selectionimport ValidationCurveDisplay\n\nalphas = np.logspace(-5, 1, 60)\ndisp = ValidationCurveDisplay.from_estimator(\n    enet,\n    X_train,\n    y_train,\n    param_name=\"alpha\",\n    param_range=alphas,\n    scoring=\"r2\",\n    n_jobs=2,\n    score_type=\"both\",\n)\ndisp.ax_.set(\n    title=r\"Validation Curve for ElasticNet (R$^2$ Score)\",\n    xlabel=r\"alpha (regularization strength)\",\n    ylabel=\"R$^2$ Score\",\n)\n\ntest_scores_mean = disp.test_scores.mean(axis=1)\nidx_avg_max_test_score = np.argmax(test_scores_mean)\ndisp.ax_.vlines(\n    alphas[idx_avg_max_test_score],\n    disp.ax_.get_ylim()[0],\n    test_scores_mean[idx_avg_max_test_score],\n    color=\"k\",\n    linewidth=2,\n    linestyle=\"--\",\n    label=f\"Optimum on test\\n$\\\\alpha$ = {alphas[idx_avg_max_test_score]:.2e}\",\n)\n_ = disp.ax_.legend(loc=\"lower right\")\n```\n\nTo find the optimal regularization parameter, we can select the value of `alpha`\nthat maximizes the validation score.\n\n## Coefficients comparison [\\#](https://scikit-learn.org/scikit-learn.org\\#coefficients-comparison)\n\nNow that we have identified the optimal regularization parameter, we can compare the\ntrue coefficients and the estimated coefficients.\n\nFirst, let\u2019s set the regularization parameter to the optimal value and fit the\nmodel on the training data. In addition, we\u2019ll show the test score for this model.\n\n```\nenet.set_params(alpha=alphas[idx_avg_max_test_score]).fit(X_train, y_train)\nprint(\n    f\"Test score: {enet.score(X_test,y_test):.3f}\",\n)\n```\n\n```\nTest score: 0.884\n\n```\n\nNow, we plot the true coefficients and the estimated coefficients.\n\n```\nimportmatplotlib.pyplotasplt\n\nfig, axs = plt.subplots(ncols=2, figsize=(12, 6), sharex=True, sharey=True)\nfor ax, coef, title in zip(axs, [true_coef, enet.coef_], [\"True\", \"Model\"]):\n    ax.stem(coef)\n    ax.set(\n        title=f\"{title} Coefficients\",\n        xlabel=\"Feature Index\",\n        ylabel=\"Coefficient Value\",\n    )\nfig.suptitle(\n    \"Comparison of the coefficients of the true generative model and \\n\"\n    \"the estimated elastic net coefficients\"\n)\n\nplt.show()\n```\n\nWhile the original coefficients are sparse, the estimated coefficients are not\nas sparse. The reason is that we fixed the `l1_ratio` parameter to 0.9. We could\nforce the model to get a sparser solution by increasing the `l1_ratio` parameter.\n\nHowever, we observed that for the estimated coefficients that are close to zero in\nthe true generative model, our model shrinks them towards zero. So we don\u2019t recover\nthe true coefficients, but we get a sensible outcome in line with the performance\nobtained on the test set.\n\n**Total running time of the script:** (0 minutes 4.564 seconds)\n\n[`Download Jupyter notebook: plot_train_error_vs_test_error.ipynb`](https://scikit-learn.org/_downloads/b49810e68af99a01e25ba2dfc951b687/plot_train_error_vs_test_error.ipynb)\n\n[`Download Python source code: plot_train_error_vs_test_error.py`](https://scikit-learn.org/_downloads/dcb776e3eb7cce048909ddcd70100917/plot_train_error_vs_test_error.py)\n\n[`Download zipped: plot_train_error_vs_test_error.zip`](https://scikit-learn.org/_downloads/899ea87b7908a2db16ce24485576e5db/plot_train_error_vs_test_error.zip)\n\nRelated examples\n\n[Ridge coefficients as a function of the L2 Regularization](https://scikit-learn.org/linear_model/plot_ridge_coeffs.html#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py)\n\nRidge coefficients as a function of the L2 Regularization\n\n[P...",
      "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html"
    },
    {
      "title": "3.1.  Cross-validation: evaluating estimator performance #",
      "text": "3.1. Cross-validation: evaluating estimator performance &#8212; scikit-learn 1.8.0 documentation\n[Skip to main content](#main-content)\n**Back to top**Ctrl+K\n[![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)![scikit-learn homepage](../_static/scikit-learn-logo-without-subtitle.svg)](../index.html)\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n**\n******\n* [**GitHub](https://github.com/scikit-learn/scikit-learn)\nChoose version\n# 3.1.Cross-validation: evaluating estimator performance[#](#cross-validation-evaluating-estimator-performance)\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called**overfitting**.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a**test set**`X\\_test,y\\_test`.\nNote that the word \u201cexperiment\u201d is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by[grid search](grid_search.html#grid-search)techniques.\n[![Grid Search Workflow](../_images/grid_search_workflow.png)](../_images/grid_search_workflow.png)\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the[`train\\_test\\_split`](generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)helper function.\nLet\u2019s load the iris data set to fit a linear support vector machine on it:\n```\n&gt;&gt;&gt;importnumpyasnp&gt;&gt;&gt;fromsklearn.model\\_selectionimporttrain\\_test\\_split&gt;&gt;&gt;fromsklearnimportdatasets&gt;&gt;&gt;fromsklearnimportsvm&gt;&gt;&gt;X,y=datasets.load\\_iris(return\\_X\\_y=True)&gt;&gt;&gt;X.shape,y.shape((150, 4), (150,))\n```\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n```\n&gt;&gt;&gt;X\\_train,X\\_test,y\\_train,y\\_test=train\\_test\\_split(...X,y,test\\_size=0.4,random\\_state=0)&gt;&gt;&gt;X\\_train.shape,y\\_train.shape((90, 4), (90,))&gt;&gt;&gt;X\\_test.shape,y\\_test.shape((60, 4), (60,))&gt;&gt;&gt;clf=svm.SVC(kernel=&#39;linear&#39;,C=1).fit(X\\_train,y\\_train)&gt;&gt;&gt;clf.score(X\\_test,y\\_test)0.96\n```\nWhen evaluating different settings (\u201chyperparameters\u201d) for estimators,\nsuch as the`C`setting that must be manually set for an SVM,\nthere is still a risk of overfitting*on the test set*because the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can \u201cleak\u201d into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called \u201cvalidation set\u201d: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\nA solution to this problem is a procedure called[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called*k*-fold CV,\nthe training set is split into*k*smaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the*k*\u201cfolds\u201d:\n* A model is trained using\\\\(k-1\\\\)of the folds as training data;\n* the resulting model is validated on the remaining part of the data\n(i.e., it is used as a test set to compute a performance measure\nsuch as accuracy).\nThe performance measure reported by*k*-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n[![A depiction of a 5 fold cross validation on a training set, while holding out a test set.](../_images/grid_search_cross_validation.png)](../_images/grid_search_cross_validation.png)\n## 3.1.1.Computing cross-validated metrics[#](#computing-cross-validated-metrics)\nThe simplest way to use cross-validation is to call the[`cross\\_val\\_score`](generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)helper function on the estimator and the dataset.\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset by splitting the data, fitting\na model and computing the score 5 consecutive times (with different splits each\ntime):\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimportcross\\_val\\_score&gt;&gt;&gt;clf=svm.SVC(kernel=&#39;linear&#39;,C=1,random\\_state=42)&gt;&gt;&gt;scores=cross\\_val\\_score(clf,X,y,cv=5)&gt;&gt;&gt;scoresarray([0.96, 1. , 0.96, 0.96, 1. ])\n```\nThe mean score and the standard deviation are hence given by:\n```\n&gt;&gt;&gt;print(&quot;%0.2faccuracy with a standard deviation of%0.2f&quot;%(scores.mean(),scores.std()))0.98 accuracy with a standard deviation of 0.02\n```\nBy default, the score computed at each CV iteration is the`score`method of the estimator. It is possible to change this by using the\nscoring parameter:\n```\n&gt;&gt;&gt;fromsklearnimportmetrics&gt;&gt;&gt;scores=cross\\_val\\_score(...clf,X,y,cv=5,scoring=&#39;&#39;f1\\_macro&#39;&#39;)&gt;&gt;&gt;scoresarray([0.96, 1., 0.96, 0.96, 1.])\n```\nSee[The scoring parameter: defining model evaluation rules](model_evaluation.html#scoring-parameter)for details.\nIn the case of the Iris dataset, the samples are balanced across target\nclasses hence the accuracy and the F1-score are almost equal.\nWhen the`cv`argument is an integer,[`cross\\_val\\_score`](generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)uses the[`KFold`](generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)or[`StratifiedKFold`](generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)strategies by default, the latter\nbeing used if the estimator derives from[`ClassifierMixin`](generated/sklearn.base.ClassifierMixin.html#sklearn.base.ClassifierMixin).\nIt is also possible to use other cross validation strategies by passing a cross\nvalidation iterator instead, for instance:\n```\n&gt;&gt;&gt;fromsklearn.model\\_selectionimportShuffleSplit&gt;&gt;&gt;n\\_samples=X.shape[0]&gt;&gt;&gt;cv=ShuffleSplit(n\\_splits=5,test\\_size=0.3,random\\_state=0)&gt;&gt;&gt;cross\\_val\\_score(clf,X,y,cv=cv)array([0.977, 0.977, 1., 0.955, 1.])\n```\nAnother option is to use an iterable yielding (train, test) splits as arrays of\nindices, for example:\n```\n&gt;&gt;&gt;defcustom\\_cv\\_2folds(X):...n=X.shape[0]...i=1...whilei&lt;=2:...idx=np.arange(n\\*(i-1)/2,n\\*i/2,dtype=int)...yieldidx,idx...i+=1...&gt;&gt;&gt;custom\\_cv=custom\\_cv\\_2folds(X)&gt;&gt;&gt;cross\\_val\\_score(clf,X,y,cv=custom\\_cv)array([1. , 0.973])\n```\nData transformation with held-out data[#](#data-transformation-with-held-out-data)\nJust as it is important to test a predictor on data held-out from\ntraining, preprocessing (such as standardization, feature selection, etc.)\nand similar[data transformations](../data_transforms.html#data-transforms)similarly should\nbe learnt from a training set and applied to held-out da...",
      "url": "https://scikit-learn.org/stable/modules/cross_validation.html"
    },
    {
      "title": "When is it ok to remove the intercept in a linear regression ...",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [When is it ok to remove the intercept in a linear regression model?](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked13 years, 3 months ago\n\nModified [1 month ago](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model?lastactivity)\n\nViewed\n230k times\n\n183\n\n$\\\\begingroup$\n\nI am running linear regression models and wondering what the conditions are for removing the intercept term.\n\nIn comparing results from two different regressions where one has the intercept and the other does not, I notice that the $R^2$ of the function without the intercept is much higher. Are there certain conditions or assumptions I should be following to make sure the removal of the intercept term is valid?\n\n- [regression](https://stats.stackexchange.com/questions/tagged/regression)\n- [linear-model](https://stats.stackexchange.com/questions/tagged/linear-model)\n- [r-squared](https://stats.stackexchange.com/questions/tagged/r-squared)\n- [intercept](https://stats.stackexchange.com/questions/tagged/intercept)\n- [faq](https://stats.stackexchange.com/questions/tagged/faq)\n\n[Share](https://stats.stackexchange.com/q/7948)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/7948/edit)\n\nFollow\n\n[edited Sep 22, 2022 at 1:13](https://stats.stackexchange.com/posts/7948/revisions)\n\n[![kjetil b halvorsen's user avatar](https://i.sstatic.net/Nri78.jpg?s=64)](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen)\n\n[kjetil b halvorsen](https://stats.stackexchange.com/users/11887/kjetil-b-halvorsen) \u2666\n\n80.3k3131 gold badges196196 silver badges639639 bronze badges\n\nasked Mar 7, 2011 at 9:14\n\n[![analyticsPierce's user avatar](https://www.gravatar.com/avatar/3b5598a61f85012ea1de2a5c8cee2e6c?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/1422/analyticspierce)\n\n[analyticsPierce](https://stats.stackexchange.com/users/1422/analyticspierce) analyticsPierce\n\n2,03144 gold badges1414 silver badges66 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- 1\n\n\n\n\n\n$\\\\begingroup$What would the intercept mean in your model? From the information in your question, it seems it would be the expected value of your response when sqft=0 and lotsize=0 and baths=0. Is that ever going to occur in reality?$\\\\endgroup$\n\n\u2013\u00a0[timbp](https://stats.stackexchange.com/users/7644/timbp)\n\nCommentedFeb 27, 2012 at 2:59\n\n- $\\\\begingroup$the intercept has no meaning. so when writing up a formula for the expected value of a house, can I leave this out?$\\\\endgroup$\n\n\u2013\u00a0[Travis](https://stats.stackexchange.com/users/9363/travis)\n\nCommentedFeb 27, 2012 at 3:02\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Instead of the y= a + b1 x1 + b2 x2 + b3x3, can I omit a?$\\\\endgroup$\n\n\u2013\u00a0[Travis](https://stats.stackexchange.com/users/9363/travis)\n\nCommentedFeb 27, 2012 at 3:03\n\n- 3\n\n\n\n\n\n$\\\\begingroup$**NB**: Some of these comments and replies address essentially the same question (framed in the context of a housing price regression) which was merged with this one as a duplicate.$\\\\endgroup$\n\n\u2013\u00a0[whuber](https://stats.stackexchange.com/users/919/whuber) \u2666\n\nCommentedFeb 27, 2012 at 4:51\n\n- 1\n\n\n\n\n\n$\\\\begingroup$A side point: $R^2$ does not have such good properties when you don't include an intercept in your model, so it's not the best statistic to use for your comparison.$\\\\endgroup$\n\n\u2013\u00a0[Charlie](https://stats.stackexchange.com/users/401/charlie)\n\nCommentedFeb 27, 2012 at 7:43\n\n\n[Add a comment](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\u00a0\\|\n\n## 11 Answers 11\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n125\n\n$\\\\begingroup$\n\nThe **shortest** answer: **never**, unless you are _sure_ that your linear approximation of the data generating process (linear regression model) either by some theoretical or any other reasons _is forced to go through the origin_. If not the other regression parameters will be biased even if intercept is statistically insignificant (strange but it is so, consult [Brooks](http://www.cambridge.org/features/economics/brooks/) _Introductory Econometrics_ for instance). Finally, as I do often explain to my students, by leaving the intercept term you insure that the residual term is zero-mean.\n\nFor your two models case we need more context. It may happen that linear model is not suitable here. For example, you need to log transform first if the model is multiplicative. Having exponentially growing processes it may occasionally happen that $R^2$ for the model without the intercept is \"much\" higher.\n\nScreen the data, test the model with RESET test or any other linear specification test, this may help to see if my guess is true. And, building the models highest $R^2$ is one of the last statistical properties I do really concern about, but it is nice to present to the people who are not so well familiar with econometrics (there are many dirty tricks to make determination close to 1 :)).\n\n[Share](https://stats.stackexchange.com/a/7950)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/7950/edit)\n\nFollow\n\nanswered Mar 7, 2011 at 10:16\n\n[![Dmitrij Celov's user avatar](https://www.gravatar.com/avatar/533a5a7b1a34f5100c8f15022a677137?s=64&d=identicon&r=PG)](https://stats.stackexchange.com/users/2645/dmitrij-celov)\n\n[Dmitrij Celov](https://stats.stackexchange.com/users/2645/dmitrij-celov) Dmitrij Celov\n\n6,28522 gold badges3030 silver badges4141 bronze badges\n\n$\\\\endgroup$\n\n4\n\n- 11\n\n\n\n\n\n$\\\\begingroup$-1 for \"never\", see example 1 of Joshuas' answer$\\\\endgroup$\n\n\u2013\u00a0[Tomas](https://stats.stackexchange.com/users/5509/tomas)\n\nCommentedDec 2, 2013 at 20:43\n\n- 7\n\n\n\n\n\n$\\\\begingroup$@Curious, \"never\" is written with \"unless\" examples below just show the exceptions when it is legal to remove intercept. When you don't know the data generating process or theory, or are not forced to go through the origin by standardization or any other special model, keep it. Keeping intercept is like using the trash bin to collect all the distortions caused by linear approximation and other simplifications. P.S. practically the response shows that you read just shortest :) Thanks a lot to Joshua (+1) for the extended examples.$\\\\endgroup$\n\n\u2013\u00a0[Dmitrij Celov](https://stats.stackexchange.com/users/2645/dmitrij-celov)\n\nCommentedMay 6, 2014 at 9:56\n\n- 5\n\n\n\n\n\n$\\\\begingroup$You missed the point of Joshua Example 1 and seem to still ignore it completely. In models with categorical covariate the removal of the intercept results in the same model with just different parametrization. This is a legitimate case when intercept can be removed.$\\\\endgroup$\n\n\u2013\u00a0[Tomas](https://stats.stackexchange.com/users/5509/tomas)\n\nCommentedMay 26, 2014 at 15:04\n\n- 3\n\n\n\n\n\n$\\\\begingroup$@Curious, in Joshua example 1, you need to add a new dummy variable for the level of the categorical variable you previously considered as baseline, and this new dummy variable will take the value of the intercept, so you are NOT removing the intercept, just renaming it and reparameterizing the rest of the parameters of the categorical covariate. Therefore the argument of Dmitrij holds.$\\\\endgroup$\n\n\u2013\u00a0[Rufo](https://stats.stackexchange.com/users/39849/rufo)\n\nCommentedApr 15, 2019 at 15:17\n\n\n[Add a comment](https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model)\u00a0\\|\n\n90\n\n$\\\\begingroup$\n\nRemoving the intercept is a different model, but there are plenty of examples where it is legitimate. Answers so far have alrea...",
      "url": "https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"
    },
    {
      "title": "Cross-Validation Tutorial - QuantDev Methodology",
      "text": "Cross-Validation Tutorial | QuantDev Methodology\n[Skip to main content](#main-content)\n# Cross-Validation Tutorial\n[Download this Tutorial**](https://quantdev.ssri.psu.edu/sites/qdev/files/CV_tutorial.html)[View in a new Window**](https://quantdev.ssri.psu.edu/sites/qdev/files/CV_tutorial.html)\n## Contributors\n[Miriam Brinberg](https://quantdev.ssri.psu.edu/people/mjb6504)\n## Related Projects\n[We R: R Users @ Penn State](https://quantdev.ssri.psu.edu/projects/we-r-r-users-penn-state)\n## Related Resource\n[Data Mining](https://quantdev.ssri.psu.edu/resources/data-mining)\n[**](#)",
      "url": "https://quantdev.ssri.psu.edu/tutorials/cross-validation-tutorial"
    },
    {
      "title": "Cross-Validation Techniques",
      "text": "# Cross-Validation Techniques\n[\u2190 Regularization Techniques](https://exploration.stat.illinois.edu/learn/Feature-Selection/Regularization-Techniques/)[Next: Principal Component Regression \u2192](https://exploration.stat.illinois.edu/learn/Feature-Selection/Principal-Component-Regression/)\n## Downsides of Selecting a Single Training and Test Dataset\n### 1. Differing Results\nAs we just observed, based on the way that a dataset is randomly split into just a*single*training and test dataset, we may get varying results when it comes to:\n1. the type of models that are built based on the training data, and\n2. the type of model that was deemed to be best and thus selected based on test data performance.### 2. Not Using the Full Dataset\nIn addition, there are inevitably going to be some observations that will never be considered when it comes to training the dataset, because they have been placed in the test dataset. And similarly there will also be some observations that will never be considered when it comes to testing the dataset, because they have been placed in the training dataset. Thus, we are not giving each observation the chance to influence our model build and selection.\n## Cross-Validation Techniques\nAs a means of making up for these shortcomings of selecting just a single training and test dataset, we introduce what we call**cross-validation techiques**. These techniques involve creating*multiple*sets of training and test datasets from the full dataset. By using these techniques, an observation will:\n* appear in*at least one*training dataset, and\n* appear in*at least one*test dataset.\nTwo of the most common cross-validation techniques include:\n1. Leave-One-Out-Cross-Validation (LOOCV)\n2. k-Fold Cross-Validation## Leave-One-Out-Cross-Validation (LOOCV)\nThe**leave-one-out cross-validation**method has every observation appear in a test dataset*exactly once*. Every observation will be in a training dataset*n-1 times*. Specifically, we use the LOOCV method to evaluate a particular model that we are considering below.\n### Choosing a Model with LOOCV\nFor a dataset with $n$ observations and a given model that we are considering (ex: one with a certain subset of explanatory variables), we do the following.\n1. **Create $n$ Training and Test Data Pairs**: For each of the $n$ observations, we do the following, creating $n$*pairs*of training and test datasets.\n* Create a test dataset with*just the one observation*.\n* Create a training dataset with*every other remaining observation*in the dataset.\n* **Train $n$ Models**: Then we create $n$ models with each of the $n$ training datasets.\n* **Test the $n$ Models**: We then test the $n$ models with each of their $n$ corresponding test datasets.*For instance, we might calculate the test data residual of it's corresponding model.*\n* **Average Model Test Performance**: Then we calculate the average test data performance (over all $n$ test datasets).*For instance, we might calculate the average test data residual*.\n* **Compare Average Model Performance**: Compare different model performances (ex: ones with different subsets of explanatory variables) based on this average model performance.*For instance, we may select the model with the lowestaveragetest model residual.*\n### Benefits of LOOCV\nThe LOOCV method has the following benefits.\n* **More Accurate Test Data Performance than Train-Test-Split Method**: The average*test data performance*is more accurate than by using just a single test and training dataset.\nWhy?\n* Recall how our training dataset predictions tend to be better than our test dataset predictions. This is because the training dataset observations were actually used to build the model. So the model was explicitly trying to minimize the error of their predictions.\n* In LOOCV, each of our training datasets is*almost*the full dataset, except for the one observation that was left out for the corresponding test dataset. Thus, we might expect each of our trained models to look pretty similar to what model we would have gotten if we used the*full dataset*to train the model. Thus, the corresponding test dataset predictions that we make with each of these trained models, will similarly reflect this higher accuracy that the full model might have achieved.\n* **No Randomness**: There is no random nature to the way in which the $n$ training and test dataset pairs are created. Thus, your model results and decisions that you make will not fluctuate based on the random seed that was selected.\n* **Low Model Variability**: Because each of the $n$ training datasets have mostly the same set of observations in them, each of your $n$ trained models should not have a high degree of variability in their slopes and intercept.\n### Drawbacks of LOOCV\nThe LOOCV method has the following drawbacks.\n* **Computationally Expensive**: This can be a very computationally expensive method, especially for large datasets, because $n$ models need to be trained.\n* **More Variable Test Data Predictions**: Your test dataset predictions*individually*may be highly variable.\n* **Inflation of Model Performance**: The average test dataset performance may also lead to an over-inflated estimate as to how well given model will perform on*new*datasets.\nWhy?\n* Recall how the average error (like RMSE) of our training dataset tends to be better than the average error (like RMSE) of our test dataset in the**train-test-split method**.\n* Because each of our training datasets of size $n-1$ in the**LOOCV method**are almost the same and almost the full dataset, we might expect each of our $n$ trained models to look very similar to the model that would have been trained on the full dataset.\n* Thus, when we calculate the error of each of our $n$ single test observations and average them, we might expect this error to look very similar to the average full dataset error of the model trained with the full dataset.\n* Thus, this average test data RMSE is likely to be lower than what it would be for a potential new dataset.## k-Fold Cross-Validation\nThe**k-fold cross-validation**method has every observation appear in a test dataset*once*. Every observation will appear in a training dataset*k-1 times*. Specifically, we use the k-Fold cross-validation method to evaluate a particular model that we are considering below.\n### Choosing a Model with k-Fold Cross-Validation\nFor a dataset with $n$ observations and a given model that we are considering (ex: one with a certain subset of explanatory variables), we do the following.\n1. **Create $k$ Folds**: Split the dataset into $k$ equally sized**folds**(ie. observation subsets).*(You may choose to randomly shuffle the rows in the full dataset first in order to have your $k$ folds be randomly selected.)*\n2. **Create $k$ Training and Test Data Pairs**: For each of the**$k$ folds**, we do the following, creating $k$*pairs*of training and test datasets.\na.\tCreate a test dataset with*just the one fold*.\nb.\tCreate a training dataset comprised of*every other remaining fold*in the dataset.\n3. **Train $n$ Models**: Then we create $k$ models with each of the $k$ training datasets.\n4. **Test the $n$ Models**: We then test the $k$ models with each of their $k$ corresponding test datasets.*For instance, we might calculate the test data R^2 of it's corresponding model.*\n5. **Average Model Test Performance**: Then we calculate the average test data performance (over all $k$ test datasets).*For instance, we might calculate the average test data R^2*.\n6. **Compare Average Model Performance**: Compare different model performances (ex: ones with different subsets of explanatory variables) based on this average model performance.*For instance, we may select the model with the highestaveragetest R^2.*\n### Benefits of k-Fold Cross-Validation\nThe k-fold cross-validation method has the following benefits.\n* **Less Computationally Complex than LOOCV**: This method is less computationally complex as LOOCV, as now only $k$ models need to be trained.\n* **More Ac...",
      "url": "https://exploration.stat.illinois.edu/learn/Feature-Selection/Cross-Validation-Techniques"
    },
    {
      "title": "Held-Out Evaluation and Cross-Validation - Stan",
      "text": "Held-Out Evaluation and Cross-Validation\n# Held-Out Evaluation and Cross-Validation\nHeld-out evaluation involves splitting a data set into two parts, a training data set and a test data set. The training data is used to estimate the model and the test data is used for evaluation. Held-out evaluation is commonly used to declare winners in predictive modeling competitions such as those run by[Kaggle](https://kaggle.com).\nCross-validation involves repeated held-out evaluations performed by partitioning a single data set in different ways. The training/test split can be done either by randomly selecting the test set, or by partitioning the data set into several equally-sized subsets and then using each subset in turn as the test data with the other folds as training data.\nHeld-out evaluation and cross-validation may involve any kind of predictive statistics, with common choices being the predictive log density on test data, squared error of parameter estimates, or accuracy in a classification task.\n## Evaluating posterior predictive densities\nGiven training data\\\\((x, y)\\\\)consisting of parallel sequences of predictors and observations and test data\\\\((\\\\tilde{x}, \\\\tilde{y})\\\\)of the same structure, the posterior predictive density is\\\\[\np(\\\\tilde{y} \\\\mid \\\\tilde{x}, x, y)\n=\n\\\\int\np(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta)\n\\\\cdot p(\\\\theta \\\\mid x, y)\n\\\\, \\\\textrm{d}\\\\theta,\n\\\\]\nwhere\\\\(\\\\theta\\\\)is the vector of model parameters. This predictive density is the density of the test observations, conditioned on both the test predictors\\\\(\\\\tilde{x}\\\\)and the training data\\\\((x, y).\\\\)\nThis integral may be calculated with Monte Carlo methods as usual,\\\\[\np(\\\\tilde{y} \\\\mid \\\\tilde{x}, x, y)\n\\\\approx\n\\\\frac{1}{M} \\\\sum\\_{m = 1}^M p(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta^{(m)}),\n\\\\]where the\\\\(\\\\theta^{(m)} \\\\sim p(\\\\theta \\\\mid x, y)\\\\)are draws from the posterior given only the training data\\\\((x, y).\\\\)\nTo avoid underflow in calculations, it will be more stable to compute densities on the log scale. Taking the logarithm and pushing it through results in a stable computation,\\\\[\\\\begin{eqnarray\\*}\n\\\\log p(\\\\tilde{y} \\\\mid \\\\tilde{x}, x, y)\n&amp;&amp; \\\\approx &amp;&amp;\n\\\\log \\\\frac{1}{M} \\\\sum\\_{m = 1}^M p(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta^{(m)}),\n\\\\\\\\[4pt]\n&amp;&amp; = &amp;&amp; -\\\\log M + \\\\log \\\\sum\\_{m = 1}^M p(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta^{(m)}),\n\\\\\\\\[4pt]\n&amp;&amp; = &amp;&amp; -\\\\log M + \\\\log \\\\sum\\_{m = 1}^M \\\\exp(\\\\log p(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta^{(m)}))\n\\\\\\\\[4pt]\n&amp;&amp; = &amp;&amp; -\\\\log M + \\\\textrm{log-sum-exp}\\_{m = 1}^M \\\\log p(\\\\tilde{y} \\\\mid \\\\tilde{x}, \\\\theta^{(m)})\n\\\\end{eqnarray\\*}\\\\]where the log sum of exponentials function is defined so as to make the above equation hold,\\\\[\n\\\\textrm{log-sum-exp}\\_{m = 1}^M \\\\, \\\\mu\\_m\n= \\\\log \\\\sum\\_{m=1}^M \\\\exp(\\\\mu\\_m).\n\\\\]The log sum of exponentials function can be implemented so as to avoid underflow and maintain high arithmetic precision as\\\\[\n\\\\textrm{log-sum-exp}\\_{m = 1}^M \\\\mu\\_m\n= \\\\textrm{max}(\\\\mu)\n+ \\\\log \\\\sum\\_{m = 1}^M \\\\exp(\\\\mu\\_m - \\\\textrm{max}(\\\\mu)).\n\\\\]Pulling the maximum out preserves all of its precision. By subtracting the maximum, the terms\\\\(\\\\mu\\_m - \\\\textrm{max}(\\\\mu) \\\\leq 0\\\\), and thus will not overflow.\n### Stan program\nTo evaluate the log predictive density of a model, it suffices to implement the log predictive density of the test data in the generated quantities block. The log sum of exponentials calculation must be done on the outside of Stan using the posterior draws of\\\\(\\\\log p(\\\\tilde{y} \\\\mid \\\\tilde{x},\n\\\\theta^{(m)}).\\\\)\nHere is the code for evaluating the log posterior predictive density in a simple linear regression of the test data\\\\(\\\\tilde{y}\\\\)given predictors\\\\(\\\\tilde{x}\\\\)and training data\\\\((x, y).\\\\)\n```\n`[](#cb1-1)data{[](#cb1-2)int&lt;lower=0&gt; N;[](#cb1-3)vector[N] y;[](#cb1-4)vector[N] x;[](#cb1-5)int&lt;lower=0&gt;&gt; N\\_tilde;[](#cb1-6)vector[N\\_tilde] x\\_tilde;[](#cb1-7)vector[N\\_tilde] y\\_tilde;[](#cb1-8)}[](#cb1-9)parameters{[](#cb1-10)realalpha;[](#cb1-11)realbeta;[](#cb1-12)real&lt;lower=0&gt; sigma;[](#cb1-13)}[](#cb1-14)model{[](#cb1-15)y \\~ normal(alpha + beta \\* x, sigma);[](#cb1-16)}[](#cb1-17)generated quantities{[](#cb1-18)reallog\\_p = normal\\_lpdf(y\\_tilde | alpha + beta \\* x\\_tilde, sigma);[](#cb1-19)}`**\n```\nOnly the training data`x`and`y`are used in the model block. The test data`y\\_tilde`and test predictors`x\\_tilde`appear in only the generated quantities block. Thus the program is not cheating by using the test data during training. Although this model does not do so, it would be fair to use`x\\_tilde`in the model block\u2014only the test observations`y\\_tilde`are unknown before they are predicted.\nGiven\\\\(M\\\\)posterior draws from Stan, the sequence`log\\_p[1:M]`will be available, so that the log posterior predictive density of the test data given training data and predictors is just`log\\_sum\\_exp(log\\_p) - log(M)`.\n## Estimation error\n### Parameter estimates\nEstimation is usually considered for unknown parameters. If the data from which the parameters were estimated came from simulated data, the true value of the parameters may be known. If\\\\(\\\\theta\\\\)is the true value and\\\\(\\\\hat{\\\\theta}\\\\)the estimate, then error is just the difference between the prediction and the true value,\\\\[\n\\\\textrm{err} = \\\\hat{\\\\theta} - \\\\theta.\n\\\\]\nIf the estimate is larger than the true value, the error is positive, and if it\u2019s smaller, then error is negative. If an estimator\u2019s unbiased, then expected error is zero. So typically, absolute error or squared error are used, which will always have positive expectations for an imperfect estimator.*Absolute error*is defined as\\\\[\n\\\\textrm{abs-err} = \\\\left| \\\\hat{\\\\theta} - \\\\theta \\\\right|\n\\\\]and*squared error*as\\\\[\n\\\\textrm{sq-err} = \\\\left( \\\\hat{\\\\theta} - \\\\theta \\\\right)^2.\n\\\\]Gneiting and Raftery ([2007](#ref-GneitingRaftery:2007))provide a thorough overview of such scoring rules and their properties.\nBayesian posterior means minimize expected square error, whereas posterior medians minimize expected absolute error. Estimates based on modes rather than probability, such as (penalized) maximum likelihood estimates or maximum a posterior estimates, do not have these properties.\n### Predictive estimates\nIn addition to parameters, other unknown quantities may be estimated, such as the score of a football match or the effect of a medical treatment given to a subject. In these cases, square error is defined in the same way. If there are multiple exchangeable outcomes being estimated,\\\\(z\\_1, \\\\ldots, z\\_N,\\\\)then it is common to report*mean square error*(MSE),\\\\[\n\\\\textrm{mse}\n= \\\\frac{1}{N} \\\\sum\\_{n = 1}^N \\\\left( \\\\hat{z}\\_n - z\\_n\\\\right)^2.\n\\\\]To put the error back on the scale of the original value, the square root may be applied, resulting in what is known prosaically as*root mean square error*(RMSE),\\\\[\n\\\\textrm{rmse} = \\\\sqrt{\\\\textrm{mean-sq-err}}.\n\\\\]\n### Predictive estimates in Stan\nConsider a simple linear regression model, parameters for the intercept\\\\(\\\\alpha\\\\)and slope\\\\(\\\\beta\\\\), along with predictors\\\\(\\\\tilde{x}\\_n\\\\). The standard Bayesian estimate is the expected value of\\\\(\\\\tilde{y}\\\\)given the predictors and training data,\\\\[\\\\begin{eqnarray\\*}\n\\\\hat{\\\\tilde{y}}\\_n\n&amp;&amp; = &amp;&amp; \\\\mathbb{E}[\\\\tilde{y}\\_n \\\\mid \\\\tilde{x}\\_n, x, y]\n\\\\\\\\[4pt]\n&amp;&amp; \\\\approx &amp;&amp; \\\\frac{1}{M} \\\\sum\\_{m = 1}^M \\\\tilde{y}\\_n^{(m)}\n\\\\end{eqnarray\\*}\\\\]where\\\\(\\\\tilde{y}\\_n^{(m)}\\\\)is drawn from the data model\\\\[\n\\\\tilde{y}\\_n^{(m)}\n\\\\sim p(\\\\tilde{y}\\_n \\\\mid \\\\tilde{x}\\_n, \\\\alpha^{(m)}, \\\\beta^{(m)}),\n\\\\]for parameters\\\\(\\\\alpha^{(m)}\\\\)and\\\\(\\\\beta^{(m)}\\\\)drawn from the posterior,\\\\[\n(\\\\alpha^{(m)}, \\\\beta^{(m)}) \\\\sim p(\\\\alpha, \\\\beta \\\\mid x, y).\n\\\\]\nIn the linear regression case, two stages of simplification can be carried out, the first of which helpfully reduces the variance of the estimator. First, rather than averaging draws\\\\(...",
      "url": "https://mc-stan.org/docs/stan-users-guide/cross-validation.html"
    },
    {
      "title": "Understanding Linear Regression Intercepts in Plain Language",
      "text": "Article\n\n# Understanding Linear Regression Intercepts in Plain Language\n\nPrashanth Southekal Published: April 26, 2024\n\nI am often asked about the role of intercepts in linear regression models \u2013 especially the negative intercepts. Here is my blog post on that topic in simple words with minimal statistical terms.\n\nRegression models are used to make predictions. The [coefficients in the equation](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/)\u00a0define the relationship between each independent variable and the dependent variable. The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero. In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero. If X sometimes equals\u00a00, the intercept is simply the expected value of Y at that value. Mathematically and pictorially, a simple linear regression (SLR) model is shown below.\n\nBut what is the business interpretation of intercept in the regression model? In business terms, an intercept represents a baseline or starting point for the dependent variable, if the independent variables are set to zero. The intercept serves as the starting point for evaluating the effects of the independent variables on the dependent variable. It reflects the portion of the dependent variable that is not influenced by the independent variables included in the model. It helps quantify the impact of changes in the independent variables from this baseline value. For example, in a sales prediction model, the intercept might represent the expected sales when all marketing efforts, i.e., the predictors are at zero. In finance, the intercept can represent fixed or overhead costs that are incurred regardless of the level of activity or other factors.\n\nTechnically, the intercept in the linear regression model can be positive, negative, or even zero.\n\n1. **Positive Intercept:** If the intercept in the regression model is positive, it means that the predicted value of the dependent variable (Y) when the independent variable (X) is zero is positive. This implies that the regression line crosses the y-axis above the zero value.\n2. **Negative Intercept:** Conversely, if the intercept in a linear regression model is negative, it means that the predicted value of Y when X is zero is negative. In this case, the regression line crosses the y-axis below the zero value.\n3. **Zero Intercept:** If the intercept in a regression model is zero, it implies that the regression line passes through the origin (0,0) on the graph.\u00a0This means that the predicted value of the dependent variable is zero when all independent variables are also zero. In other words, there is no additional constant term in the regression equation. This situation is extremely rate and very theoretical.\n\nBasically, you deal with negative or positive intercepts, and when you come across the negative intercept you deal with the negative intercept the same way as you would deal with a positive intercept.\u00a0But in practical terms, a negative intercept may or may not make sense depending on the context of the data being analyzed. For example, if you are analyzing the day\u2019s temperature (X) and sales of ice cream (Y), a negative intercept would not be meaningful since it is impossible to have negative sales. However, in other domains such as financial analysis, a negative intercept could make sense.\n\nBelow are some approaches you can consider when you have negative intercepts:\n\n1. Check for data errors and assumptions: Before making any adjustments, ensure that the regression assumptions are met. This includes linearity, independence, homoscedasticity (pertaining to residuals), normality of the data variables and residuals, outliers, and more. If these assumptions are violated, it is necessary to address them first.\n2. Apply business acumen and commonsense and check if the interpretation of the negative intercept makes practical sense. A negative intercept might make sense depending on what the intercept represents. For example, in financial data, a negative intercept could indicate a starting point below zero, which may be perfectly reasonable. But\u00a0if you are analyzing data on the temperature and sales of ice cream, a negative intercept would not be meaningful since it is impossible to have negative sales.\n3. Center the variables. Regression models are valid only for a given range of data values. But sometimes, the values of the independent and the dependent variables can be outside of the given range. In this regard, centering involves subtracting a constant value or arithmetic mean of a variable (independent) from each of its values. This can make interpretation easier, especially if the independent variables (Xs) have zero values. Basically, by centering the variables around their means, the intercept represents the predicted value of the dependent variable when the independent variables are at their mean values. Also, in some cases, extreme values or outliers in the data can lead to numerical instability in the regression models. Centering variables can mitigate these issues by reducing the scale of the variables and making the regression model more stable.\n4. Ensure that confounding variables are in the regression model. Adding additional explanatory variables or confounding variables to the regression model may help explain the negative intercept.\n\nOverall, it is important to note that linear regression models are based on assumptions. Firstly, they assume a linear relationship between variables, which may not always hold true in real-world scenarios. Additionally, linear regression depends on normally distributed data and is very sensitive to outliers. Last but not least, linear regression may not perform well with nonlinear relationships, and in such cases, more complex models like polynomial regression or non-linear regression may be more appropriate.\n\n**Reference**\n\n- \u201cAnalytics Bets Practices\u201d by Dr. Southekal\n- \u201cRegression Analysis\u201d by Jim Frost\n- [youtube.com/watch?v=h2nkqtLjjZY](https://www.youtube.com/watch?v=h2nkqtLjjZY)\n- [dataversity.net/demystifying-data-analytics-models/](https://www.dataversity.net/demystifying-data-analytics-models/)\n\nAbout the author\n\n### Prashanth Southekal\n\n**Prashanth H. Southekal, Founder,\u00a0[DBP-Institute](http://www.dbp-institute.com/)**\n\nDr. Prashanth H. Southekal is the founder of\u00a0[DBP-Institute,](http://www.dbp-institute.com/) a data and analytics consulting, research, and education firm based in Calgary, Canada. He is a consultant, author, keynote speaker, board member, and professor of data and analytics. Dr. Southekal has advised more than 80 organizations, including P&G, GE, Shell, and Apple. He is the author of three books: \u201cData for Business Performance,\u201d \u201cAnalytics Best Practices,\u201d and \u201cData Quality. \u201d He writes regularly on data, analytics, and machine learning for Forbes, SAP Insider, DATAVERSITY, and CFO University.\n\nBesides his consulting and advisory pursuits, Dr. Southekal has trained over 3,500 professionals worldwide in data and analytics. He has also developed an on-demand data analytics course for DATAVERSITY \u2013 \u201cBusiness Analytics in Action Learning Plan\u201d \u2013 taken by hundreds of data and business professionals. Dr. Southekal is also an adjunct professor of data and analytics at IE Business School (Madrid, Spain), and CDO Magazine included him in the top 75 global academic data leaders of 2022. In addition, CIOLook listed Dr. Southekal as one of the \u201cten most influential leaders in tech to follow in 2023.\u201d Dr. Southekal holds a Ph.D. from ESC Lille (FR), an MBA from Kellogg School of Management (US), and an ICD.D designation from the Rotman School of Management (CA). He splits his time between Calgary, Canada and Madrid, Spain with his wife, two children, and a high-energy Goldendoodle dog. Outside work, he loves...",
      "url": "https://www.dataversity.net/articles/understanding-linear-regression-intercepts-in-plain-language"
    }
  ]
}