{
  "query": "How can I improve generalization to unseen chemical solvents in reaction yield prediction when leave-one-out cross-validation shows large gap to test performance?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving the ability of a yield\u2011prediction model to extrapolate to solvents that were never seen during training requires a combination of richer solvent representations, training\u2011time strategies that expose the model to out\u2011of\u2011distribution (OOD) conditions, and post\u2011hoc techniques that make the predictions more reliable.  Recent work on the **Catechol benchmark** shows that treating solvent selection as a few\u2011shot learning problem\u2014e.g. pre\u2011training a regression model on the full 1\u202f200\u2011reaction flow dataset and then fine\u2011tuning it on a handful of examples for a new solvent\u2014substantially narrows the gap between leave\u2011one\u2011out cross\u2011validation and true test performance ([arxiv.org/html/2506.07619v1](https://arxiv.org/html/2506.07619v1)).  Graph\u2011based encodings of the solvent molecule (or of solvent\u2013reactant complexes) are especially effective; a graph neural network (GNN) benchmark on the same catechol rearrangement data demonstrated that continuous solvent descriptors learned by a GNN capture subtle polarity and hydrogen\u2011bonding effects that simple categorical labels miss ([arxiv.org/abs/2512.19530](https://arxiv.org/abs/2512.19530)).  Adding physicochemical features (dielectric constant, Kamlet\u2011Taft parameters, Hansen solubility parameters) as auxiliary inputs further improves transferability, because the model can infer relationships between solvents that share similar properties.\n\nBeyond representation, **data\u2011augmentation and uncertainty\u2011aware training** are key to closing the validation\u2011test gap.  IBM\u2019s 2020 study showed that augmenting the training set with chemically plausible reaction variants (e.g., swapping a solvent for a close analogue, perturbing temperature or concentration) and using test\u2011time augmentation to generate an ensemble of predictions yields both higher accuracy and calibrated uncertainty estimates that correlate with actual errors ([research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty](https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty)).  Probabilistic models such as **DIONYSUS** further improve calibration on low\u2011data solvent subsets, allowing you to flag predictions that are likely out of scope ([pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  Active\u2011learning loops\u2014selecting the most informative solvent\u2011reaction pairs for experimental measurement\u2014can also be used to enrich the training distribution where it matters most, as demonstrated in the Catechol benchmark\u2019s active\u2011learning experiments.\n\nFinally, adopt **OOD\u2011focused validation** rather than pure leave\u2011one\u2011out splits.  Time\u2011split or \u201cnew\u2011patent\u201d splits, as used in the \u201cChallenging reaction prediction models to generalize to novel chemistry\u201d study, reveal how models behave when confronted with truly unseen chemistry and guide hyper\u2011parameter or architecture choices that improve robustness ([arxiv.org/abs/2501.06669](https://arxiv.org/abs/2501.06669)).  Ablation studies from the BOOM benchmark highlight that pre\u2011training on large, diverse reaction corpora, careful hyper\u2011parameter tuning, and using inductive\u2011bias\u2011rich architectures (e.g., GNNs with edge\u2011wise solvent\u2013reactant interactions) are the most effective levers for OOD generalization ([openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf](https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf)).  By combining (1) solvent\u2011aware graph representations, (2) transfer\u2011learning/few\u2011shot fine\u2011tuning, (3) targeted data augmentation and uncertainty calibration, and (4) OOD\u2011oriented validation, you can substantially reduce the performance gap between cross\u2011validation and real\u2011world solvent\u2011unseen test sets.",
      "url": ""
    },
    {
      "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning",
      "text": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\n# The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning\nToby Boyne1, Juan S. Campos1, Becky D. Langdon1, Jixiang Qing1, Yilin Xie1\nShiqiang Zhang1, Calvin Tsay1, Ruth Misener1, Daniel W. Davies2, Kim E. Jelfs2\nSarah Boyall3, Thomas M. Dixon3, Linden Schrecker3, Jose Pablo Folch3\u2020\nDepartment of Computing, Imperial College, London, UK1\nDepartment of Chemistry, Imperial College, London, UK2\nSOLVE Chemistry, London, UK3t.boyne23@imperial.ac.uk;\u2020jose@solvechemistry.com\n###### Abstract\nMachine learning has promised to change the landscape of laboratory chemistry, with impressive results in molecular property prediction and reaction retro-synthesis. However, chemical datasets are often inaccessible to the machine learning community as they tend to require cleaning, thorough understanding of the chemistry, or are simply not available. In this paper, we introduce a novel dataset for yield prediction, providing the first-ever transient flow dataset for machine learning benchmarking, covering over 1200 process conditions. While previous datasets focus on discrete parameters, our experimental set-up allow us to sample a large number of continuous process conditions, generating new challenges for machine learning models. We focus on solvent selection, a task that is particularly difficult to model theoretically and therefore ripe for machine learning applications. We showcase benchmarking for regression algorithms, transfer-learning approaches, feature engineering, and active learning, with important applications towards solvent replacement and sustainable manufacturing.\n## 1Introduction\nMachine learning (ML) and artificial intelligence (AI) have showcased enormous potential in empowering the world of the natural sciences: from famous examples such as AlphaFold for protein predictions> [\n[> 1\n](https://arxiv.org/html/2506.07619v1#bib.bib1)> ]\n, to fusion reactor control> [\n[> 2\n](https://arxiv.org/html/2506.07619v1#bib.bib2)> ]\n, disease detection> [\n[> 3\n](https://arxiv.org/html/2506.07619v1#bib.bib3)> ]\n, battery design> [\n[> 4\n](https://arxiv.org/html/2506.07619v1#bib.bib4)> ]\n, and material discovery> [\n[> 5\n](https://arxiv.org/html/2506.07619v1#bib.bib5)> ]\n, among many more. However, we seldom see the machine learning community benchmark new methods in physical science datasets, mostly due to the difficulty in cleaning real-world data, the need for interdisciplinary understanding to correctly benchmark, and most importantly, how expensive the data can be to produce, resulting in many datasets being locked behind closed doors by large companies.\nAIchemy ([https://aichemy.ac.uk](https://aichemy.ac.uk)) is an interdisciplinary UK hub with the mission of transforming the chemistry-AI interface via aiding the collaboration of chemists and AI researchers, as well as addressing gaps in data standards, curation, and availability for AI use. In partnership with SOLVE Chemistry ([https://www.solvechemistry.com](https://www.solvechemistry.com)), we present a first important step into addressing the dataset gap with the introduction of a new and unique open dataset for benchmarking low-data machine learning algorithms for chemistry.\nSolvent selection is one of the biggest challenges for chemical manufacturing, with solvents often being the main source of waste in the manufacturing process> [\n[> 6\n](https://arxiv.org/html/2506.07619v1#bib.bib6)> ]\n. Increased regulation on solvents and a drive to making process manufacturing more sustainable led to an interest in the discovery of greener solvents and for improved solvent replacement tools. However, most of the solvent replacement tools focus purely on learning unsupervised representations of solvents, with the hope that experimentalists can find solvents with similar properties to replace those with environmental concerns. A much stronger approach would consider the interaction of a variety of different solvents with a reaction of interest to directly predict reaction yields, in such a way that the best possible solvent can be selected according to a yield-sustainability trade-off.\nMachine learning approaches have been shown to be a powerful tool for the prediction of chemical reaction conditions. Success has been reported in retro-synthesis> [\n[> 7\n](https://arxiv.org/html/2506.07619v1#bib.bib7)> , [> 8\n](https://arxiv.org/html/2506.07619v1#bib.bib8)> ]\n, condition recommendations> [\n[> 9\n](https://arxiv.org/html/2506.07619v1#bib.bib9)> ]\n, product predictions> [\n[> 10\n](https://arxiv.org/html/2506.07619v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2506.07619v1#bib.bib11)> ]\n, among others. While yield prediction has proven to be more difficult due to large inconsistencies in procedure and data reporting> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\n, we have still seen promising yield prediction results for smaller and more carefully curated datasets> [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> , [> 14\n](https://arxiv.org/html/2506.07619v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2506.07619v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2506.07619v1#bib.bib16)> ]\n. However, these datasets lack the continuous reaction conditions, such as temperature and residence time, that are required to scale-up processes to practical manufacturing conditions.\nIn this paper, we release the first machine-learning-ready transient flow dataset, a framework that allows for quick and efficient screening of continuous reaction conditions. We specifically provide yield data over the uni-molecular allyl substituted catechol reaction, shown in Figure[1](https://arxiv.org/html/2506.07619v1#S1.F1), with dense measurements across the residence time, temperature, and solvent space. We further showcase how this type ofkinetic dataposes new challenges to current machine learning methods for chemistry, and identify how the challenges can potentially be tackled by the community.\n![Refer to caption](extracted/6524982/figures/Project2_rxn.png)Figure 1:Data was gathered on the rearrangement of allyl substituted catechol. By subjecting the reaction mixture to high temperatures, we begin a cascade reaction forming multiple rearrangement products. We investigate the yield of the reaction for a range of different solvents. Product 1 was not observed and reacted immediately to form Product 2 and later 3.\n### 1.1Related works\nReaction datasets are common in chemistry research, but their suitability for machine learning benchmarking tends to be poor. This can be a result of improper formatting or documentation, incomplete information about reaction conditions or the experimental set-up, or the lack of machine readability, leading to limited usage by the ML community. However, some effort has been made to address this, with the biggest example being the creation of the Open Reaction Database (ORD)> [\n[> 17\n](https://arxiv.org/html/2506.07619v1#bib.bib17)> ]\n, a repository containing over 2M different reactions, many of which come from US patent data (USPTO)> [\n[> 18\n](https://arxiv.org/html/2506.07619v1#bib.bib18)> ]\n. However, the dataset falls short in some aspects, in particular with respect to machine learning readiness and data inconsistencies across reactions.\nORDerly> [\n[> 12\n](https://arxiv.org/html/2506.07619v1#bib.bib12)> ]\nallows for easy cleaning and preparation of ORD data, showing the promise of the dataset for forward and retro-synthetic prediction using transformers; however, it also shows that yield prediction cannot be done well due to data inconsistencies.> Schwaller et\u00a0al. [\n[> 13\n](https://arxiv.org/html/2506.07619v1#bib.bib13)> ]\ndrew similar conclusions when using the USPTO dataset, stating that reaction conditions such as temperature, concentrations, and duration have a significant effect on yield. The assumption that every reaction in the dataset is optimized for reaction param...",
      "url": "https://arxiv.org/html/2506.07619v1"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2512.19530] Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2512.19530\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2512.19530**(cs)\n[Submitted on 22 Dec 2025]\n# Title:Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement\nAuthors:[Hongsheng Xing](https://arxiv.org/search/cs?searchtype=author&amp;query=Xing,+H),[Qiuxin Si](https://arxiv.org/search/cs?searchtype=author&amp;query=Si,+Q)\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n[View PDF](https://arxiv.org/pdf/2512.19530)[HTML (experimental)](https://arxiv.org/html/2512.19530v1)> > Abstract:\n> Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n> Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $&gt;25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning. Comments:|13 pages, 6 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nMSCclasses:|68T07, 92E20, 62M45|\nACMclasses:|I.2.1; I.2.6; J.2|\nCite as:|[arXiv:2512.19530](https://arxiv.org/abs/2512.19530)[cs.LG]|\n|(or[arXiv:2512.19530v1](https://arxiv.org/abs/2512.19530v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2512.19530](https://doi.org/10.48550/arXiv.2512.19530)\nFocus to learn more\narXiv-issued DOI via DataCite (pending registration)\n|\n## Submission history\nFrom: Hongsheng Xing [[view email](https://arxiv.org/show-email/9dc7457b/2512.19530)]\n**[v1]**Mon, 22 Dec 2025 16:19:01 UTC (2,198 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement, by Hongsheng Xing and Qiuxin Si\n* [View PDF](https://arxiv.org/pdf/2512.19530)\n* [HTML (experimental)](https://arxiv.org/html/2512.19530v1)\n* [TeX Source](https://arxiv.org/src/2512.19530)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2512.19530&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2512.19530&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-12](https://arxiv.org/list/cs.LG/2025-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2512.19530?context=cs)\n[cs.AI](https://arxiv.org/abs/2512.19530?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2512.19530)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2512.19530)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2512.19530)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2512.19530&amp;description=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2512.19530&amp;title=Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.or...",
      "url": "https://arxiv.org/abs/2512.19530"
    },
    {
      "title": "",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2501.06669** (cs)\n\n\\[Submitted on 11 Jan 2025\\]\n\n# Title:Challenging reaction prediction models to generalize to novel chemistry\n\nAuthors: [John Bradshaw](https://arxiv.org/search/cs?searchtype=author&query=Bradshaw,+J), [Anji Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+A), [Babak Mahjour](https://arxiv.org/search/cs?searchtype=author&query=Mahjour,+B), [David E. Graff](https://arxiv.org/search/cs?searchtype=author&query=Graff,+D+E), [Marwin H.S. Segler](https://arxiv.org/search/cs?searchtype=author&query=Segler,+M+H), [Connor W. Coley](https://arxiv.org/search/cs?searchtype=author&query=Coley,+C+W)\n\nView a PDF of the paper titled Challenging reaction prediction models to generalize to novel chemistry, by John Bradshaw and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2501.06669) [HTML (experimental)](https://arxiv.org/html/2501.06669v1)\n\n> Abstract:Deep learning models for anticipating the products of organic reactions have found many use cases, including validating retrosynthetic pathways and constraining synthesis-based molecular design tools. Despite compelling performance on popular benchmark tasks, strange and erroneous predictions sometimes ensue when using these models in practice. The core issue is that common benchmarks test models in an in-distribution setting, whereas many real-world uses for these models are in out-of-distribution settings and require a greater degree of extrapolation. To better understand how current reaction predictors work in out-of-distribution domains, we report a series of more challenging evaluations of a prototypical SMILES-based deep learning model. First, we illustrate how performance on randomly sampled datasets is overly optimistic compared to performance when generalizing to new patents or new authors. Second, we conduct time splits that evaluate how models perform when tested on reactions published in years after those in their training set, mimicking real-world deployment. Finally, we consider extrapolation across reaction classes to reflect what would be required for the discovery of novel reaction types. This panel of tasks can reveal the capabilities and limitations of today's reaction predictors, acting as a crucial first step in the development of tomorrow's next-generation models capable of reaction discovery.\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Chemical Physics (physics.chem-ph) |\n| Cite as: | [arXiv:2501.06669](https://arxiv.org/abs/2501.06669) \\[cs.LG\\] |\n| (or [arXiv:2501.06669v1](https://arxiv.org/abs/2501.06669v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2501.06669](https://doi.org/10.48550/arXiv.2501.06669) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: John Bradshaw \\[ [view email](https://arxiv.org/show-email/47bd441c/2501.06669)\\] **\\[v1\\]**\nSat, 11 Jan 2025 23:49:14 UTC (2,297 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Challenging reaction prediction models to generalize to novel chemistry, by John Bradshaw and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2501.06669)\n- [HTML (experimental)](https://arxiv.org/html/2501.06669v1)\n- [TeX Source](https://arxiv.org/src/2501.06669)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2501.06669&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2501.06669&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-01](https://arxiv.org/list/cs.LG/2025-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2501.06669?context=cs) [physics](https://arxiv.org/abs/2501.06669?context=physics) [physics.chem-ph](https://arxiv.org/abs/2501.06669?context=physics.chem-ph)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2501.06669)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2501.06669)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2501.06669)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2501.06669) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2501.06669"
    },
    {
      "title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020",
      "text": "Authors Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020 https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty\nData augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020\nAuthors\n2020-12-06T00:00:00Z\n## Abstract\nChemical reactions describe how precursor molecules react together and transform into products. The reaction yield describes the percentage of the precursors successfully transformed into products relative to the theoretical maximum. The prediction of reaction yields can help chemists navigate reaction space and accelerate the design of more effective routes. Here, we investigate the best-studied high-throughput experiment data set and show how data augmentation on chemical reactions can improve yield predictions' accuracy, even when only small data sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In this manuscript, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the random training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Related\nWorkshop paper\n### [Monitoring the Impact of Wildfires on Tree Species with Deep Learning](https://research.ibm.com/publications/monitoring-the-impact-of-wildfires-on-tree-species-with-deep-learning)\nWang Zhou, Levente Klein\nNeurIPS 2020\nWorkshop paper\n### [Structure Discovery in (Causal) Proximal Graphical Event Models](https://research.ibm.com/publications/structure-discovery-in-causal-proximal-graphical-event-models)\nDebarun Bhattacharjya, Karthikeyan Shanmugam, et al.\nNeurIPS 2020\nWorkshop paper\n### [Differentially Private Stochastic Coordinate Descent](https://research.ibm.com/publications/differentially-private-stochastic-coordinate-descent--1)\nGeorgios Damaskinos, Celestine Mendler-D\u00fcnner, et al.\nNeurIPS 2020\nWorkshop paper\n### [Long-Range Seasonal Forecasting of 2m Temperature with Machine Learning](https://research.ibm.com/publications/long-range-seasonal-forecasting-of-2m-temperature-with-machine-learning)\nEtienne Eben Vos, Ashley Daniel Gritzman, et al.\nNeurIPS 2020\n[View all publications](https://research.ibm.com/publications)",
      "url": "https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "",
      "text": "BOOM: Benchmarking Out-Of-distribution Molecular\nProperty Predictions of Machine Learning Models\nEvan R. Antoniuk\u2020\u2217 Shehtab Zaman\u2021\u2217 Tal Ben-Nun\u2020 Peggy Li\u2020\nJames Diffenderfer\u2020 Busra Demirci\u2021 Obadiah Smolenski\u2021 Tim Hsu\u2020\nAnna M. Hiszpanski\u2020 Kenneth Chiu\u2021 Bhavya Kailkhura\u2020\nBrian Van Essen\u2020\n\u2020 Lawrence Livermore National Laboratory, Livermore, CA\n\u2021 Binghamton University, Binghamton, NY\nAbstract\nData-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML)\nand generative modeling to filter and design novel molecules. Discovering novel molecules\nrequires accurate out-of-distribution (OOD) predictions, but ML models struggle to general\u0002ize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks.\nWe present BOOM, benchmarks for out-of-distribution molecular property predictions: a\nchemically-informed benchmark for OOD performance on common molecular property\nprediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning\nmodels on OOD performance. Overall, we find that no existing model achieves strong gener\u0002alization across all tasks: even the top-performing model exhibited an average OOD error 3\u00d7\nhigher than in-distribution. Current chemical foundation models do not show strong OOD ex\u0002trapolation, while models with high inductive bias can perform well on OOD tasks with sim\u0002ple, specific properties. We perform extensive ablation experiments, highlighting how data\ngeneration, pre-training, hyperparameter optimization, model architecture, and molecular\nrepresentation impact OOD performance. Developing models with strong out-of-distribution\n(OOD) generalization is a new frontier challenge in chemical machine learning (ML). This\nopen-source benchmark is available at https://github.com/FLASK-LLNL/BOOM.\n1 Introduction\nMolecular discovery pipelines have increasingly relied upon machine learning (ML) models [Bohacek\net al., 1996, Reymond, 2015, Kailkhura et al., 2019]. These models discover new molecules by\neither screening a list of enumerated molecules or by guiding a generative model towards molecules\nof interest [Wang et al., 2023a]. Molecular discovery is inherently an out-of-distribution (OOD)\nprediction problem, since the molecules need to either (i) exhibit properties that extrapolate beyond\nthe training dataset, or (ii) possess a previously unconsidered chemical substructure. In either case,\nsuccess depends on the learned model\u2019s ability to make accurate predictions on samples that are not\nin the same distribution as the training data.\nDespite the importance of OOD performance to real-world molecular discovery, the OOD per\u0002formance of common ML models for molecular property prediction has yet to be systematically\nexplored. Due to the lack of standardized splits for testing models, especially splits based on the\ndata distribution, we believe that current ML models are optimizing in-distribution performance on\n\u2217Equal Contribution\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\ninsufficiently challenging datasets that do not adequately measure real-world performance. Currently,\nlittle empirical knowledge exists about how choices regarding the pretraining task, model architecture,\nand/or dataset diversity impact the generalization performance of chemistry foundation models that\nare expected to generalize across all chemical systems.\nIn this work, we develop BOOM, benchmarks for out-of-distribution molecular property predictions,\na standardized benchmark for assessing the OOD generalization performance of molecule property\nprediction models. Our work consists of the following main contributions:\n\u2022 We develop a general and robust methodology for evaluating the performance of chemical\nproperty prediction models for property values beyond their training distribution. We intro\u0002duce OOD-specific metrics such as binned R2\nto allow comparisons of OOD performance\nacross all models.\n\u2022 We perform the first large-scale OOD performance benchmarking of state-of-the-art ML\nchemical property prediction models. Across 10 diverse OOD tasks and 15 models, we\ndo not find any existing models that show strong OOD generalization across all tasks. We\ntherefore put forth BOOM OOD property prediction as a frontier challenge for chemical\nfoundation models.\n\u2022 Our work highlights insights into how pretraining strategies, model architecture, molecular\nrepresentation, and data augmentation impact OOD performance. These findings point\ntowards strategies for the chemistry community to achieve chemical foundation models with\nstrong OOD generalization across all chemical systems.\n2 BOOM\nDefining Out-of-distribution. Consider a supervised dataset D with N molecules M \u2208\n{M1,M2, ...,MN } and associated labels or properties y \u2208 {y1, y2, ..., yN }. The problem of\nout-of-distribution prediction can be defined as the mismatch in the probability distribution, P of the\ntraining and test sets, Dtrain and Dtest such that,\nP(M, y|Dtest) \u0338= P(M, y|Dtrain) (1)\nThe key question is defining the density function P(M, y) over a set of molecules and their respective\nproperties. The density can be defined over the chemical structure or molecule features, or over the\nproperties. Formally, we define out-of-distribution as low-density regions over the property space,\nsuch that:\n0 < P(ytest) \u2264 P(ytrain) (2)\nFarquhar and Gal [2022] define this as a complement distribution conditioned on the targets. This is\nknown as concept or label shift as well [Liu et al., 2024]. While we focus on designing splits with a\nconcept shift, it is important to note that depending on the property, this may result in a covariate shit,\nresulting in a structural or chemical imbalance. The probability density over the labels is determined\nusing kernel density estimation (KDE), allowing us to generalize to multimodal distributions. The\nsplit strategy algorithm for each dataset is detailed in Appendix A.1. The lowest probability samples\nfrom the KDE estimated distribution are held-out (see Fig. 1) to evaluate the consistency of ML\nmodels to discover molecules with state-of-the-art properties that extrapolate beyond the training\ndata.\nDatasets. BOOM consists of 10 quantum chemical molecular property datasets derived from\nQM9 [Ramakrishnan et al., 2014] and the 10k Dataset [Antoniuk et al., 2025], derived from the Cam\u0002bridge Structural Database. The 10k Dataset was sourced from 10,206 experimentally synthesized,\nsmall organic molecules and contains the density functional theory calculated values of their molecu\u0002lar density and solid heat of formation (HoF). We collect 8 molecular property datasets from the QM9\nDataset: isotropic polarizability (\u03b1), heat capacity (Cv), highest occupied molecular orbital (HOMO)\nenergy, lowest unoccupied molecular orbital (LUMO) energy, HOMO-LUMO gap, dipole moment\n(\u00b5), electronic spatial extent (\nR2\n\u000b\n), and zero point vibrational energy (ZPVE). We also select a\nrandom subset of the dataset to serve as the ID test set, detailed in Appendix A. To further expand the\napplication space of BOOM, we also perform benchmarking on the Lipophilicity dataset[Wu et al.,\n2018] of 4,200 experimental measurements of the octanol/water distribution coefficient, which is of\nrelevance for drug compounds. The inclusion of the Lipophilicity dataset serves as an exemplary\n2\nFigure 1: (Left) An example OOD dataset included in the BOOM benchmark. To assess OOD\nperformance, we split each chemical property dataset into an out-of-distribution (OOD) Test Set\n(blue), an in-distribution (ID) Test Set (orange) and a Train Set (green), as described in Section 2.\n(Right) Example model predictions on this task exhibiting weak correlation on the OOD samples.\ndataset for performing OOD evaluations on experimentally measured properties, rather than only\ncomputed physicochemical properties (See Table 9).\nMetrics. We also propose standardized metrics over the ID and OOD to compare models. We\nuse root mean square error (RMSE) over respective data spli...",
      "url": "https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf"
    },
    {
      "title": "Towards out-of-distribution generalizable predictions of chemical kinetic properties",
      "text": "Towards out-of-distribution generalizable predictions of chemical kinetic properties\nLicense: arXiv.org perpetual non-exclusive license\narXiv:2310.03152v2 [cs.LG] 04 Dec 2023\n# Towards out-of-distribution generalizable predictions of chemical kinetic properties\nZihao Wang\nCSE, HKUST\nzwanggc@cse.ust.hk\n&amp;Yongqiang Chen\\*{}^{\\*}start\\_FLOATSUPERSCRIPT \\* end\\_FLOATSUPERSCRIPT\nCSE, CUHK\nyqchen@cse.cuhk.edu.hk\n&amp;Yang Duan, Weijiang Li\nCS, UIUC\n{yangd4,wl13}@illinois.eduBo Han\nCS, HKBU\nbhanml@comp.hkbu.edu.hk\n&amp;James Cheng\nCSE, CUHK\njcheng@cse.cuhk.edu.hk\n&amp;Hanghang Tong\nCS, UIUC\nhtong@illinois.eduEqual contribution.\n###### Abstract\nMachine Learning (ML) techniques have found applications in estimating chemical kinetic properties. With the accumulated drug molecules identified through \u201cAI4drug discovery\u201d, the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets and benchmarks can further support research in this direction. The github repository for code and data can be found in[https://github.com/zihao-wang/ReactionOOD](https://github.com/zihao-wang/ReactionOOD).\n## 1Introduction\nIn recent years, graph machine learning has been widely used in scientific discovery> (Wang et\u00a0al., [> 2023\n](#bib.bib40)> ; Zhang et\u00a0al., [> 2023\n](#bib.bib45)> )\nand gained particular success in chemistry> (Gilmer et\u00a0al., [> 2017\n](#bib.bib10)> ; Jumper et\u00a0al., [> 2021\n](#bib.bib19)> ; Mullowney et\u00a0al., [> 2023\n](#bib.bib26)> )\n.\nThe underlying rationale is the long-standing structure-property relationship> (Mihali\u0107 and Trinajsti\u0107, [> 1992\n](#bib.bib25)> )\nin chemistry.\nFor example, Graph Neural Networks (GNNs) can efficiently encode information at both the molecular structure level and the atom level within a molecule, which reveal compelling properties of the molecule> (Gilmer et\u00a0al., [> 2017\n](#bib.bib10)> )\n.\nSuch methods yield efficient, cheap, but still effective predictions of the properties ofunseen moleculesbefore expensive experiments or computations, which can serve as valuable reference information for drug discovery> (Mullowney et\u00a0al., [> 2023\n](#bib.bib26)> )\n.\nOne of thenextquestions to be answered after the discovery of a proper but unseen molecule isHow to efficiently obtain unseen molecules through chemical synthesis.In contrast to the molecule property estimation problem that concerns asinglemolecule, chemical synthesis processes involve the proper arrangement of various reactions that encompassmultiplemolecules under optimal conditions. Therefore, the very first step of achieving a high-throughput synthesis of unseen molecules is to estimate the properties of chemical reactions> (Warr, [> 2014\n](#bib.bib41)> )\n, especially kinetics properties that describe the \u201crate\u201d of reactions.\nThis prediction task is expected to be Out-Of-Distribution (OOD) generalizable so that the kinetic properties of OOD reactions with unseen molecules can be well predicted.\nRecently, machine learning methods have been applied to predict the kinetic properties of reactions> (Heid and Green, [> 2021\n](#bib.bib14)> )\n. In existing studies, chemical reactions are assumed to be Independently and Identically Distributed (IID), and models are trained and tested within random splits> (Heid and Green, [> 2021\n](#bib.bib14)> ; Stuyver and Coley, [> 2022\n](#bib.bib35)> ; Heid et\u00a0al., [> 2023\n](#bib.bib15)> )\n. However, results from such IID assumptions provide little credible insight into the performances of existing ML methods in OOD reaction property prediction.\nMeanwhile, existing theoretical and empirical studies for OOD generalization on graphs> (Ji et\u00a0al., [> 2022\n](#bib.bib17)> ; Gui et\u00a0al., [> 2022\n](#bib.bib11)> )\n, are restricted to problems with a single graph. How OOD methods perform reaction properties prediction with multiple molecules is still unknown.\nTo fill this gap, this paper discusses the out-of-distribution generalization issue when applying machine learning methods to the prediction of chemical reaction properties. We propose three levels of OOD shifts for ML-based reaction prediction: Structure OOD, Mechanism OOD, and Conditional OOD. Then, we reorganize recent reaction kinetic databases> (Johnson et\u00a0al., [> 2022\n](#bib.bib18)> )\nand create a comprehensive dataset in three levels of OOD. Furthermore, we empirically justify the performance of state-of-the-art kinetic property prediction produced by state-of-the-art OOD methods for general and graph inputs. Our results demonstrated that there remain huge ID-OOD performance gaps under different distribution shifts in chemical reactions for existing OOD methods.\n## 2Related works\nIncreasing efforts have been made to devise machine learning approaches for various aspects of chemical reaction systems> (Davies, [> 2019\n](#bib.bib6)> ; Stocker et\u00a0al., [> 2020\n](#bib.bib33)> ; Meuwly, [> 2021\n](#bib.bib23)> ; Strieth-Kalthoff et\u00a0al., [> 2022\n](#bib.bib34)> )\n, such as reaction classification> (Schwaller et\u00a0al., [> 2021b\n](#bib.bib30)> ; Bur\u00e9s and Larrosa, [> 2023\n](#bib.bib3)> )\n, reaction optimization> (Felton et\u00a0al., [> 2021\n](#bib.bib8)> )\n, atom mapping> (Schwaller et\u00a0al., [> 2021a\n](#bib.bib29)> )\n, and the most fundamentally, reaction property prediction> (Heid and Green, [> 2021\n](#bib.bib14)> )\n. With the burst of chemical reaction data> (von Rudorff et\u00a0al., [> 2020\n](#bib.bib39)> ; Spiekermann et\u00a0al., [> 2022\n](#bib.bib32)> ; Johnson et\u00a0al., [> 2022\n](#bib.bib18)> ; Choi, [> 2023\n](#bib.bib5)> ; Stuyver et\u00a0al., [> 2023\n](#bib.bib36)> ; Zhao et\u00a0al., [> 2023\n](#bib.bib46)> )\n, the Graph Neural Network (GNN) based methods> (Heid and Green, [> 2021\n](#bib.bib14)> ; Stuyver and Coley, [> 2022\n](#bib.bib35)> ; Heid et\u00a0al., [> 2023\n](#bib.bib15)> )\nare demonstrated its clear advantage over traditional methods by leveraging the structure of reactants and products.\nOut-of-distribution shift is one of the long-standing problems in machine learning> (Vapnik, [> 1991\n](#bib.bib38)> ; Quinonero-Candela et\u00a0al., [> 2008\n](#bib.bib27)> ; Shen et\u00a0al., [> 2021\n](#bib.bib31)> )\n. Recently, OOD generalizable graph neural networks have been discussed extensively> (Bevilacqua et\u00a0al., [> 2021\n](#bib.bib2)> ; Zhu et\u00a0al., [> 2021\n](#bib.bib47)> ; Wu et\u00a0al., [> 2022b\n](#bib.bib43)> , [> a\n](#bib.bib42)> ; Chen et\u00a0al., [> 2022\n](#bib.bib4)> )\n. When it comes to scientific discovery, out-of-distribution generalization capabilities enable machine learning methods to find more reliable discoveries from existing observations. A thorough investigation of the intersection of OOD and drug discovery can be found at> Ji et\u00a0al. (\n[> 2022\n](#bib.bib17)> )\nand> Gui et\u00a0al. (\n[> 2022\n](#bib.bib11)> )\n. However, as we will identify in the incoming parts, the out-of-distribution shifts for chemical reactions are radically different from those with existing graph OOD settings> (Gui et\u00a0al., [> 2022\n](#bib.bib11)> )\n, and existing OOD methods do not perform well.\n## 3Preliminary\n### 3.1Chemical reactions and kinetic property prediction\nA chemical reaction\u211c\u211c\\\\mathfrak{R}fraktur\\_Ris described by the reactantsr1,\u2026,rmsubscript\ud835\udc5f1\u2026subscript\ud835\udc5f\ud835\udc5ar\\_{1},\\\\dots,r\\_{m}italic\\_r start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , \u2026, italic\\_r start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT, productsp1,\u2026,pnsubscript\ud835\udc5d1\u2026sub...",
      "url": "https://arxiv.org/html/2310.03152v2"
    },
    {
      "title": "Challenging reaction prediction models to generalize to novel chemistry",
      "text": "# Challenging reaction prediction models to generalize to novel chemistry\n\nJohn Bradshaw\u2020\u2020\\\\dagger\u2020\n\nAnji Zhang\u2020\u2020\\\\dagger\u2020\n\nBabak Mahjour\u2020\u2020\\\\dagger\u2020\n\nDavid E. Graff\u266f\u266f\\\\sharp\u266f\u2020\u2020\\\\dagger\u2020\n\nMarwin H.S. Segler\u00a7\u00a7\\\\mathsection\u00a7\n\nConnor W. Coley\u2020\u2020\\\\dagger\u2020\u2021\u2021\\\\ddagger\u2021\u2020\u2020\\\\dagger\u2020 Department of Chemical Engineering, Massachusetts Institute of Technology ({jbrad, anji\\_z, bmahjour, ccoley}@mit.edu);\n\u266f\u266f\\\\sharp\u266f Department of Chemistry and Chemical Biology, Harvard University (deg711@g.harvard.edu);\n\u00a7\u00a7\\\\mathsection\u00a7 Microsoft Research AI for Science;\n\u2021\u2021\\\\ddagger\u2021 Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.\n\n###### Abstract\n\nDeep learning models for anticipating the products of organic reactions have found many use\ncases, including validating retrosynthetic pathways and constraining synthesis-based molecular design tools.\nDespite compelling performance on popular benchmark tasks, strange and erroneous predictions sometimes ensue when using these models in practice.\nThe core issue is that common\nbenchmarks test models in an _in-distribution_ setting, whereas many real-world uses for these models are\nin _out-of-distribution_ settings and require a greater degree of extrapolation.\nTo better understand how current reaction predictors work in out-of-distribution domains, we report a\nseries of more challenging evaluations of a prototypical SMILES-based deep learning model.\nFirst, we illustrate how performance on randomly sampled datasets is overly optimistic compared to performance when generalizing to new patents or new authors.\nSecond, we conduct time splits that evaluate how models perform when tested on reactions published in years after those in their training set, mimicking real-world deployment.\nFinally, we consider extrapolation across reaction classes to reflect what would be required for the discovery of novel reaction types.\nThis panel of tasks can reveal the capabilities and limitations of today\u2019s reaction predictors, acting as\na crucial first step in the\ndevelopment of tomorrow\u2019s next-generation models capable of reaction discovery.\n\n## 1 Introduction\n\nReaction prediction\u2014the task of anticipating in silico the products of a chemical reaction given the reactants (Fig.\u00a0[1](https://arxiv.org/html/2501.06669v1#S1.F1); \\[ [90](https://arxiv.org/html/2501.06669v1#bib.bib90), [22](https://arxiv.org/html/2501.06669v1#bib.bib22), [65](https://arxiv.org/html/2501.06669v1#bib.bib65), [84](https://arxiv.org/html/2501.06669v1#bib.bib84)\\])\u2014is a crucial technology in (a) the validation of retrosynthetic pathways \\[ [2](https://arxiv.org/html/2501.06669v1#bib.bib2), [66](https://arxiv.org/html/2501.06669v1#bib.bib66), [97](https://arxiv.org/html/2501.06669v1#bib.bib97), [32](https://arxiv.org/html/2501.06669v1#bib.bib32)\\], (b) as a component of synthesis-based de novo design algorithms \\[ [18](https://arxiv.org/html/2501.06669v1#bib.bib18), [8](https://arxiv.org/html/2501.06669v1#bib.bib8), [12](https://arxiv.org/html/2501.06669v1#bib.bib12), [86](https://arxiv.org/html/2501.06669v1#bib.bib86), [38](https://arxiv.org/html/2501.06669v1#bib.bib38), [23](https://arxiv.org/html/2501.06669v1#bib.bib23), [29](https://arxiv.org/html/2501.06669v1#bib.bib29), [77](https://arxiv.org/html/2501.06669v1#bib.bib77), [7](https://arxiv.org/html/2501.06669v1#bib.bib7), [87](https://arxiv.org/html/2501.06669v1#bib.bib87)\\], and potentially (c) for the discovery of new reactions \\[ [5](https://arxiv.org/html/2501.06669v1#bib.bib5), [28](https://arxiv.org/html/2501.06669v1#bib.bib28), [68](https://arxiv.org/html/2501.06669v1#bib.bib68), [89](https://arxiv.org/html/2501.06669v1#bib.bib89), [48](https://arxiv.org/html/2501.06669v1#bib.bib48)\\].\nEncouragingly, there has been a burst of recent works developing a variety of machine learning\u2013based reaction predictors that achieve very high accuracies on common benchmark tasks \\[ [65](https://arxiv.org/html/2501.06669v1#bib.bib65), [14](https://arxiv.org/html/2501.06669v1#bib.bib14), [33](https://arxiv.org/html/2501.06669v1#bib.bib33), [61](https://arxiv.org/html/2501.06669v1#bib.bib61), [4](https://arxiv.org/html/2501.06669v1#bib.bib4), [83](https://arxiv.org/html/2501.06669v1#bib.bib83), [6](https://arxiv.org/html/2501.06669v1#bib.bib6), [91](https://arxiv.org/html/2501.06669v1#bib.bib91), [31](https://arxiv.org/html/2501.06669v1#bib.bib31), [15](https://arxiv.org/html/2501.06669v1#bib.bib15), [50](https://arxiv.org/html/2501.06669v1#bib.bib50), [79](https://arxiv.org/html/2501.06669v1#bib.bib79), [16](https://arxiv.org/html/2501.06669v1#bib.bib16), [35](https://arxiv.org/html/2501.06669v1#bib.bib35)\\].\nWith the best of these models matching or outperforming human chemists (see, e.g., \\[ [33](https://arxiv.org/html/2501.06669v1#bib.bib33), \u00a74.2\\]) and reporting top-5 accuracies above 95% (meaning that the correct answer is found in the top five predictions of the model over 95% of the time; see, e.g., \\[ [79](https://arxiv.org/html/2501.06669v1#bib.bib79), p.9\\]), performance seems to have saturated. Distinguishing best-performing models has become challenging. It is also natural to wonder if the task of reaction prediction has been \u201csolved\u201d to a meaningful degree.\n\nWhen using these models in practice, it quickly becomes apparent that the answer is a resounding no.\nIn fact, when using reaction predictors in new domains, not only might a model make an incorrect prediction, it might hallucinate a product preposterous to a human chemist.\nThe discrepancy between the reported performance on benchmarks with the subjective performance that can be seen in practice can be explained by the setting in which the model is evaluated.\nBenchmark tasks (such as USPTO\\_Stereo, USPTO\\_MIT, Pistachio, etc. \\[ [47](https://arxiv.org/html/2501.06669v1#bib.bib47), [64](https://arxiv.org/html/2501.06669v1#bib.bib64), [33](https://arxiv.org/html/2501.06669v1#bib.bib33), [49](https://arxiv.org/html/2501.06669v1#bib.bib49)\\]) evaluate models on in-distribution (ID) data, where the reactions in the test set come from the same distribution as that used to train the model, for example, using a random partition of a reaction dataset.\nHowever, in practice we often want to evaluate a model on out-of-distribution (OOD) data, meaning the test reactions are sampled from a different distribution than that used to train the model (Fig.\u00a0[1](https://arxiv.org/html/2501.06669v1#S1.F1)). In fact, using these models for reaction discovery is by definition an out-of-distribution task.\n\nThe unrealistic nature of current evaluations not only robs us of a sense of how existing methods perform, but it does so in such a way that overstates performance, stymieing analysis of where methods fall short and how to improve them.\nTo address this, we reassess what it means to evaluate a reaction predictor. We discuss and develop new tasks to test how well reaction predictors can do in different out-of-distribution domains, investigating when and how they are able to generalize and extrapolate in such settings.\nConcretely, we seek to answer the following questions:\n\n1. 1.\n\n\nHow over-optimistic are the random splits that are currently the most popular style of split for this task, and what is a more realistic evaluation of a reaction predictor\u2019s performance?\n\n2. 2.\n\n\nIf we want to use reaction predictors trained today on future datasets, how should we design benchmarks to test models prospectively?\n\n3. 3.\n\n\nWhen, and under what circumstances, might reaction predictors be able to discover new reactions?\n\n\n{subcaptiongroup}\\\\phantomcaption\n\n\\\\phantomcaption\n\nFigure 1: [1](https://arxiv.org/html/2501.06669v1#S1.F1)\nReaction prediction, in the context of this manuscript, is the task of predicting the major product(s) of a reaction given the reactants.\n(Note that by \u201creaction\u201d we mean specific reported reaction examples, rather than generic reaction \u201ctypes\u201d or \u201cclasses\u201d that cover a large group of related specific examples\u2014we will come back to the conc...",
      "url": "https://arxiv.org/html/2501.06669v1"
    }
  ]
}