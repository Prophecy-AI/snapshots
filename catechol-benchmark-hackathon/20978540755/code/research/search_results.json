{
  "query": "How can adversarial validation identify features causing distribution shift between train and test data in machine learning?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Adversarial validation pinpoints the features that make the training and test sets look different by treating the origin\u2011label (train\u202f=\u202f0, test\u202f=\u202f1) as a prediction problem and then inspecting what the classifier relies on to separate the two groups\u202f([unfoldai.com](https://unfoldai.com/adversarial-validation)).\n\n**Steps to identify shift\u2011causing features**\n\n1. **Merge the data** \u2013 Stack the training and test rows together, **dropping the original target column** so the model only sees the input features.  \n2. **Create a binary label** \u2013 Add a new column that marks every original training example as\u202f0 and every test example as\u202f1.  \n3. **Train a simple binary classifier** \u2013 Use a fast model (e.g., logistic regression or a small tree\u2011based model) to predict this label\u202f([unfoldai.com](https://unfoldai.com/adversarial-validation)).  \n4. **Check the classifier\u2019s performance** \u2013 High accuracy (significantly above random) indicates a measurable distribution shift; low accuracy suggests the sets are similar\u202f([unfoldai.com](https://unfoldai.com/adversarial-validation)).  \n5. **Extract feature importance** \u2013 Examine the trained model\u2019s coefficients, permutation importance, or SHAP values. Features with the largest absolute weights or importance scores are the ones the classifier uses to tell train from test, i.e., the **features driving the shift**.  \n6. **Validate and act** \u2013 Optionally verify these features by visual inspection or statistical tests, then consider re\u2011sampling, re\u2011weighting, or removing them to reduce the shift\u202f([NeurIPS\u202f2019 \u201cFailing Loudly\u201d](https://papers.nips.cc/paper/8420-failing-loudly-an-empirical-study-of-methods-for-detecting-dataset-shift)).  \n\nFollowing this pipeline lets you not only detect that a train\u2011test mismatch exists but also **pinpoint the exact variables responsible**, enabling targeted mitigation.",
      "url": ""
    },
    {
      "title": "Addressing train-test divergence: The power of adversarial validation",
      "text": "<div><article>\n<header>\n<p> <span> 145</span>\n</p>\n</header>\n<div>\n<h2><span>Introduction</span></h2>\n<p>Ensuring that training and testing data originate from similar distributions is paramount for achieving reliable and generalizable models in machine learning. Imagine you have six months of sales data, training your model in the first five months and testing it in the last month. If the model performs well on training data but poorly on test data, it can be a sign of data distribution mismatch. This discrepancy often manifests as a significant gap in accuracy between the train and test sets. While this might initially suggest overfitting, a closer look might reveal it\u2019s actually due to differences in the data distribution.</p>\n<p>Adversarial validation provides a simple yet powerful technique to assess and address this potential mismatch in the similarity between training and test data.</p>\n<div><p></p><p>High accuracy on the training data compared to the significantly lower accuracy on the test data</p></div>\n<h2><span><span>The adversarial validation approach</span></span></h2>\n<p><span>Adversarial validation is a clever trick that utilizes a binary classifier to distinguish between training and test data. This technique involves the following steps:</span></p>\n<ul>\n<li><span><strong>Combining datasets:</strong> Merge your training and test datasets, excluding the target column. The goal is not to predict the original target; instead, we focus on data distribution analysis.</span></li>\n<li><span><strong>Creating a new target:</strong> Introduce a binary feature that labels training data as 0 and test data as 1. This new target provides an explicit identifier for the origin of each data point.</span></li>\n<li><span><strong>Training a binary classifier:</strong> Employ a simple model, such as logistic regression, to predict this newly created binary target.</span></li>\n<li><span><strong>Evaluating the model:</strong> If the classifier can accurately differentiate between training and test data, it suggests a <strong>distribution mismatch</strong>.</span></li>\n</ul>\n<p><span>To quantify the effectiveness of the binary classifier, we can utilize the ROC-AUC score, which measures its ability to distinguish between the two data sets. A score close to 0.5 indicates similar distributions, while a score near 1.0 suggests distinct distributions.</span></p>\n<h2><span><span>Delving into ROC-AUC for the evaluation</span></span></h2>\n<p><span>The ROC-AUC score plays a crucial role in assessing the performance of our adversarial validation model. It helps us determine if the classifier can effectively distinguish between training and test data. A score close to 0.5 implies the model struggles to tell them apart, suggesting similar distributions. Conversely, a score near 1.0 indicates the model can easily separate the two sets, hinting at distinct distributions.</span></p>\n<div><p></p><p>The curve represents the trade-off between the true positive rate (TPR) and the false positive rate (FPR) at various threshold settings. The area under the curve (AUC) is a measure of the model\u2019s ability to distinguish between the classes.</p></div>\n<h2><span><span>Real-world scenarios</span></span></h2>\n<p><span>This practical application is essential for ensuring data integrity in model training.</span></p>\n<ul>\n<li><span><strong>Feature importance analysis</strong>: By computing the importance of each feature, we can systematically remove the most significant ones from the training dataset. This process helps pinpoint the root causes of distribution differences. For such analysis, I found <a href=\"https://github.com/shap/shap\">https://github.com/shap/shap</a> to be a handy repo.</span></li>\n<li><span><strong>Retraining the model:</strong> After removing the identified features, we retrain the model and evaluate its ROC-AUC score. If the score moves closer to 0.5, eliminating these features has reduced the distribution mismatch.</span></li>\n<li><span><strong>Data collection review:</strong> If the ROC-AUC score remains high, we may need to reconsider our data collection methods or revise our feature engineering techniques.</span></li>\n</ul>\n<div><p></p><p>SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model.</p></div>\n<h2><span><span>Conclusion</span></span></h2>\n<p><span>Adversarial validation is not just a diagnostic tool; it\u2019s a powerful lens through which we can scrutinize and improve the integrity of our data. By ensuring that our training and test sets are comparable, we lay the foundation for more reliable, effective machine learning models.\u00a0</span></p>\n<p><span>Remember, the strength of a model lies not just in its algorithms but also in the quality and consistency of the data it learns from.</span></p>\n</div>\n</article></div>",
      "url": "https://unfoldai.com/adversarial-validation"
    },
    {
      "title": "Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift",
      "text": "Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift\n# Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift\nStephan Rabanser, Stephan G\u00fcnnemann, Zachary Lipton\n[Advances in Neural Information Processing Systems 32 (NeurIPS 2019)](https://papers.nips.cc/paper_files/paper/2019)\n[AuthorFeedback](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-AuthorFeedback.pdf)[Bibtex](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Bibtex.bib)[MetaReview](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-MetaReview.html)[Metadata](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Metadata.json)[Paper](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Paper.pdf)[Reviews](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Reviews.html)[Supplemental](https://papers.nips.cc/paper_files/paper/2019/file/846c260d715e5b854ffad5f70a516c88-Supplemental.zip)\n## Abstract\nWe might hope that when faced with unexpected inputs, well-designed software systems would fire off warnings. Machine learning (ML) systems, however, which depend strongly on properties of their inputs (e.g. the i.i.d. assumption), tend to fail silently. This paper explores the problem of building ML systems that fail loudly, investigating methods for detecting dataset shift, identifying exemplars that most typify the shift, and quantifying shift malignancy. We focus on several datasets and various perturbations to both covariates and label distributions with varying magnitudes and fractions of data affected. Interestingly, we show that across the dataset shifts that we explore, a two-sample-testing-based approach, using pre-trained classifiers for dimensionality reduction, performs best. Moreover, we demonstrate that domain-discriminating approaches tend to be helpful for characterizing shifts qualitatively and determining if they are harmful.\n#### Name Change Policy\n&times;\nRequests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\nUse the \"Report an Issue\" link to request a name change.\nDo not remove: This comment is monitored to verify that the site is working properly",
      "url": "https://papers.nips.cc/paper/8420-failing-loudly-an-empirical-study-of-methods-for-detecting-dataset-shift"
    },
    {
      "title": "Computer Science > Computation and Language",
      "text": "[2310.11324] Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2310.11324\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2310.11324**(cs)\n[Submitted on 17 Oct 2023 ([v1](https://arxiv.org/abs/2310.11324v1)), last revised 1 Jul 2024 (this version, v2)]\n# Title:Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting\nAuthors:[Melanie Sclar](https://arxiv.org/search/cs?searchtype=author&amp;query=Sclar,+M),[Yejin Choi](https://arxiv.org/search/cs?searchtype=author&amp;query=Choi,+Y),[Yulia Tsvetkov](https://arxiv.org/search/cs?searchtype=author&amp;query=Tsvetkov,+Y),[Alane Suhr](https://arxiv.org/search/cs?searchtype=author&amp;query=Suhr,+A)\nView a PDF of the paper titled Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting, by Melanie Sclar and 3 other authors\n[View PDF](https://arxiv.org/pdf/2310.11324)[HTML (experimental)](https://arxiv.org/html/2310.11324v2)> > Abstract:\n> As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats. Comments:|ICLR 2024 Camera Ready version. With respect to the original submission, we added text generation experiments, plots of entire accuracy distributions for each task + stdev computations, and prompt length correlation with spread analysis|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2310.11324](https://arxiv.org/abs/2310.11324)[cs.CL]|\n|(or[arXiv:2310.11324v2](https://arxiv.org/abs/2310.11324v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2310.11324](https://doi.org/10.48550/arXiv.2310.11324)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Melanie Sclar [[view email](https://arxiv.org/show-email/e2487a18/2310.11324)]\n**[[v1]](https://arxiv.org/abs/2310.11324v1)**Tue, 17 Oct 2023 15:03:30 UTC (1,110 KB)\n**[v2]**Mon, 1 Jul 2024 22:28:01 UTC (1,221 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting, by Melanie Sclar and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2310.11324)\n* [HTML (experimental)](https://arxiv.org/html/2310.11324v2)\n* [TeX Source](https://arxiv.org/src/2310.11324)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2310.11324&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2310.11324&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2023-10](https://arxiv.org/list/cs.CL/2023-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2310.11324?context=cs)\n[cs.AI](https://arxiv.org/abs/2310.11324?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2310.11324?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.11324)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.11324)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.11324)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2310.11324&amp;description=Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2310.11324&amp;title=Quantifying Language Models&#39; Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](ht...",
      "url": "https://arxiv.org/abs/2310.11324"
    },
    {
      "title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2210.10769"
    },
    {
      "title": "Diagnosing Model Performance Under Distribution Shift",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2303.02011"
    },
    {
      "title": "",
      "text": "Examining and Combating Spurious Features under Distribution Shift\nChunting Zhou 1 Xuezhe Ma 2 Paul Michel 1 Graham Neubig 1\nAbstract\nA central goal of machine learning is to learn\nrobust representations that capture the causal rela\u0002tionship between inputs features and output labels.\nHowever, minimizing empirical risk over finite or\nbiased datasets often results in models latching\non to spurious correlations between the training\ninput/output pairs that are not fundamental to the\nproblem at hand. In this paper, we define and\nanalyze robust and spurious representations us\u0002ing the information-theoretic concept of minimal\nsufficient statistics. We prove that even when\nthere is only bias of the input distribution (i.e. co\u0002variate shift), models can still pick up spurious\nfeatures from their training data. Group distri\u0002butionally robust optimization (DRO) provides\nan effective tool to alleviate covariate shift by\nminimizing the worst-case training loss over a\nset of pre-defined groups. Inspired by our anal\u0002ysis, we demonstrate that group DRO can fail\nwhen groups do not directly account for various\nspurious correlations that occur in the data. To\naddress this, we further propose to minimize the\nworst-case losses over a more flexible set of dis\u0002tributions that are defined on the joint distribution\nof groups and instances, instead of treating each\ngroup as a whole at optimization time. Through\nextensive experiments on one image and two lan\u0002guage tasks, we show that our model is signif\u0002icantly more robust than comparable baselines\nunder various partitions. Our code is available\nat https://github.com/violet-zct/\ngroup-conditional-DRO.\n1. Introduction\nMany machine learning models that minimize the average\ntraining loss via empirical risk minimization (ERM) are\n1Language Technologies Institute, Carnegie Mellon Univer\u0002sity, Pittsburgh, USA 2\nInformation Sciences Institute, University\nof Southern California, Log Angeles, USA. Correspondence to:\nChunting Zhou <chuntinz@cs.cmu.edu>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n(a) ERM\n(average=93.3% robust=83.4%)\n(b) Group DRO [clean]\n(average=96.5% robust=95.8%)\n(c) Group DRO [imperfect]\n(average=93.3%, robust=83.1%)\n(d) Our Method [imperfect]\n(average=94.0%, robust=93.8%)\nFigure 1. Consider data points x in R\n2 with two classes y. The\nvertical axis of x is a spurious feature that highly correlates with\ny, and the horizontal axis is the robust feature. There are two\nsubclasses in each class, where the top-right and lower-left are\ntwo minority subclasses. The robust accuracy is test worst-case\naccuracy over the four subclasses. We train a linear classifier with\ndifferent methods. For models trained with the clean partitions,\neach subclass is a group. For the imperfect partitions, dots with\nthe same shape is a group (best viewed in color).\ntrained and evaluated on randomly shuffled and split train\u0002ing and test sets. However, such in-distribution learning\nsetups can hide critical issues: models that achieve high\naccuracy on average often underperform when the test dis\u0002tribution drifts away from the training one (Hashimoto et al.,\n2018; Koenecke et al., 2020; Koh et al., 2020). Such mod\u0002els are often \u201cright for the wrong reasons\u201d due to reliance\non spurious correlations (or \u201cdataset biases\u201d) (Torralba &\nEfros, 2011; Goyal et al., 2017; McCoy et al., 2019; Guru\u0002rangan et al., 2018), heuristics that hold for most training\nexamples but are not inherent to the task of interest, such as\nstrong associations between the presence of green pastures\nbackground with the label \u201ccows\u201d in image classification.\nNaturally, models that use such features will fail when tested\non data where the correlation does not hold.\nRecent work has investigated how models trained with ERM\nlearn spurious features that do not generalize, from the\npoints of view of causality (Arjovsky et al., 2019), under\u0002standing model overparameterization (Sagawa et al., 2020b)\narXiv:2106.07171v1 [cs.LG] 14 Jun 2021\nExamining and Combating Spurious Features under Distribution Shift\nand information theory (Lovering et al., 2021). However,\nthese works have not characterized the idea of spurious fea\u0002tures mathematically. In this paper, we characterize spurious\nfeatures from an information-theoretic perspective. We con\u0002sider prediction of target random variable Y \u2208 Y from input\nvariable X \u2208 X and characterize spurious features learned\nunder changes to the input distribution p(X) (i.e. covariate\nshift).\nA central goal of machine learning is to learn true causal re\u0002lationships between X and Y in a manner robust to spurious\nfactors concerning the variables. We assume that there ex\u0002ists an \u201cideal\u201d data distribution pideal (short for pideal(X, Y )\nbelow) which contains data from all possible experimental\nconditions concerning the confounders that cause spurious\ncorrelations, both observable and hypothetical (Lewis, 2013;\nArjovsky et al., 2019; Bellot & van der Schaar, 2020). For\nexample, consider the problem of classifying images of\ncows and camels (Beery et al., 2018). Under the ideal con\u0002ditions, we assume that pictures of cows and camels on\nany background can be collected, including cows in deserts\nand camels in green pastures. Therefore, under pideal the\nbackground of the image X is no longer a spurious factor\nof the label Y . However, such an \u201cideal\u201d distribution pideal\nis not accessible in practice (Bahng et al., 2020; Koh et al.,\n2020; McCoy et al., 2019), and our training distribution\nptrain (often, in practice, an associated empirical distribu\u0002tion) does not match pideal. ERM-based learning algorithms\nindiscriminately fit all correlations found in ptrain, including\nspurious correlations based on confounders (Tenenbaum,\n2018; Lopez-Paz, 2016).\nTo investigate the spurious features learned under the distri\u0002bution shift from pideal to ptrain, we first characterize those\nfeatures of X which most efficiently capture all possible in\u0002formation needed to predict Y . We define these robust\nfeatures using the notion of minimal sufficient statistic\n(MSS) (Dynkin, 2000; Cvitkovic & Koliander, 2019) un\u0002der pideal. We then examine whether the features learned\nunder ptrain contain spurious features compared to the MSS\nlearned under pideal. Through our analysis, we find that even\nonly with covariate shift, the features learned on ptrain can\ncontain spurious features or miss robust features of pideal.\nModels that fit spurious correlations in ptrain can be vulnera\u0002ble to groups (subpopulations of pideal/ptest) where the corre\u0002lation does not hold. A common approach to avoid learning\na model that suffers high worst group errors is group dis\u0002tributionally robust optimization (group DRO), a training\nprocedure that efficiently minimizes the worst expected loss\nover a set of groups in the training data (Oren et al., 2019;\nSagawa et al., 2020a). The partition of groups can be defined\nin several ways, such as by presence of manually identified\npotentially spurious features (Sagawa et al., 2020a), data do\u0002mains (Koh et al., 2020), or topics of text (Oren et al., 2019).\nIn a typical setup, the groups of interest in the test set align\nwith those used to partition the training data. Under such\nsetups, group DRO usually outperforms ERM with respect\nto the worst-group accuracy. We contend that this is because\nit promotes learning robust features that perform uniformly\nwell across all groups. However, in many tasks, we can\nnot collect clean group membership of training examples\ndue to expensive annotation cost or privacy concerns regard\u0002ing e.g. demographic identities of users or other sensitive\ninformation.\nInspired by our analysis of spurious features, we demon\u0002strate that group DRO can fail under \u201cimperfect\u201d partitions\nof training data that are not consistent with the test set, es\u0002pecially when reducing spurious correlation in one group\ncould exacerbate the spurious correlations in another (\u00a74.2),\nas shown in Fig. 1. This...",
      "url": "https://arxiv.org/pdf/2106.07171"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2403.05652] &#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2403.05652\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2403.05652**(cs)\n[Submitted on 8 Mar 2024 ([v1](https://arxiv.org/abs/2403.05652v1)), last revised 22 Sep 2025 (this version, v3)]\n# Title:&#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts\nAuthors:[Varun Babbar](https://arxiv.org/search/cs?searchtype=author&amp;query=Babbar,+V),[Zhicheng Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+Z),[Cynthia Rudin](https://arxiv.org/search/cs?searchtype=author&amp;query=Rudin,+C)\nView a PDF of the paper titled &#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts, by Varun Babbar and 2 other authors\n[View PDF](https://arxiv.org/pdf/2403.05652)[HTML (experimental)](https://arxiv.org/html/2403.05652v3)> > Abstract:\n> The performance of machine learning models relies heavily on the quality of input data, yet real-world applications often face significant data-related challenges. A common issue arises when curating training data or deploying models: two datasets from the same domain may exhibit differing distributions. While many techniques exist for detecting such distribution shifts, there is a lack of comprehensive methods to explain these differences in a human-understandable way beyond opaque quantitative metrics. To bridge this gap, we propose a versatile framework of interpretable methods for comparing datasets. Using a variety of case studies, we demonstrate the effectiveness of our approach across diverse data modalities-including tabular data, text data, images, time-series signals -- in both low and high-dimensional settings. These methods complement existing techniques by providing actionable and interpretable insights to better understand and address distribution shifts. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2403.05652](https://arxiv.org/abs/2403.05652)[cs.LG]|\n|(or[arXiv:2403.05652v3](https://arxiv.org/abs/2403.05652v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2403.05652](https://doi.org/10.48550/arXiv.2403.05652)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|J. Mach. Learn. Res. 26(180):1-64, 2025|\n## Submission history\nFrom: Zhicheng Guo [[view email](https://arxiv.org/show-email/5c9c0065/2403.05652)]\n**[[v1]](https://arxiv.org/abs/2403.05652v1)**Fri, 8 Mar 2024 19:52:39 UTC (40,168 KB)\n**[[v2]](https://arxiv.org/abs/2403.05652v2)**Wed, 29 Jan 2025 17:10:45 UTC (24,760 KB)\n**[v3]**Mon, 22 Sep 2025 21:20:37 UTC (22,963 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled &#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts, by Varun Babbar and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2403.05652)\n* [HTML (experimental)](https://arxiv.org/html/2403.05652v3)\n* [TeX Source](https://arxiv.org/src/2403.05652)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2403.05652&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2403.05652&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-03](https://arxiv.org/list/cs.LG/2024-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2403.05652?context=cs)\n[cs.AI](https://arxiv.org/abs/2403.05652?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.05652)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.05652)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.05652)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2403.05652&amp;description=&#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2403.05652&amp;title=&#34;What is Different Between These Datasets?&#34; A Framework for Explaining Data Distribution Shifts>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](http...",
      "url": "https://arxiv.org/abs/2403.05652"
    },
    {
      "title": "Finding and Fixing Spurious Patterns with Explanations",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/abs/2106.02112"
    }
  ]
}