{
  "query": "How do Kaggle competition winners handle large gap between local cross-validation and public leaderboard score when intercept is too high?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on insights from Kaggle winners and the community, the usual way to close a large gap between a strong local cross\u2011validation (CV) score and a weak public\u2011leaderboard score\u2014especially when the model\u2019s intercept (bias) appears too high\u2014is to\u202fre\u2011engineer the validation and post\u2011process the predictions as follows\u202f([Chioka\u202f2024](https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio);\u202f[DataScience\u202fStackExchange\u202f2017](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition);\u202f[Stack\u202fOverflow\u202f2017](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition)):\n\n1. **Match the validation split to the test distribution** \u2013 use time\u2011based or stratified folds that mimic the public\u2011test set (e.g., train on earlier periods, validate on later periods) instead of a random split. This reduces distribution shift that often inflates CV scores.  \n\n2. **Increase CV robustness** \u2013 run many folds (e.g., 10\u2011fold) and repeat the whole CV several times, then average the scores. A stable CV metric is a better proxy for the leaderboard and helps spot an overly optimistic intercept.  \n\n3. **Calibrate the intercept on a hold\u2011out slice** \u2013 keep a small, untouched validation slice, compute the mean prediction error (the intercept bias), and adjust all predictions by adding/subtracting this offset before submission.  \n\n4. **Blend diverse models** \u2013 combine at least two fundamentally different models (e.g., tree\u2011based + linear) so that their individual intercept errors cancel out when averaged.  \n\n5. **Trust the refined CV metric over the public leaderboard** \u2013 the public leaderboard is computed on a tiny subset of the test data; a well\u2011validated CV score (even after intercept correction) is a more reliable indicator of final performance.  \n\n6. **Submit a final calibrated ensemble and monitor the private leaderboard** \u2013 after the competition ends, the private leaderboard will confirm whether the intercept adjustment closed the gap.  \n\nFollowing these steps lets winners turn an inflated local CV score into a realistic, competitive public\u2011leaderboard result.",
      "url": ""
    },
    {
      "title": "Gap leaderboard score and model scoring on a Competition",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Gap leaderboard score and model scoring on a Competition](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 4 months ago\n\nModified [6 years, 4 months ago](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?lastactivity)\n\nViewed\n694 times\n\n3\n\n$\\\\begingroup$\n\nI'm working on a Veolia challenge on Ens Data Challenge [ens-data](https://challengedata.ens.fr/en/home) (equivalent to Kaggle) the goal is to classify very rare binary events (the failure of a pipeline ) for 2014 and 2015 (y={2014,2015}). In input we have 5 features 3 categorical features (which I turned into dummy variable) and two continuous. The score is average AUC, $0.6\\*AUC\\_1 + 0.4\\*AUC\\_2$.\n\nMy problem is the following, when I compute each AUC (for 2014 and 2015) with a stratified kfold cross validation and I compute the average AUC I get roughly 0.88 and when I submit on the website I end up with 0.67, I guess there is a problem in my code.\n\nHere is my code for choosing the best model for 2014:\n\nRk: to predict on the test set (2014,2015 unknown), I first predict with all 5 features, 2014. Then I add the prediction of 2014 to my feature to predict 2015\n\n```\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LG', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\n\n# stratifiedkfold is defined by default when there is an integer\nscoring = 'roc_auc'\nnum_folds = 10\n\nfor name, model in models:\n    cv_results = cross_validation.cross_val_score(model, X, Y, cv=num_folds, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [python](https://datascience.stackexchange.com/questions/tagged/python)\n\n[Share](https://datascience.stackexchange.com/q/16789)\n\n[Improve this question](https://datascience.stackexchange.com/posts/16789/edit)\n\nFollow\n\n[edited Feb 7, 2017 at 9:17](https://datascience.stackexchange.com/posts/16789/revisions)\n\nbouritosse\n\nasked Feb 6, 2017 at 20:09\n\n[![bouritosse's user avatar](https://www.gravatar.com/avatar/d05cc42fc90ecb44bf614b1f69a90fcc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/19065/bouritosse)\n\n[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse) bouritosse\n\n9366 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- $\\\\begingroup$You mean you have a model scoring 0.88 while the leaderboard(the score on the website) shows 0.67?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 2:27\n\n- $\\\\begingroup$yes that is correct$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 9:15\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Your test data(the 0.88 one) is different from the test data on the website(the 0.67 one). There must be some difference between their scores. Are you concerning about over-fitting?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 10:13\n\n- $\\\\begingroup$kfold cross validation is not supposed to downsize the overfitting ?$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 12:21\n\n- $\\\\begingroup$Can you add your score on training set to your question, that's crucial.$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 12:49\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nAs you have commented, you are concerning about over-fitting.\n\nIn fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find:\n\n1. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) (the famous ResNet paper). Checkout figure.1\n2. This [kernel](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006) of [Two Sigma Financial Modeling Challenge](https://www.kaggle.com/c/two-sigma-financial-modeling) on [Kaggle](https://www.kaggle.com/) says:\n\n> we are getting a public score of 0.0169 which is slightly better than the previous one.\n> Submitting this model to the LB gave me a score of 0.006\n\nIn my practice, I am more willing to concern the scoring gap between TRAIN, VALIDATION(DEVELOP) and TEST dataset. Personally speaking, TRAIN>VALIDATION=TEST is better than TRAIN=VALIDATION>TEST.\n\nEdit: For class imbalance problem, there are some resources:\n\n1. [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This blog shows a common workflow dealing with imbalanced class issue.\n\n2. [Class Imbalance Problem in Data Mining: Review](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This paper compares several algorithms created for solving the class imbalance problem.\n\n\n[Share](https://datascience.stackexchange.com/a/16808)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/16808/edit)\n\nFollow\n\n[edited Feb 8, 2017 at 1:58](https://datascience.stackexchange.com/posts/16808/revisions)\n\nanswered Feb 7, 2017 at 12:48\n\n[![Icyblade's user avatar](https://www.gravatar.com/avatar/0696854959b681c4cefaf494015e8bf4?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/28628/icyblade)\n\n[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade) Icyblade\n\n4,33611 gold badge2424 silver badges3434 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$On the training I get 0.847700964724 AUC and on the validation set LG: 0.909566 (0.032773). With LogisticRegression(class\\_weight='balanced')$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 13:09\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I think the problem is that my class are really imbalance I have 0.19% of failure in class 2014$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 20:06\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f16789%2fgap-leaderboard-score-and-model-scoring-on-a-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [pri...",
      "url": "https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition"
    },
    {
      "title": "Why good local validation gives bad score on Kaggle Competition?",
      "text": "2024 Developer survey is here and we would like to hear from you!\n[Take the 2024 Developer Survey](https://stackoverflow.com/dev-survey/start?utm_medium=referral&utm_source=stackexchange-community&utm_campaign=dev-survey-2024&utm_content=announcement-banner)\n\n##### Collectives\u2122 on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n\n[Learn more about Collectives](https://stackoverflow.com/collectives)\n\n**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\nGet early access and see previews of new features.\n\n[Learn more about Labs](https://stackoverflow.co/labs/)\n\n# [Why good local validation gives bad score on Kaggle Competition?](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition)\n\n[Ask Question](https://stackoverflow.com/questions/ask)\n\nAsked6 years, 6 months ago\n\nModified [6 years, 6 months ago](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition?lastactivity)\n\nViewed\n297 times\n\n1\n\nThis might be a general question.\n\nI was trying to build a predictive model in a Kaggle competition. I used some of the traditional methods like Xgboost Lightgbm and Random Forest. I tried to split the train data to train and validation into 7:3.\n\n```\nX_train, X_vali, Y_train, Y_vali = cross_validation.train_test_split\\\n     (x_train, y_train, test_size=0.3, random_state=42);\n\n```\n\nThen build model and test if the parameters are the best.\n\n```\nmodel = Model.fit(x_train,y_train)\nprint(log_loss(y_true=Y_vali,y_pred=model.predict_proba(X_vali)))\n\n```\n\nEverything works well but I had a very bad score on the final submission. It seems like worse validation results even get a better score on Kaggle's kernel(Doesn't seems like an overfitting problem I guess). I don't know how to optimize my model because the Kaggle results seem unpredictable.\n\nDoes someone know why this problem happened?\n\n- [python](https://stackoverflow.com/questions/tagged/python)\n- [data-analysis](https://stackoverflow.com/questions/tagged/data-analysis)\n- [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation)\n- [data-science](https://stackoverflow.com/questions/tagged/data-science)\n- [kaggle](https://stackoverflow.com/questions/tagged/kaggle)\n\n[Share](https://stackoverflow.com/q/47838954)\n\n[Improve this question](https://stackoverflow.com/posts/47838954/edit)\n\nFollow\n\nasked Dec 15, 2017 at 19:43\n\n[![Jiayu Zhang's user avatar](https://lh4.googleusercontent.com/--sptEe0bMuY/AAAAAAAAAAI/AAAAAAAAAA4/B0Oa481JYu0/photo.jpg?sz=64)](https://stackoverflow.com/users/8816642/jiayu-zhang)\n\n[Jiayu Zhang](https://stackoverflow.com/users/8816642/jiayu-zhang) Jiayu Zhang\n\n71988 silver badges2424 bronze badges\n\n2\n\n- Which dataset/competition are you working on? It would be great if we can reproduce your case.\n\n\u2013\u00a0[Muhammad Nizami](https://stackoverflow.com/users/5571136/muhammad-nizami)\n\nCommentedDec 18, 2017 at 0:51\n\n- Hi, @MuhammadNizami. It's KKBox's churn prediction. Competition ends now. Maybe you can download the dataset from the Kaggle page.\n\n\u2013\u00a0[Jiayu Zhang](https://stackoverflow.com/users/8816642/jiayu-zhang)\n\nCommentedDec 19, 2017 at 0:46\n\n\n[Add a comment](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition)\u00a0\\|\n\nRelated questions\n\n[6\\\n\\\nHow to Cross Validate Properly](https://stackoverflow.com/questions/27990846/how-to-cross-validate-properly)\n\n[2\\\n\\\nSklearn cross validation produces different results than manual execution](https://stackoverflow.com/questions/29223149/sklearn-cross-validation-produces-different-results-than-manual-execution)\n\n[1\\\n\\\nPython+SciKit -> different results for manual and cross\\_val\\_score prediction](https://stackoverflow.com/questions/32160049/pythonscikit-different-results-for-manual-and-cross-val-score-prediction)\n\nRelated questions\n\n[6\\\n\\\nHow to Cross Validate Properly](https://stackoverflow.com/questions/27990846/how-to-cross-validate-properly)\n\n[2\\\n\\\nSklearn cross validation produces different results than manual execution](https://stackoverflow.com/questions/29223149/sklearn-cross-validation-produces-different-results-than-manual-execution)\n\n[1\\\n\\\nPython+SciKit -> different results for manual and cross\\_val\\_score prediction](https://stackoverflow.com/questions/32160049/pythonscikit-different-results-for-manual-and-cross-val-score-prediction)\n\n[1\\\n\\\nSciKit-Learn: Very Different Results with Cross Validation](https://stackoverflow.com/questions/45148220/scikit-learn-very-different-results-with-cross-validation)\n\n[12\\\n\\\nValidation and Testing accuracy widely different](https://stackoverflow.com/questions/48718663/validation-and-testing-accuracy-widely-different)\n\n[0\\\n\\\nSubmission on Kaggle](https://stackoverflow.com/questions/49049834/submission-on-kaggle)\n\n[2\\\n\\\nk fold cross validation model assessment](https://stackoverflow.com/questions/51247739/k-fold-cross-validation-model-assessment)\n\n[1\\\n\\\nWhy Cross Validation performs poorer than testing?](https://stackoverflow.com/questions/56794711/why-cross-validation-performs-poorer-than-testing)\n\n[1\\\n\\\nbad accuracy with cross\\_val\\_score](https://stackoverflow.com/questions/60549349/bad-accuracy-with-cross-val-score)\n\n[0\\\n\\\nWhy there is very large difference between cross validation scores?](https://stackoverflow.com/questions/60713337/why-there-is-very-large-difference-between-cross-validation-scores)\n\nLoad 7 more related questions\nShow fewer related questions\n\n## 0\n\nSorted by:\n[Reset to default](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Trending (recent votes count more)Date modified (newest first)Date created (oldest first)\n\n## Know someone who can answer? Share a link to this [question](https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition) via [email](https://stackoverflow.com/cdn-cgi/l/email-protection\\#655a1610070f00061158361104060e4057552a13001703090a1240575534100016110c0a0b430408155e070a011c58320d1c405755020a0a01405755090a0604094057551304090c0104110c0a0b405755020c13001640575507040140575516060a17004057550a0b4057552e0402020900405755260a081500110c110c0a0b4056034055240d11111516405604405703405703161104060e0a13001703090a124b060a084057031440570351525d565d5c505140560316000840560157), [Twitter](https://twitter.com/share?url=https%3a%2f%2fstackoverflow.com%2fq%2f47838954%3fstw%3d2), or [Facebook](https://www.facebook.com/sharer.php?u=https%3a%2f%2fstackoverflow.com%2fq%2f47838954%3fsfb%3d2).\n\n## Your Answer\n\n**Reminder:** Answers generated by artificial intelligence tools are not allowed on Stack Overflow. [Learn more](https://stackoverflow.com/help/gen-ai-policy)\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stackoverflow.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f47838954%2fwhy-good-local-validation-gives-bad-score-on-kaggle-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Browse other questions tagged  - [python](https://stackoverflow.com/questions/tagged/python) - [data-analysis](https://stackoverflow.com/questions/tagged/data-analysis) - [cross-validation](https://stackoverflow.com/questions/tagged/cross-validation) - [data-science](https://stackoverflow.com/questions/tagged/data-science) - [kaggle](https://stackoverflow.com/questions/tagged/kaggle)   or [a...",
      "url": "https://stackoverflow.com/questions/47838954/why-good-local-validation-gives-bad-score-on-kaggle-competition"
    },
    {
      "title": "How to Select Your Final Models in a Kaggle Competition",
      "text": "Did your rank just drop sharp in the private leaderboard in a Kaggle\u00a0competition?\n[![picard palm](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)](https://www.chioka.in/wp-content/uploads/2014/10/picard-palm.jpg)\nI\u2019ve been through that, too. We all learn about overfitting when we started machine learning,\u00a0but Kaggle makes\u00a0you really feel the pain of overfitting.\u00a0Should I have been more careful in the [Higgs Boson Machine Learning competition](http://www.kaggle.com/c/higgs-boson/), I would have selected a solution that would gave me a rank 4 than rank 22.\nI vow to come out with some principles systematically select\u00a0final models. Here are the lessons learnt:\n- **Always do cross-validation to get a reliable metric.**\u00a0If you don\u2019t, the validation score you get on a single\u00a0validation set\u00a0is unlikely to reflect the model performance in general. Then, you will likely see\u00a0a model improvement in that single validation set, but actually performs worse in general. **_Keep in mind the CV score can be optimistic, but your model is still overfitting._**\n- **Trust your CV\u00a0score, and not LB\u00a0score.** The leaderboard score\u00a0is scored only on a small percentage of the full test set. In some cases, it\u2019s only a few hundred test cases. Your cross-validation score will be much more reliable in general.\n- If your CV score is not stable (perhaps due to ensembling methods), you can\u00a0run your CV with more folds and multiple times to\u00a0take average.\n- If a single\u00a0CV\u00a0run is very slow, use a subset of the data to run\u00a0the CV. This will help your CV loop to run faster. Of course, the subset should not be too small or else the CV score will not be representative.\n- **For the final 2 models, pick very different models.** Picking two very similar solutions means that your solutions either fail together or win together, effectively meaning that you only pick one model. You should reduce your risk by picking two confident but very different models.\u00a0_**You should not depend on the leaderboard score at all.**_\n- Try to group your solutions by methodologies. Then, pick the best CV score model from each group. Then compare these best candidates of each group, pick two.\n- Example: I have different groups 1)\u00a0Bagging of SVMs 2) RandomForest 3) Neural Networks 4) LinearModels. Then, each group should produce one single best model, then you pick 2 out of these.\n- **Pick a robust methodology.** Here is the tricky part\u00a0which depends on experience, even if you have done cross validation, you can still get burned:\u00a0Sketchy methods of improving the CV score like\u00a0making cubic features, cubic root features, boosting like crazy, magical\u00a0numbers(without understanding it), etc, will likely be a bad model to pick even if the CV score is good. Unfortunately, you will probably have to make this mistake once to know what this means. =\\]\nApplying the above\u00a0principles to the recent competition\u00a0[Africa Soil Property Prediction Challenge](http://www.kaggle.com/c/afsis-soil-properties), plus a bit of luck, I picked the top 1 and top 2 models.\n[![top score](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)](https://www.chioka.in/wp-content/uploads/2014/10/top-score.png)\nSorted by private score\nI ended up Top 10% with a rank of\u00a090 by spending just\u00a0a week time and mostly in Mexico in a vacation. I guess,\u00a0not too bad?",
      "url": "https://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio"
    },
    {
      "title": "A note on \"proper\" Cross-Validation techniques for Kaggle ...",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/getting-started/398047"
    },
    {
      "title": "Cross-Validation - Kaggle",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at b.onerror.b.onload (https://www.kaggle.com/static/assets/runtime.js?v=f2bc5978f08b75213e7a:1:11021)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/alexisbcook/cross-validation"
    },
    {
      "title": "1st Place Solution | Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings/writeups/1st-place-solution"
    },
    {
      "title": "Tutorial: K Fold Cross Validation - Kaggle",
      "text": "menu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nCreate\n\nsearch\u200b\n\n- [explore\\\n\\\nHome](https://www.kaggle.com/)\n\n- [emoji\\_events\\\n\\\nCompetitions](https://www.kaggle.com/competitions)\n\n- [table\\_chart\\\n\\\nDatasets](https://www.kaggle.com/datasets)\n\n- [tenancy\\\n\\\nModels](https://www.kaggle.com/models)\n\n- [code\\\n\\\nCode](https://www.kaggle.com/code)\n\n- [comment\\\n\\\nDiscussions](https://www.kaggle.com/discussions)\n\n- [school\\\n\\\nLearn](https://www.kaggle.com/learn)\n\n\n- [expand\\_more\\\n\\\nMore](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation)\n\n\nauto\\_awesome\\_motion\n\nView Active Events\n\nmenu\n\n[Skip to\\\n\\\ncontent](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#site-content)\n\n[![Kaggle](https://www.kaggle.com/static/images/site-logo.svg)](https://www.kaggle.com/)\n\nsearch\u200b\n\n[Sign In](https://www.kaggle.com/account/login?phase=startSignInTab&returnUrl=%2Fcode%2Fsatishgunjal%2Ftutorial-k-fold-cross-validation)\n\n[Register](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2Fcode%2Fsatishgunjal%2Ftutorial-k-fold-cross-validation)\n\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\nSatish Gunjal \u00b7 4y ago \u00b7 92,586 views\n\narrow\\_drop\\_up131\n\nCopy & Edit249\n\n![gold medal](https://www.kaggle.com/static/images/medals/notebooks/goldl@1x.png)\n\nmore\\_vert\n\n# Tutorial: K Fold Cross Validation\n\n## Tutorial: K Fold Cross Validation\n\n[Notebook](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/notebook) [Input](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/input) [Output](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/output) [Logs](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/log) [Comments (5)](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation/comments)\n\nhistoryVersion 7 of 7chevron\\_right\n\n## Runtime\n\nplay\\_arrow\n\n2m 35s\n\n## Input\n\nCOMPETITIONS\n\n![](https://www.kaggle.com/competitions/5407/images/thumbnail)\n\nHouse Prices - Advanced Regression Techniques\n\n![](https://www.kaggle.com/competitions/3136/images/thumbnail)\n\nTitanic - Machine Learning from Disaster\n\n## Tags\n\n[Classification](https://www.kaggle.com/code?tagIds=13302-Classification) [Regression](https://www.kaggle.com/code?tagIds=14203-Regression)\n\n## Language\n\nPython\n\n## Table of Contents\n\n[Index](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Index) [Introduction](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Introduction-) [Inner Working of Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Inner-Working-of-Cross-Validation-) [K Fold Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold-Cross-Validation-) [Stratified K Fold Cross Validation](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Stratified-K-Fold-Cross-Validation-) [Hyperparameter Tuning and Model Selection](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Hyperparameter-Tuning-and-Model-Selection-) [Advantages](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Advantages-) [Disadvantages](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Disadvantages-) [K Fold: Regression Example](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold:-Regression-Example-) [Import Libraries](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Import-Libraries-) [Load Dataset](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Load-Dataset-) [Understanding the Data](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Understanding-the-Data-) [Model Score Using KFold](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Model-Score-Using-KFold-) [Model Tuning using KFold](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Model-Tuning-using-KFold-) [K Fold: Classification Example](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#K-Fold:-Classification-Example-) [Using Logistic Regression](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Logistic-Regression-) [Using Decision Classifier](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Decision-Classifier-) [Using Random Forest Classifier](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Using-Random-Forest-Classifier-) [Decision Tree Classifier Tuning](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Decision-Tree-Classifier-Tuning-) [Reference](https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation#Reference-)\n\n![Profile picture for undefined](https://storage.googleapis.com/kaggle-competitions/kaggle/3136/logos/thumb76_76.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1739697666&Signature=JVVWuDBwsOVtiIWfxdVES7uWSyf%2Bps87MWLfWJ4GthhX9l2wvME9fn8paJlGoTXB%2B61Ng1U%2F9Dws7W9CvZLWlNwwBrzkLSOK1MuCj%2FR3z4XcSWskcH8Ntiy74IZbEqlkNoEWj%2FI%2FVrI%2Fb1tGr3RWFKn7s61Z0MJ4ZbFPcPEO8kd5aLgU1t3IIiua3V8i1kbO%2B6iqnK92oZfS1io1qVBCS2L77pxDaV%2F0TCelxozeV%2B5e0B4Um7uVOg5mipGLJ74%2BGnkdf562hcqIUI3yQFBS68SRnnbbyMwbhIpfWtpHK56itpjIA5o6kbQwjg%2BMyKxnyCPPXNKEtDC93IVbQZvXWg%3D%3D)\n\nCompetition Notebook\n\n[Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n\n[iframe](https://www.kaggleusercontent.com/kf/48636546/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..NNcXRlz1r7wVA_ZK7hc27A.fenyRROP0MgZU1Tk_PeeaOXMw36nI8G_UByr6iwE5r3uL9ZGOxHPxUrpb82HBc2ig7ZhFkyRQcXV-3-nyGWrXUhjUsFKXK7Xjjt4d3FEzA6Xenj3IXnN4K8f_ZcqQTdXSkkaTPTTSRixgMLYi2KoE0J8sWKlQ-63xQaj8ancpZ0H7B-QWFnfLvTmwqUDOaaenkJ73CwKvLmUVFNxP3Q7ki2xSCozzg5G6d3SLaj6v-dR4D8qYvWRr-vNiOkZJVZixLTk1vx5H-SVNTzxka7JSHpPCcpPj2KTtq9ngeuLjO_bHOX1ldNbGLq_uRIu9LbkV3sRFz2_IJs2pdU6-qtOU_WjhkAqyR6v9sXudACSV1x-MXbrttJvEB2i3kBoJzXcNRN8XH5K1Sss5-OdY2xj2q8PQCnqDlJBnWIWx4RSJeFxpaSPb1x3U7sWTF_4EYsOLCKM9Jo6QmWCFOUYLpwZ-vJVX5lEmSoHWiRGillmyCKXwGHTCvC9vApjgvYcwj6Cklu8EvuldNOYKp1Q6h6B0GKkjM2xoWDyflCBN6KdxVieDeYSY1kXLBrcT9eWdaHwmzhva1zHPVDZSocdHapOAfHFunmzkYnusqHpI66mFybxcKWRDTZBSDQSjEet401qkneujsqnaiHPiuUef9YLl1goUwXngf2oI0NwHICm0-Y.98Dh2ehJc4THR-HE0iz1SQ/__results__.html?sharingControls=true)\n\n## License\n\nThis Notebook has been released under the [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0) open source license.\n\n## Continue exploring\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/input_light.svg)\n\n\n\n\n\n\n\nInput\n\n2 files\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/output_light.svg)\n\n\n\n\n\n\n\nOutput\n\n0 files\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/logs_light.svg)\n\n\n\n\n\n\n\nLogs\n\n154.6 second run - successful\n\n\n\n\narrow\\_right\\_alt\n\n- ![](https://www.kaggle.com/static/images/kernel/viewer/comments_light.svg)\n\n\n\n\n\n\n\nComments\n\n5 comments\n\n\n\n\narrow\\_right\\_alt",
      "url": "https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation"
    },
    {
      "title": "Overfitting the leaderboard",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=77c6c69f1305124c7a10:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/caseyftw/overfitting-the-leaderboard"
    }
  ]
}