{
  "query": "How to implement Graph Attention Network for molecular property prediction in PyTorch with SMILES input?",
  "results": [
    {
      "title": "ANSWER",
      "text": "**Based on recent papers on graph\u2011attention models for chemistry\u202f([Rohan\u202fMehrotra\u202f2020](http://cs230.stanford.edu/projects_winter_2020/reports/32642951.pdf)), the MolGAT implementation on GitHub ([mesfind/molgnn](https://github.com/mesfind/molgnn)) and the official PyTorch\u2011Geometric\u202fAttentiveFP example ([pyg\u2011team/attentive_fp.py](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)) together with the library documentation ([torch_geometric.nn.models.AttentiveFP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html)), you can build a GAT\u2011based molecular property predictor from SMILES in PyTorch as follows:**\n\n1. **Convert SMILES \u2192 molecular graph**  \n   * Use\u202fRDKit to parse each SMILES string, extract atom features (e.g., atomic number, degree, hybridization) and bond features (bond type, aromaticity).  \n   * Build a `torch_geometric.data.Data` object: `x` = tensor of atom features, `edge_index` = 2\u2011\u00d7\u202fE adjacency matrix, `edge_attr` = bond feature tensor.  \n   *(The MolGAT repo follows this pipeline for creating graph inputs from SMILES.)*\u30104\u2020L1-L4\u3011\n\n2. **Create a PyTorch\u2011Geometric Dataset / DataLoader**  \n   * Wrap the list of `Data` objects in a `torch_geometric.data.InMemoryDataset` (or `Dataset`) and use `torch_geometric.loader.DataLoader` for batching.  \n\n3. **Define the GAT\u2011based model**  \n   * The simplest ready\u2011made architecture is **AttentiveFP**, which implements a graph\u2011attention layer (GATConv) plus read\u2011out pooling.  \n   * Example definition (from the docs):  \n\n   ```python\n   from torch_geometric.nn import AttentiveFP\n\n   model = AttentiveFP(\n       in_channels=atom_feat_dim,      # size of atom feature vector\n       hidden_channels=128,            # hidden dimension\n       out_channels=1,                 # number of property targets\n       edge_dim=bond_feat_dim,         # size of bond feature vector\n       num_layers=3,\n       num_timesteps=3,\n       dropout=0.1)\n   ```  \n   *(Parameters and class signature are documented in the AttentiveFP API.)*\u301010\u2020L1-L9\u3011\n\n   * If you prefer the original GAT formulation, you can replace `AttentiveFP` with a stack of `torch_geometric.nn.GATConv` layers (see the GATConv source for the forward/edge\u2011update logic).*\u30109\u2020L1-L12\u3011\n\n4. **Add a read\u2011out and prediction head**  \n   * After the GAT layers, pool node embeddings to a graph embedding (e.g., `global_add_pool`).  \n   * Feed the pooled vector through a linear layer to obtain the final property prediction.  \n\n   ```python\n   import torch.nn.functional as F\n   from torch_geometric.nn import global_add_pool\n\n   class MolGAT(torch.nn.Module):\n       def __init__(self, ...):\n           super().__init__()\n           self.gat = AttentiveFP(...)\n           self.lin = torch.nn.Linear(128, 1)   # adjust size to hidden_channels\n\n       def forward(self, data):\n           h = self.gat(data.x, data.edge_index, data.edge_attr)\n           h = global_add_pool(h, data.batch)\n           return self.lin(h).squeeze(-1)\n   ```\n\n5. **Train the model**  \n   * Use a standard regression or classification loss (e.g., `MSELoss` for continuous properties, `BCEWithLogitsLoss` for binary toxicity).  \n   * Optimizer: `torch.optim.Adam(model.parameters(), lr=1e-3)`.  \n   * Typical training loop (mirroring the AttentiveFP example):  \n\n   ```python\n   for epoch in range(num_epochs):\n       model.train()\n       for batch in loader:\n           optimizer.zero_grad()\n           pred = model(batch)\n           loss = loss_fn(pred, batch.y)\n           loss.backward()\n           optimizer.step()\n   ```\n\n6. **Evaluate & save**  \n   * Compute metrics on a held\u2011out test set (RMSE, ROC\u2011AUC, etc.).  \n   * Save the trained weights with `torch.save(model.state_dict(), 'molgat.pt')`.  \n\n7. **Optional: fine\u2011tune or extend**  \n   * The MolGAT repository shows how to add residual connections or deeper stacks (ResGAT) for improved performance on larger datasets\u30101\u2020L1-L4\u3011.  \n   * You can experiment with different atom/bond feature sets or increase `num_layers`/`hidden_channels` as needed.\n\n**In summary:** parse SMILES with RDKit \u2192 build PyTorch\u2011Geometric graph objects \u2192 instantiate an attention\u2011based model (AttentiveFP or custom GATConv stack) \u2192 pool node embeddings \u2192 train on your property labels. All required components are available in the cited open\u2011source code and documentation.",
      "url": ""
    },
    {
      "title": "[PDF] Predicting Molecular Properties with Graph Attention Networks",
      "text": "Predicting Molecular Properties with Graph Attention\nNetworks\nRohan Mehrotra\nDepartment of Computer Science\nStanford University\nStanford, CA 94305\nrohanm2@stanford.edu\nKevin Guo\nDepartment of Computer Science\nStanford University\nStanford, CA 94305\nkyguo@stanford.edu\nAbstract\nPredicting properties of molecules without the need for lab experiments is a desirable objective that\nhas the potential to revolutionize the development of drugs and other new molecules. Previously,\nseveral research groups have worked on this problem, and graph learning methods have emerged as\nthe most promising approach. In this paper, we propose that a recently developed graph learning\ntechnique \u2013 Graph Attention Networks (GAT\u2019s) \u2013 could be used to further improve property prediction.\nWe build a GAT model to predict the toxicity of molecules in the Tox21 dataset, and show that it\noutperforms several baseline models. Furthermore, we look closer at the GAT model\u2019s predictions\nand find that it performs best on smaller, linear molecules, but still has room for improvement on\nlarger, aromatic molecules \u2013 a potential area for further research.\n1 Introduction\nThe design of new molecules with particular therapeutic properties is the central goal of the drug development process.\nToday, biochemists rely on tedious laboratory experiments in order to screen molecules to evaluate their pharmaceutical\npotential. As a result, the drug development process is both cost-inefficient and time consuming, with a single drug\nrequiring 12 years and $2.6 billion on average to be developed and approved.[1]\nArtificial intelligence has the potential to revolutionize the drug development pipeline. There has recently been\nsignificant interest in \u201cmolecular machine learning\u201d: using artificial intelligence to predict molecular properties\nautomatically and efficiently. If the properties of a molecule could be predicted solely based on the molecule\u2019s atomic\nstructure, rather than from wet lab experiments, it would represent a major breakthrough in molecule design \u2013 with\napplications in drug discovery and beyond.\nIn 2014, the NIH\u2019s \u201cToxicology in the 21st Century\u201d (Tox21) initiative published a public dataset reporting the toxicity\nof a number of compounds.[2] The dataset contains toxicity measurements for 7830 compounds on 12 different target\ntasks. The NIH released the dataset with the intention of establishing a standard dataset that researchers could use to\nbenchmark models they build for predicting properties based on molecular structures. In this paper, we present a graph\nattention network-based model for molecular property prediction on the Tox21 dataset. We first review related work,\nthen present our network design, and finally evaluate and rationalize its performance.\n2 Related Work\nSince the release of Tox21, there have been several reports on the effectiveness of various machine learning approaches\non the dataset. Previous groups have developed models based on a number of approaches including logistic regression,\nrandom forests, SVM, and graph networks for Tox21, with varying degrees of success.[3, 4, 5]\nIn 2017, Wu et al.[6] published MoleculeNet, a benchmark designed for testing machine learning methods on molecular\nproperties. MoleculeNet curated the key previously proposed algorithms for Tox21 prediction (and other chemical\ndatasets), and integrated them into an open-source Python package called DeepChem. Out of all the models in\nMoleculeNet, the most effective one was graph convolutional networks (GCN\u2019s). This suggests that graph learning\nmethods may be the most effective approach to molecular property prediction. Such a result is unsurprising, because\nmolecules are particularly amenable to graph representations. Specifically, molecules can be represented as graphs\nwith nodes representing the atoms and edges representing the bonds between them. The node features and adjacency\nmatrices indicating edges can be fed into graph learning techniques for property prediction.\n3 Background on Graph Learning\nGraph learning is a quickly evolving field that has seen rapid algorithmic advances in recent years. In 2016, a key\nbreakthrough occurred when Defferrard et al. proposed a \u201cgraph convolutional\u201d operation, analogous to convolutions\nused in CNN\u2019s for computer vision.[7] These graph convolutional networks (GCN\u2019s) use both node features and\ntopological structural information to make predictions, and have proven to greatly outperform traditional methods for\ngraph learning.\nBeyond GCN\u2019s, in 2017, Velickovic et al. published a landmark paper introducing attention mechanisms to graph\nlearning, thus proposing introducing a new architecture for graph learning called graph attention networks (GAT\u2019s).[8]\nThrough an attention mechanism on neighborhoods, GAT\u2019s can more effectively aggregate node information. Recent\nresults have shown that GAT\u2019s perform even better than standard GCN\u2019s at many graph learning tasks.\nIn MoleculeNet and other papers, standard GCN\u2019s have already been implemented for Tox21 prediction. In this\npaper, we seek to go beyond standard GCN\u2019s by applying the recently developed attention mechanism to Tox21. We\nhypothesized that, because GAT\u2019s learn weights for node aggregation rather than fixing a node aggregation formula,\nthey would be able to more effectively learn local information in molecular structures that hold insights into their overall\nproperties.\n4 Dataset\n4.1 Overview\nThe dataset we are using for molecular property prediction is the Tox21 dataset. The Tox21 dataset comprises 7830\nchemical compounds. For each sample, there are 12 binary labels (active/inactive) representing the outcome of 12\ndifferent toxicological assays. The objective is to build a model that can accurately predict these 12 toxicological\nproperties of molecules based on their structures.\n4.2 Featurization\nIn the NIH\u2019s release of Tox21, the molecules are represented as SMILES strings (a standard method of encoding\nstructures). However, because we wanted to use graph learning methods, we needed to convert these SMILES strings to\ngraph representations.\nWe built our models using two Python libraries: DeepChem (for the baseline models) and DGL (for the GAT model).\nBoth DeepChem and DGL contain methods to transform the SMILES strings into featurized representations that can be\nfed into graph networks. In particular, these methods convert SMILES strings into graph objects, which consist of an an\nn-dimensional node feature vector (where n is the number of atoms in the molecule), and a list containing the node\npairs [u, v] that are connected with an edge (bond).\n5 Methods\n5.1 Baseline Models\nWe first implemented several baseline models using the DeepChem package. In particular, we built a Logistic Regression\nmodel, SVM model, and GCN model. DeepChem includes implementations for each of these models that are optimized\nfor Tox21, which we leveraged to build and test the baseline models. The purpose of these baselines was to set a\nbenchmark which we could compare our GAT model\u2019s performance to.\n5.2 GAT Model\nNext, we implemented the GAT model. We decided to build the model using a PyTorch-based library called DGL,\nwhich is designed to make the implementation of graph learning methods easier. As a preface, we will first describe the\n2\nmathematical intuition behind GAT\u2019s, then describe the architecture and hyperparameters that we used in our model for\nTox21. We would also like to acknowledge the original GAT paper[8], whose code inspired our own implementation.\nThe key difference between GAT\u2019s and GCN\u2019s is how the information from node neighbors is aggregated. For GCN\u2019s,\nthe graph convolution produces the normalized sum of the node features of neighbors. GAT introduces the attention\nmechanism as a substitute for the statically normalized convolution.\nAs described in the original GAT paper, a GAT layer consists of the following equations to compute the node embedding\nof layer l + 1 from the embeddings of layer l:\n[8]\nEquation (1) is...",
      "url": "http://cs230.stanford.edu/projects_winter_2020/reports/32642951.pdf"
    },
    {
      "title": "mesfind/molgnn - GitHub",
      "text": "[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[mesfind](https://github.com/mesfind)/ **[molgnn](https://github.com/mesfind/molgnn)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fmesfind%2Fmolgnn) You must be signed in to change notification settings\n- [Fork\\\n1](https://github.com/login?return_to=%2Fmesfind%2Fmolgnn)\n- [Star\\\n3](https://github.com/login?return_to=%2Fmesfind%2Fmolgnn)\n\n\n### License\n\n[MIT license](https://github.com/mesfind/molgnn/blob/main/LICENSE.md)\n\n[3\\\nstars](https://github.com/mesfind/molgnn/stargazers) [1\\\nfork](https://github.com/mesfind/molgnn/forks) [Branches](https://github.com/mesfind/molgnn/branches) [Tags](https://github.com/mesfind/molgnn/tags) [Activity](https://github.com/mesfind/molgnn/activity)\n\n[Star](https://github.com/login?return_to=%2Fmesfind%2Fmolgnn)\n\n[Notifications](https://github.com/login?return_to=%2Fmesfind%2Fmolgnn) You must be signed in to change notification settings\n\n# mesfind/molgnn\n\nmain\n\n[Branches](https://github.com/mesfind/molgnn/branches) [Tags](https://github.com/mesfind/molgnn/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit<br>## History<br>[71 Commits](https://github.com/mesfind/molgnn/commits/main/) |\n| [data](https://github.com/mesfind/molgnn/tree/main/data) | [data](https://github.com/mesfind/molgnn/tree/main/data) |\n| [final\\_models](https://github.com/mesfind/molgnn/tree/main/final_models) | [final\\_models](https://github.com/mesfind/molgnn/tree/main/final_models) |\n| [utils](https://github.com/mesfind/molgnn/tree/main/utils) | [utils](https://github.com/mesfind/molgnn/tree/main/utils) |\n| [LICENSE.md](https://github.com/mesfind/molgnn/blob/main/LICENSE.md) | [LICENSE.md](https://github.com/mesfind/molgnn/blob/main/LICENSE.md) |\n| [README.md](https://github.com/mesfind/molgnn/blob/main/README.md) | [README.md](https://github.com/mesfind/molgnn/blob/main/README.md) |\n| [\\_\\_init\\_\\_.py](https://github.com/mesfind/molgnn/blob/main/__init__.py) | [\\_\\_init\\_\\_.py](https://github.com/mesfind/molgnn/blob/main/__init__.py) |\n| [app.py](https://github.com/mesfind/molgnn/blob/main/app.py) | [app.py](https://github.com/mesfind/molgnn/blob/main/app.py) |\n| [custom\\_pygdata.py](https://github.com/mesfind/molgnn/blob/main/custom_pygdata.py) | [custom\\_pygdata.py](https://github.com/mesfind/molgnn/blob/main/custom_pygdata.py) |\n| [molfeatures.py](https://github.com/mesfind/molgnn/blob/main/molfeatures.py) | [molfeatures.py](https://github.com/mesfind/molgnn/blob/main/molfeatures.py) |\n| [molgat.py](https://github.com/mesfind/molgnn/blob/main/molgat.py) | [molgat.py](https://github.com/mesfind/molgnn/blob/main/molgat.py) |\n| [molgnn.py](https://github.com/mesfind/molgnn/blob/main/molgnn.py) | [molgnn.py](https://github.com/mesfind/molgnn/blob/main/molgnn.py) |\n| [output.txt](https://github.com/mesfind/molgnn/blob/main/output.txt) | [output.txt](https://github.com/mesfind/molgnn/blob/main/output.txt) |\n| [pygdata.py](https://github.com/mesfind/molgnn/blob/main/pygdata.py) | [pygdata.py](https://github.com/mesfind/molgnn/blob/main/pygdata.py) |\n| [requirements.txt](https://github.com/mesfind/molgnn/blob/main/requirements.txt) | [requirements.txt](https://github.com/mesfind/molgnn/blob/main/requirements.txt) |\n| [setup.py](https://github.com/mesfind/molgnn/blob/main/setup.py) | [setup.py](https://github.com/mesfind/molgnn/blob/main/setup.py) |\n| [solfeatures.py](https://github.com/mesfind/molgnn/blob/main/solfeatures.py) | [solfeatures.py](https://github.com/mesfind/molgnn/blob/main/solfeatures.py) |\n| [test\\_molgat.py](https://github.com/mesfind/molgnn/blob/main/test_molgat.py) | [test\\_molgat.py](https://github.com/mesfind/molgnn/blob/main/test_molgat.py) |\n| [train.py](https://github.com/mesfind/molgnn/blob/main/train.py) | [train.py](https://github.com/mesfind/molgnn/blob/main/train.py) |\n| View all files |\n\n## Repository files navigation\n\n# MolGAT Model\n\nThe MolGAT model is a deep learning model for molecular property prediction. It is based on the Graph Attention Networks (GAT) architecture and is designed to work with molecular graphs.\n\nUnlike the GAT model, the MolGAT model takes n-dimensional edge features as input, which allows it to incorporate additional information about the chemical bonds in the molecular graph.\n\nThis model takes as input a molecular graph represented as a node feature matrix, an edge feature matrix, and an adjacency matrix, and learns to predict a molecular property. It consists of several Graph Attention Layers (GATs) followed by fully connected layers, and its architecture is defined by the MolGAT class.\n\nThe **init** method of the MolGAT class initializes the layers of the model. It takes as input the following parameters:\n\n- `node_features`: the dimensionality of the input node features.\n- `hidden_dim`: the dimensionality of the hidden representations.\n- `edge_features`: the dimensionality of the input edge features.\n- `num_heads`: the number of attention heads used in each GAT layer.\n- `dropout`: the dropout probability used in the GAT layers.\n- `num_conv_layers`: the number of GAT layers.\n- `num_fc_layers`: the number of fully connected layers.\n\nThe forward method of the MolGAT class performs a forward pass through the layers of the model. It takes as input the following arguments:\n\n- `x`: the node feature matrix of shape (num\\_nodes, node\\_features).\n- `edge_index`: the edge index tensor of shape (2, num\\_edges).\n- `batch_index`: the batch index tensor of shape (num\\_nodes,).\n- `edge_attr`: the edge feature matrix of shape (num\\_edges, edge\\_features).\n\nThe forward method performs the following steps:\n\n- Applies a GAT layer to the input node features and edge features.\n- ReLU activation function is applied to the output of the GAT layer.\n- Dropout is applied to the output of the GAT layer.\n- Graph-level max pooling and mean pooling are applied to the output of the GAT layer.\n- The output of the GAT layer is concatenated with the output of the graph-level pooling.\n- Fully connected layers with ReLU activation functions are applied to the concatenated output.\n- Dropout is applied to the output of the fully connected layers.\n- The output of the fully connected layers is passed through a linear layer to obtain the final output.\n\nOverall, the MolGAT model learns a hierarchical representation of the molecular graph using GAT layers and fully connected layers, and uses attention-based graph aggregation to summarize the node-level features and generate a graph-level representation, which is then used to make a prediction of the molecular property.\n\n## Requirements\n\nThe following packages are required to run the MolGAT model:\n\n- Python 3.x\n- PyTorch\n- RDKit\n- PyTorch\\_geometric(PyG)\n- numpy\n- pandas\n- scikit-learn\n\n## Model Architecture\n\nThe MolGAT model consists of multiple graph attention convolutional layers, followed by fully connected layers. The number of convolutional layers and fully connected layers can be specified as input parameters. Each convolutional layer is followed by a batch normalization layer and ReLU activation. The output of the last convolutional layer is concatenated with the global maximum pooling and global average pooling of node features, and passed through the fully connected layers to obtain the final prediction.\n\n## Training\n\nThe model is trained using mean squared error (MSE) loss, and optimized using the Adam optimizer with a custom learning rate schedule. The training and testing data are loaded using PyTorch data loaders. During training, the model is put into training mode, and gradients are computed and updated after each batch. During testi...",
      "url": "https://github.com/mesfind/molgnn"
    },
    {
      "title": "Search code, repositories, users, issues, pull requests...",
      "text": "pytorch\\_geometric/examples/attentive\\_fp.py at master \u00b7pyg-team/pytorch\\_geometric \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n</option></form>\nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n</option></form>\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n</option></form>\nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=pyg-team/pytorch_geometric)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[pyg-team](https://github.com/pyg-team)/**[pytorch\\_geometric](https://github.com/pyg-team/pytorch_geometric)**Public\n* [Notifications](https://github.com/login?return_to=/pyg-team/pytorch_geometric)You must be signed in to change notification settings\n* [Fork3.9k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n* [Star23.3k](https://github.com/login?return_to=/pyg-team/pytorch_geometric)\n</turbo-frame></main>\nYou can\u2019t perform that action at this time.\n</div>",
      "url": "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py"
    },
    {
      "title": "torch_geometric.nn.models.AttentiveFP \u2014 pytorch_geometric documentation",
      "text": "torch_geometric.nn.models.AttentiveFP \u2014 pytorch_geometric documentation https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html\ntorch_geometric.nn.models.AttentiveFP \u2014 pytorch_geometric documentation\nNone\n2025-01-01T00:00:00Z\n# torch_geometric.nn.models.AttentiveFP [](pytorch-geometric.readthedocs.io/en/latest/...)\n_class_ AttentiveFP( _in_channels:[int](https://docs.python.org/3/library/functions.html#int)_, _hidden_channels:[int](https://docs.python.org/3/library/functions.html#int)_, _out_channels:[int](https://docs.python.org/3/library/functions.html#int)_, _edge_dim:[int](https://docs.python.org/3/library/functions.html#int)_, _num_layers:[int](https://docs.python.org/3/library/functions.html#int)_, _num_timesteps:[int](https://docs.python.org/3/library/functions.html#int)_, _dropout:[float](https://docs.python.org/3/library/functions.html#float)=0.0_) [[source]](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/attentive_fp.html#AttentiveFP) [](pytorch-geometric.readthedocs.io/en/latest/...)\nBases: [`Module`](https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module)\nThe Attentive FP model for molecular representation learning from the\n[\u201cPushing the Boundaries of Molecular Representation for Drug Discovery\nwith the Graph Attention Mechanism\u201d](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) paper, based on\ngraph attention mechanisms.\nParameters:\n- **in_channels** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Size of each input sample.\n- **hidden_channels** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Hidden node feature dimensionality.\n- **out_channels** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Size of each output sample.\n- **edge_dim** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Edge feature dimensionality.\n- **num_layers** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Number of GNN layers.\n- **num_timesteps** ( [_int_](https://docs.python.org/3/library/functions.html#int)) \u2013 Number of iterative refinement steps for global\nreadout.\n- **dropout** ( [_float_](https://docs.python.org/3/library/functions.html#float) _,_ _optional_) \u2013 Dropout probability. (default: `0.0`)\nforward( _x:[Tensor](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor)_, _edge_index:[Tensor](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor)_, _edge_attr:[Tensor](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor)_, _batch:[Tensor](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor)_)\u2192[Tensor](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor) [[source]](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/attentive_fp.html#AttentiveFP.forward) [](pytorch-geometric.readthedocs.io/en/latest/...)Return type:\n[`Tensor`](https://docs.pytorch.org/docs/main/tensors.html#torch.Tensor)\nreset_parameters() [[source]](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/attentive_fp.html#AttentiveFP.reset_parameters) [\uf0c1]\nResets all learnable parameters of the module.",
      "url": "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html"
    },
    {
      "title": "Residual Graph Attention Networks for molecular property prediction",
      "text": "Advertisement\n\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n\n# ResGAT: Residual Graph Attention Networks for molecular property prediction\n\n- Regular Research Paper\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 03 September 2024\n\n- Volume\u00a016,\u00a0pages 491\u2013503, (2024)\n- [Cite this article](https://link.springer.com/link.springer.com#citeas)\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s12293-024-00423-5.pdf)\n\nYou have full access to this [open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research) article\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/12293?as=webp)Memetic Computing](https://link.springer.com/journal/12293) [Aims and scope](https://link.springer.com/journal/12293/aims-and-scope) [Submit manuscript](https://submission.nature.com/new-submission/12293/3)\n\nResGAT: Residual Graph Attention Networks for molecular property prediction\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s12293-024-00423-5.pdf)\n\n## Abstract\n\nMolecular property prediction is an important step in the drug discovery pipeline. Numerous computational methods have been developed to predict a wide range of molecular properties. While recent approaches have shown promising results, no single architecture can comprehensively address all tasks, making this area persistently challenging and requiring substantial time and effort. Beyond traditional machine learning and deep learning architectures for regular data, several deep learning architectures have been designed for graph-structured data to overcome the limitations of conventional methods. Utilizing graph-structured data in quantitative structure\u2013activity relationship (QSAR) modeling allows models to effectively extract unique features, especially where connectivity information is crucial. In our study, we developed residual graph attention networks (ResGAT), a deep learning architecture for molecular graph-structured data. This architecture is a combination of graph attention networks and shortcut connections to address both regression and classification problems. It is also customizable to adapt to various dataset sizes, enhancing the learning process based on molecular patterns. When tested multiple times with both random and scaffold sampling strategies on nine benchmark molecular datasets, QSAR models developed using ResGAT demonstrated stability and competitive performance compared to state-of-the-art methods.\n\n### Similar content being viewed by others\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10822-021-00421-6/MediaObjects/10822_2021_421_Fig1_HTML.png)\n\n### [Simplified, interpretable graph convolutional neural networks for small molecule activity prediction](https://link.springer.com/10.1007/s10822-021-00421-6?fromPaywallRec=false)\n\nArticleOpen access24 November 2021\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-02590-y/MediaObjects/41598_2025_2590_Fig1_HTML.png)\n\n### [Prediction of reproductive and developmental toxicity using an attention and gate augmented graph convolutional network](https://link.springer.com/10.1038/s41598-025-02590-y?fromPaywallRec=false)\n\nArticleOpen access25 May 2025\n\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00521-023-08403-5/MediaObjects/521_2023_8403_Fig1_HTML.png)\n\n### [Few-shot learning via graph embeddings with convolutional networks for low-data molecular property prediction](https://link.springer.com/10.1007/s00521-023-08403-5?fromPaywallRec=false)\n\nArticleOpen access10 March 2023\n\n### Explore related subjects\n\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\n\n- [Molecular Modelling](https://link.springer.com/subjects/molecular-modelling)\n- [Neural encoding](https://link.springer.com/subjects/neural-encoding)\n- [Predictive markers](https://link.springer.com/subjects/predictive-markers)\n- [Protein structure predictions](https://link.springer.com/subjects/protein-structure-predictions)\n- [Structure Prediction](https://link.springer.com/subjects/structure-prediction)\n- [Targeted resequencing](https://link.springer.com/subjects/targeted-resequencing)\n\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=12293)\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nProperty prediction is an important step in modern drug discovery, and it continues to capture researchers\u2019 attention\u00a0\\[ [1](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR1)\\]. Accurate molecular property determination speeds up screening processes for potential drug candidates, resulting in cost and time savings\u00a0\\[ [2](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR2)\\]. Because molecular structures and biological activities (or properties) are closely related, many computational approaches have been developed to predict these properties using structural information. Among these approaches, quantitative structure\u2013activity relationship (QSAR) modeling is a low-cost computational method commonly used to predict a wide range of molecular properties (e.g., lipophilicity, hydrophobicity, solubility)\u00a0\\[ [3](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR3)\\]. These QSAR models are flexible in design and optimized for efficient learning of complex structural patterns. Despite initial successes, these modeling tasks remain difficult due to the complexity of chemical structures, class imbalance, high-dimensional data representation, and limited data volume. To address these challenges, robust computational methods and interdisciplinary collaboration are critical.\n\nThe graph neural network (GNN)\u00a0\\[ [4](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR4)\\], which was specifically designed to handle molecular graphs \\[ [5](https://link.springer.com/link.springer.com#ref-CR5), [6](https://link.springer.com/link.springer.com#ref-CR6), [7](https://link.springer.com/link.springer.com#ref-CR7), [8](https://link.springer.com/link.springer.com#ref-CR8), [9](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR9)\\], has made a breakthrough over the last two decades. The development of efficient GNN variants allows for the emergence of graph-based representation learning \\[ [10](https://link.springer.com/link.springer.com#ref-CR10), [11](https://link.springer.com/link.springer.com#ref-CR11), [12](https://link.springer.com/link.springer.com#ref-CR12), [13](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR13)\\]. For years, numerous studies have used GNN and its variants to predict molecular properties. Scarselli et\u00a0al. \\[ [4](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR4)\\] proposed the first version of GNN in 2009. Although GNN can handle graph-structured data, their applications have not been widespread due to their relatively low learning efficiency until the graph convolutional network (GCN) was presented \\[ [14](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR14)\\]. The introduction of GNN has sparked a large number of further research and extensive practice in graph-based deep learning (DL) architectures. Wieder et\u00a0al. \\[ [15](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR15)\\] conducted a critical review to summarize DL architectures used for molecular property prediction. Gilmer et al. \\[ [16](https://link.springer.com/article/10.1007/s12293-024-00423-5#ref-CR16)\\] developed neural fingerprints using CNN customized for graph-structured data. After critically surveying various GNN-based models, Yang et al. \\[ [17]...",
      "url": "https://link.springer.com/article/10.1007/s12293-024-00423-5"
    },
    {
      "title": "Building A Graph Convolutional Network for Molecular Property ...",
      "text": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F978b0ae10ec4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fdata-science%2Fbuilding-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**TDS Archive**](https://medium.com/data-science?source=post_page---publication_nav-7f60cf5620c9-978b0ae10ec4---------------------------------------)\n\n\u00b7\n\nAn archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former Towards Data Science Medium publication.\n\n## Artificial Intelligence\n\n# Building A Graph Convolutional Network for Molecular Property Prediction\n\n## Tutorial to make molecular graphs and develop a simple PyTorch-based GCN\n\n[Gaurav Deshmukh](https://medium.com/@ChemAndCode?source=post_page---byline--978b0ae10ec4---------------------------------------)\n\n17 min read\n\n\u00b7\n\nDec 23, 2023\n\n--\n\n6\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nPhoto by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n\nArtificial intelligence has taken the world by storm. Every week, new models, tools, and applications emerge that promise to push the boundaries of human endeavor. The availability of open-source tools that enable users to train and employ complex machine learning models in a modest number of lines of code have truly democratized AI; at the same time, while many of these off-the-shelf models may provide excellent predictive capabilities, their usage as black box models may deprive inquisitive students of AI of a deeper understanding of how they work and why they were developed in the first place. This understanding is particularly important in the natural sciences, where knowing that a model is accurate is not enough \u2014 it is also essential to know its connection to other physical theories, its limitations, and its generalizability to other systems. In this article, we will explore the basics of one particular ML model \u2014 a graph convolutional network \u2014 through the lens of chemistry. This is not meant to be a mathematically rigorous exploration; instead, we will try to compare features of the network with traditional models in the natural sciences and think about why it works as well as it does.\n\n## 1\\. The need for graphs and graph neural networks\n\nA model in chemistry or physics is usually a continuous function, say _y=f(x\u2081, x\u2082, x\u2083, \u2026, x\u2099)_, in which _x\u2081, x\u2082, x\u2083, \u2026, x\u2099_ are the inputs and _y_ is the output. An example of such a model is the equation that determines the electrostatic interaction (or force) between two point charges _q\u2081_ and _q\u2082_ separated by a distance _r_ present in a medium with relative permittivity _\u03b5\u1d63_, commonly termed as Coulomb\u2019s law.\n\nPress enter or click to view image in full size\n\nFigure 1: The Coulomb equation as a model for electrostatic interactions between point charges (Image by author)\n\nIf we did not know this relationship but, hypothetically, had multiple datapoints each including the interaction between point charges (the output) and the corresponding inputs, we could fit an artificial neural network to predict the interaction for any given point charges for any given separation in a medium with a specified permittivity. In the case of this problem, admittedly ignoring some important caveats, creating a data-driven model for a physical problem is relatively straightforward.\n\nNow consider the problem of prediction of a particular property, say solubility in water, from the structure of a molecule. First, there is no obvious set of inputs to describe a molecule. You could use various features, such as bond lengths, bond angles, number of different types of elements, number of rings, and so forth. However, there is no guarantee that any such arbitrary set is bound to work well for all molecules.\n\nSecond, unlike the example of the point charges, the inputs may not necessarily reside in a continuous space. For example, we can think of methanol, ethanol, and propanol as a set of molecules with increasing chain lengths; there is no notion, however, of anything between them \u2014 chain length is a discrete parameter and there is no way to interpolate between methanol and ethanol to get other molecules. Having a continuous space of inputs is essential to calculate derivatives of the model, which can then be used for optimization of the chosen property.\n\nTo overcome these problems, various methods for encoding molecules have been proposed. One such method is textual representation using schemes such as SMILES and SELFIES. There is a large body of literature on this representation, and I direct the interested reader to this [helpful review](https://www.cell.com/patterns/pdf/S2666-3899(22)00206-9.pdf). The second method involves representing molecules as graphs. While each method has its advantages and shortcomings, graph representations feel more intuitive for chemistry.\n\nA graph is a mathematical structure consisting of nodes connected by edges that represent relationships between nodes. Molecules fit naturally into this structure \u2014 atoms become nodes, and bonds become edges. Each node in the graph is represented by a vector that encodes properties of the corresponding atom. Usually, a one-hot encoding scheme suffices (more on this in the next section). These vectors can be stacked to create a _node matrix._ Relationships between nodes \u2014 denoted by edges \u2014 can be delineated through a square _adjacency matrix,_ wherein every element _a\u1d62\u2c7c_ is either 1 or 0 depending on whether the two nodes _i_ and _j_ are connected by an edge or not respectively. The diagonal elements are set to 1, indicating a self-connection, which makes the matrix amenable to convolutions (as you will see in the next section). More complex graph representations can be developed, in which edge properties are also one-hot encoded in a separate matrix, but we shall leave that for another article. These node and adjacency matrices will serve as inputs to our model.\n\nPress enter or click to view image in full size\n\nFigure 2: Representation of an acetamide molecule as a graph with one-hot encodings of atomic numbers of nodes (Image by author)\n\nTypically, artificial neural network models accept a 1-dimensional vector of inputs. For multidimensional inputs, such as images, a class of models called convolutional neural networks was developed. In our case too we have 2-dimensional matrices as inputs, and therefore, need a modified network that can accept these as inputs. Graph neural networks were developed to operate on such node and adjacency matrices to convert them into appropriate 1-dimensional vectors that can then be passed through hidden layers of a vanilla artificial neural network to generate outputs. There are many types of graph neural networks, such as graph convolutional networks, message passing networks, graph attention networks, and so forth, which primarily differ ...",
      "url": "https://medium.com/data-science/building-a-graph-convolutional-network-for-molecular-property-prediction-978b0ae10ec4"
    },
    {
      "title": "Enhancing property and activity prediction and interpretation using ...",
      "text": "Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01155-w.pdf)\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01155-w.pdf)\n\n### Subjects\n\n- [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n- [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n- [Virtual screening](https://www.nature.com/subjects/virtual-screening)\n\n## Abstract\n\nGraph Neural Networks (GNNs) excel in compound property and activity prediction, but the choice of molecular graph representations significantly influences model learning and interpretation. While atom-level molecular graphs resemble natural topology, they overlook key substructures or functional groups and their interpretation partially aligns with chemical intuition. Recent research suggests alternative representations using reduced molecular graphs to integrate higher-level chemical information and leverages both representations for model. However, there is a lack of studies about applicability and impact of different molecular graphs on model learning and interpretation. Here, we introduce MMGX (Multiple Molecular Graph eXplainable discovery), investigating the effects of multiple molecular graphs, including Atom, Pharmacophore, JunctionTree, and FunctionalGroup, on model learning and interpretation with various perspectives. Our findings indicate that multiple graphs relatively improve model performance, but in varying degrees depending on datasets. Interpretation from multiple graphs in different views provides more comprehensive features and potential substructures consistent with background knowledge. These results help to understand model decisions and offer valuable insights for subsequent tasks. The concept of multiple molecular graph representations and diverse interpretation perspectives has broad applicability across tasks, architectures, and explanation techniques, enhancing model learning and interpretation for relevant applications in drug discovery.\n\n### Similar content being viewed by others\n\n### [Hierarchical Molecular Graph Self-Supervised Learning for property prediction](https://www.nature.com/articles/s42004-023-00825-5?fromPaywallRec=false)\n\nArticleOpen access17 February 2023\n\n### [Advancing molecular machine learning representations with stereoelectronics-infused molecular graphs](https://www.nature.com/articles/s42256-025-01031-9?fromPaywallRec=false)\n\nArticle23 May 2025\n\n### [Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning](https://www.nature.com/articles/s42256-021-00403-1?fromPaywallRec=false)\n\nArticle18 October 2021\n\n## Introduction\n\nAdvanced artificial intelligence (AI) techniques have been integrated into drug discovery to facilitate various tasks, particularly the prediction of chemical properties and activities. These methods exhibit the capacity to handle multidimensional data and complex chemical space using mathematical techniques, thereby accelerate the research process. The applications of AI enable high-throughput results, cost reduction, time savings, and minimization of unintentional human errors during biochemical experiments[1](https://www.nature.com/www.nature.com#ref-CR1), [2](https://www.nature.com/www.nature.com#ref-CR2), [3](https://www.nature.com/articles/s42004-024-01155-w#ref-CR3). Several deep learning techniques for molecular property and activity prediction have been proposing using different kinds of molecular featurizations, for instance, SMILES-based[4](https://www.nature.com/articles/s42004-024-01155-w#ref-CR4), fingerprint-based[5](https://www.nature.com/articles/s42004-024-01155-w#ref-CR5), knowledge-based[6](https://www.nature.com/articles/s42004-024-01155-w#ref-CR6), functional group-based[7](https://www.nature.com/articles/s42004-024-01155-w#ref-CR7), or image-based[8](https://www.nature.com/articles/s42004-024-01155-w#ref-CR8) methods. One of the potential AI techniques that has been widely used in this field is graph neural network (GNN) which encodes the compounds with molecular graph representation[9](https://www.nature.com/articles/s42004-024-01155-w#ref-CR9). GNNs helps leveraging the relationships and aggregated information between nodes and edges of the molecular graph and have demonstrated remarkable performance across numerous tasks in property and activity prediction[10](https://www.nature.com/articles/s42004-024-01155-w#ref-CR10), [11](https://www.nature.com/articles/s42004-024-01155-w#ref-CR11). When employing GNNs for compound property and activity prediction, careful consideration must be given to the way of representing molecules in graph structures as it highly influences both model learning and model interpretation.\n\nGenerally, the chemical compounds in GNNs are encoded in form of atom-level molecular graph representation by transforming atoms into nodes and bonds into edges, similar to natural form of molecules. This representation has been utilized in many research areas, such as, molecular property prediction and drug-target affinity prediction[12](https://www.nature.com/articles/s42004-024-01155-w#ref-CR12), [13](https://www.nature.com/articles/s42004-024-01155-w#ref-CR13). While the common topology of compound can be captured by this representation, it overlooks higher-level information pertaining to chemical substructures, such as functional groups, chemical fragments, or pharmacophoric features, which are the relevant characteristics for identifying compound property and interaction[14](https://www.nature.com/articles/s42004-024-01155-w#ref-CR14). This limitation can impede model from effectively recognizing information from molecular graphs. To address this, the learning layer of GNNs should be increased to encompass larger substructures. However, increasing number of layers can lead to other challenges like over-smoothing, neighbors-explosion, and over-squashing[15](https://www.nature.com/articles/s42004-024-01155-w#ref-CR15), while reducing number of layers may also cause under-reaching as well[16](https://www.nature.com/articles/s42004-024-01155-w#ref-CR16). Besides, because of the representation at atom-level, the interpretation are sometimes scattered and inconsistent within the same functional groups or substructures which may cause confusion[17](https://www.nature.com/articles/s42004-024-01155-w#ref-CR17). Therefore, many current researchers proposed alternative graph representations using reduction techniques. These representations encode original atom-level molecular graph with higher level of abstraction by simplifying subgraphs into single nodes, while preserving topological properties through predefined rules, for example, functional group, structure-based transformation, or pharmacophoric features[18](https://www.nature.com/articles/s42004-024-01155-w#ref-CR18), [19](https://www.nature.com/articles/s42004-024-01155-w#ref-CR19). Several reduced molecular graphs have been purposed, offering varying degrees of information, specificity and aggregation. Due to the coarsening of features in reduced graph, some information is discarded, and the resulted graph may be incomplete. Obviously, with different advantages and drawbacks from both graphs, choosing the approach for representing molecular structure is the essential task which affects model learning and interpretation.\n\nFor the effects on model learning, because of pros and cons from both graphs, many studies exploit the multiple molecular graph combination model by using both atom-level and another reduced graph representations in feature construction to support the model training. This approach leads to the improvement of the performance in many drug discovery tasks. For example, some studies apply the integration of pharmacophore-related graphs/features for molecular property prediction[19](https://www.nature.com/a...",
      "url": "https://www.nature.com/articles/s42004-024-01155-w"
    },
    {
      "title": "3D graph contrastive learning for molecular property prediction",
      "text": "3D graph contrastive learning for molecular property prediction | Bioinformatics | Oxford Academic[Skip to Main Content](#skipNav)\nAdvertisement\n[![Oxford Academic](https://oup.silverchair-cdn.com/UI/app/svg/umbrella/oxford-academic-logo.svg)](https://academic.oup.com/)\n[Journals](https://academic.oup.com/journals)\n[Books](https://academic.oup.com/books)\n* [*Search Menu*](javascript:;)\n* [*AI Discovery Assistant*](javascript:;)\n* [![Information](https://oup.silverchair-cdn.com/UI/app/svg/i.svg)](https://academic.oup.com/pages/information)\n* [![Account](https://oup.silverchair-cdn.com/UI/app/svg/account.svg)](javascript:;)\n* [*Menu*](javascript:;)\n* [![Information](https://oup.silverchair-cdn.com/UI/app/svg/i.svg)](https://academic.oup.com/pages/information)\n* [![Account](https://oup.silverchair-cdn.com/UI/app/svg/account.svg)](javascript:;)\n* [Sign in through your institution](javascript:;)\nRemember my\ninstitution\nNavbar Search FilterBioinformaticsThis issueBioinformatics JournalsBioinformatics and Computational BiologyBooksJournalsOxford AcademicMobile Enter search term[Search](javascript:;)\n* [Issues](https://academic.oup.com/bioinformatics/issue)\n* [Advance articles](https://academic.oup.com/bioinformatics/advance-articles)\n* [Submit**](javascript:;)**\n* [Author Guidelines](https://academic.oup.com/bioinformatics/pages/author-guidelines)\n* [Submission Site](http://mc.manuscriptcentral.com/bioinformatics)\n* [Open Access](https://academic.oup.com/bioinformatics/pages/open-access)\n* [Why publish with this journal?](https://academic.oup.com/bioinformatics/pages/why-publish)\n* [Alerts](https://academic.oup.com/my-account/email-alerts)\n* [About**](javascript:;)**\n* [About Bioinformatics](https://academic.oup.com/bioinformatics/pages/About)\n* [Journals Career Network](http://science-and-mathematics-careernetwork.oxfordjournals.org/jobseeker/search/results/?t730=&amp;t732=470053&amp;t731=&amp;t733=&amp;t735=&amp;t737=&amp;max=25&amp;site_id=20106&amp;search=)\n* [Editorial Board](https://academic.oup.com/bioinformatics/pages/Editorial_Board)\n* [Author Resource Centre](https://academic.oup.com/bioinformatics/pages/author-resource-centre)\n* [Advertising and Corporate Services](<https://academic.oup.com/advertising-and-corporate-services/pages/bioinformatics-media-kit >)\n* [Self-Archiving Policy](https://academic.oup.com/journals/pages/access_purchase/rights_and_permissions/self_archiving_policy_b)\n* [Dispatch Dates](http://www.oxfordjournals.org/en/access-purchase/dispatch-dates.html)\n* [Journals on Oxford Academic](https://academic.oup.com/journals)\n* [Books on Oxford Academic](https://academic.oup.com/books)\n[Bioinformatics Journals](https://academic.oup.com/bioinformaticsjournals)\n[![Bioinformatics](https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/bioinformatics/bioinformatics_title-205396838.svg)](https://academic.oup.com/bioinformatics)\n[![International Society for Computational Biology](https://oup.silverchair-cdn.com/data/SiteBuilderAssets/Live/Images/bioinformatics/bioinformatics_h11820233911.svg)](http://www.iscb.org/)\n[Close](javascript:;)\nNavbar Search FilterBioinformaticsThis issueBioinformatics JournalsBioinformatics and Computational BiologyBooksJournalsOxford AcademicEnter search term[Search](javascript:;)\n[Advanced Search](https://academic.oup.com/bioinformatics/advanced-search?page=1&amp;fl_SiteID=5139&amp;SearchSourceType=1)\n[Search Menu](javascript:;)\n[**AI Discovery Assistant](javascript:;)\nDiscover the most relevant content quickly with our AI Discovery Assistant\n**\nArticle Navigation\nClose mobile search navigation\nArticle Navigation\n**[\n![Bioinformatics Cover Image for Volume 39, Issue 6](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/Issue/39/6/37/m_bioinfo_39_6cover.jpeg?Expires=1771300085&amp;Signature=2b2kv-3EVG3xEFc9ym4h-RH8Zx5mBIFdkWtok2KJxLid9RYpczhtsAEPQZirwkwIELCJP6owJDqeFBEegfjigWNVwUDeiZ7zNWiwuJEV-v8awEKvAoLn5mghLXgHdufTZjv~4ZVzXwe75dwTptwTmBlX6McNcQfeji2Gx4ufQft9gTPnNZnuKq~rNNJ9m3V0tXCof8vYl2BTRtTsXEZAWj-eVfYM-Ipg1uokVT0mEpfl134Y0fQ2G3XFr~8aX2VZuyg0g7FuSqLsk65ilO5Oy8leR5XWKY8BXNvHsC3uY9pG~6b~w5et85ZFV7H58zJau1NA9JDnjGX31w2cxCgKYw__&amp;Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)\nVolume 39\nIssue 6\nJune 2023\n](https://academic.oup.com/bioinformatics/issue/39/6)\n### Article Contents\n* [Abstract](#474790673)\n* [1 Introduction](#474790675)\n* [2 Related works](#474790680)\n* [3 Materials and methods](#474790701)\n* [4 Experiments](#474790713)\n* [5 Additional experiments](#474790741)\n* [6 Conclusion and further works](#474790760)\n* [Supplementary data](#474790763)\n* [Conflict of interest](#474790765)\n* [Funding](#474790767)\n* [References](#474790769)\n* [&lt; Previous](https://academic.oup.com/bioinformatics/article/39/6/btad369/7192170)\n* [Next &gt;](https://academic.oup.com/bioinformatics/article/39/6/btad355/7186502)\nArticle Navigation\nArticle Navigation\nJournal Article\n# 3D graph contrastive learning for molecular property prediction*Open Access*\n[Kisung Moon](javascript:;),\nKisung Moon\nDepartment of Information Convergence Engineering, Pusan National University\n, Yangsan 50612,\nKorea\n[![ORCID logo](https://oup.silverchair-cdn.com/Themes/Silver/app/img/mini-icon.png)https://orcid.org/0000-0003-3608-0017](https://orcid.org/0000-0003-3608-0017)\nSearch for other works by this author on:\n[Oxford Academic](https://academic.oup.com/bioinformatics/search-results?f_Authors=Kisung+Moon)\n[PubMed](<http://www.ncbi.nlm.nih.gov/pubmed?cmd=search&amp;term=Moon K>)\n[Google Scholar](<http://scholar.google.com/scholar?q=author:\"Moon Kisung\">)\n[Hyeon-Jin Im](javascript:;),\nHyeon-Jin Im\nDepartment of Information Convergence Engineering, Pusan National University\n, Yangsan 50612,\nKorea\n[![ORCID logo](https://oup.silverchair-cdn.com/Themes/Silver/app/img/mini-icon.png)https://orcid.org/0000-0003-1402-855X](https://orcid.org/0000-0003-1402-855X)\nSearch for other works by this author on:\n[Oxford Academic](https://academic.oup.com/bioinformatics/search-results?f_Authors=Hyeon-Jin+Im)\n[PubMed](<http://www.ncbi.nlm.nih.gov/pubmed?cmd=search&amp;term=Im H>)\n[Google Scholar](<http://scholar.google.com/scholar?q=author:\"Im Hyeon-Jin\">)\n[Sunyoung Kwon](javascript:;)\nSunyoung Kwon[](javascript:;)\nDepartment of Information Convergence Engineering, Pusan National University\n, Yangsan 50612,\nKorea\nSchool of Biomedical Convergence Engineering, Pusan National University\n, Yangsan 50612,\nKorea\nCenter for Artificial Intelligence Research, Pusan National University\n, Busan 46241,\nKorea\nCorresponding author.School of Biomedical Convergence Engineering, Pusan National University, Yangsan 50612, Korea. E-mail:[sy.kwon@pusan.ac.kr](mailto:sy.kwon@pusan.ac.kr)\n[![ORCID logo](https://oup.silverchair-cdn.com/Themes/Silver/app/img/mini-icon.png)https://orcid.org/0000-0003-3433-1409](https://orcid.org/0000-0003-3433-1409)\nSearch for other works by this author on:\n[Oxford Academic](https://academic.oup.com/bioinformatics/search-results?f_Authors=Sunyoung+Kwon)\n[PubMed](<http://www.ncbi.nlm.nih.gov/pubmed?cmd=search&amp;term=Kwon S>)\n[Google Scholar](<http://scholar.google.com/scholar?q=author:\"Kwon Sunyoung\">)\n*Bioinformatics*, Volume 39, Issue 6, June 2023, btad371,[https://doi.org/10.1093/bioinformatics/btad371](https://doi.org/10.1093/bioinformatics/btad371)\nPublished:\n08 June 2023\n[Article history**](javascript:;)\nReceived:\n23 September 2022\nRevision received:\n14 May 2023\nEditorial decision:\n18 May 2023\nAccepted:\n07 June 2023\nPublished:\n08 June 2023\nCorrected and typeset:\n21 June 2023\n* [![pdf](https://oup.silverchair-cdn.com/UI/app/svg/pdf.svg)PDF](https://academic.oup.com/bioinformatics/article-pdf/39/6/btad371/58549346/btad371.pdf)\n* [**Split View](javascript:;)\n* [**\nViews\n**\n](javascript:;)\n* [Article contents](javascript:;)\n* [Figures &amp; tables](javascript:;)\n* [Video](javascript:;)\n* [Audio](javascript:;)\n* [Supplementary Data](javascript:;)\n* [**Cite](#)\n### Cite\nKisung Moon, Hyeon-Jin Im, Sunyoung Kwon, 3D graph contr...",
      "url": "https://academic.oup.com/bioinformatics/article/39/6/btad371/7192173"
    }
  ]
}