{
  "query": "How can I improve machine learning model generalization when there is a large gap between cross-validation and test performance?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving generalization when the cross\u2011validation score is far higher than the held\u2011out test score means you are over\u2011fitting the validation process.  The literature recommends tightening the validation protocol and adding regularization techniques that reduce model variance and capacity\u202f([Cawley\u202f&\u202fTalbot,\u202f2010](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf);\u202f[Roelofs\u202fet\u202fal.,\u202f2022](https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf);\u202f[Zhang\u202flecture\u202f9,\u202f2018](https://www.cs.toronto.edu/~lczhang/321/notes/notes09.pdf)).  \n\n**Steps to close the CV\u2011test gap**\n\n1. **Use a strict, unbiased evaluation pipeline**  \n   * Split the data once into a *training* set, a *validation* set (used only for hyper\u2011parameter tuning), and a final *test* set that is never touched until the very end.  \n   * Apply **nested cross\u2011validation** (inner CV for tuning, outer CV for estimating performance) to avoid leaking information from the validation folds into model selection\u202f([Cawley\u202f&\u202fTalbot,\u202f2010](https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf)).  \n\n2. **Limit model capacity**  \n   * Choose a simpler architecture or reduce the number of parameters (e.g., shallower trees, fewer layers/neurons).  \n   * Apply **weight decay / L2 regularization** or **L1 sparsity** to penalize large weights\u202f([Zhang\u202flecture\u202f9,\u202f2018](https://www.cs.toronto.edu/~lczhang/321/notes/notes09.pdf)).  \n\n3. **Introduce regularization that reduces variance**  \n   * **Early stopping**: monitor validation loss and stop training when it stops improving.  \n   * **Stochastic regularization**: use dropout, data augmentation, or random noise injection during training\u202f([Zhang\u202flecture\u202f9,\u202f2018](https://www.cs.toronto.edu/~lczhang/321/notes/notes09.pdf)).  \n\n4. **Ensemble or bootstrap**  \n   * Train multiple models on different bootstrap resamples of the training data and average their predictions.  The \u201cDeep Bootstrap Framework\u201d shows that good online learners (trained on bootstrapped streams) tend to generalize better offline\u202f([Nakkiran\u202fet\u202fal.,\u202f2020](https://arxiv.org/abs/2010.08127)).  \n\n5. **Control hyper\u2011parameter search breadth**  \n   * Restrict the number of hyper\u2011parameter configurations explored; excessive tuning can over\u2011fit the validation folds (the \u201cdanger of cross\u2011validation\u201d when thousands of models are evaluated)\u202f([Rao\u202fet\u202fal.,\u202f2008](https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf)).  \n\n6. **Validate assumptions about data distribution**  \n   * Verify that training, validation, and test data are i.i.d. and share the same underlying distribution; if not, consider domain\u2011adaptation or collect more representative data\u202f([Google\u202foverfitting guide,\u202f2022](https://developers.google.com/machine-learning/crash-course/overfitting/overfitting)).  \n\n7. **Re\u2011estimate performance with unbiased variance estimators**  \n   * Use variance\u2011aware estimators for the generalization error (as proposed by Bengio\u202f2003) to obtain more reliable confidence intervals before trusting the CV score\u202f([Bengio\u202f2003](https://link.springer.com/article/10.1023/A:1024068626366)).  \n\nFollowing these steps\u2014especially a clean train/validation/test split with nested CV, reducing model complexity, applying strong regularization, and limiting the scope of hyper\u2011parameter search\u2014will narrow the gap between cross\u2011validation and true test performance and yield a model that generalizes reliably.",
      "url": ""
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2010.08127] The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2010.08127\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2010.08127**(cs)\n[Submitted on 16 Oct 2020 ([v1](https://arxiv.org/abs/2010.08127v1)), last revised 19 Feb 2021 (this version, v2)]\n# Title:The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers\nAuthors:[Preetum Nakkiran](https://arxiv.org/search/cs?searchtype=author&amp;query=Nakkiran,+P),[Behnam Neyshabur](https://arxiv.org/search/cs?searchtype=author&amp;query=Neyshabur,+B),[Hanie Sedghi](https://arxiv.org/search/cs?searchtype=author&amp;query=Sedghi,+H)\nView a PDF of the paper titled The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers, by Preetum Nakkiran and 2 other authors\n[View PDF](https://arxiv.org/pdf/2010.08127)> > Abstract:\n> We propose a new framework for reasoning about generalization in deep learning. The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in offline learning to the problem of optimization in online learning. We then give empirical evidence that this gap between worlds can be small in realistic deep learning settings, in particular supervised image classification. For example, CNNs generalize better than MLPs on image distributions in the Real World, but this is &#34;because&#34; they optimize faster on the population loss in the Ideal World. This suggests our framework is a useful tool for understanding generalization in deep learning, and lays a foundation for future research in the area. Comments:|Accepted to ICLR 2021|\nSubjects:|Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Statistics Theory (math.ST); Machine Learning (stat.ML)|\nCite as:|[arXiv:2010.08127](https://arxiv.org/abs/2010.08127)[cs.LG]|\n|(or[arXiv:2010.08127v2](https://arxiv.org/abs/2010.08127v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2010.08127](https://doi.org/10.48550/arXiv.2010.08127)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Preetum Nakkiran [[view email](https://arxiv.org/show-email/f2556ae8/2010.08127)]\n**[[v1]](https://arxiv.org/abs/2010.08127v1)**Fri, 16 Oct 2020 03:07:49 UTC (17,486 KB)\n**[v2]**Fri, 19 Feb 2021 03:24:24 UTC (18,002 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers, by Preetum Nakkiran and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2010.08127)\n* [TeX Source](https://arxiv.org/src/2010.08127)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2010.08127&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2010.08127&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-10](https://arxiv.org/list/cs.LG/2020-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2010.08127?context=cs)\n[cs.CV](https://arxiv.org/abs/2010.08127?context=cs.CV)\n[cs.NE](https://arxiv.org/abs/2010.08127?context=cs.NE)\n[math](https://arxiv.org/abs/2010.08127?context=math)\n[math.ST](https://arxiv.org/abs/2010.08127?context=math.ST)\n[stat](https://arxiv.org/abs/2010.08127?context=stat)\n[stat.ML](https://arxiv.org/abs/2010.08127?context=stat.ML)\n[stat.TH](https://arxiv.org/abs/2010.08127?context=stat.TH)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.08127)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.08127)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.08127)\n### [2 blog links](https://arxiv.org/tb/2010.08127)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-08127)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-08127)\n[Preetum Nakkiran](<https://dblp.uni-trier.de/search/author?author=Preetum Nakkiran>)\n[Behnam Neyshabur](<https://dblp.uni-trier.de/search/author?author=Behnam Neyshabur>)\n[Hanie Sedghi](<https://dblp.uni-trier.de/search/author?author=Hanie Sedghi>)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2010.08127&amp;description=The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2010.08127&amp;title=The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle...",
      "url": "https://arxiv.org/abs/2010.08127"
    },
    {
      "title": "",
      "text": "Journal of Machine Learning Research 11 (2010) 2079-2107 Submitted 10/09; Revised 3/10; Published 7/10\nOn Over-fitting in Model Selection and Subsequent Selection Bias in\nPerformance Evaluation\nGavin C. Cawley GCC@CMP.UEA.AC.UK\nNicola L. C. Talbot NLCT@CMP.UEA.AC.UK\nSchool of Computing Sciences\nUniversity of East Anglia\nNorwich, United Kingdom NR4 7TJ\nEditor: Isabelle Guyon\nAbstract\nModel selection strategies for machine learning algorithms typically involve the numerical opti\u0002misation of an appropriate model selection criterion, often based on an estimator of generalisation\nperformance, such as k-fold cross-validation. The error of such an estimator can be broken down\ninto bias and variance components. While unbiasedness is often cited as a beneficial quality of a\nmodel selection criterion, we demonstrate that a low variance is at least as important, as a non\u0002negligible variance introduces the potential for over-fitting in model selection as well as in training\nthe model. While this observation is in hindsight perhaps rather obvious, the degradation in perfor\u0002mance due to over-fitting the model selection criterion can be surprisingly large, an observation that\nappears to have received little attention in the machine learning literature to date. In this paper, we\nshow that the effects of this form of over-fitting are often of comparable magnitude to differences\nin performance between learning algorithms, and thus cannot be ignored in empirical evaluation.\nFurthermore, we show that some common performance evaluation practices are susceptible to a\nform of selection bias as a result of this form of over-fitting and hence are unreliable. We dis\u0002cuss methods to avoid over-fitting in model selection and subsequent selection bias in performance\nevaluation, which we hope will be incorporated into best practice. While this study concentrates\non cross-validation based model selection, the findings are quite general and apply to any model\nselection practice involving the optimisation of a model selection criterion evaluated over a finite\nsample of data, including maximisation of the Bayesian evidence and optimisation of performance\nbounds.\nKeywords: model selection, performance evaluation, bias-variance trade-off, selection bias, over\u0002fitting\n1. Introduction\nThis paper is concerned with two closely related topics that form core components of best practice in\nboth the real world application of machine learning methods and the development of novel machine\nlearning algorithms, namely model selection and performance evaluation. The majority of machine\nlearning algorithms are based on some form of multi-level inference, where the model is defined\nby a set of model parameters and also a set of hyper-parameters (Guyon et al., 2009), for example\nin kernel learning methods the parameters correspond to the coefficients of the kernel expansion\nand the hyper-parameters include the regularisation parameter, the choice of kernel function and\nany associated kernel parameters. This division into parameters and hyper-parameters is typically\n\rc 2010 Gavin C. Cawley and Nicola L. C. Talbot.\nCAWLEY AND TALBOT\nperformed for computational convenience; for instance in the case of kernel machines, for fixed\nvalues of the hyper-parameters, the parameters are normally given by the solution of a convex\noptimisation problem for which efficient algorithms are available. Thus it makes sense to take\nadvantage of this structure and fit the model iteratively using a pair of nested loops, with the hyper\u0002parameters adjusted to optimise a model selection criterion in the outer loop (model selection) and\nthe parameters set to optimise a training criterion in the inner loop (model fitting/training). In\nour previous study (Cawley and Talbot, 2007), we noted that the variance of the model selection\ncriterion admitted the possibility of over-fitting during model selection as well as the more familiar\nform of over-fitting that occurs during training and demonstrated that this could be ameliorated to\nsome extent by regularisation of the model selection criterion. The first part of this paper discusses\nthe problem of over-fitting in model selection in more detail, providing illustrative examples, and\ndescribes how to avoid this form of over-fitting in order to gain the best attainable performance,\ndesirable in practical applications, and required for fair comparison of machine learning algorithms.\nUnbiased and robust1 performance evaluation is undoubtedly the cornerstone of machine learn\u0002ing research; without a reliable indication of the relative performance of competing algorithms,\nacross a wide range of learning tasks, we cannot have the clear picture of the strengths and weak\u0002nesses of current approaches required to set the direction for future research. This topic is consid\u0002ered in the second part of the paper, specifically focusing on the undesirable optimistic bias that\ncan arise due to over-fitting in model selection. This phenomenon is essentially analogous to the\nselection bias observed by Ambroise and McLachlan (2002) in microarray classification, due to\nfeature selection prior to performance evaluation, and shares a similar solution. We show that some,\napparently quite benign, performance evaluation protocols in common use by the machine learning\ncommunity are susceptible to this form of bias, and thus potentially give spurious results. In order\nto avoid this bias, model selection must be treated as an integral part of the model fitting process and\nperformed afresh every time a model is fitted to a new sample of data. Furthermore, as the differ\u0002ences in performance due to model selection are shown to be often of comparable magnitude to the\ndifference in performance between learning algorithms, it seems no longer meaningful to evaluate\nthe performance of machine learning algorithms in isolation, and we should instead compare learn\u0002ing algorithm/model selection procedure combinations. However, this means that robust unbiased\nperformance evaluation is likely to require more rigorous and computationally intensive protocols,\nsuch a nested cross-validation or \u201cdouble cross\u201d (Stone, 1974).\nNone of the methods or algorithms discussed in this paper are new; the novel contribution\nof this work is an empirical demonstration that over-fitting at the second level of inference (i.e.,\nmodel selection) can have a very substantial deleterious effect on the generalisation performance of\nstate-of-the-art machine learning algorithms. Furthermore the demonstration that this can lead to a\nmisleading optimistic bias in performance evaluation using evaluation protocols in common use in\nthe machine learning community is also novel. The paper is intended to be of some tutorial value in\npromoting best practice in model selection and performance evaluation, however we also hope that\nthe observation that over-fitting in model selection is a significant problem will encourage much\nneeded algorithmic and theoretical development in this area.\nThe remainder of the paper is structured as follows: Section 2 provides a brief overview of the\nkernel ridge regression classifier used as the base classifier for the majority of the experimental work\n1. The term \u201crobust\u201d is used here to imply insensitivity to irrelevant experimental factors, such as the sampling and par\u0002titioning of the data to form training, validation and test sets; this is normally achieved by computationally expensive\nresampling schemes, for example, cross-validation (Stone, 1974) and the bootstrap (Efron and Tibshirani, 1994).\n2080\nOVERFITTING IN MODEL SELECTION AND SELECTION BIAS IN PERFORMANCE EVALUATION\nand Section 3 describes the data sets used. Section 4 demonstrates the importance of the variance\nof the model selection criterion, as it can lead to over-fitting in model selection, resulting in poor\ngeneralisation performance. A number of methods to avoid over-fitting in model selection are also\ndiscussed. Section 5 shows that over-fitting in model selection...",
      "url": "https://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf"
    },
    {
      "title": "",
      "text": "A Meta-Analysis of Overfitting in Machine Learning\nRebecca Roelofs\u2217\nUC Berkeley\nroelofs@berkeley.edu\nSara Fridovich-Keil\u2217\nUC Berkeley\nsfk@berkeley.edu\nJohn Miller\nUC Berkeley\nmiller_john@berkeley.edu\nVaishaal Shankar\nUC Berkeley\nvaishaal@berkeley.edu\nMoritz Hardt\nUC Berkeley\nhardt@berkeley.edu\nBenjamin Recht\nUC Berkeley\nbrecht@berkeley.edu\nLudwig Schmidt\nUC Berkeley\nludwig@berkeley.edu\nAbstract\nWe conduct the first large meta-analysis of overfitting due to test set reuse in the\nmachine learning community. Our analysis is based on over one hundred machine\nlearning competitions hosted on the Kaggle platform over the course of several\nyears. In each competition, numerous practitioners repeatedly evaluated their\nprogress against a holdout set that forms the basis of a public ranking available\nthroughout the competition. Performance on a separate test set used only once\ndetermined the final ranking. By systematically comparing the public ranking\nwith the final ranking, we assess how much participants adapted to the holdout set\nover the course of a competition. Our study shows, somewhat surprisingly, little\nevidence of substantial overfitting. These findings speak to the robustness of the\nholdout method across different data domains, loss functions, model classes, and\nhuman analysts.\n1 Introduction\nThe holdout method is central to empirical progress in the machine learning community. Competitions,\nbenchmarks, and large-scale hyperparameter search all rely on splitting a data set into multiple pieces\nto separate model training from evaluation. However, when practitioners repeatedly reuse holdout\ndata, the danger of overfitting to the holdout data arises [6, 13].\nDespite its importance, there is little empirical research into the manifested robustness and validity\nof the holdout method in practical scenarios. Real-world use cases of the holdout method often\nfall outside the guarantees of existing theoretical bounds, making questions of validity a matter of\nguesswork.\nRecent replication studies [16] demonstrated that the popular CIFAR-10 [10] and ImageNet [5, 18]\nbenchmarks continue to support progress despite years of intensive use. The longevity of these\nbenchmarks perhaps suggests that overfitting to holdout data is less of a concern than reasoning from\nfirst principles might have suggested. However, this is evidence from only two, albeit important,\ncomputer vision benchmarks. It remains unclear whether the observed phenomenon is specific to the\ndata domain, model class, or practices of vision researchers. Unfortunately, these replication studies\n\u2217Equal contribution\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nrequired assembling new test sets from scratch, resulting in a highly labor-intensive analysis that is\ndifficult to scale.\nIn this paper, we empirically study holdout reuse at a significantly larger scale by analyzing data\nfrom 120 machine learning competitions on the popular Kaggle platform [2]. Kaggle competitions\nare a particularly well-suited environment for studying overfitting since data sources are diverse,\ncontestants use a wide range of model families, and training techniques vary greatly. Moreover,\nKaggle competitions use public and private test data splits which provide a natural experimental setup\nfor measuring overfitting on various datasets.\nTo provide a detailed analysis of each competition, we introduce a coherent methodology to char\u0002acterize the extent of overfitting at three increasingly fine scales. Our approach allows us both to\ndiscuss the overall \u201chealth\u201d of a competition across all submissions and to inspect signs of overfitting\nseparately among the top submissions. In addition, we develop a statistical test specific to the\nclassification competitions on Kaggle to compare the submission scores to those arising in an ideal\nnull model that assumes no overfitting. Observed data that are close to data predicted by the null\nmodel is strong evidence against overfitting.\nOverall, we conclude that the classification competitions on Kaggle show little to no signs of\noverfitting. While there are some outlier competitions in the data, these competitions usually have\npathologies such as non-i.i.d. data splits or (effectively) small test sets. Among the remaining\ncompetitions, the public and private test scores show a remarkably good correspondence. The\npicture becomes more nuanced among the highest scoring submissions, but the overall effect sizes\nof (potential) overfitting are typically small (e.g., less than 1% classification accuracy). Thus, our\nfindings show that substantial overfitting is unlikely to occur naturally in regular machine learning\nworkflows.\n2 Background and setup\nBefore we delve into the analysis of the Kaggle data, we briefly define the type of overfitting we\nstudy and then describe how the Kaggle competition format naturally lends itself to investigating\noverfitting in machine learning competitions.\n2.1 Adaptive overfitting\n\u201cOverfitting\u201d is often used as an umbrella term to describe any unwanted performance drop of a\nmachine learning model. Here, we focus on adaptive overfitting, which is overfitting caused by test\nset reuse. While other phenomena under the overfitting umbrella are also important aspects of reliable\nmachine learning (e.g. performance drops due to distribution shifts), they are beyond the scope of our\npaper since they require an experimental setup different from ours.\nFormally, let f : X \u2192 Y be a trained model that maps examples x \u2208 X to output values y \u2208 Y\n(e.g., class labels or regression targets). The standard approach to measuring the performance of\nsuch a trained model is to define a loss function L : Y \u00d7 Y \u2192 R and to draw samples S =\n{(x1, y1), . . . ,(xn, yn)} from a data distribution D which we then use to evaluate the test loss\nLS(f) = Pn\ni=1 L(f(xi), yi). As long as the model f does not depend on the test set S, standard\nconcentration results [19] show that LS(f) is a good approximation of the true performance given by\nthe population loss LD(f) = ED[L(f(x), y)].\nHowever, machine learning practitioners often undermine the assumption that f does not depend\non the test set by selecting models and tuning hyperparameters based on the test loss. Especially\nwhen algorithm designers evaluate a large number of different models on the same test set, the final\nclassifier may only perform well on the specific examples in the test set. The failure to generalize to\nthe entire data distribution D manifests itself in a large adaptivity gap LD(f) \u2212 LS(f) and leads to\noverly optimistic performance estimates.\n2.2 Kaggle\nKaggle is the most widely used platform for machine learning competitions, currently hosting 1,461\nactive and completed competitions. Various organizations (companies, educators, etc.) provide the\n2\ndatasets and evaluation rules for the competitions, which are generally open to any participant. Each\ncompetition is centered around a dataset consisting of a training set and a test set.\nConsidering the danger of overfitting to the test set in a competitive environment, Kaggle subdivides\neach test set into public and private components. The subsets are randomly shuffled together and\nthe entire test set is released without labels, so that participants should not know which test samples\nbelong to which split. Hence participants submit predictions for the entire test set. The Kaggle server\nthen internally evaluates each submission on both public and private splits and updates the public\ncompetition leaderboard only with the score on the public split. At the end of the competition, Kaggle\nreleases the private scores, which determine the winner.\nKaggle has released the MetaKaggle dataset2, which contains detailed information about competitions,\nsubmissions, etc. on the Kaggle platform. The structure of Kaggle competitions makes MetaKaggle\na useful dataset for investigating overfitting empirically at a large scale. In particular, we can view\nthe public test ...",
      "url": "https://papers.neurips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning.pdf"
    },
    {
      "title": "",
      "text": "On the Dangers of Cross-Validation. An Experimental Evaluation\nR. Bharat Rao\nIKM CKS Siemens Medical Solutions USA\nGlenn Fung\nIKM CKS Siemens Medical Solutions USA\nRomer Rosales\nIKM CKS Siemens Medical Solutions USA\nAbstract\nCross validation allows models to be tested using the\nfull training set by means of repeated resampling; thus,\nmaximizing the total number of points used for testing\nand potentially, helping to protect against overfitting.\nImprovements in computational power, recent reduc\u0002tions in the (computational) cost of classification algo\u0002rithms, and the development of closed-form solutions\n(for performing cross validation in certain classes of\nlearning algorithms) makes it possible to test thousand\nor millions of variants of learning models on the data.\nThus, it is now possible to calculate cross validation per\u0002formance on a much larger number of tuned models than\nwould have been possible otherwise. However, we em\u0002pirically show how under such large number of models\nthe risk for overfitting increases and the performance\nestimated by cross validation is no longer an effective\nestimate of generalization; hence, this paper provides\nan empirical reminder of the dangers of cross valida\u0002tion. We use a closed-form solution that makes this\nevaluation possible for the cross validation problem of\ninterest. In addition, through extensive experiments we\nexpose and discuss the effects of the overuse/misuse of\ncross validation in various aspects, including model se\u0002lection, feature selection, and data dimensionality. This\nis illustrated on synthetic, benchmark, and real-world\ndata sets.\n1 Introduction\nIn a general classification problem, the goal is to learn a\nclassifier that performs well on unseen data drawn from\nthe same distribution as the available data 1; in other\nwords, to learn classifiers with good generalization. One\ncommon way to estimate generalization capabilities is\nto measure the performance of the learned classifier on\ntest data that has not been used to train the classifier.\nWhen a large test data set cannot be held out or easily\n1We concentrate on performance on data drawn from the same\ndistribution but performance on a different distribution is also a\n(less explored) problem of interest.\nacquired, resampling methods, such as cross validation,\nare commonly used to estimate the generalization er\u0002ror. The resulting estimates of generalization can also\nbe used for model selection by choosing from various\npossible classification algorithms (models) the one that\nhas the lowest cross validation error (and hence the low\u0002est expected generalization error).\nA strong argument in favor of using cross validation\nis the potential of using the entire training set for\ntesting (albeit not at once), creating the largest possible\ntest set for a fixed training data set. Essentially,\nthe classifier is trained on a subset of the training\ndata set, and tested on the remainder. This process\nis repeated systematically so that all the points in\nthe training set are tested. There has been much\nstudy on the empirical behavior of cross-validation for\nerror estimation and model selection, and more recently\ntheoretical bounds on the error in the leave-one-out\ncross-validation (loocv) estimate. Much of the focus\nhas been on the expected value of this error over all\ntraining sets of a given sample size and the asymptotic\nbehavior as the sample size increases. In this paper we\nempirically address the pitfalls of using cross validation\nerror to select among a large number of classification\nalgorithms.\nResampling methods, such as bootstrapping or\ncross validation (Stone, 1977; Kohavi, 1995a; Weiss &\nKulikowski, 1991; Efron & Tibshirani, 1993) have typ\u0002ically been used to measure the generalization perfor\u0002mance of a chosen algorithm, or possibly to select be\u0002tween a limited set of algorithms. Until the last decade,\ncross validation experiments could reasonable be per\u0002formed only on a small set of algorithms or possible\nmodels; a k-fold or loocv run for a single algorithm,\neven on a small dataset, typically ran for several hours,\nif not days. As computers have become more power\u0002ful and due to recent advances regarding the compu\u0002tational efficiency of popular classification algorithms\nand techniques (for example: linear training time for\nSVMs (Joachims, 2006) and n log(n) kernel computa\u0002tion (Raykar & Duraiswami, 2005)), cross validation\nperformance can be quickly computed on several thou\u0002sands or even millions of algorithms. Recent develop\u0002ments in grid computing now allow computers distrib\u0002uted in a large geographic area to be harnessed for a spe\u0002cific task, exponentially increasing the computing power\nat hand.\nIt is a commonly held believe that cross validation,\nlike any other tool or metric, can be abused (Ng,\n1997). Some basic heuristic procedures have been\nemployed to avoid these problems. For example, when\npossible a sequestered test set is kept aside. This\nset is finally used only after training to verify that\nthe chosen classifier indeed has superior generalization.\nAny modeling decisions based upon experiments on the\ntraining set, even cross validation estimates, are suspect,\nuntil independently verified.\nDespite certain general knowledge about the draw\u0002backs attached to cross validation, there has not been a\nsufficiently clear experimental (practical) investigation\non the behavior of the estimate of generalization error\nfor a fixed data set.\nIn this paper we provide an empirical reminder\nof a fact that is known but usually underestimated:\nwhen the set of algorithms to be considered becomes\nlarge, cross validation is no longer a good measure\nof generalization performance, and accordingly can no\nlonger be used for algorithm or feature selection. In\naddition we experimentally show the impact of cross\nvalidation as the data dimensionality increases and\nfor feature selection. We provide experimental results\non synthetic, standardized benchmark (from the UCI\nrepository), and a real-world dataset related to clinical\ndiagnosis in virtual colonoscopy.\n2 Related Research\nA fundamental issue in machine learning is to obtain\nan accurate estimate of the generalization error of a\nmodel trained on a finite data set. Precisely estimating\nthe accuracy of a model is not only important to ex\u0002amine the generalization performance of an algorithm,\nbut also for choosing an algorithm from a variety of\nlearning algorithms. Empirical estimators based upon\nresampling, which include bootstrap (Efron & Tibshi\u0002rani, 1993), cross validation (Stone, 1977) estimates are\npopular, and Holdout estimates where a test set is se\u0002questered until the model is frozen are also used.\nA fair amount of research has focused on the\nempirical performance of leave-one-out cross validation\n(loocv) and k-fold CV on synthetic and benchmark\ndata sets. Experiments by (Breiman & Spector, 1992)\nshow that k-fold CV has better empirical performance\nthan loocv for feature selection for linear regression.\n(Kohavi, 1995b) also obtains results in favor of 10-\nfold cross validation using decision trees and Naive\nBayes, and demonstrates the bias-variance trade-off for\ndifferent values of k on multiple benchmark data sets.\n(Kohavi & Wolpert, 1996) discuss the bias-variance\ntrade-off for classifiers using a misclassification loss\nfunction. Our work, while not directly related to the\nbias-variance trade-off is closely related to the notion of\nvariance.\nFrom a theoretical perspective, the most general\ntheoretical results for training error estimates are pro\u0002vided by (Vapnik, 1982) who proved that the training\nerror estimate is less than O(\np\nV C/n) away from the\ntrue test error where V C is the VC dimension of a hy\u0002pothesis space. More recently, the task of developing\nupper bounds on the loocv error for a specific method\u0002ology has drawn the attention in the learning theory\ncommunity. For example, (Zhang, 2003) has derived\nupper bounds on the expected loocv error to show con\u0002sistency for a set of kernel methods. These consistenc...",
      "url": "https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf"
    },
    {
      "title": "",
      "text": "Lecture 9: Generalization\nRoger Grosse\n1 Introduction\nWhen we train a machine learning model, we don\u2019t just want it to learn to\nmodel the training data. We want it to generalize to data it hasn\u2019t seen\nbefore. Fortunately, there\u2019s a very convenient way to measure an algorithm\u2019s\ngeneralization performance: we measure its performance on a held-out test\nset, consisting of examples it hasn\u2019t seen before. If an algorithm works well\non the training set but fails to generalize, we say it is overfitting. Improving\ngeneralization (or preventing overfitting) in neural nets is still somewhat of\na dark art, but this lecture will cover a few simple strategies that can often\nhelp a lot.\n1.1 Learning Goals\n\u2022 Know the difference between a training set, validation set, and test\nset.\n\u2022 Be able to reason qualitatively about how training and test error de\u0002pend on the size of the model, the number of training examples, and\nthe number of training iterations.\n\u2022 Understand the motivation behind, and be able to use, several strate\u0002gies to improve generalization:\n\u2013 reducing the capacity\n\u2013 early stopping\n\u2013 weight decay\n\u2013 ensembles\n\u2013 input transformations\n\u2013 stochastic regularization\n2 Measuring generalization\nSo far in this course, we\u2019ve focused on training, or optimizing, neural net\u0002works. We defined a cost function, the average loss over the training set:\n1\nN\nX\nN\ni=1\nL(y(x\n(i)\n), t(i)). (1)\nBut we don\u2019t just want the network to get the training examples right; we\nalso want it to generalize to novel instances it hasn\u2019t seen before.\nFortunately, there\u2019s an easy way to measure a network\u2019s generalization\nperformance. We simply partition our data into three subsets:\n1\n\u2022 A training set, a set of training examples the network is trained on.\nThere are lots of variants on this\nbasic strategy, including something\ncalled cross-validation. Typically,\nthese alternatives are used in\nsituations with small datasets,\ni.e. less than a few thousand\nexamples. Most applications of\nneural nets involve datasets large\nenough to split into training,\nvalidation and test sets.\n\u2022 A validation set, which is used to tune hyperparameters such as the\nnumber of hidden units, or the learning rate.\n\u2022 A test set, which is used to measure the generalization performance.\nThe losses on these subsets are called training, validation, and test\nloss, respectively. Hopefully it\u2019s clear why we need separate training and\ntest sets: if we train on the test data, we have no idea whether the network\nis correctly generalizing, or whether it\u2019s simply memorizing the training\nexamples. It\u2019s a more subtle point why we need a separate validation set.\n\u2022 We can\u2019t tune hyperparameters on the training set, because we want\nto choose values that will generalize. For instance, suppose we\u2019re\ntrying to choose the number of hidden units. If we choose a very large\nvalue, the network will be able to memorize the training data, but will\ngeneralize poorly. Tuning on the training data could lead us to choose\nsuch a large value.\n\u2022 We also can\u2019t tune them on the test set, because that would be \u201ccheat\u0002ing.\u201d We\u2019re only allowed to use the test set once, to report the final\nperformance. If we \u201cpeek\u201d at the test data by using it to tune hyper\u0002parameters, it will no longer give a realistic estimate of generalization\nperformance.1\nThe most basic strategy for tuning hyperparameters is to do a grid\nsearch: for each hyperparameter, choose a set of candidate values. Sep\u0002arately train models using all possible combinations of these values, and\nchoose whichever configuration gives the best validation error. A closely\nrelated alternative is random search: train a bunch of networks using\nrandom configurations of the hyperparameters, and pick whichever one has\nthe best validation error. The advantage of random search over grid search\nis as follows: suppose your model has 10 hyperparameters, but only two\nof them are actually important. (You don\u2019t know which two.) It\u2019s infea\u0002sible to do a grid search in 10 dimensions, but random search still ought\nto provide reasonable coverage of the 2-dimensional space of the important\nhyperparameters. On the other hand, in a scientific setting, grid search has\nthe advantage that it\u2019s easy to reproduce the exact experimental setup.\n3 Reasoning about generalization\nIf a network performs well on the training set but generalizes badly, we\nsay it is overfitting. A network might overfit if the training set contains\naccidental regularities. For instance, if the task is to classify handwritten\ndigits, it might happen that in the training set, all images of 9\u2019s have pixel\nnumber 122 on, while all other examples have it off. The network might\n1Actually, there\u2019s some fascinating recent work showing that it\u2019s possible to use a test\nset repeatedly, as long as you add small amounts of noise to the average error. This hasn\u2019t\nyet become a standard technique, but it may sometime in the future. See Dwork et al.,\n2015, \u201cThe reusable holdout: preserving validity in adaptive data analysis.\u201d\n2\nFigure 1: (left) Qualitative relationship between the number of training\nexamples and training and test error. (right) Qualitative relationship be\u0002tween the number of parameters (or model capacity) and training and test\nerror.\ndecide to exploit this accidental regularity, thereby correctly classifying all\nthe training examples of 9\u2019s, without learning the true regularities. If this\nproperty doesn\u2019t hold on the test set, the network will generalize badly.\nAs an extreme case, remember the network we constructed in Lecture\n5, which was able to learn arbitrary Boolean functions? It had a separate\nhidden unit for every possible input configuration. This network architec\u0002ture is able to memorize a training set, i.e. learn the correct answer for\nevery training example, even though it will have no idea how to classify\nnovel instances. The problem is that this network has too large a capac\u0002ity, i.e. ability to remember information about its training data. Capacity\nisn\u2019t a formal term, but corresponds roughly to the number of trainable\nparameters (i.e. weights). The idea is that information is stored in the net\u0002work\u2019s trainable parameters, so networks with more parameters can store\nmore information.\nIn order to reason qualitatively about generalization, let\u2019s think about\nhow the training and generalization error vary as a function of the number\nof training examples and the number of parameters. Having more train\u0002ing data should only help generalization: for any particular test example,\nthe larger the training set, the more likely there will be a closely related\ntraining example. Also, the larger the training set, the fewer the accidental\nregularities, so the network will be forced to pick up the true regularities.\nTherefore, generalization error ought to improve as we add more training\nexamples. If the test error increases with the\nnumber of training examples,\nthat\u2019s a sign that you have a bug\nin your code or that there\u2019s\nsomething wrong with your model.\nOn the other hand, small training sets are easier to memorize\nthan large ones, so training error tends to increase as we add more exam\u0002ples. As the training set gets larger, the two will eventually meet. This is\nshown qualitatively in Figure 1.\nNow let\u2019s think about the model capacity. As we add more parameters,\nit becomes easier to fit both the accidental and the true regularities of the\ntraining data. Therefore, training error improves as we add more parame\u0002ters. The effect on generalization error is a bit more subtle. If the network\nhas too little capacity, it generalizes badly because it fails to pick up the\nregularities (true or accidental) in the data. If it has too much capacity, it\nwill memorize the training set and fail to generalize. Therefore, the effect\n3\nof capacity on test error is non-monotonic: it decreases, and then increases.\nWe would like to design network architectures which have enough capacity\nto learn the true regularities in the training data, but not enough capacity\nto simply memorize the train...",
      "url": "https://www.cs.toronto.edu/~lczhang/321/notes/notes09.pdf"
    },
    {
      "title": "Inference for the Generalization Error",
      "text": "<div><div>\n <header>\n <ul>\n \n \n \n \n <li><a href=\"#article-info\">Published: <time>September 2003</time></a></li>\n </ul>\n \n \n \n <p>\n \n <a href=\"/journal/10994\"><i>Machine Learning</i></a>\n <b>volume\u00a052</b>,\u00a0pages 239\u2013281 (2003)<a href=\"#citeas\">Cite this article</a>\n </p>\n \n \n <div>\n <ul>\n \n <li>\n <p>6125 Accesses</p>\n </li>\n \n \n <li>\n <p>502 Citations</p>\n </li>\n \n \n \n <li>\n <p>15 Altmetric</p>\n </li>\n \n \n <li>\n <p><a href=\"/article/10.1023/A:1024068626366/metrics\">Metrics details</a></p>\n </li>\n </ul>\n </div>\n \n \n \n \n </header>\n </div><div>\n <div><h2>Abstract</h2><p>In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the <i>variability due to the choice of training set</i> and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the <i>training set</i> as well as <i>test examples</i>. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.</p></div>\n \n \n \n \n \n \n \n \n \n \n <div><h2>References</h2><div><ul><li><p>Blake, C., Keogh, E., &amp; Merz, C.-J. (1998). UCI repository of machine learning databases.</p></li><li><p>Breiman, L. (1996). Heuristics of instability and stabilization in model selection. <i>Annals of Statistics</i>,<i>24:6</i>, 2350\u20132383.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Heuristics%20of%20instability%20and%20stabilization%20in%20model%20selection&amp;journal=Annals%20of%20Statistics&amp;volume=24&amp;issue=6&amp;pages=2350-2383&amp;publication_year=1996&amp;author=Breiman%2CL.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Breiman, L., Friedman, J., Olshen, R., &amp; Stone, C. (1984). <i>Classification and regression trees</i>. Wadsworth International Group.</p></li><li><p>Burges, C. (1998). A tutorial on support vector machines for pattern recognition. <i>Data Mining and Knowledge Discovery</i>, <i>2:2</i>, 1\u201347.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial%20on%20support%20vector%20machines%20for%20pattern%20recognition&amp;journal=Data%20Mining%20and%20Knowledge%20Discovery&amp;volume=2&amp;issue=2&amp;pages=1-47&amp;publication_year=1998&amp;author=Burges%2CC.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Devroye, L., Gyr\u00f6fi, L., &amp; Lugosi, G. (1996). <i>A probabilistic theory of pattern recognition</i>. Springer-Verlag.</p></li><li><p>Dietterich, T. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. <i>Neural Computation</i>, <i>10:7</i>, 1895\u20131924.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Approximate%20statistical%20tests%20for%20comparing%20supervised%20classification%20learning%20algorithms&amp;journal=Neural%20Computation&amp;volume=107&amp;pages=1895-1924&amp;publication_year=1998&amp;author=Dietterich%2CT.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Efron, B., &amp; Tibshirani, R. J. (1993). <i>An introduction to the bootstrap</i>. Monographs on Statistics and Applied Probability 57. New York, NY: Chapman &amp; Hall.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20the%20bootstrap&amp;publication_year=1993&amp;author=Efron%2CB.&amp;author=Tibshirani%2CR.%20J.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Everitt, B. (1977). <i>The analysis of contingency tables</i>. London: Chapman &amp; Hall.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20analysis%20of%20contingency%20tables&amp;publication_year=1977&amp;author=Everitt%2CB.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Goutte, C. (1997). Note on free lunches and cross-validation. <i>Neural Computation</i>, <i>9:6</i>, 1053\u20131059.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Note%20on%20free%20lunches%20and%20cross-validation&amp;journal=Neural%20Computation&amp;volume=9&amp;issue=6&amp;pages=1053-1059&amp;publication_year=1997&amp;author=Goutte%2CC.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Hinton, G., Neal, R., Tibshirani, R., &amp; DELVE team members. (1995). Assessing learning procedures using DELVE. Technical report, University of Toronto, Department of Computer Science.</p></li><li><p>Kearns, M., &amp; Ron, D. (1997). Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. <i>Tenth Annual Conference on Computational Learning Theory</i> (pp. 152\u2013162). Morgan Kaufmann.</p></li><li><p>Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. <i>Proceeding of the Fourteenth International Joint Conference on Artificial Intelligence</i> (pp. 1137\u20131143). Morgan Kaufmann.</p></li><li><p>Kolen, J. &amp; Pollack, J. (1991). Back propagation is sensitive to initial conditions. <i>Advances in Neural Information Processing Systems</i> (pp. 860\u2013867). San Francisco, CA: Morgan Kauffmann.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Back%20propagation%20is%20sensitive%20to%20initial%20conditions&amp;pages=860-867&amp;publication_year=1991&amp;author=Kolen%2CJ.&amp;author=Pollack%2CJ.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Nadeau, C., &amp; Bengio, Y. (1999). Inference for the generalisation error. Technical report 99s-25, CIRANO.</p></li><li><p>Vapnik, V. (1982). <i>Estimation of dependences based on empirical data</i>. Berlin: Springer-Verlag.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20of%20dependences%20based%20on%20empirical%20data&amp;publication_year=1982&amp;author=Vapnik%2CV.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>White, H. (1982). Maximum likelihood estimation of misspecified models. <i>Econometrica</i>, <i>50</i>, 1\u201325.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=Maximum%20likelihood%20estimation%20of%20misspecified%20models&amp;journal=Econometrica&amp;volume=50&amp;pages=1-25&amp;publication_year=1982&amp;author=White%2CH.\">\n Google Scholar</a>\u00a0\n </p></li><li><p>Wolpert, D., &amp; Macready,W. (1995). No free lunch theorems for search. Technical report SFI-TR-95-02-010, The Santa Fe Institute.</p></li><li><p>Zhu, H., &amp; Rohwer, R. (1996). No free lunch for cross validation. <i>Neural Computation</i>, <i>8:7</i>, 1421\u20131426.</p><p><a href=\"http://scholar.google.com/scholar_lookup?&amp;title=No%20free%20lunch%20for%20cross%20validation&amp;journal=Neural%20Computation&amp;volume=8&amp;issue=7&amp;pages=1421-1426&amp;publication_year=1996&amp;author=Zhu%2CH.&amp;author=Rohwer%2CR.\">\n Google Scholar</a>\u00a0\n </p></li></ul><p><a href=\"https://citation-needed.springer.com/v2/references/10.1023/A:1024068626366?format=refman&amp;flavour=references\">Download references</a></p></div></div><div><h2>Author information</h2><div><h3>Authors and Affiliations</h3><ol><li><p>Health Canada, AL0900B1, Ottawa, ON, Canada, K1A 0L2</p><p>Claude Nadeau</p></li><li><p>CIRANO and Dept. IRO, Universit\u00e9 de Montr\u00e9al, C.P. 6128 Succ. Centre-Ville, Montr\u00e9al, Qu...",
      "url": "https://link.springer.com/article/10.1023/A:1024068626366"
    },
    {
      "title": "Overfitting \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
      "text": "Overfitting | Machine Learning | Google for Developers[Skip to main content](#main-content)\n* [Machine Learning](https://developers.google.com/machine-learning)\n/\n* English\n* Deutsch\n* Espa\u00f1ol\n* Espa\u00f1ol \u2013Am\u00e9rica Latina\n* Fran\u00e7ais\n* Indonesia\n* Italiano\n* Polski\n* Portugu\u00eas \u2013Brasil\n* Ti\u00ea\u0301ng Vi\u00ea\u0323t\n* T\u00fcrk\u00e7e\n* \u0420\u0443\u0441\u0441\u043a\u0438\u0439* \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430* \u05e2\u05d1\u05e8\u05d9\u05ea* \u0627\u0644\u0639\u0631\u0628\u064a\u0651\u0629* \u0641\u0627\u0631\u0633\u06cc* \u0939\u093f\u0902\u0926\u0940* \u09ac\u09be\u0982\u09b2\u09be* \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22* \u4e2d\u6587\u2013\u7b80\u4f53* \u4e2d\u6587\u2013\u7e41\u9ad4* \u65e5\u672c\u8a9e* \ud55c\uad6d\uc5b4Sign in\n* [ML Concepts](https://developers.google.com/machine-learning/crash-course)\n* [Home](https://developers.google.com/)\n* [Products](https://developers.google.com/products)\n* [Machine Learning](https://developers.google.com/machine-learning)\n* [ML Concepts](https://developers.google.com/machine-learning/crash-course)\n* [Crash Course](https://developers.google.com/machine-learning/crash-course/prereqs-and-prework)\nSend feedback# OverfittingStay organized with collectionsSave and categorize content based on your preferences.\n![Spark icon](https://developers.google.com/_static/images/icons/spark.svg)\n## Page Summary\noutlined\\_flag\n* Overfitting occurs when a model performs well on training data but poorly on new, unseen data.\n* A model is considered to generalize well if it accurately predicts on new data, indicating it hasn&#39;t overfit.\n* Overfitting can be detected by observing diverging loss curves for training and validation sets on a generalization curve.\n* Common causes of overfitting include unrepresentative training data and overly complex models.\n* Dataset conditions for good generalization include examples being independent, identically distributed, and stationary, with similar distributions across partitions.\n[**Overfitting**](https://developers.google.com/machine-learning/glossary#overfitting)means creating a model\nthat matches (*memorizes*) the[**training set**](https://developers.google.com/machine-learning/glossary#training-set)so\nclosely that the model fails to make correct predictions on new data.\nAn overfit model is analogous to an invention that performs well in the lab but\nis worthless in the real world.\n**Tip:**Overfitting is a common problem in machine learning, not an academic\nhypothetical.\nIn Figure 11, imagine that each geometric shape represents a tree&#39;s position\nin a square forest. The blue diamonds mark the locations of healthy trees,\nwhile the orange circles mark the locations of sick trees.\n![Figure 11. This figure contains about 60 dots, half of which are\nhealthy trees and the other half sick trees.\nThe healthy trees are mainly in the northeast quadrant, though a few\nhealthy trees sneak into the northwest quadrants. The sick trees\nare mainly in the southeast quadrant, but a few of the sick trees\nspill into other quadrants.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTrainingSet.svg)**Figure 11.**Training set: locations of healthy and sick trees in a square forest.\nMentally draw any shapes\u2014lines, curves, ovals...anything\u2014to separate the\nhealthy trees from the sick trees. Then, expand the next line to examine\none possible separation.\n#### Expand to see one possible solution (Figure 12).\n![Figure 12. The same arrangement of healthy and sick trees as in\nFigure 11. However, a model of complex geometric shapes separates\nnearly all of the healthy trees from the sick trees.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTrainingSetComplexModel.svg)**Figure 12.**A complex model for distinguishing sick from healthy trees.\nThe complex shapes shown in Figure 12 successfully categorized all but two of\nthe trees. If we think of the shapes as a model, then this is a fantastic\nmodel.\nOr is it? A truly excellent model successfully categorizes*new*examples.\nFigure 13 shows what happens when that same model makes predictions on new\nexamples from the test set:\n![Figure 13. A new batch of healthy and sick trees overlaid on the\nmodel shown in Figure 12. The model miscategorizes many of the\ntrees.](https://developers.google.com/static/machine-learning/crash-course/images/TreesTestSetComplexModel.svg)**Figure 13.**Test set: a complex model for distinguishing sick from healthy trees.\nSo, the complex model shown in Figure 12 did a great job on the training set\nbut a pretty bad job on the test set. This is a classic case of a model*overfitting*to the training set data.\n## Fitting, overfitting, and underfitting\nA model must make good predictions on*new*data.\nThat is, you&#39;re aiming to create a model that &quot;fits&quot; new data.\nAs you&#39;ve seen, an overfit model makes excellent predictions on the training\nset but poor predictions on new data. An[**underfit**](https://developers.google.com/machine-learning/glossary#underfitting)model\ndoesn&#39;t even make good predictions on the training data. If an overfit model is\nlike a product that performs well in the lab but poorly in the real world,\nthen an underfit model is like a product that doesn&#39;t even do well in\nthe lab.\n![Figure 14. Cartesian plot. X-axis is labeled 'quality of predictions\non training set.' Y-axis is labeled 'quality of predictions on\nreal-world data.' A curve starts at the origin and rises gradually,\nbut then falls just as quickly. The lower-left portion of the curve\n(low quality of predictions on real-world data and low quality of\npredictions on training set) is labeled 'underfit models.' The\nlower-right portion of the curve (low quality of predictions on\nreal-world data but high quality of predictions on training set)\nis labeled 'overfit models.' The peak of the curve (high quality\nof predictions on real-world data and medium quality of predictions\non training set) is labeled 'fit models.'](https://developers.google.com/static/machine-learning/crash-course/images/underfit_fit_overfit.svg)**Figure 14.**Underfit, fit, and overfit models.\n[**Generalization**](https://developers.google.com/machine-learning/glossary#generalization)is the\nopposite of overfitting. That is, a model that*generalizes well*makes good\npredictions on new data. Your goal is to create a model that generalizes\nwell to new data.\n## Detecting overfitting\nThe following curves help you detect overfitting:\n* loss curves\n* generalization curves\nA[**loss curve**](https://developers.google.com/machine-learning/glossary#loss-curve)plots a model&#39;s loss\nagainst the number of training iterations.\nA graph that shows two or more loss curves is called a[**generalization\ncurve**](https://developers.google.com/machine-learning/glossary#generalization-curve). The following\ngeneralization curve shows two loss curves:\n![Figure 15. The loss function for the training set gradually\ndeclines. The loss function for the validation set also declines,\nbut then it starts to rise after a certain number of iterations.](https://developers.google.com/static/machine-learning/crash-course/images/RegularizationTwoLossFunctions.png)**Figure 15.**A generalization curve that strongly implies overfitting.\nNotice that the two loss curves behave similarly at first and then diverge.\nThat is, after a certain number of iterations, loss declines or\nholds steady (converges) for the training set, but increases\nfor the validation set. This suggests overfitting.\nIn contrast, a generalization curve for a well-fit model shows two loss curves\nthat have similar shapes.\n## What causes overfitting?\nVery broadly speaking, overfitting is caused by one or both of the following\nproblems:\n* The training set doesn&#39;t adequately represent real life data (or the\nvalidation set or test set).\n* The model is too complex.## Generalization conditions\nA model trains on a training set, but the real test of a model&#39;s worth is how\nwell it makes predictions on new examples, particularly on real-world data.\nWhile developing a model, your test set serves as a proxy for real-world data.\nTraining a model that generalizes well implies the following dataset conditions:\n* Examples must be[**independently and identically distributed**](https://developers.google.com/machine-learning/glossary#independentl...",
      "url": "https://developers.google.com/machine-learning/crash-course/overfitting/overfitting"
    },
    {
      "title": "Chapter 20 Evaluating Machine Learning Models and Their Diagnostic Value",
      "text": "Evaluating Machine Learning Models and Their Diagnostic Value - Machine Learning for Brain Disorders - NCBI Bookshelf\n![U.S. flag](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/favicons/favicon-57.png)\nAn official website of the United States government\nHere's how you know\n![Dot gov](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-dot-gov.svg)\n**The .gov means it's official.**\nFederal government websites often end in .gov or .mil. Before\nsharing sensitive information, make sure you're on a federal\ngovernment site.\n![Https](https://www.ncbi.nlm.nih.gov/coreutils/uswds/img/icon-https.svg)\n**The site is secure.**\nThe**https://**ensures that you are connecting to the\nofficial website and that any information you provide is encrypted\nand transmitted securely.\n[![NIH NLM Logo](https://www.ncbi.nlm.nih.gov/coreutils/nwds/img/logos/AgencyLogo.svg)](https://www.ncbi.nlm.nih.gov/)\n[Log in](https://account.ncbi.nlm.nih.gov)Show account info\nClose#### Account\nLogged in as:\n**username**\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* [Log out](https://www.ncbi.nlm.nih.gov/account/signout/)\n[Access keys](https://www.ncbi.nlm.nih.gov/guide/browsers/#ncbi_accesskeys)[NCBI Homepage](https://www.ncbi.nlm.nih.gov)[MyNCBI Homepage](https://www.ncbi.nlm.nih.gov/myncbi/)[Main Content](#maincontent)[Main Navigation](#)\n# [Bookshelf](https://www.ncbi.nlm.nih.gov/books/)\n## Search databaseBooksAll DatabasesAssemblyBiocollectionsBioProjectBioSampleBooksClinVarConserved DomainsdbGaPdbVarGeneGenomeGEO DataSetsGEO ProfilesGTRIdentical Protein GroupsMedGenMeSHNLM CatalogNucleotideOMIMPMCProteinProtein ClustersProtein Family ModelsPubChem BioAssayPubChem CompoundPubChem SubstancePubMedSNPSRAStructureTaxonomyToolKitToolKitAllToolKitBookgh\nSearch term\nSearch\n* [Browse Titles](https://www.ncbi.nlm.nih.gov/books/browse/)\n* [Advanced](https://www.ncbi.nlm.nih.gov/books/advanced/)\n* [Help](https://www.ncbi.nlm.nih.gov/books/NBK3833/)\n* [Disclaimer](https://www.ncbi.nlm.nih.gov/books/about/disclaimer/)\nNCBI Bookshelf. A service of the National Library of Medicine, National Institutes of Health.\nColliot O, editor. Machine Learning for Brain Disorders [Internet]. New York, NY: Humana; 2023. doi: 10.1007/978-1-0716-3195-9\\_20\n[![Cover of Machine Learning for Brain Disorders](https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/bookshelf/thumbs/th-spr9781071631959-lrg.png)](https://www.ncbi.nlm.nih.gov/books/n/spr9781071631959/)\n## Machine Learning for Brain Disorders [Internet].\n[Show details](#__NBK597473_dtls__)\nColliot O, editor.\nNew York, NY:[Humana](http://www.springer.com); 2023.\n* [Contents](https://www.ncbi.nlm.nih.gov/books/n/spr9781071631959/)\nSearch term\n[&lt; Prev](https://www.ncbi.nlm.nih.gov/books/n/spr9781071631959/p4/)[Next &gt;](https://www.ncbi.nlm.nih.gov/books/n/spr9781071631959/ch21/)\n# Chapter 20Evaluating Machine Learning Models and Their Diagnostic Value\nGael VaroquauxandOlivier Colliot.\n[Author Information and Affiliations](#__NBK597473_ai__)\n#### Authors\nGael Varoquaux![corresponding author](https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif)1andOlivier Colliot2.\n#### Affiliations\n1Soda, Inria, Saclay,France\nEmail:[rf.airni@xuauqorav.leag](mailto:dev@null)\n2Sorbonne Universit&#x000e9;, Institut du Cerveau &#x02013; Paris Brain Institute &#x02013; ICM, CNRS, Inria, Inserm, AP-HP, H&#x000f4;pital de la Piti&#x000e9;-Salp&#x000ea;tri&#x000e8;re, Paris,France\n![corresponding author](https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/corrauth.gif)Corresponding author.\nPublished online: July 23, 2023.\nThis chapter describes model validation, a crucial part of machine learning whether it is to select the best model or to assess performance of a given model. We start by detailing the main performance metrics for different tasks (classification, regression), and how they may be interpreted, including in the face of class imbalance, varying prevalence, or asymmetric cost&#x02013;benefit trade-offs. We then explain how to estimate these metrics in an unbiased manner using training, validation, and test sets. We describe cross-validation procedures&#x02014;to use a larger part of the data for both training and testing&#x02014;and the dangers of data leakage&#x02014;optimism bias due to training data contaminating the test set. Finally, we discuss how to obtain confidence intervals of performance metrics, distinguishing two situations: internal validation or evaluation of learning algorithms and external validation or evaluation of resulting prediction models.\n#### Key words:\nValidation, Performance metrics, Cross-validation, Data leakage, External validation\n## 1. Introduction\nA machine learning (ML) model is validated by evaluating its prediction performance. Ideally, this evaluation should be representative of how the model would perform when deployed in a real-life setting. This is an ambitious goal that goes beyond the settings of academic research. Indeed, a perfect validation would probe robustness to any possible variation of the input data that may include different acquisition devices and protocols, different practices that vary from one country to another, from one hospital to another, and even from one physician to another. A less ambitious goal for validation is to provide an unbiased estimate of the model performance on new&#x02014;never before seen&#x02014;data similar to that used for training (but not the same data!). By similar, we mean data that have similar clinical or sociodemographic characteristics and that have been acquired using similar devices and protocols. To go beyond such*internal validity*, external validation would evaluate generalization to data from different sources (for example, another dataset, data from another hospital).\nThis chapter addresses the following questions. How to quantify the performance of the model? This will lead us to present, in Subheading[2](#ch20.Sec2), different performance metrics that are adequate for different ML tasks (classification, regression, &#x02026;). How to estimate these performance metrics? This will lead to the presentation of different validation strategies (Subheading[3](#ch20.Sec18)). We will also explain how to derive confidence intervals for the estimated performance metrics, drawing the distinction between evaluating a learning algorithm or a resulting prediction model. We will present various caveats that pertain to the use of performance metrics on medical data as well as to data leakage, which can be particularly insidious.\n## 2. Performance Metrics\nMetrics allow to quantify the performance of an ML model. In this section, we describe metrics for classification and regression tasks. Other tasks (segmentation, generation, detection,&#x02026;) can use some of these but will often require other metrics that are specific to these tasks. The reader may refer to Chap.[13](http://dx.crossref.org/10.1007/978-1-0716-3195-9_13)for metrics dedicated to segmentation and to Subheading[6](http://dx.crossref.org/10.1007/978-1-0716-3195-9_23)of Chap.[23](http://dx.crossref.org/10.1007/978-1-0716-3195-9_23)for metrics dedicated to segmentation, classification, and detection.\n### 2.1. Metrics for Classification\n#### 2.1.1. Binary Classification\nFor classification tasks, the results can be summarized in a matrix called the confusion matrix (Fig.[1](https://www.ncbi.nlm.nih.gov/books/NBK597473/figure/ch20.Fig1/?report=objectonly)). For binary classification, the confusion matrix divides the test samples into four categories, depending on their true and predicted labels:\n* **True Positives (TP)**: Samples for which the true and predicted labels are both 1. Example: The patient has cancer (1), and the model classifies this sample as cancer (1).\n* **True Negatives (TN)**: Samples for which the true and predicted labels are both 0. Example: The pat...",
      "url": "https://www.ncbi.nlm.nih.gov/books/NBK597473"
    }
  ]
}