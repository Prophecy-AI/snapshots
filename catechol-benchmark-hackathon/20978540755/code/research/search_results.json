{
  "query": "How can domain adaptation techniques reduce distribution shift between training and test data in chemical property prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Domain adaptation tackles the covariate\u2011shift that arises when the molecular space covered by the training set differs from that encountered at test time.  In practice the most common strategy is to learn a representation that is **invariant across domains**: a feature extractor is trained jointly with a domain\u2011discriminator that tries to predict whether a molecule comes from the source (training) or the target (test) distribution, while the extractor is penalised for providing discriminative cues.  This adversarial objective forces the model to map both domains onto a shared latent space, so that a downstream predictor trained on source labels can be applied to target molecules with reduced bias\u202f([arxiv.org](https://arxiv.org/abs/1505.07818)).  Recent work on transformer\u2011based chemical language models shows that adding such a domain\u2011adaptation stage after large\u2011scale pre\u2011training on datasets like ZINC or ChEMBL yields consistent gains on ADME endpoints, even when the pre\u2011training data size is saturated\u202f([arxiv.org](https://arxiv.org/html/2503.03360v1)).\n\nBeyond pure adversarial alignment, several complementary techniques have been proposed for chemical property prediction.  **Meta\u2011learning with unlabeled data** interpolates between in\u2011distribution (ID) and out\u2011of\u2011distribution (OOD) compounds, allowing the model to \u201clearn how to learn\u201d on the target domain and thereby mitigate covariate shift\u202f([arxiv.org](https://arxiv.org/abs/2506.11877)).  **Task\u2011specific adaptation** augments a pretrained graph neural network with auxiliary heads that are fine\u2011tuned on the target task while keeping the backbone largely frozen, which improves generalisation to novel chemotypes\u202f([jcheminf.biomedcentral.com](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7)).  Finally, **transferability mapping** quantifies the similarity between source and target property datasets via a principal\u2011gradient metric; selecting source tasks that are close in this space reduces the risk of negative transfer and makes the adaptation more effective\u202f([nature.com](https://www.nature.com/articles/s42004-024-01169-4)).\n\nTogether, these domain\u2011adaptation strategies align feature distributions, exploit unlabeled target data, and choose appropriate source tasks, thereby narrowing the gap between training and test chemical spaces and delivering more reliable property predictions for unseen molecules.",
      "url": ""
    },
    {
      "title": "Transformers for Molecular Property Prediction: Domain Adaptation Efficiently Improves Performance.",
      "text": "Transformers for Molecular Property Prediction: Domain Adaptation Efficiently Improves Performance.\n# Transformers for Molecular Property Prediction: Domain Adaptation Efficiently Improves Performance.\nAfnan SultanMax Rausch-DupontShahrukh KhanSpoken Language Systems, Saarland Informatics Campus, Saarland University, Saarbr\u00fccken, GermanyMedical Faculty, Saarland University, 66421, Homburg, Saarland, Germany; Center for Bioinformatics, Saarland University, 66123,\nSaarbr\u00fccken, Saarland, GermanyOlga KalininaMedical Faculty, Saarland University, 66421, Homburg, Saarland, Germany; Center for Bioinformatics, Saarland University, 66123,\nSaarbr\u00fccken, Saarland, GermanyAndrea VolkamerData Driven Drug Design, Center for Bioinformatics, Saarland University, Saarbr\u00fccken, GermanyDietrich Klakow\n###### Abstract\nOver the past six years, molecular transformer models have become part of the computational toolbox for drug design tasks. Most of the current transformer-based chemical language models are pre-trained on millions to billions of molecules from unlabeled datasets like ZINC, or ChEMBL. However, the improvement from such scaling in dataset size is not confidently linked to improved molecular property prediction.\nThe aim of this study was to investigate and overcome some of the limitations of transformer models in predicting molecular properties. Specifically, we examine the impact of pre-training dataset size and diversity on the performance of transformer models, and investigate the use of domain adaptation as a technique for improving model performance.\nFirst, our findings indicate that increasing pre-training dataset size beyond\u223c400\u2062Ksimilar-toabsent400\ud835\udc3e\\\\sim 400K\u223c 400 italic\\_Kmolecules (i.e., 30%) from the pre-training dataset does not result in a significant improvement on four ADME endpoints, namely, solubility, permeability, microsomal stability, and plasma protein binding.\nFurther analysis indicates that the limited benefits of increasing the pre-training dataset size may be attributed to its lower diversity in Murcko scaffolds compared to the downstream datasets.\nSecond, our results demonstrate that using domain adaptation by further training the transformer model on a small set of domain-relevant molecules, i.e., a few hundred to a few thousand, using multi-task regression of physicochemical properties was sufficient to significantly improve performance for three out of the four investigated ADME endpoints (P-value&lt;0.001absent0.001&lt;0.001&lt; 0.001).\nFinally, we observe that a model pre-trained on\u223c400similar-toabsent400\\\\sim 400\u223c 400K molecules and domain adopted on a few hundred/thousand molecules perform similarly (P-value&gt;0.05absent0.05&gt;0.05&gt; 0.05) to more complicated transformer models like MolBERT> [\n[> 1\n](https://arxiv.org/html/2503.03360v1#bib.bib1)> ]\n(pre-trained on\u223c1.3similar-toabsent1.3\\\\sim 1.3\u223c 1.3M molecules) and MolFormer> [\n[> 2\n](https://arxiv.org/html/2503.03360v1#bib.bib2)> ]\n(pre-trained on\u223c100similar-toabsent100\\\\sim 100\u223c 100M molecules). A comparison to a random forest model trained on basic physicochemical properties showed similar performance to the examined transformer models.\nWe believe that current transformer models can be improved through further systematic analysis on pre-training and downstream data, pre-training objectives, and scaling laws, ultimately leading to better and more helpful models.\n\u2020\u2020footnotetext:\u2020\u2020\\\\dagger\u2020Co-first authors.\u2020\u2020footnotetext:\u00a7\u00a7\\\\S\u00a7Co-corresponding authors.\n## 1Introduction\nMolecular property prediction (MPP)is a task at the heart of diverse cheminformatics and drug design challenges. Molecular properties can range from inherent physical features of the molecule like lipophilicity or solubility to more complex results of the physiological properties like toxic effects of the molecule on an organism> [\n[> 3\n](https://arxiv.org/html/2503.03360v1#bib.bib3)> ]\n. Supervised learning (SL) has been used to map predefined or heuristic molecular descriptors to such properties> [\n[> 3\n](https://arxiv.org/html/2503.03360v1#bib.bib3)> , [> 4\n](https://arxiv.org/html/2503.03360v1#bib.bib4)> ]\n. However, freely available datasets for MPP tasks usually consist of only a few hundred to a few thousand molecules> [\n[> 3\n](https://arxiv.org/html/2503.03360v1#bib.bib3)> , [> 5\n](https://arxiv.org/html/2503.03360v1#bib.bib5)> ]\ndue to the complex and expensive experimental processes to generate the data> [\n[> 6\n](https://arxiv.org/html/2503.03360v1#bib.bib6)> ]\n.\nExisting property prediction methods suffer from limitations regarding data representation. On the one end, human-made descriptors like predefined fingerprints require expert knowledge and are restricted to known rules or patterns> [\n[> 7\n](https://arxiv.org/html/2503.03360v1#bib.bib7)> ]\n. On the other end, data-driven methods like deep learning require a large amount of labeled data> [\n[> 8\n](https://arxiv.org/html/2503.03360v1#bib.bib8)> ]\n.\nSelf-supervised learning (SSL) has been used as an alternative to learning from labeled data as in supervised learning> [\n[> 9\n](https://arxiv.org/html/2503.03360v1#bib.bib9)> ]\n. In SSL, the model is initially trained on large unlabeled data to learn intrinsic relationships within the input. These relationships can be obtained by using tasks like recovering parts of the input using information from other parts of the input> [\n[> 10\n](https://arxiv.org/html/2503.03360v1#bib.bib10)> , [> 11\n](https://arxiv.org/html/2503.03360v1#bib.bib11)> ]\n. Such an SSL model is then thought of as a foundation model that can be generalized to different downstream tasks> [\n[> 11\n](https://arxiv.org/html/2503.03360v1#bib.bib11)> , [> 12\n](https://arxiv.org/html/2503.03360v1#bib.bib12)> ]\n. The past decade has seen a breakthrough in the field of Natural Language Processing (NLP) with the introduction of the SSL-based transformer model> [\n[> 13\n](https://arxiv.org/html/2503.03360v1#bib.bib13)> ]\n, which has inspired multiple works to adopt similar schemes for sequence-based representations of molecules> [\n[> 14\n](https://arxiv.org/html/2503.03360v1#bib.bib14)> , [> 15\n](https://arxiv.org/html/2503.03360v1#bib.bib15)> , [> 16\n](https://arxiv.org/html/2503.03360v1#bib.bib16)> ]\n.\nThetransformer model> [\n[> 13\n](https://arxiv.org/html/2503.03360v1#bib.bib13)> ]\nis a sequence-to-sequence model composed of an encoder-decoder architecture and trained on the next token prediction (NTP) objective. In this objective, the model is optimized such that the decoder model correctly predicts the next token (i.e., subword) given the previous tokens. While the transformer model was built for machine translation, BERT (Bidirectional Encoder Representations from Transformers)> [\n[> 17\n](https://arxiv.org/html/2503.03360v1#bib.bib17)> ]\nintroduced the concept of transfer learning. BERT was pre-trained on a large corpus of generic, unlabeled data (e.g., Wikipedia), and then fine-tuned on smaller, labeled downstream datasets to generalize across various tasks. One of the pre-training objectives used in BERT> [\n[> 17\n](https://arxiv.org/html/2503.03360v1#bib.bib17)> ]\nis masked language modeling (MLM). In this objective, a percentage of the tokens of each sequence is masked randomly, and the model is optimized to correctly predict these masked tokens.\nAlthough the transfer learning scheme employed in BERT has yielded promising results for numerous tasks> [\n[> 17\n](https://arxiv.org/html/2503.03360v1#bib.bib17)> , [> 18\n](https://arxiv.org/html/2503.03360v1#bib.bib18)> ]\n, its effectiveness is limited when applied to downstream tasks that fall outside the domain of the pre-training corpus.\nFor example, a model trained on Wikipedia will not be able to capture the nuances of medical or legal languages.\nTo address this, specialized models have been pre-trained on domain-specific corpus rather than general-purpose> [\n[> 19\n](https://arxiv.org/html/2503.03360v1#bib.bib19)> , [> 20\n](https://arxiv.org/html/2503.03360v1#bib.bib20)> ]\n.\nHowever, suc...",
      "url": "https://arxiv.org/html/2503.03360v1"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Statistics > Machine Learning",
      "text": "[1505.07818] Domain-Adversarial Training of Neural Networks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1505.07818\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1505.07818**(stat)\n[Submitted on 28 May 2015 ([v1](https://arxiv.org/abs/1505.07818v1)), last revised 26 May 2016 (this version, v4)]\n# Title:Domain-Adversarial Training of Neural Networks\nAuthors:[Yaroslav Ganin](https://arxiv.org/search/stat?searchtype=author&amp;query=Ganin,+Y),[Evgeniya Ustinova](https://arxiv.org/search/stat?searchtype=author&amp;query=Ustinova,+E),[Hana Ajakan](https://arxiv.org/search/stat?searchtype=author&amp;query=Ajakan,+H),[Pascal Germain](https://arxiv.org/search/stat?searchtype=author&amp;query=Germain,+P),[Hugo Larochelle](https://arxiv.org/search/stat?searchtype=author&amp;query=Larochelle,+H),[Fran\u00e7ois Laviolette](https://arxiv.org/search/stat?searchtype=author&amp;query=Laviolette,+F),[Mario Marchand](https://arxiv.org/search/stat?searchtype=author&amp;query=Marchand,+M),[Victor Lempitsky](https://arxiv.org/search/stat?searchtype=author&amp;query=Lempitsky,+V)\nView a PDF of the paper titled Domain-Adversarial Training of Neural Networks, by Yaroslav Ganin and 7 other authors\n[View PDF](https://arxiv.org/pdf/1505.07818)> > Abstract:\n> We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application. Comments:|Published in JMLR:[this http URL](http://jmlr.org/papers/v17/15-239.html)|\nSubjects:|Machine Learning (stat.ML); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:1505.07818](https://arxiv.org/abs/1505.07818)[stat.ML]|\n|(or[arXiv:1505.07818v4](https://arxiv.org/abs/1505.07818v4)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1505.07818](https://doi.org/10.48550/arXiv.1505.07818)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|Journal of Machine Learning Research 2016, vol. 17, p. 1-35|\n## Submission history\nFrom: Pascal Germain [[view email](https://arxiv.org/show-email/58512ed6/1505.07818)]\n**[[v1]](https://arxiv.org/abs/1505.07818v1)**Thu, 28 May 2015 19:34:53 UTC (9,333 KB)\n**[[v2]](https://arxiv.org/abs/1505.07818v2)**Fri, 29 May 2015 13:32:12 UTC (9,334 KB)\n**[[v3]](https://arxiv.org/abs/1505.07818v3)**Fri, 27 Nov 2015 16:57:53 UTC (5,139 KB)\n**[v4]**Thu, 26 May 2016 19:56:08 UTC (5,140 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Domain-Adversarial Training of Neural Networks, by Yaroslav Ganin and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1505.07818)\n* [TeX Source](https://arxiv.org/src/1505.07818)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1505.07818&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1505.07818&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2015-05](https://arxiv.org/list/stat.ML/2015-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/1505.07818?context=cs)\n[cs.LG](https://arxiv.org/abs/1505.07818?context=cs.LG)\n[cs.NE](https://arxiv.org/abs/1505.07818?context=cs.NE)\n[stat](https://arxiv.org/abs/1505.07818?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1505.07818)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1505.07818)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1505.07818)\n### [1 blog link](https://arxiv.org/tb/1505.07818)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/1505.07818&amp;description=Domain-Adversarial Training of Neural Networks>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/1505.07818&amp;title=Domain-Adversarial Training of Neural Networks>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Tog...",
      "url": "https://arxiv.org/abs/1505.07818"
    },
    {
      "title": "Enhancing molecular property prediction with auxiliary learning and task-specific adaptation",
      "text": "Search all BMC articles\n\nSearch\n\nEnhancing molecular property prediction with auxiliary learning and task-specific adaptation\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00880-7.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00880-7.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00880-7.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00880-7.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 24 July 2024\n\n# Enhancing molecular property prediction with auxiliary learning and task-specific adaptation\n\n- [Vishal Dey](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Vishal-Dey-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1) &\n- [Xia Ning](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Xia-Ning-Aff1-Aff2-Aff3) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff3)\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a085 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 1535 Accesses\n\n- 2 Citations\n\n- 1 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7/metrics)\n\n\n## Abstract\n\nPretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients (\\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\)), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.\n\n**Scientific contribution**\n\nWe introduce a novel framework for adapting pretrained GNNs to molecular tasks using auxiliary learning to address the critical issue of negative transfer. Leveraging novel gradient surgery techniques such as \\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\), the proposed adaptation framework represents a significant departure from the dominant pretraining fine-tuning approach for molecular GNNs. Our contributions are significant for drug discovery research, especially for tasks with limited data, filling a notable gap in the efficient adaptation of pretrained models for molecular GNNs.\n\n## Introduction\n\nAccurate prediction of molecular properties is pivotal in drug discovery \\[ [39](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR39)\\], as it accelerates the identification of potential molecules with desired properties. Developing computational models for property prediction relies on learning effective representations of molecules \\[ [5](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR5)\\]. In this regard, Graph Neural Networks (GNNs) have shown impressive results in learning effective representations for molecular property prediction tasks \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR12), [37](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR37)\\]. Inspired by the paradigm of pretraining followed by fine-tuning, widely recognized for its impact in natural language understanding \\[ [27](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR27), [38](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR38)\\], molecular GNNs are often pretrained \\[ [17](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR17)\\] on a large corpus of molecules. Such a corpus might encompass irrelevant data for the target property prediction task. This can lead the GNNs to learn features that do not benefit the target task. Consequently, pretrained GNNs are fine-tuned with the target task to encode task-specific features. However, vanilla fine-tuning can potentially lead to poor generalization, particularly when dealing with diverse downstream tasks, limited data, and the need to generalize across varying scaffolds \\[ [40](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR40)\\].\n\nTo improve generalization, auxiliary learning has recently garnered attention \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR8), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR20), [21](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR21)\\]. Auxiliary learning leverages informative signals from self-supervised tasks on unlabeled data, to improve the performance of the target tasks. However, its application in the context of molecular graphs, specifically for molecular property prediction, remains largely unexplored. Following this line of work, in this paper, we explore how to adapt pretrained molecular GNNs by combining widely-used self-supervised tasks with the target task using respective task-specific data (with self-supervised and target task labels). However, a critical challenge in such an adaptation is caused by negative transfer \\[ [29](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR29)\\], where auxiliary tasks might impede rather than aid the target task \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR9), [30](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR30)\\].\n\nTo address this challenge, we develop novel gradient surgery-based adaptation strategies, referred to as Rotation of Conflicting Gradients (\\\\(\\\\mathop {\\\\texttt{RCGrad}}\\\\limits\\\\)) and Bi-level Optimization with Gradient Rotation (\\\\(\\\\mathop {\\\\texttt{BLO}\\\\text {+}\\\\texttt{RCGrad}}\\\\limits\\\\)). Such strategies mitigate negative transfer from auxiliary tasks by learning to align conflicting gradients. Overall, our adaptation strategies improved the target task performance by as much as 7.7% over vanilla fine-tuning. Moreover, our findings indicate that the developed adaptation strategies are particularly effective in tasks with limited labeled data, which is a common challenge in molecular property prediction tasks. Our comprehensive investigation of multiple adaptation strategies for pretrained molecular GNNs represents a notable contribution in addressing the limited benefit of pretrained GNNs \\[ [34](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7#ref-CR34)\\], and in improving generalizability across a diverse set of downstream tasks with limited data.\n\n## Related work\n\n### Pretraining an...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00880-7"
    },
    {
      "title": "Fast and effective molecular property prediction with transferability map",
      "text": "Fast and effective molecular property prediction with transferability map\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n[Download PDF](https://www.nature.com/articles/s42004-024-01169-4.pdf)\n\n### Subjects\n\n- [Computational chemistry](https://www.nature.com/subjects/computational-chemistry)\n- [Drug discovery and development](https://www.nature.com/subjects/drug-discovery-and-development)\n\n## Abstract\n\nEffective transfer learning for molecular property prediction has shown considerable strength in addressing insufficient labeled molecules. Many existing methods either disregard the quantitative relationship between source and target properties, risking negative transfer, or require intensive training on target tasks. To quantify transferability concerning task-relatedness, we propose Principal Gradient-based Measurement (PGM) for transferring molecular property prediction ability. First, we design an optimization-free scheme to calculate a principal gradient for approximating the direction of model optimization on a molecular property prediction dataset. We have analyzed the close connection between the principal gradient and model optimization through mathematical proof. PGM measures the transferability as the distance between the principal gradient obtained from the source dataset and that derived from the target dataset. Then, we perform PGM on various molecular property prediction datasets to build a quantitative transferability map for source dataset selection. Finally, we evaluate PGM on multiple combinations of transfer learning tasks across 12 benchmark molecular property prediction datasets and demonstrate that it can serve as fast and effective guidance to improve the performance of a target task. This work contributes to more efficient discovery of drugs, materials, and catalysts by offering a task-relatedness quantification prior to transfer learning and understanding the relationship between chemical properties.\n\n### Similar content being viewed by others\n\n### [Knowledge graph-enhanced molecular contrastive learning with functional prompt](https://www.nature.com/articles/s42256-023-00654-0?fromPaywallRec=false)\n\nArticleOpen access04 May 2023\n\n### [A knowledge-guided pre-training framework for improving molecular representation learning](https://www.nature.com/articles/s41467-023-43214-1?fromPaywallRec=false)\n\nArticleOpen access21 November 2023\n\n### [Enhancing property and activity prediction and interpretation using multiple molecular graph representations with MMGX](https://www.nature.com/articles/s42004-024-01155-w?fromPaywallRec=false)\n\nArticleOpen access05 April 2024\n\n## Introduction\n\nMolecular property prediction, which involves identifying molecules with desired properties[1](https://www.nature.com/articles/s42004-024-01169-4#ref-CR1), [2](https://www.nature.com/articles/s42004-024-01169-4#ref-CR2), poses a critical challenge prevalent across various scientific fields. It holds particular significance in chemistry for designing drugs, catalysts, and materials. In recent years, artificial intelligence (AI) technologies have come mainstream in this area, and AI-guided chemical design can efficiently explore chemical space while improving performance based on experimental feedback, showing promise from laboratory research to real-world industry applications[3](https://www.nature.com/articles/s42004-024-01169-4#ref-CR3). However, it is common that the experimental data size is small as producing labeled data requires time-consuming and expensive experiments[4](https://www.nature.com/articles/s42004-024-01169-4#ref-CR4), [5](https://www.nature.com/articles/s42004-024-01169-4#ref-CR5). In contrast, transfer learning[6](https://www.nature.com/articles/s42004-024-01169-4#ref-CR6) has become a powerful paradigm for addressing data scarcity problem by exploiting the knowledge from related datasets across fields such as natural language processing[7](https://www.nature.com/articles/s42004-024-01169-4#ref-CR7), [8](https://www.nature.com/articles/s42004-024-01169-4#ref-CR8), computer vision[9](https://www.nature.com/articles/s42004-024-01169-4#ref-CR9), [10](https://www.nature.com/articles/s42004-024-01169-4#ref-CR10), and biomedcine[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12). In chemistry, transfer learning leverages pre-trained models on extensive or related datasets to facilitate efficient exploration of vast chemical space[13](https://www.nature.com/articles/s42004-024-01169-4#ref-CR13), [14](https://www.nature.com/articles/s42004-024-01169-4#ref-CR14) for various downstream tasks. It has been used to predict properties[15](https://www.nature.com/articles/s42004-024-01169-4#ref-CR15), [16](https://www.nature.com/articles/s42004-024-01169-4#ref-CR16), plan synthesis[17](https://www.nature.com/articles/s42004-024-01169-4#ref-CR17), [18](https://www.nature.com/articles/s42004-024-01169-4#ref-CR18), and explore the space of chemical reactions[19](https://www.nature.com/www.nature.com#ref-CR19), [20](https://www.nature.com/www.nature.com#ref-CR20), [21](https://www.nature.com/www.nature.com#ref-CR21), [22](https://www.nature.com/articles/s42004-024-01169-4#ref-CR22).\n\nTransfer learning can enhance molecular property prediction in limited data sets by borrowing knowledge from sufficient source data sets, thus improving both model accuracy and computation efficiency. Although several previous works have explored the power of transfer learning to enhance molecular property prediction[11](https://www.nature.com/articles/s42004-024-01169-4#ref-CR11), [12](https://www.nature.com/articles/s42004-024-01169-4#ref-CR12), [23](https://www.nature.com/www.nature.com#ref-CR23), [24](https://www.nature.com/www.nature.com#ref-CR24), [25](https://www.nature.com/articles/s42004-024-01169-4#ref-CR25), challenges remain. One major challenge is negative transfer, which occurs when the performance after transfer learning is adversely affected due to minimal similarity between the source and target tasks[26](https://www.nature.com/articles/s42004-024-01169-4#ref-CR26), [27](https://www.nature.com/articles/s42004-024-01169-4#ref-CR27). For example, Hu et al.[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23) observed that pretrained GNN (at both node-level and graph-level) performed well but yielded negative transfer when pretrained at the level of either entire graphs or individual nodes. Additionally, some supervised pre-training tasks unrelated to the downstream task of interest can even degrade the downstream performance[23](https://www.nature.com/articles/s42004-024-01169-4#ref-CR23), [28](https://www.nature.com/articles/s42004-024-01169-4#ref-CR28).\n\nNegative transfer primarily stems from suboptimal model and layer choices, as well as insufficient task relatedness, highlighting the need to evaluate transferability prior to applying transfer learning. In computer vision, some researchers have recently focused on selecting the best model from a pool of options by estimating the transferability of each model[29](https://www.nature.com/www.nature.com#ref-CR29), [30](https://www.nature.com/www.nature.com#ref-CR30), [31](https://www.nature.com/www.nature.com#ref-CR31), [32](https://www.nature.com/articles/s42004-024-01169-4#ref-CR32). In molecular property prediction, recent efforts involve investigating the relatedness of the source task to the target task. To maximize the performance on a target task and prevent negative transfer, existing methods mainly rely on a molecular distance metric to measure the similarity of molecules, such as Tanimoto coefficient (based on molecular fingerprint)[33](https://www.nature.com/articles/s42004-024-01169-4#ref-CR33), [34](https://www.nature.com/articles/s42004-024-01169-4#ref-CR34) and a chemical distance measure (based on fingerprint and subgraph)[35](https://www.nature.com/articles/s42004-024-01169-...",
      "url": "https://www.nature.com/articles/s42004-024-01169-4"
    },
    {
      "title": "Understanding and Mitigating Distribution Shifts \n For Machine Learning Force Fields",
      "text": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
      "url": "https://arxiv.org/html/2503.08674v2"
    },
    {
      "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
      "text": "# Physics > Chemical Physics\n\n**arXiv:2306.10060** (physics)\n\n\\[Submitted on 14 Jun 2023 ( [v1](https://arxiv.org/abs/2306.10060v1)), last revised 16 Apr 2024 (this version, v4)\\]\n\n# Title:MUBen: Benchmarking the Uncertainty of Molecular Representation Models\n\nAuthors: [Yinghao Li](https://arxiv.org/search/physics?searchtype=author&query=Li,+Y), [Lingkai Kong](https://arxiv.org/search/physics?searchtype=author&query=Kong,+L), [Yuanqi Du](https://arxiv.org/search/physics?searchtype=author&query=Du,+Y), [Yue Yu](https://arxiv.org/search/physics?searchtype=author&query=Yu,+Y), [Yuchen Zhuang](https://arxiv.org/search/physics?searchtype=author&query=Zhuang,+Y), [Wenhao Mu](https://arxiv.org/search/physics?searchtype=author&query=Mu,+W), [Chao Zhang](https://arxiv.org/search/physics?searchtype=author&query=Zhang,+C)\n\nView a PDF of the paper titled MUBen: Benchmarking the Uncertainty of Molecular Representation Models, by Yinghao Li and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.10060) [HTML (experimental)](https://arxiv.org/html/2306.10060v4)\n\n> Abstract:Large molecular representation models pre-trained on massive unlabeled data have shown great success in predicting molecular properties. However, these models may tend to overfit the fine-tuning data, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have included UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different UQ methods for state-of-the-art backbone molecular representation models to investigate their capabilities. By fine-tuning various backbones using different molecular descriptors as inputs with UQ methods from different categories, we assess the influence of architectural decisions and training strategies. Our study offers insights for selecting UQ for backbone models, which can facilitate research on uncertainty-critical applications in fields such as materials science and drug discovery.\n\n|     |     |\n| --- | --- |\n| Comments: | 58 pages, 10 figures, 39 tables, in TMLR |\n| Subjects: | Chemical Physics (physics.chem-ph); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2306.10060](https://arxiv.org/abs/2306.10060) \\[physics.chem-ph\\] |\n|  | (or [arXiv:2306.10060v4](https://arxiv.org/abs/2306.10060v4) \\[physics.chem-ph\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2306.10060](https://doi.org/10.48550/arXiv.2306.10060)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yinghao Li \\[ [view email](https://arxiv.org/show-email/20907fbf/2306.10060)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2306.10060v1)**\nWed, 14 Jun 2023 13:06:04 UTC (1,923 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2306.10060v2)**\nMon, 2 Oct 2023 16:44:32 UTC (1,751 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2306.10060v3)**\nSat, 16 Mar 2024 15:57:19 UTC (1,636 KB)\n\n**\\[v4\\]**\nTue, 16 Apr 2024 22:40:40 UTC (1,692 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MUBen: Benchmarking the Uncertainty of Molecular Representation Models, by Yinghao Li and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.10060)\n- [HTML (experimental)](https://arxiv.org/html/2306.10060v4)\n- [TeX Source](https://arxiv.org/src/2306.10060)\n- [Other Formats](https://arxiv.org/format/2306.10060)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nphysics.chem-ph\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.10060&function=prev&context=physics.chem-ph)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.10060&function=next&context=physics.chem-ph)\n\n[new](https://arxiv.org/list/physics.chem-ph/new) \\| [recent](https://arxiv.org/list/physics.chem-ph/recent) \\| [2023-06](https://arxiv.org/list/physics.chem-ph/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.10060?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2306.10060?context=cs.LG)\n\n[physics](https://arxiv.org/abs/2306.10060?context=physics)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.10060)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.10060)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.10060)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.10060&description=MUBen: Benchmarking the Uncertainty of Molecular Representation Models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.10060&title=MUBen: Benchmarking the Uncertainty of Molecular Representation Models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.10060) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2306.10060"
    },
    {
      "title": "Robust Molecular Property Prediction via  Densifying Scarce Labeled Data",
      "text": "Robust Molecular Property Prediction via\nDensifying Scarce Labeled Data\nJina Kim * 1 Jeffrey Willette * 1 Bruno Andreis * 1 Sung Ju Hwang 1 2\nAbstract\nA widely recognized limitation of molecular pre\u0002diction models is their reliance on structures ob\u0002served in the training data, resulting in poor gener\u0002alization to out-of-distribution compounds. Yet in\ndrug discovery, the compounds most critical for\nadvancing research often lie beyond the training\nset, making the bias toward the training data par\u0002ticularly problematic. This mismatch introduces\nsubstantial covariate shift, under which standard\ndeep learning models produce unstable and in\u0002accurate predictions. Furthermore, the scarcity\nof labeled data\u2014stemming from the onerous and\ncostly nature of experimental validation\u2014further\nexacerbates the difficulty of achieving reliable\ngeneralization. To address these limitations, we\npropose a novel meta-learning-based approach\nthat leverages unlabeled data to interpolate be\u0002tween in-distribution (ID) and out-of-distribution\n(OOD) data, enabling the model to meta-learn\nhow to generalize beyond the training distribution.\nWe demonstrate significant performance gains on\nchallenging real-world datasets with substantial\ncovariate shift, supported by t-SNE visualizations\nhighlighting our interpolation method.\n1. Introduction\nMolecular property prediction plays a central role in drug\ndiscovery pipelines, enabling researchers to prioritize com\u0002pounds for costly and time-consuming experimental valida\u0002tion. Accurate computational models have the potential to\ndramatically accelerate early-stage discovery by predicting\ncritical attributes such as bioactivity, toxicity, and solubility\n*Equal contribution 1Korea Advanced Institute of Sci\u0002ence and Technology (KAIST), South Korea 2Deepauto.ai,\nSouth Korea. Correspondence to: Jina Kim <ji\u0002nakim@kaist.ac.kr>, Jeffrey Willette <jwillette@kaist.ac.kr>,\nBruno Andreis <andries@kaist.ac.kr>, Sung Ju Hwang\n<sungju.hwang@kaist.ac.kr>.\nProceedings of the Workshop on Generative AI for Biology at the\n42 nd International Conference on Machine Learning, Vancouver,\nCanada. PMLR 267, 2025. Copyright 2025 by the author(s).\nIn-Distribution Out-Of-Distribution\nGeneralizing OOD\ntrain point\n, \nContext guided Interpolation\ncontext point\nRobust\nPrediction\nCovariate Shift\nOOD data point\nlogP: 1.2 | Tox: low logP: 4.2 | Tox: high\nlogP: 2.5 | Tox: medium\nFigure 1. Concept. We densify the train dataset using external\nunlabeled data (context point) for robust generalization across\ncovariate shift. Notation details are provided in Section 4.\nbefore synthesis (Schneider, 2018; Vamathevan et al., 2019).\nHowever, building reliable predictive models generalizing to\nnovel, unseen compounds remains a fundamental challenge.\nStandard molecular property prediction models tend to rely\nheavily on patterns observed within the training distribution,\nresulting in poor generalization to out-of-distribution com\u0002pounds (Klarner et al., 2023; Ovadia et al., 2019; Koh et al.,\n2021). In drug discovery, this limitation is particularly prob\u0002lematic, since the compounds most crucial for advancing re\u0002search often lie far beyond the chemical spaces represented\nduring training (Lee et al., 2023). The resulting covariate\nshift introduces significant obstacles to reliable prediction,\nwith models frequently producing unstable outputs when\nextrapolating to new regions of chemical space. Further\ncompounding these challenges, experimental validation of\nmolecular properties is both costly and resource-intensive,\nleading to a scarcity of labeled data and increasing reliance\non computational exploration (Altae-Tran et al., 2017). Also,\navailable labeled data is typically concentrated in narrow\nregions of chemical space, introducing bias that hampers\ngeneralization to unseen compounds (Klarner et al., 2023).\nWhile vast collections of unlabeled molecular structures\nare readily available (Sterling & Irwin, 2015; Kim et al.,\n2021), offering rich information about the structure of\nchemical space, existing methods often fail to fully ex\u0002ploit this resource to improve generalization (Klarner et al.,\n1\nRobust Molecular Property Prediction via Densifying Scarce Labeled Data\n2023). Therefore, we propose a novel meta-learning based\nmethod that leverages unlabeled data to densify the scarce\ntrain dataset and guide the model toward sensible behav\u0002ior in unexplored regions of chemical space. Our code\ncan be found at https://github.com/JinA0218/\ndrugood-densify.\n2. Methodology\nPreliminaries. We consider the problem of molecular\nproperty prediction under covariate shift. Given a small\nlabeled dataset Dtrain = {(xi, yi)}\nn\ni=1 and abundant unla\u0002beled molecules Dunlabeled = {xj}\nm\nj=1, the goal is to learn a\npredictive model f : X \u2192 Y that reliably generalizes to a\ndistributionally shifted test set Dtest.\nScarce Data Densification with Unlabeled Data To ad\u0002dress this, we propose a meta-learning based framework that\ninterpolates the training distribution Dtrain with an exoge\u0002nous distribution Dunlabeled. Our objective is to leverage the\ncheaper and more abundant distribution Dunlabeled to densify\nthe scarce labeled distribution Dtrain in a way that encour\u0002ages the model to generalize robustly under covariate shift,\nparticularly in out-of-distribution scenarios where we have\nno label information and therefore high uncertainty. For this,\nwe utilize subsets of Dunlabeled as Dcontext and Dmvalid, where\nDcontext is a domain-informed external task distribution for\ninterpolating to Dtrain, and Dmvalid is a meta-validation set\nused to guide the interpolation function. Inspired by (Lee\net al., 2024), we introduce a permutation invariant learnable\nset function (Zaheer et al., 2017; Lee et al., 2019) \u00b5\u03bb as a\nmixer (interpolator), which learns to mix each point from\nxi \u223c Dtrain with the context points {cij}\nmi\nj=1 in a way that\ndensifies Dtrain, where\n(xi, yi) \u223c Dtrain, {cij}\nmi\nj=1 \u223c Dcontext, i \u2208 {1, . . . , B}\nand B denotes the minibatch size, mi \u223c Uint(0, M) where\nM controls the maximum number of context samples drawn\nfrom Dcontext for each minibatch. Given a feature dimen\u0002sion D, for each i, the input consists of, xi \u2208 R\nB\u00d71\u00d7D\nand {cij}\nmi\nj=1 \u2208 R\nB\u00d71\u00d7D, where the set {cij}\nmi\nj=1 can be\norganized into a tensor Ci \u2208 R\nB\u00d7mi\u00d7D.\nOverall, our model has two main components: (1) a meta\u0002learner f\u03b8l\n, which is a standard MLP at the l\nth layer, that\nmaps input data x\n(l\u22121)\ni \u2208 R\nB\u00d71\u00d7D to the feature space of\nthe (l + 1)th layer, producing x\n(l)\ni = f\u03b8l\n(x\n(l\u22121)\ni\n), and (2) a\nlearnable set function \u00b5\u03bb which mixes x\n(lmix)\ni\nand C\n(lmix)\ni\nas\na set and outputs a single pooled representation x\u02dc\n(lmix)\ni =\n\u00b5\u03bb({x\n(lmix)\ni\n, C(lmix)\ni\n}) \u2208 R\nB\u00d71\u00d7H, where H is the hidden\ndimension and lmix is the layer where the mixing happens.\nThe full model structure with L layers can be expressed as\n\u02c6f\u03b8,\u03bb := f\u03b8L\n\u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03b8lmix+1 \u25e6 \u00b5\u03bb \u25e6 f\u03b8lmix\u22121\n\u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03b81.\nWe utilize bilevel optimization for training meta-learner f\u03b8l,\nand treat the set function parameter \u00b5\u03bb as a hyperparameter\nto be optimized in the outer loop (Lorraine et al., 2019). As\nshown in Table 2 (w/o bilevel optimization), simply opti\u0002mizing the meta-learner parameters \u03b8 and the set function\nparameters \u03bb jointly can lead to overfitting to the task dis\u0002tribution and harms test-time generalization. Following the\nsetting of (Lorraine et al., 2019), during training, we only\nupdate the parameter \u03b8 in the inner loop and we only update\nthe parameter \u03bb in the outer loop (see Figure 9b for the\ndetailed model structure of the bilevel optimization).\nIn the inner loop, the model accepts xi \u2208 R\nB\u00d71\u00d7H\nand Ci \u2208 R\nB\u00d7mi\u00d7H and the set encoder \u00b5\u03bb mixes\n{x\n(lmix)\ni\n, C(lmix)\ni\n} and outputs x\u02dc\n(lmix)\ni \u2208 R\nB\u00d71\u00d7H. Since Ci\nis used to introduce a domain-informed external context to\ndensify Dtrain, we utilize the original label yi from Dtrain to\ntrain the task learner parameters f\u03b8l, with the mixed x\u02dc\n(lmix)\ni\n.\nIn the outer loop, we train the set encoder using hypergra\u0002dient (Lorraine et al., 2019), w...",
      "url": "https://openreview.net/pdf/70bee4e7da17329c1a51ab89274f7974d10cc74d.pdf"
    }
  ]
}