{
  "query": "What causes large gap between cross-validation score and leaderboard score in Kaggle competitions and how to fix it?",
  "results": [
    {
      "title": "ANSWER",
      "text": "A large discrepancy between the CV score you see while developing a model and the score that appears on the Kaggle leaderboard is almost always a symptom of **over\u2011fitting to the data that is being used for evaluation**. In most competitions the public leaderboard is computed on a small, hidden slice of the test set, while the private leaderboard (the final ranking) is evaluated on the remaining portion. Because the public slice is only a sample, a model that has been tuned aggressively on it can exploit its idiosyncrasies \u2013 for example by selecting hyper\u2011parameters that happen to work well on that particular subset or by inadvertently leaking information from the training set. When the private data has a slightly different distribution, the same model\u2019s performance drops, creating the gap you observe ([gregpark.io](https://gregpark.io/blog/Kaggle-Psychopathy-Postmortem), [stats.stackexchange.com](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of)). The problem is amplified when the public leaderboard is based on a very small hold\u2011out, making its score a noisy estimate of true out\u2011of\u2011sample performance ([blog.mrtz.org](https://blog.mrtz.org/2015/03/09/competition.html)).\n\nTo narrow the gap you need a validation scheme that **mirrors the competition\u2019s test conditions** rather than the convenient split you might use by default. Common fixes include:\n\n* **Stratified / group / time\u2011aware CV** that respects the same data leakage constraints (e.g., no same user or time period appears in both train and validation folds).  \n* **Adversarial validation** to detect and correct distribution shifts between your training data and the public test slice, then re\u2011sample or engineer features accordingly.  \n* **Regularization and early stopping** to keep model complexity in check, preventing the model from memorising the public hold\u2011out.  \n* **Limiting the number of leaderboard submissions** and treating the public score as a noisy signal rather than a target for hyper\u2011parameter search.  \n* **Ensembling and stacking** of diverse models, which tends to be more robust to small sample noise.  \n\nThese practices are highlighted as effective ways to avoid over\u2011fitting in Kaggle contests ([safjan.com](https://safjan.com/avoiding-overfitting-in-Kaggle-competitions)), and the Kaggle handbook stresses the importance of building a validation strategy that reflects the \u201csecond stage\u201d where the private leaderboard is computed ([medium.com](https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8)). By aligning your CV methodology with the true test distribution and applying regularization, you can substantially reduce the public\u2011private score gap.",
      "url": ""
    },
    {
      "title": "Beat Overfitting in Kaggle Competitions - Proven Techniques",
      "text": "## Overfitting problem in Kaggle competitions\n\nOverfitting is a common issue in Kaggle competitions where the goal is to develop a classification model that performs well on unseen data. Overfitting occurs when a model is trained too well on the training data, and as a result, it becomes too complex and starts to memorize the training data, instead of learning the underlying patterns. This can lead to poor performance on the test data, which is the ultimate goal in Kaggle competitions.\n\nTo avoid overfitting, it's essential to evaluate the model during the training process, and select the best model that generalizes well to unseen data. Here are some effective techniques to achieve this:\n\n- [Popular methods for avoiding overfitting](http://safjan.com/safjan.com#popular-methods-for-avoiding-overfitting)\n- [Cross-validation](http://safjan.com/safjan.com#cross-validation)\n- [Early Stopping](http://safjan.com/safjan.com#early-stopping)\n- [Regularization](http://safjan.com/safjan.com#regularization)\n- [Ensemble methods](http://safjan.com/safjan.com#ensemble-methods)\n- [Stacking](http://safjan.com/safjan.com#stacking)\n- [Feature Selection](http://safjan.com/safjan.com#feature-selection)\n- [Advanced methods for avoiding overfitting](http://safjan.com/safjan.com#advanced-methods-for-avoiding-overfitting)\n- [Adversarial Validation](http://safjan.com/safjan.com#adversarial-validation)\n- [Model Uncertainty](http://safjan.com/safjan.com#model-uncertainty)\n- [Dropout (regularization)](http://safjan.com/safjan.com#dropout-regularization)\n- [Transfer Learning - for improving performance](http://safjan.com/safjan.com#transfer-learning---for-improving-performance)\n- [AutoML - for selecting and tuning models](http://safjan.com/safjan.com#automl---for-selecting-and-tuning-models)\n- [Bayesian Optimization - for hyperparameters tunnig](http://safjan.com/safjan.com#bayesian-optimization---for-hyperparameters-tunnig)\n- [Notable mentions](http://safjan.com/safjan.com#notable-mentions)\n- [Bagging](http://safjan.com/safjan.com#bagging)\n- [Boosting](http://safjan.com/safjan.com#boosting)\n- [Conclusion](http://safjan.com/safjan.com#conclusion)\n\n## Popular methods for avoiding overfitting\n\n### Cross-validation\n\nIt is a technique used to assess the performance of a model on the unseen data. The idea is to divide the data into multiple folds, and train the model on k-1 folds, and validate it on the kth fold. This process is repeated multiple times, and the average performance is used as the final score.\n\n### Early Stopping\n\nIt is a technique used to stop the training process when the model performance on a validation set stops improving. The idea is to monitor the performance on the validation set during the training process, and stop the training when the performance plateaus or starts to decline.\n\n### Regularization\n\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function. The idea is to encourage the model to learn simple representations, instead of complex ones. Common regularization techniques include L1 and L2 regularization.\n\n### Ensemble methods\n\nEnsemble methods are techniques used to combine the predictions of multiple models to produce a single prediction. Ensemble methods are known to be effective in preventing overfitting, as they combine the strengths of multiple models and reduce the risk of overfitting to a single model.\n\n### Stacking\n\nStacking is an ensemble technique that combines the predictions of multiple models to produce a single prediction. It involves training multiple models on different portions of the training data and then using their predictions as features to train a meta-model. This technique can lead to improved performance compared to using a single model.\n\n### Feature Selection\n\nFeature selection is a technique used to select the most relevant features for a classification problem. The idea is to remove redundant and irrelevant features, which can improve the model's performance and prevent overfitting.\n\n## Advanced methods for avoiding overfitting\n\n### Adversarial Validation\n\nAdversarial Validation is a technique used to evaluate the generalization performance of a model by creating a validation set that is similar to the test set. The idea is to train the model on the training set, and then evaluate its performance on the validation set, which is obtained by combining samples from the training set and the test set.\n\nReferences:\n\n- [Adversarial Validation \\| Zak Jost](https://blog.zakjost.com/post/adversarial_validation/)\n- [What is Adversarial Validation? \\| Kaggle](https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation)\n\n### Model Uncertainty\n\nModel Uncertainty is a technique used to evaluate the uncertainty in the model predictions. The idea is to use Bayesian techniques to estimate the uncertainty in the model parameters, and use this information to rank the predictions made by the model.\n\nReferences:\n\n- [Counterfactual explanation of Bayesian model uncertainty \\| SpringerLink](https://link.springer.com/article/10.1007/s00521-021-06528-z)\n- [A Gentle Introduction to Uncertainty in Machine Learning - MachineLearningMastery.com](https://machinelearningmastery.com/uncertainty-in-machine-learning/)\n- [Uncertainty Assessment of Predictions with Bayesian Inference \\| by Georgi Ivanov \\| Towards Data Science](https://towardsdatascience.com/uncertainty-quantification-of-predictions-with-bayesian-inference-6192e31a9fa9)\n\n### Dropout (regularization)\n\nDropout is a regularization technique that involves randomly dropping out units in a neural network during training. The idea is to prevent the network from becoming too complex and memorizing the training data, which can lead to overfitting.\n\n### Transfer Learning - for improving performance\n\nTransfer Learning is a technique used to transfer knowledge from one task to another. The idea is to fine-tune a pre-trained model on the target task, instead of training the model from scratch. This technique can lead to improved performance by leveraging the knowledge learned from related tasks.\n\nReferences:\n\n- [Transfer learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n- [A Gentle Introduction to Transfer Learning for Deep Learning - MachineLearningMastery.com](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)\n\n### AutoML - for selecting and tuning models\n\nAutoML is the use of machine learning algorithms to automate the process of selecting and tuning machine learning models. AutoML has been used by many Kaggle competition winners and data science expert professionals to streamline the model selection and hyperparameter tuning process, and to find the best models with less human intervention, thereby reducing the risk of overfitting.\nExamples of python AutoML libraries: [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://epistasislab.github.io/tpot/), [HyperOpt](http://hyperopt.github.io/hyperopt-sklearn/), [AutoKeras](https://autokeras.com/)\n\nReferences:\n\n- [Automated Machine Learning (AutoML) Libraries for Python - MachineLearningMastery.com](https://machinelearningmastery.com/automl-libraries-for-python/)\n- [4 Python AutoML Libraries Every Data Scientist Should Know \\| by Andre Ye \\| Towards Data Science](https://towardsdatascience.com/4-python-automl-libraries-every-data-scientist-should-know-680ff5d6ad08)\n- [Top 10 AutoML Python packages to automate your machine learning tasks](https://www.activestate.com/blog/the-top-10-automl-python-packages-to-automate-your-machine-learning-tasks/)\n- [Python AutoML Library That Outperforms Data Scientists \\| Towards Data Science](https://towardsdatascience.com/python-automl-sklearn-fd85d3b3c5e)\n\n### Bayesian Optimization - for hyperparameters tunnig\n\nBayesian Optimization is a probabilistic model-based optimization technique used to tune the hyperparameters of a model. This technique has been used by many Kaggle competition winners a...",
      "url": "https://safjan.com/avoiding-overfitting-in-Kaggle-competitions"
    },
    {
      "title": "The dangers of overfitting: a Kaggle postmortem",
      "text": "# The dangers of overfitting: a Kaggle postmortem\n\nJuly 06, 2012\n\nOver the last month and a half, the [Online Privacy Foundation](http://www.onlineprivacyfoundation.org) hosted a [Kaggle](http://www.kaggle.com) competition, in which competitors attempted to predict the [psychopathy](http://en.wikipedia.org/wiki/Psychopathy) levels of Twitter users. The bigger goal here is to learn how much of our personality is actually revealed through services like Twitter, and by hosting on Kaggle, the Online Privacy Foundation can sit back and watch competitors squeeze every bit of information out of the data.\n\nCompetitors can submit two sets of predictions each day, and each submission is scored from 0 (worst) to 1 (best) using a metric known as [average precision](http://www.kaggle.com/c/twitter-psychopathy-prediction/details/Evaluation). Essentially, a submission that predicts the correct ranking of psychopathy scores across all Twitter accounts will receive an average precision score of 1.\n\nI made 42 submissions, making it my biggest Kaggle effort yet. Each submission is instantly scored, and competitors are ranked on a public leaderboard by their best submission. However, the public leaderboard score isn\u2019t actually the _true_ score\u2014it is only an estimate based on a small portion of the submission. When the competition ends, five submissions from each competitor are compared to the full set of test data, and the highest scoring submission from each user is used to calculate the final score. By the end of the contest, I had slowly worked my way up to 2nd place on the public leaderboard, shown below.\n\n![Public Leaderboard](https://gregpark.io/assets/images/PublicLeaderboard.png)_Top of the public leaderboard. The public leaderboard scores are calculated during the competition by comparing users\u2019 predictions to a small subset of the test data._\n\nI held this spot for the last week and felt confident that I would maintain a decent spot on the private, or true, leaderboard. Soon after the competition closed, the private leaderboard was revealed. Here\u2019s what I saw at the top:\n\n![Private Leaderboard](https://gregpark.io/assets/images/PrivateLeaderboard.png)_Top of the private leaderboard. The private leaderboard is the \u201creal\u201d leaderboard, revealed after the contest is closed. Scores are calculated by comparing users\u2019 predictions to the full set of test data._\n\nWhere\u2019d I go? I scrolled down the leaderboard\u2026. further\u2026 and further\u2026 and finally found my name:\n\n![Final Standing](https://gregpark.io/assets/images/Final-Standing.png)_My place on the private leaderboard. I dropped from 2nd place on the public leaderboard to 52nd on the private leaderboard. Notice I placed below the random forest benchmark!_\n\nSomehow I managed to fall from 2nd all the way down to 52nd! I wasn\u2019t the only one who took a big fall: the top five users on the public leaderboard ended up in 64th, 52nd, 58th, 16th, and 57th on the private leaderboard, respectively. I even placed below the _random forest benchmark_, a solution publicly available from the start of the competition.\n\n## What happened?\n\nAfter getting over the initial shock of dropping 50 places, I began sifting through the ashes to figure out what went so wrong. I think those with more experience already know, but one clue is in the screenshot of the pack leaders on the public leaderboard. Notice that the top five users, including myself, have a lot of submissions. For context, the median number of submissions in this contest was six. Contrast this with the (real) leaders on the private leaderboard \u2013 most have less than 12 submissions. Below, I\u2019ve plotted the number of entries from each user against their final standing on the public and private leaderboards and added a trend line to each plot.\n\n![Ranks vs. Entries](https://gregpark.io/assets/images/rankvsentriesgrid1.png)\n\nOn the public leaderboard, more submissions are consistently related to a better standing. It could be that the public leaderboard actually reflects the amount of brute force from a competitor rather than predictive accuracy. If you throw enough mud at a wall, eventually some of it will start to stick. The problem is that submissions that score well using this approach probably will not generalize to the full set of test data when the competition closes. It\u2019s possible to overfit the portion of test data used to calculate the public leaderboard, and it looks like that\u2019s exactly what I did.\n\nCompare that trend in the public leaderboard to the U-shaped curve in the plot of the private leaderboard and number of submissions. After about 25 submissions or so, private leaderboard standings get worse with the number of submissions.\n\n## Poor judgments under uncertainty\n\nOverfitting the public leaderboard is not unheard of, and I knew that it was a possibility all along. So why did I continue to hammer away at competition with so many submissions, knowing that I could be slowly overfitting to the leaderboard?\n\nMany competitors with estimate the quality of their submissions prior to uploading them using [cross-validation](http://en.wikipedia.org/wiki/Cross-validation_(statistics)). Because the public leaderboard is only based on a small portion of the test data, it is only a rough estimate of the true quality of a submission, and cross-validation gives a sort of second opinion of a submission\u2019s quality. For most of my submissions, I used 10-fold cross-validation to estimate the average precision. So throughout the contest, I could observe both the public leaderboard score and my own estimated score from cross-validation. After the contest closes, Kaggle reveals the private or true score of each submission. Below, I\u2019ve plotted the public, private, and cv-estimated score of each submission by date. The dotted line is the private score of the winning submission.\n\n![Results by Time](https://gregpark.io/assets/images/ResultsByTime.png)\n\nThere are a few things worth pointing out here:\n\n- My cross-validation (CV) estimated scores (the orange line) gradually improve over time. So, as far as I know, my submissions are actually getting better as I go.\n- The private or _true_ scores actually get worse over time. In fact, my first two submissions to the contest turned out to be my best (and I did not choose them as any of my final five submissions)\n- The public scores reach a peak and then slowly get worse at the end of the contest.\n- It is very difficult to see a relationship between any of these trends.\n\nBelow, I\u2019ve replaced the choppy lines with smoothed lines to show the general trends.\n\n![Smoothed Results by Time](https://gregpark.io/assets/images/ResultsByTime_smooth.png)_An alternate plot of the submission scores over time using smoothers to depict some general trends._\n\nBased on my experience with past contests, I knew that the public leaderboard could not be trusted fully, and this is why I used cross-validation. I assumed that there was a stronger relationship between the cross-validation estimates and the private leaderboard than between the public and private leaderboard. Below, I\u2019ve created scatterplots to show the relationships between each pair of score types.\n\n![Score Scatterplots](https://gregpark.io/assets/images/ScoresMatrix3.png)_Scatterplots of three types of scores for each submission: 10-fold CV estimated score, public leaderboard score, and private or \u201ctrue\u201d score._\n\nThe scatterplots tell a different story. It turned out that my cross-validation estimates were not related to the private scores at all (notice the horizontal linear trends in those scatterplots), and the public leaderboard wasn\u2019t any better. I already guessed that the public leaderboard would be a poor estimate of the true score, but why didn\u2019t cross-validation do any better?\n\nI suspect this is because as the competition went on, I began to use much more feature selection and preprocessing. However, I made the classic mistake in my cross-validation method by not including this in the cross-validation f...",
      "url": "https://gregpark.io/blog/Kaggle-Psychopathy-Postmortem"
    },
    {
      "title": "Is Kaggle's private leaderboard a good predictor of out-of-sample performance of the winning model?",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Is Kaggle's private leaderboard a good predictor of out-of-sample performance of the winning model?](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of)\n\n[Ask Question](https://stats.stackexchange.com/questions/ask)\n\nAsked7 years ago\n\nModified [6 years, 2 months ago](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of?lastactivity)\n\nViewed\n1k times\n\n18\n\n$\\\\begingroup$\n\nWhile the results of the private test set can not be used to refine the model further, isn't model selection out of a huge number of models being performed based on the private test set results? Would you not, through that process alone, end up overfitting to the private test set?\n\nAccording to [\"Pseudo-Mathematics and Financial Charlatanism: The Effects of\\\nBacktest Overfitting on Out-of-Sample Performance\"](http://www.ams.org/notices/201405/rnoti-p458.pdf)\nby Bailey et.al. it is relatively easy to \"overfit\" when selecting the best out of a large number of models evaluated on the same dataset. Is that not happening with Kaggle's private leaderboard?\n\n- What are the statistical justifications for the best performing models on the private leaderboard being the models that generalize the best to out-of-sample data?\n- Do companies actually end up using the winning models, or is the private leaderboard there just to provide the \"rules of the game\", and the companies are actually more interested in the insight that arises from the discussion of the problem?\n\n- [model-selection](https://stats.stackexchange.com/questions/tagged/model-selection)\n- [overfitting](https://stats.stackexchange.com/questions/tagged/overfitting)\n- [out-of-sample](https://stats.stackexchange.com/questions/tagged/out-of-sample)\n\n[Share](https://stats.stackexchange.com/q/284232)\n\nCite\n\n[Improve this question](https://stats.stackexchange.com/posts/284232/edit)\n\nFollow\n\n[edited Jun 8, 2017 at 16:24](https://stats.stackexchange.com/posts/284232/revisions)\n\n[![Kodiologist's user avatar](https://i.sstatic.net/281CZ.png?s=64)](https://stats.stackexchange.com/users/14076/kodiologist)\n\n[Kodiologist](https://stats.stackexchange.com/users/14076/kodiologist)\n\n20.3k22 gold badges4242 silver badges7777 bronze badges\n\nasked Jun 8, 2017 at 10:21\n\n[![rinspy's user avatar](https://www.gravatar.com/avatar/cdf215d785625823e70cbea13fe41993?s=64&d=identicon&r=PG&f=y&so-version=2)](https://stats.stackexchange.com/users/154925/rinspy)\n\n[rinspy](https://stats.stackexchange.com/users/154925/rinspy) rinspy\n\n3,3701616 silver badges4343 bronze badges\n\n$\\\\endgroup$\n\n3\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Somewhat related: [stats.stackexchange.com/q/235591](https://stats.stackexchange.com/q/235591)$\\\\endgroup$\n\n\u2013\u00a0[Kodiologist](https://stats.stackexchange.com/users/14076/kodiologist)\n\nCommentedJun 8, 2017 at 16:27\n\n- 2\n\n\n\n\n\n$\\\\begingroup$You could look at the difference between private and public scores. One could argue that a non-overfitted model should achieve similar performance on both data sets.$\\\\endgroup$\n\n\u2013\u00a0[shadowtalker](https://stats.stackexchange.com/users/36229/shadowtalker)\n\nCommentedJun 8, 2017 at 16:28\n\n- 2\n\n\n\n\n\n$\\\\begingroup$@shadowtalker That indeed would be a good way to detect overfitting, but what we are actually interested in is the out-of-sample predictive power of the model, not the degree of overfitting. An overfit model - i.e. one that works much better in-sample than out-of-sample - may have better out-of-sample performance than a model that is not overfit. I do not have a reference on hand, but I believe that is often the case in complex domains, e.g. computer vision, when using complex models, e.g. CNNs.$\\\\endgroup$\n\n\u2013\u00a0[rinspy](https://stats.stackexchange.com/users/154925/rinspy)\n\nCommentedFeb 1, 2018 at 10:00\n\n\n[Add a comment](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n9\n\n$\\\\begingroup$\n\nWell the points you present are fair, however I think that there is a far more real issue with people **overfitting on the public leaderboard**.\n\nThis may happen when you do 100 or so submissions, the public test set will eventually _bleed out_ on to your hyperparameter selection and thus overfit. I think that the private leaderboard is necessary in that respect.\n\n[Share](https://stats.stackexchange.com/a/340026)\n\nCite\n\n[Improve this answer](https://stats.stackexchange.com/posts/340026/edit)\n\nFollow\n\nanswered Apr 12, 2018 at 2:53\n\nuser204013user204013\n\n$\\\\endgroup$\n\n[Add a comment](https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://stats.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fstats.stackexchange.com%2fquestions%2f284232%2fis-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [privacy policy](https://stackoverflow.com/legal/privacy-policy).\n\n## Not the answer you're looking for? Browse other questions tagged  - [model-selection](https://stats.stackexchange.com/questions/tagged/model-selection) - [overfitting](https://stats.stackexchange.com/questions/tagged/overfitting) - [out-of-sample](https://stats.stackexchange.com/questions/tagged/out-of-sample)   or [ask your own question](https://stats.stackexchange.com/questions/ask).\n\n- Featured on Meta\n- [Upcoming sign-up experiments related to tags](https://meta.stackexchange.com/questions/400648/upcoming-sign-up-experiments-related-to-tags)\n\n\n#### Linked\n\n[2](https://stats.stackexchange.com/q/235591) [Nested nested cross validation for model selection](https://stats.stackexchange.com/questions/235591/nested-nested-cross-validation-for-model-selection?noredirect=1)\n\n#### Related\n\n[15](https://stats.stackexchange.com/q/82664) [Bayesian vs MLE, overfitting problem](https://stats.stackexchange.com/questions/82664/bayesian-vs-mle-overfitting-problem)\n\n[22](https://stats.stackexchange.com/q/188125) [\"Semi supervised learning\" - is this overfitting?](https://stats.stackexchange.com/questions/188125/semi-supervised-learning-is-this-overfitting)\n\n[32](https://stats.stackexchange.com/q/204489) [Why is xgboost overfitting in my task? Is it fine to accept this overfitting?](https://stats.stackexchange.com/questions/204489/why-is-xgboost-overfitting-in-my-task-is-it-fine-to-accept-this-overfitting)\n\n[5](https://stats.stackexchange.com/q/318930) [Nested Cross Validation: Choosing between different best hyperparameters](https://stats.stackexchange.com/questions/318930/nested-cross-validation-choosing-between-different-best-hyperparameters)\n\n[4](https://stats.stackexchange.com/q/459770) [Why my model overfits despite selecting best hyperparameters value in each tuning step?](https://stats.stackexchange.com/questions/459770/why-my-model-overfits-despite-selecting-best-hyperparameters-value-in-each-tunin)\n\n[15](https://stats.stackexchange.com/q/540454) [Is it wrong to compare multiple models on the same test set and choose the best model?](https://stats.stackexchange...",
      "url": "https://stats.stackexchange.com/questions/284232/is-kaggles-private-leaderboard-a-good-predictor-of-out-of-sample-performance-of"
    },
    {
      "title": "Competing in a data science contest without reading the data",
      "text": "Machine learning competitions have become an extremely popular format for\nsolving prediction and classification problems of all sorts. The most famous\nexample is perhaps the Netflix prize. An even better example is\n[Kaggle](http://www.kaggle.com), an awesome startup that\u2019s\norganized more than a hundred competitions over the past few years.\n\nThe central component of any competition is the public leaderboard. Competitors can repeatedly submit a list of predictions and see how their predictions perform on a set of _holdout labels_ not available to them. The leaderboard ranks all teams according to their prediction accuracy on the holdout labels. Once the competition closes all teams are scored on a final test set not used so far. The resulting ranking, called private leaderboard, determines the winner.\n\nPublic leaderboard of the Heritage Health Prize ( [Source](http://www.heritagehealthprize.com/c/hhp/leaderboard/public))\n\nIn this post, I will describe a method to climb the public leaderboard _without even looking at the data_. The algorithm is so simple and natural that an unwitting analyst might just run it. We will see that in Kaggle\u2019s famous Heritage Health Prize competition this might have propelled a participant from rank around 150 into the top 10 on the public leaderboard without making progress on the actual problem. The Heritage Health Prize competition ran for two years and had a prize pool of 3 million dollars. Keep in mind though that the standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that accurately reflects the true performance of each team is a difficult and deep problem. While there are decades of work on estimating the true performance of a model (or set of models) from a finite sample, the leaderboard application highlights some\nchallenges that while fundamental have only recently seen increased attention. A follow-up post will describe a [recent paper](http://arxiv.org/abs/1502.04585) with Avrim Blum that gives an algorithm for maintaining a (provably) accurate public leaderboard.\n\nLet me be very clear that my point is _not_ to criticize Kaggle or anyone else organizing machine learning competitions. On the contrary, I\u2019m amazed by how well Kaggle competitions work. In my opinion, they have contributed a tremendous amount of value to both industry and education. I also know that Kaggle has some very smart people thinking hard about how to anticipate problems with competitions.\n\n## The Kaggle leaderboard mechanism\n\nAt first sight, the Kaggle mechanism looks like the classic _holdout method_. Kaggle partitions the data into two sets: a training set and a holdout set. The training set is publicly available with both the individual instances and their corresponding class labels. The instances of the holdout set are publicly available as well, but the class labels are withheld. Predicting these missing class labels is the goal of the participant and a valid submission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N labels to a numerical score, which we assume to be in . Think of the score as prediction error (smaller is better). For concreteness, let\u2019s fix it to be the _misclassification rate_. That is a prediction incurs loss 0 if it matches the corresponding unknown label and loss 1 if it does not match it. We divide by the number of predictions to get a score in .\n\nKaggle further splits its \\\\(N\\\\) private labels randomly into \\\\(n\\\\) holdout labels and \\\\(N-n\\\\) test labels. Typically, \\\\(n=0.3N\\\\). The public leaderboard is a sorting of all teams according to their score computed only on the \\\\(n\\\\) holdout labels (without using the test labels), while the private leaderboard is the ranking induced by the test labels. I will let \\\\(s\\_H(y)\\\\) denote the public score of a submission \\\\(y\\\\), i.e., the score according to the public leaderboard. Typically, Kaggle rounds all scores to 5 or 6 digits of precision.\n\n## The cautionary tale of wacky boosting\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole machine learning craze. So, I sign up for a Kaggle competition to get some skills. Kaggle tells me that there\u2019s an unknown set of labels \\\\(y\\\\in\\\\{0,1\\\\}^N\\\\) that I need to predict. Well, I know nothing about \\\\(y\\\\). So here\u2019s what I\u2019m going to do. I try out a bunch of random vectors and keep all those that give me a slightly better than expected score. If we\u2019re talking about misclassification rate, the expected score of a random binary vector is 0.5. So, I\u2019m keeping all the vectors with score less than 0.5. Then I recall something about boosting. It tells me that I can boost my accuracy by aggregating all predictors into a single predictor using the majority function. Slightly more formally, here\u2019s what I do:\n\n**Algorithm** (Wacky Boosting):\n\n1. Choose \\\\(y\\_1,\\\\dots,y\\_k\\\\in\\\\{0,1\\\\}^N\\\\) uniformly at random.\n2. Let \\\\(I = \\\\{ i\\\\in\\[k\\] \\\\colon s\\_H(y\\_i) < 0.5 \\\\}\\\\).\n3. Output \\\\(\\\\hat y=\\\\mathrm{majority} \\\\{ y\\_i \\\\colon i \\\\in I \\\\} \\\\), where the majority is component-wise.\n\nLo and behold, this is what happens:\n\nIn this plot, \\\\(n=4000\\\\) and all numbers are averaged over 5 independent repetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I keep climbing the leaderboard! Who would\u2019ve thought that this machine learning thing was so easy? So, I go write a blog post on Medium about Big Data and score a job at DeepCompeting.ly, the latest data science startup in the city. Life is pretty sweet. I pick up indoor rock climbing, sign up for wood working classes; I read Proust and books about espresso. Two months later the competition closes and Kaggle releases the final score. What an embarrassment! Wacky boosting did nothing whatsoever on the final test set. I get fired from DeepCompeting.ly days before the buyout. My spouse dumps me. The lease expires. I get evicted from my apartment in the Mission. Inevitably, I hike the Pacific Crest Trail and write a novel about it.\n\n### What just happened\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific Crest Trail. To start out with, each \\\\(y\\_i\\\\) has loss around \\\\(1/2\\\\pm1/\\\\sqrt{n}\\\\). We\u2019re selecting the ones that are biased below a half. This introduces a bias in the score and the conditional expected bias of each selected vector \\\\(w\\_i\\\\) is roughly \\\\(1/2-c/\\\\sqrt{n}\\\\) for some positive constant \\\\(c>0\\\\). Put differently, each selected \\\\(y\\_i\\\\) is giving us a guess about each label in the unknown holdout set \\\\(H\\\\subseteq \\[N\\]\\\\) that\u2019s correct with probability \\\\(1/2 + \\\\Omega(1/\\\\sqrt{n})\\\\). Since the public score doesn\u2019t depend on labels outside of \\\\(H\\\\), the conditioning does not affect the final test set. The labels outside of \\\\(H\\\\) are still unbiased. Finally, we need to argue that the majority vote \u201cboosts\u201d our slightly biased coin tosses into a stronger bias. More formally, we can show that \\\\(\\\\hat y\\\\) gives us a guess for each label in \\\\(H\\\\) that\u2019s correct with probability\n\\\\\\[\n\\\\frac12 + \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nHence, the public score of \\\\(y\\\\) satisfies\n\\\\\\[\ns\\_H(y) < \\\\frac12 - \\\\Omega\\\\left(\\\\sqrt{k/n}\\\\right).\n\\\\\\]\nOutside of \\\\(H\\\\), however, we\u2019re just random guessing with no advantage.\nTo summarize, wacky boosting gives us _a bias of \\\\(\\\\sqrt{k}\\\\) standard deviations on the public score with \\\\(k\\\\) submissions_.\n\nWhat\u2019s important is that the same algorithm still \u201cworks\u201d even if we don\u2019t get exact answers. All we need are answers that are accurate to an additive error of \\\\(1/\\\\sqrt{n}\\\\). This is important since Kaggle rounds its answers to 5 digits of precision. In particular, this attack will work so long as \\\\(n< 10^{10}\\\\).\n\n### Why the holdout method breaks down\n\nThe idea behind the holdout method is that the holdout data serve as a fresh sample providing an unbiased ...",
      "url": "https://blog.mrtz.org/2015/03/09/competition.html"
    },
    {
      "title": "Kaggle Handbook: Fundamentals to Survive a Kaggle Shake-up",
      "text": "<div><div><div><h2>A guide to Kaggle competitions with tips &amp; tricks to get on the good side of the \u201cShake-up\u201d.</h2><div><a href=\"https://medium.com/@ertugruldemir?source=post_page---byline--3dec0c085bc8---------------------------------------\"><div><p></p></div></a></div></div><p>If you\u2019re interested in data science, you probably heard about Kaggle, a platform where hundreds of DS and ML enthusiasts meet, discuss, share and compete.</p><p>In the first post of this series, I\u2019m going to talk briefly about the format of Kaggle competitions and then move on to the fundamental techniques that will help you end up on the better side of the biggest pitfalls of Kaggle competitions: <em>Shake-up</em>s.</p><h2><strong>How do Kaggle competitions work?</strong></h2><p><strong><em>The competition process is basically as follows:</em></strong></p><ul><li>An individual, or more often an organization, identifies a problem and assumes that this issue can be solved by machine learning.</li><li>The data for the problem might already exist or be collected/generated for the competition. Then, a proper metric is established.</li><li>Depending on the sponsors and the complexity of the problem, a prize pool is set.</li><li>The organizers provide Kaggle staff with the necessary data and materials, and finally, the organizers become the Kaggle competition host.</li><li>Participants submit their solutions during the competition phase and receive their final ranking based on the competition metrics.</li></ul><h2>This looks straightforward<strong>. What\u2019s the catch?</strong></h2><p><strong><em>The catch is in the second stage of the competition where your models are tested on data they have never seen before.</em></strong></p><figure><figcaption>Public vs. Private LB, Source: Kaggle</figcaption></figure><p>Kaggle competitions usually consist of two stages. The results of the first stage are displayed on the <strong>Public Leaderboard</strong> \u2014 a live scoreboard \u2014 and the results of the second stage are displayed on the <strong>Private Leaderboard</strong>. The results of the first stage are not decisive for the final ranking, but they are your guide for the private leaderboard. Organizers usually leave only a small part of the test set for the public leaderboard, and the rest is reserved for the private leaderboard, which is the most important for the final rankings. The private leaderboard is hidden from the participants until the competition ends.</p><p>In terms of restrictions and complexity, there are similar contests on Kaggle called <strong>Code Competitions</strong>. In these contests, the test set features are hidden from the participants, which takes the restrictions one step further by limiting the inference times and preventing hand labeling or exploratory data analysis on test data.</p><h2>You were talking about shake-what?!</h2><p><strong><em>That\u2019s the part where your final outcome reveals.</em></strong></p><p>The shake-up is the difference in rank between public and private leaderboards. One of the main goals of Kaggle competitions is to <em>survive </em>the shake-ups. Is it sheer luck to end up on the good side of shake-ups, or are there better ways? How can we assess our chances of survival in shake-ups? What techniques can we use?</p><p>Data scientists are not the kind of people who shut their eyes and hope for the best. So let us figure out how to better <strong><em>estimate</em></strong> the unknown.</p><h2><strong>How do I know what my model will do on the private leaderboard?</strong></h2><p><strong><em>Let\u2019s talk about fundamentals.</em></strong></p><p><strong>Bias-Variance Tradeoff and Overfitting</strong></p><p>Today we can easily train models with a large number of parameters and create highly complex models. Complex models can be very useful, but they can also be very punishing if we are not careful about the <strong>bias-variance tradeoff</strong>. Such models are risky because they are prone to overfitting. Overfitting occurs when a model memorizes the noise of training data instead of learning about the actual pattern behind it. We want our models to <strong>generalize </strong>and capture the actual nature of the data. That\u2019s what we want in Kaggle competitions too, so we can make predictions on a test set that our model hasn\u2019t trained on. The results of these predictions determine your private leaderboard standings. To assess the generalization capabilities of our model, we need to apply <strong>validation techniques</strong> and understand how our model acts in different cases.</p><p>Bias-Variance Tradeoff is beyond the scope of this post, but I highly recommend you take a look if you are not familiar with this concept.</p><h2>What are these validation techniques?</h2><p><strong><em>There are numerous validation techniques but we\u2019re going to talk about more common and Kaggle-specific ones.</em></strong></p><p><strong>Validation Set Approach</strong></p><p>The validation set approach is pretty straightforward. You separate away a fraction of your training data and this new set is called <strong>Hold-Out</strong>:</p><blockquote><p>The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.</p></blockquote><p><em>\u2014 Hands-On Machine Learning with Scikit-Learn and TensorFlow, 2nd Edition.</em></p><p>The latter part of the quote above bears a resemblance to the public leaderboard concept. We can evaluate our final model on the public test set to get an estimate of the generalization error. However, this approach has its own downsides:</p><ul><li>The validation estimate of our model metric can be highly variable, depending on which observations are included in the training set and which are included in the validation set. This is especially true for the <strong>Public Test Set</strong> since we don\u2019t know if the Private Test Set is coming from similar distribution.</li><li>If we take a subset of our training data as the validation set, we decrease the number of observations to train on, which can hamper our model generalization again.</li></ul><p><strong>Cross Validation Approach</strong></p><figure><figcaption>General k-fold cross validation approach with Kaggle competition additions.</figcaption></figure><blockquote><p>This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k \u2212 1 folds.</p></blockquote><p><em>\u2014 Page 203, An Introduction to Statistical Learning, 2021.</em></p><p>This method can solve the problems with the single hold-out set approach that I mentioned above. By creating multiple <strong>folds</strong>, we can test the metric on different cases while observing the variance and eventually using the whole training data! This way, we can have stronger indicators for our model and estimate private leaderboard scores more confidently. Of course, we still assume training and private test set come from similar distributions. However, we train our models on different sets, so together they\u2019re more likely to generalize better than a single holdout set approach.</p><p><strong>Trust Your CV (Cross Validation)</strong></p><p><strong>Trust Your CV </strong>became a pretty popular tagline among Kaggle competitors, reminding them not to build their models based on public leaderboard scores, and I concur. I have participated in quite a few Kaggle competitions over the last few years, and...",
      "url": "https://medium.com/global-maksimum-data-information-technologies/kaggle-handbook-fundamentals-to-survive-a-kaggle-shake-up-3dec0c085bc8"
    },
    {
      "title": "Overfitting the leaderboard",
      "text": "Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n\n[Learn more](https://www.kaggle.com/cookies)\n\nOK, Got it.\n\n###### Something went wrong and this page crashed!\n\nIf the issue persists, it's likely a problem on our side.\n\n```\n\nLoading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)keyboard_arrow_upcontent_copyChunkLoadError: Loading CSS chunk 8198 failed.\n(error: https://www.kaggle.com/static/assets/KernelViewer.56ec1322d855ad0f5482.css)\n    at r.onerror.r.onload (https://www.kaggle.com/static/assets/runtime.js?v=77c6c69f1305124c7a10:1:11004)\n```\n\nRefresh",
      "url": "https://www.kaggle.com/code/caseyftw/overfitting-the-leaderboard"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition",
      "text": "## Introduction\n\n[Kaggle](https://www.kaggle.com/) is the best place to learn from other data scientists. Many companies provide data and prize money to set up data science competitions on Kaggle. Recently I had my first shot on Kaggle and **ranked 98th (~ 5%) among 2125 teams**. Being my Kaggle debut, I feel quite satisfied with the result. Since many Kaggle beginners set 10% as their first goal, I want to share my two cents on how to achieve that.\n\n_This post is also available in [Chinese](https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/)._\n\n**Updated on Oct 28th, 2016:** I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore.\n\n![Kaggle Profile](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-profile.png)\n\nMost Kagglers use Python or R. I prefer Python, but R users should have no difficulty in understanding the ideas behind tools and languages.\n\nFirst let\u2019s go through some facts about Kaggle competitions in case you are not familiar with them.\n\n- Different competitions have different tasks: classifications, regressions, recommendations\u2026 Training set and testing set will be open for download after the competition launches.\n\n- A competition typically lasts for 2 ~ 3 months. Each team can submit for a limited number of times per day. Usually it\u2019s 5 times a day.\n\n- There will be a 1st submission deadline one week before the end of the competition, after which you cannot merge teams or enter the competition. Therefore **be sure to have at least one valid submission before that.**\n\n- You will get you score immediately after the submission. Different competitions use different scoring metrics, which are explained by the question mark on the leaderboard.\n\n- The score you get is calculated on a subset of testing set, which is commonly referred to as a **Public LB** score. Whereas the final result will use the remaining data in the testing set, which is referred to as a **Private LB** score.\n\n- The score you get by local cross validation is commonly referred to as a **CV** score. Generally speaking, CV scores are more reliable than LB scores.\n\n- Beginners can learn a lot from **Forum** and **Scripts**. Do not hesitate to ask about anything. Kagglers are in general very kind and helpful.\n\n\nI assume that readers are familiar with basic concepts and models of machine learning. Enjoy reading!\n\n## General Approach\n\nIn this section, I will walk you through the process of a Kaggle competition.\n\n### Data Exploration\n\nWhat we do at this stage is called **EDA (Exploratory Data Analysis)**, which means analytically exploring data in order to provide some insights for subsequent processing and modeling.\n\nUsually we would load the data using **[Pandas](http://pandas.pydata.org/)** and make some visualizations to understand the data.\n\n#### Visualization\n\nFor plotting, **[Matplotlib](http://matplotlib.org/)** and **[Seaborn](https://stanford.edu/~mwaskom/software/seaborn/)** should suffice.\n\nSome common practices:\n\n- Inspect the distribution of target variable. Depending on what scoring metric is used, **an [imbalanced](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907) distribution of target variable might harm the model\u2019s performance**.\n- For **numerical variables**, use **box plot** and **scatter plot** to inspect their distributions and check for outliers.\n- For classification tasks, plot the data with points colored according to their labels. This can help with feature engineering.\n- Make pairwise distribution plots and examine their correlations.\n\nBe sure to read [this inspiring tutorial of exploratory visualization](https://www.kaggle.com/benhamner/d/uciml/iris/python-data-visualizations) before you go on.\n\n#### Statistical Tests\n\nWe can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i.i.d. data in real world. So we have to be careful about which test to use and how we interpret the findings.\n\nIn many competitions public LB scores are not very consistent with local CV scores due to noise or non-i.i.d. distribution. You can use test results to **roughly set a threshold for determining whether an increase of score is due to genuine improvment or randomness**.\n\n### Data Preprocessing\n\nIn most cases, we need to preprocess the dataset before constructing features. Some common steps are:\n\n- Sometimes several files are provided and we need to join them.\n- Deal with **[missing data](https://en.wikipedia.org/wiki/Missing_data)**.\n- Deal with **[outliers](https://en.wikipedia.org/wiki/Outlier)**.\n- Encode **[categorical variables](https://en.wikipedia.org/wiki/Categorical_variable)** if necessary.\n- Deal with noise. For example you may have some floats derived from raw figures. The loss of precision during floating-point arithemics can bring much noise into the data: two seemingly different values might be the same before conversion. Sometimes noise harms model and we would want to avoid that.\n\nHow we choose to perform preprocessing largely depends on what we learn about the data in the previous stage. In practice, I recommend using **[Jupyter Notebook](http://ipython.org/notebook.html)** for data manipulation and mastering usage of frequently used Pandas operations. The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all [reproducible results](https://en.wikipedia.org/wiki/Reproducibility) are very important in data science.\n\nLet\u2019s see some examples.\n\n#### Outlier\n\n![Outlier Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-outlier-example.png)\n\nThe plot shows some scaled coordinates data. We can see that there are some outliers in the top-right corner. Exclude them and the distribution looks good.\n\n#### Dummy Variables\n\nFor categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**. For a categorical variable with `n` possible values, we create a group of `n` dummy variables. Suppose a record in the data takes one value for this variable, then the corresponding dummy variable is set to `1` while other dummies in the same group are all set to `0`.\n\n![Dummies Example](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-guide-dummies-example.png)\n\nIn this example, we transform `DayOfWeek` into 7 dummy variables.\n\nNote that when the categorical variable can take many values (hundreds or more), this might not work well. It\u2019s difficult to find a general solution to that, but I\u2019ll discuss one scenario in the next section.\n\n### Feature Engineering\n\nSome describe the essence of Kaggle competitions as **feature engineering supplemented by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature engineering gets your very far.** Yet it is how well you know about the domain of given data that decides how far you can go. For example, in a competition where data is mainly consisted of texts, Natural Language Processing teachniques are a must. The approach of constructing useful features is something we all have to continuously learn in order to do better.\n\nBasically, **when you feel that a variable is intuitively useful for the task, you can include it as a feature**. But how do you know it actually works? The simplest way is to plot it against the target variable like this:\n\n![Checking Feature Validity](https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en/kaggle-visualize-feature-correlation.png)\n\n#### Feature Selection\n\nGenerally speaking, **we should tr...",
      "url": "https://dnc1994.github.io/2016/05/rank-10-percent-in-first-kaggle-competition-en"
    },
    {
      "title": "How to Rank 10% in Your First Kaggle Competition - KDnuggets",
      "text": "# How to Rank 10% in Your First Kaggle Competition\n\nThis post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project.\n\nPages: [1](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html) [2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2) 3 [4](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/4)\n\n* * *\n\n**Model Training**\n\nWe can improve a model\u2019s performance by tuning its parameters. A model usually have many parameters, but only a few of them are significant to its performance. For example, the most important parameters for a random forset is the number of trees in the forest and the maximum number of features used in developing each tree.\u00a0**We need to understand how models work and what impact does each parameter have to the model\u2019s performance, be it accuracy, robustness or speed.**\n\nNormally we would find the best set of parameters by a process called\u00a0**[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**. Actually what it does is simply iterating through all the possible combinations and find the best one.\n\nBy the way, random forest usually reach optimum when\u00a0`max_features`\u00a0is set to the square root of the total number of features.\n\nHere I\u2019d like to stress some points about tuning XGB. These parameters are generally considered to have real impacts on its performance:\n\n- `eta`: Step size used in updating weights. Lower\u00a0`eta`\u00a0means slower training but better convergence.\n- `num_round`: Total number of iterations.\n- `subsample`: The ratio of training data used in each iteration. This is to combat overfitting.\n- `colsample_bytree`: The ratio of features used in each iteration. This is like\u00a0`max_features`\u00a0in\u00a0`RandomForestClassifier`.\n- `max_depth`: The maximum depth of each tree. Unlike random forest,\u00a0**gradient boosting would eventually overfit if we do not limit its depth**.\n- `early_stopping_rounds`: If we don\u2019t see an increase of validation score for a given number of iterations, the algorithm will stop early. This is to combat overfitting, too.\n\nUsual tuning steps:\n\n1. Reserve a portion of training set as the validation set.\n2. Set\u00a0`eta`\u00a0to a relatively high value (e.g. 0.05 ~ 0.1),\u00a0`num_round`\u00a0to 300 ~ 500.\n3. Use grid search to find the best combination of other parameters.\n4. Gradually lower\u00a0`eta`\u00a0until we reach the optimum.\n5. **Use the validation set as\u00a0`watch_list`\u00a0to re-train the model with the best parameters. Observe how score changes on validation set in each iteration. Find the optimal value for\u00a0`early_stopping_rounds`.**\n\nFinally, note that models with randomness all have a parameter like\u00a0`seed`\u00a0or\u00a0`random_state`\u00a0to control the random seed.\u00a0**You must record this**\u00a0with all other parameters when you get a good model. Otherwise you wouldn\u2019t be able to reproduce it.\n\n**Cross Validation**\n\n**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**\u00a0is an essential step in model training. It tells us whether our model is at high risk of overfitting. In many competitions, public LB scores are not very reliable. Often when we improve the model and get a better local CV score, the LB score becomes worse.\u00a0**It is widely believed that we should trust our CV scores under such situation.** Ideally we would want\u00a0**CV scores obtained by different approaches to improve in sync with each other and with the LB score**, but this is not always possible.\n\nUsually\u00a0**5-fold CV**\u00a0is good enough. If we use more folds, the CV score would become more reliable, but the training takes longer to finish as well. However, we shouldn\u2019t use too many folds if our training data is limited. Otherwise we would have too few samples in each fold to guarantee statistical significance.\n\nHow to do CV properly is not a trivial problem. It requires constant experiment and case-by-case discussion. Many Kagglers share their CV approaches (like\u00a0[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)) after competitions when they feel that reliable CV is not easy.\n\n**Ensemble Generation**\n\n[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\u00a0refers to the technique of combining different models. It\u00a0**reduces both bias and variance of the final model**\u00a0(you can find a proof\u00a0[here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)), thus\u00a0**increasing the score and reducing the risk of overfitting**. Recently it became virtually impossible to win prize without using ensemble in Kaggle competitions.\n\nCommon approaches of ensemble learning are:\n\n- **Bagging**: Use different random subsets of training data to train each base model. Then all the base models vote to generate the final predictions. This is how random forest works.\n- **Boosting**: Train base models iteratively, modify the weights of training samples according to the last iteration. This is how gradient boosted trees work. (Actually it\u2019s not the whole story. Apart from boosting, GBTs try to learn the residuals of earlier iterations.) It performs better than bagging but is more prone to overfitting.\n- **Blending**: Use non-overlapping data to train different base models and take a weighted average of them to obtain the final predictions. This is easy to implement but uses less data.\n- **Stacking**: To be discussed next.\n\nIn theory, for the ensemble to perform well, two factors matter:\n\n- **Base models should be as unrelated as possibly**. This is why we tend to include non-tree-based models in the ensemble even though they don\u2019t perform as well. The math says that the greater the diversity, and less bias in the final ensemble.\n- **Performance of base models shouldn\u2019t differ to much.**\n\nActually we have a\u00a0**trade-off**\u00a0here. In practice we may end up with highly related models of comparable performances. Yet we ensemble them anyway because it usually increase the overall performance.\n\n**Stacking**\n\nCompared with blending, stacking makes better use of training data. Here\u2019s a diagram of how it works:\n\n[![Stacking](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%2099%200'%3E%3C/svg%3E)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)\n\n_(Taken from\u00a0[Faron](https://www.kaggle.com/mmueller). Many thanks!)_\n\nIt\u2019s much like cross validation. Take 5-fold stacking as an example. First we split the training data into 5 folds. Next we will do 5 iterations. In each iteration, train every base model on 4 folds and predict on the hold-out fold.\u00a0**You have to keep the predictions on the testing data as well.**\u00a0This way, in each iteration every base model will make predictions on 1 fold of the training data and all of the testing data. After 5 iterations we will obtain a matrix of shape\u00a0`#(samples in training data) X #(base models)`. This matrix is then fed to the stacker (it\u2019s just another model) in the second level. After the stacker is fitted, use the predictions on testing data by base models ( **each base model is trained 5 times, therefore we have to take an average to obtain a matrix of the same shape**) as the input for the stacker and obtain our final predictions.\n\nMaybe it\u2019s better to just show the codes:\n\nPrize winners usually have larger and much more complicated ensembles. For beginner, implementing a correct 5-fold stacking is good enough.\n\n**\\*Pipeline**\n\nWe can see that the workflow for a Kaggle competition is quite complex, especially for model selection and ensemble. Ideally, we need a highly automated pipeline capable of:\n\n- **Modularized feature transformations**. We only need to write a few lines of codes (or better, rules / DSLs) and the new feature is added to the training set.\n- **Automated grid search**. We only need to set up models and parameter grid, the search w...",
      "url": "https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3"
    }
  ]
}