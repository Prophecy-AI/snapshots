{
  "query": "How do Gaussian Process Regression models perform for small dataset chemical property prediction compared to neural networks?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Gaussian\u2011process regression (GPR) is generally favoured over standard neural networks when only a few hundred (or fewer) experimental measurements are available for a molecular property.  Because GPR is a Bayesian, non\u2011parametric model it can fit the data without the massive parameter count of a neural net and it provides well\u2011calibrated predictive uncertainties, which is especially valuable for low\u2011data chemistry\u202f([Stack\u202fExchange\u202f2024](https://stats.stackexchange.com/questions/645580)).\n\nEmpirical studies on three small molecular datasets show that GPR\u2011based tools (e.g., FlowMO) achieve predictive errors that are on par with deep\u2011learning models while delivering markedly better uncertainty calibration\u202f([FlowMO\u202f2020](https://ui.adsabs.harvard.edu/abs/2020arXiv201001118M/abstract)).  A recent benchmark of probabilistic models on low\u2011data chemical tasks (DIONYSUS) confirms that GPR\u2011type approaches retain high generalisability and superior calibration compared with neural\u2011network baselines\u202f([DIONYSUS\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).  In contrast, neural networks typically need larger training sets to avoid over\u2011fitting and to reach comparable accuracy, and their uncertainty estimates are less reliable unless additional techniques (e.g., ensembles or Bayesian NN) are employed\u202f([Hyperstack\u202f2023](https://www.hyperstack.cloud/blog/case-study/introduction-to-neural-networks-in-machine-learning)).\n\nThus, for small\u2011dataset chemical property prediction, GPR usually matches or exceeds neural\u2011network accuracy and consistently outperforms them in uncertainty quantification, while neural nets become competitive only as the dataset grows beyond the regime where GPR\u2019s cubic\u2011scaling cost becomes prohibitive\u202f([GPR review\u202f2021](https://pmc.ncbi.nlm.nih.gov/articles/PMC8391963)).",
      "url": ""
    },
    {
      "title": "Basics of Neural Networks in Machine Learning - Hyperstack",
      "text": "Basics of Neural Networks in Machine Learning\n![Reserve here](https://www.hyperstack.cloud/hubfs/hyperstack_2023/home/gpu.svg)\nNVIDIAH100 SXMsOn-Demand at$2.40/hour -Reserve from just$1.90/hour.[Reserve here](https://www.hyperstack.cloud/nvidia-hgx-sxm5-h100-reserve-now)\n![Reserve here](https://www.hyperstack.cloud/hubfs/hyperstack_2023/home/gpu.svg)\nDeploy 8 to 16,384 NVIDIA H100 SXM GPUs on the AI Supercloud.[Learn More](https://www.hyperstack.cloud/nvidia-h100-sxm)\n![alert](https://www.hyperstack.cloud/hs-fs/hubfs/hyperstack_2025/home/alert.webp?width=315&amp;height=90&amp;name=alert.webp)\nWe\u2019ve been made aware of a**fraudulent website**impersonating Hyperstack at**hyperstack.my**.\nThis domain**is not affiliated with Hyperstack**or**NexGen Cloud**.\nIf you\u2019ve been approached or interacted with this site, please contact our team immediately at[support@hyperstack.cloud](mailto:support@hyperstack.cloud).\n![close](https://www.hyperstack.cloud/hubfs/hyperstack_2025/home/close.svg)\n[Skip to content](#main-content)\n[\n](https://www.hyperstack.cloud)\n[+44 (0) 203 137 5256](<tel:+44 (0) 203 137 5256>)\n[Login](https://console.hyperstack.cloud/)\n* [Why Hyperstack](https://www.hyperstack.cloud/why-hyperstack)\n* [GPU Pricing](https://www.hyperstack.cloud/gpu-pricing)\n* [AI Studio](https://www.hyperstack.cloud/ai-studio)\n* [Cloud GPUs](javascript:;)\n* [Spot VMs](https://www.hyperstack.cloud/spot-vms)\n* [NVIDIA RTX Pro 6000 SE](https://www.hyperstack.cloud/rtx-pro-6000-se)\n* [NVIDIA H200 SXM](https://www.hyperstack.cloud/nvidia-h200-sxm)\n* [NVIDIA H100 SXM](https://www.hyperstack.cloud/nvidia-h100-sxm)\n* [NVIDIA H100 PCIe](https://www.hyperstack.cloud/h100-pcie)\n* [NVIDIA GB200 NVL72](https://www.hyperstack.cloud/nvidia-blackwell-gb200)\n* [NVIDIA HGX B200](https://www.hyperstack.cloud/nvidia-blackwell-b200)\n* [NVIDIA A100 SXM](https://www.hyperstack.cloud/a100-sxm)\n* [NVIDIA A100](https://www.hyperstack.cloud/a100)\n* [NVIDIA L40](https://www.hyperstack.cloud/l40)\n* [NVIDIA RTX A6000](https://www.hyperstack.cloud/rtx-a6000)\n* [Product](javascript:;)\n* [Object Storage](https://www.hyperstack.cloud/object-storage)\n* [On-Demand Kubernetes](https://www.hyperstack.cloud/on-demand-kubernetes)\n* [Resources](javascript:;)\n* [Blog](https://www.hyperstack.cloud/blog)\n* [Product Updates](https://www.hyperstack.cloud/technical-resources/product-updates)\n* [Hyperstack Tutorials](https://www.hyperstack.cloud/technical-resources/tutorials)\n* [Performance Benchmarks](https://www.hyperstack.cloud/technical-resources/performance-benchmarks)\n* [Case Studies](https://www.hyperstack.cloud/blog/case-study)\n* [Events](https://www.hyperstack.cloud/events)\n* [Documentation](https://infrahub-doc.nexgencloud.com/docs/intro)\n* [DevOps Tools](javascript:;)\n* [LLM Inference Toolkit](https://github.com/NexGenCloud/hyperstack-llm-inference-toolkit)\n* [GPU Selector for LLMs](https://www.hyperstack.cloud/llm-gpu-selector)\n* [Terraform Provider](https://github.com/NexGenCloud/terraform-provider-hyperstack)\n* [SDKs](javascript:;)\n* [Go SDK](https://github.com/NexGenCloud/hyperstack-sdk-go)\n* [Python SDK](https://github.com/NexGenCloud/hyperstack-sdk-python)\n* [Support Hub](https://portal.hyperstack.cloud/)\n* [Feature Requests](<https://feature-requests.hyperstack.cloud/?utm_campaign=172899518-feature requests&amp;utm_source=website&amp;utm_medium=navbar>)\n* [Contact](https://www.hyperstack.cloud/contact)\n[+44 (0) 203 137 5256](<tel:+44 (0) 203 137 5256>)\n[Sign-Up / Login](https://console.hyperstack.cloud/)\n[![Hyperstack](https://www.hyperstack.cloud/hubfs/hyperstack_2023/home/Hyperstack.svg)](https://www.hyperstack.cloud/)\n[Call us](<tel:+44 (0) 203 475 3402>)\n[Sign-Up / Login](https://console.hyperstack.cloud)\n* [Why Hyperstack](https://www.hyperstack.cloud/why-hyperstack)\n* [GPU Pricing](https://www.hyperstack.cloud/gpu-pricing)\n* [AI Studio](https://www.hyperstack.cloud/ai-studio)\n* [Cloud GPUs](javascript:;)\n* [Spot VMs](https://www.hyperstack.cloud/spot-vms)\n* [NVIDIA RTX Pro 6000 SE](https://www.hyperstack.cloud/rtx-pro-6000-se)\n* [NVIDIA H200 SXM](https://www.hyperstack.cloud/nvidia-h200-sxm)\n* [NVIDIA H100 SXM](https://www.hyperstack.cloud/nvidia-h100-sxm)\n* [NVIDIA H100 PCIe](https://www.hyperstack.cloud/h100-pcie)\n* [NVIDIA GB200 NVL72](https://www.hyperstack.cloud/nvidia-blackwell-gb200)\n* [NVIDIA HGX B200](https://www.hyperstack.cloud/nvidia-blackwell-b200)\n* [NVIDIA A100 SXM](https://www.hyperstack.cloud/a100-sxm)\n* [NVIDIA A100](https://www.hyperstack.cloud/a100)\n* [NVIDIA L40](https://www.hyperstack.cloud/l40)\n* [NVIDIA RTX A6000](https://www.hyperstack.cloud/rtx-a6000)\n* [Product](javascript:;)\n* [Object Storage](https://www.hyperstack.cloud/object-storage)\n* [On-Demand Kubernetes](https://www.hyperstack.cloud/on-demand-kubernetes)\n* [Resources](javascript:;)\n* [Blog](https://www.hyperstack.cloud/blog)\n* [Product Updates](https://www.hyperstack.cloud/technical-resources/product-updates)\n* [Hyperstack Tutorials](https://www.hyperstack.cloud/technical-resources/tutorials)\n* [Performance Benchmarks](https://www.hyperstack.cloud/technical-resources/performance-benchmarks)\n* [Case Studies](https://www.hyperstack.cloud/blog/case-study)\n* [Events](https://www.hyperstack.cloud/events)\n* [Documentation](https://infrahub-doc.nexgencloud.com/docs/intro)\n* [DevOps Tools](javascript:;)\n* [LLM Inference Toolkit](https://github.com/NexGenCloud/hyperstack-llm-inference-toolkit)\n* [GPU Selector for LLMs](https://www.hyperstack.cloud/llm-gpu-selector)\n* [Terraform Provider](https://github.com/NexGenCloud/terraform-provider-hyperstack)\n* [SDKs](javascript:;)\n* [Go SDK](https://github.com/NexGenCloud/hyperstack-sdk-go)\n* [Python SDK](https://github.com/NexGenCloud/hyperstack-sdk-python)\n* [Support Hub](https://portal.hyperstack.cloud/)\n* [Feature Requests](<https://feature-requests.hyperstack.cloud/?utm_campaign=172899518-feature requests&amp;utm_source=website&amp;utm_medium=navbar>)\n* [Contact](https://www.hyperstack.cloud/contact)\n![Damanpreet Kaur Vohra](https://www.hyperstack.cloud/hubfs/daman.webp)\n[Damanpreet Kaur Vohra](https://www.hyperstack.cloud/blog/case-study/author/damanpreet-kaur-vohra)\n|\nUpdated on 15 Dec 2025\n# Introduction to Neural Networks in Machine Learning\nTABLE OF CONTENTS\nNVIDIA H100 SXM On-Demand\n[Sign up/Login](https://console.hyperstack.cloud)\n![summary](https://www.hyperstack.cloud/hubfs/hyperstack_2025/blog/summary.svg)\nIn this article, we explored how neural networks in machine learning mimic the human brain to recognise patterns, learn from data, and power applications from image recognition to autonomous vehicles. We traced their evolution from the 1940s to the deep learning revolution, explained their architecture and training techniques, and highlighted different types like CNNs, RNNs, and GANs. Whether it\u2019s transforming NLP, powering AI assistants, or enabling predictive analytics, neural networks continue to redefine what's possible with machine intelligence.\nTechnology has never been more intimately connected to the human experience than through the concept of neural networks in machine learning. Neural networks are sophisticated computational models that draw inspiration from the biological neural networks found in the human brain. These intricate systems are designed to mimic the way neurons in our brains process information and help them learn from vast amounts of data and recognise complex patterns.\nNeural Networks in Machine Learning have proven remarkably successful in various domains, including image and speech recognition, natural language processing, and even gaming. According to[**a study by Stanford researchers**](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), neural networks could accurately determine a person's sexual orientation from facial images with startling accuracy - 81% for men and 74% for women from a single image, outperforming the human perception which was 61% for ...",
      "url": "https://www.hyperstack.cloud/blog/case-study/introduction-to-neural-networks-in-machine-learning"
    },
    {
      "title": "Gaussian Process Molecule Property Prediction with FlowMO",
      "text": "Now on home page\n## ADS\n## Gaussian Process Molecule Property Prediction with FlowMO\n- [Moss, Henry B.](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Moss%2C+Henry+B.%22);\n- [Griffiths, Ryan-Rhys](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Griffiths%2C+Ryan-Rhys%22)\n#### Abstract\nWe present FlowMO: an open-source Python library for molecular property prediction with Gaussian Processes. Built upon GPflow and RDKit, FlowMO enables the user to make predictions with well-calibrated uncertainty estimates, an output central to active learning and molecular design applications. Gaussian Processes are particularly attractive for modelling small molecular datasets, a characteristic of many real-world virtual screening campaigns where high-quality experimental data is scarce. Computational experiments across three small datasets demonstrate comparable predictive performance to deep learning methods but with superior uncertainty calibration.\nPublication:\narXiv e-prints\nPub Date:October 2020DOI:\n[10.48550/arXiv.2010.01118](https://ui.adsabs.harvard.edu/link_gateway/2020arXiv201001118M/doi:10.48550/arXiv.2010.01118)\narXiv:[arXiv:2010.01118](https://ui.adsabs.harvard.edu/link_gateway/2020arXiv201001118M/arXiv:2010.01118)Bibcode:[2020arXiv201001118M](https://ui.adsabs.harvard.edu/abs/2020arXiv201001118M/abstract)Keywords:\n- Computer Science - Machine Learning;\n- Statistics - Machine Learning\nfull text sources\nPreprint\n\\|\n\ud83c\udf13",
      "url": "https://ui.adsabs.harvard.edu/abs/2020arXiv201001118M/abstract"
    },
    {
      "title": "Gaussian Process Regression for Materials and Molecules",
      "text": "Gaussian Process Regression for Materials and Molecules - PMC[Skip to main content](#main-content)\n![](https://pmc.ncbi.nlm.nih.gov/static/img/us_flag.svg)\nAn official website of the United States government\nHere's how you know\nHere's how you know\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-dot-gov.svg)\n**Official websites use .gov**\nA**.gov**website belongs to an official\ngovernment organization in the United States.\n![](https://pmc.ncbi.nlm.nih.gov/static/img/icon-https.svg)\n**Secure .gov websites use HTTPS**\nA**lock**(LockLocked padlock icon) or**https://**means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n[![NCBI home page](https://pmc.ncbi.nlm.nih.gov/static/img/ncbi-logos/nih-nlm-ncbi--white.svg)](https://www.ncbi.nlm.nih.gov/)\nSearch\nLog in\n* [Dashboard](https://www.ncbi.nlm.nih.gov/myncbi/)\n* [Publications](https://www.ncbi.nlm.nih.gov/myncbi/collections/bibliography/)\n* [Account settings](https://www.ncbi.nlm.nih.gov/account/settings/)\n* Log out\nSearch\u2026Search NCBI\n[](https://pmc.ncbi.nlm.nih.gov/)\nSearch PMC Full-Text ArchiveSearch in PMC![Search](https://pmc.ncbi.nlm.nih.gov/static/img/usa-icons-bg/search--white.svg)\n* [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n* [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n* * [](https://doi.org/10.1021/acs.chemrev.1c00022)\n* [](pdf/cr1c00022.pdf)\n* * * ## PERMALINK\nCopy\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)|[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n![ACS AuthorChoice logo](https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-acssd.png)\nChem Rev\n. 2021 Aug 16;121(16):10073\u201310141. doi:[10.1021/acs.chemrev.1c00022](https://doi.org/10.1021/acs.chemrev.1c00022)\n# Gaussian Process Regression for Materials and Molecules\n[Volker L Deringer](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Deringer VL\"[Author]>)\n### Volker L Deringer\n\u2020Department\nof Chemistry, Inorganic Chemistry Laboratory, University of Oxford, Oxford OX1 3QR, United Kingdom\nFind articles by[Volker L Deringer](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Deringer VL\"[Author]>)\n\u2020,\\*,[Albert P Bart\u00f3k](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bart\u00f3k AP\"[Author]>)\n### Albert P Bart\u00f3k\n\u2021Department\nof Physics and Warwick Centre for Predictive Modelling, School of\nEngineering, University of Warwick, Coventry CV4 7AL, United Kingdom\nFind articles by[Albert P Bart\u00f3k](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bart\u00f3k AP\"[Author]>)\n\u2021,\\*,[Noam Bernstein](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bernstein N\"[Author]>)\n### Noam Bernstein\n\u00a7Center\nfor Computational Materials Science, U.S.\nNaval Research Laboratory, Washington D.C. 20375, United States\nFind articles by[Noam Bernstein](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Bernstein N\"[Author]>)\n\u00a7,[David M Wilkins](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wilkins DM\"[Author]>)\n### David M Wilkins\n\u2225Atomistic\nSimulation Centre, School of Mathematics and Physics, Queen\u2019s University Belfast, Belfast BT7 1NN, Northern Ireland, United Kingdom\nFind articles by[David M Wilkins](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Wilkins DM\"[Author]>)\n\u2225,[Michele Ceriotti](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ceriotti M\"[Author]>)\n### Michele Ceriotti\n\u22a5Laboratory\nof Computational Science and Modeling, IMX, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne 1015, Switzerland\n#National\nCentre for Computational Design and Discovery of Novel Materials (MARVEL), \u00c9cole Polytechnique F\u00e9d\u00e9rale\nde Lausanne, Lausanne, Switzerland\nFind articles by[Michele Ceriotti](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Ceriotti M\"[Author]>)\n\u22a5,#,[G\u00e1bor Cs\u00e1nyi](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Cs\u00e1nyi G\"[Author]>)\n### G\u00e1bor Cs\u00e1nyi\n7Engineering\nLaboratory, University of Cambridge, Cambridge CB2 1PZ, United Kingdom\nFind articles by[G\u00e1bor Cs\u00e1nyi](<https://pubmed.ncbi.nlm.nih.gov/?term=\"Cs\u00e1nyi G\"[Author]>)\n7,\\*\n* Author information\n* Article notes\n* Copyright and License information\n\u2020Department\nof Chemistry, Inorganic Chemistry Laboratory, University of Oxford, Oxford OX1 3QR, United Kingdom\n\u2021Department\nof Physics and Warwick Centre for Predictive Modelling, School of\nEngineering, University of Warwick, Coventry CV4 7AL, United Kingdom\n\u00a7Center\nfor Computational Materials Science, U.S.\nNaval Research Laboratory, Washington D.C. 20375, United States\n\u2225Atomistic\nSimulation Centre, School of Mathematics and Physics, Queen\u2019s University Belfast, Belfast BT7 1NN, Northern Ireland, United Kingdom\n\u22a5Laboratory\nof Computational Science and Modeling, IMX, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, Lausanne 1015, Switzerland\n#National\nCentre for Computational Design and Discovery of Novel Materials (MARVEL), \u00c9cole Polytechnique F\u00e9d\u00e9rale\nde Lausanne, Lausanne, Switzerland\n7Engineering\nLaboratory, University of Cambridge, Cambridge CB2 1PZ, United Kingdom\n\\*\nEmail:volker.deringer@chem.ox.ac.uk.\n\\*\nEmail:apbartok@gmail.com.\n\\*\nEmail:gc121@cam.ac.uk.\nReceived 2021 Jan 8; Issue date 2021 Aug 25.\n\u00a92021 The Authors. Published by American Chemical Society\nPermits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained ([https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)).\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\nPMCID: PMC8391963\u00a0\u00a0PMID:[34398616](https://pubmed.ncbi.nlm.nih.gov/34398616/)\n## Abstract\n![graphic file with name cr1c00022_0053.jpg](https://cdn.ncbi.nlm.nih.gov/pmc/blobs/51dd/8391963/a2d2dd86efb1/cr1c00022_0053.jpg)\nWe provide an introduction\nto Gaussian process regression (GPR)\nmachine-learning methods in computational materials science and chemistry.\nThe focus of the present review is on the regression of atomistic\nproperties: in particular, on the construction of interatomic potentials,\nor force fields, in the Gaussian Approximation Potential (GAP) framework;\nbeyond this, we also discuss the fitting of arbitrary scalar, vectorial,\nand tensorial quantities. Methodological aspects of reference data\ngeneration, representation, and regression, as well as the question\nof how a data-driven model may be validated, are reviewed and critically\ndiscussed. A survey of applications to a variety of research questions\nin chemistry and materials science illustrates the rapid growth in\nthe field. A vision is outlined for the development of the methodology\nin the years to come.\n## 1. Introduction\nAt the heart of chemistry is the need to understand the nature,\ntransformations, and macroscopic effects of atomistic structure. This\nis true for*materials*\u2014crystals, glasses, nanostructures,\ncomposites\u2014as well as for*molecules*, from\nthe simplest industrial feedstocks to entire proteins. And with the\noften-quoted role of chemistry as the \u201ccentral science\u201d,[1](#ref1),[2](#ref2)its emphasis on atomistic understanding has a bearing on many neighboring\ndisciplines: candidate drug molecules are made by synthetic chemists\nbased on an atomic-level knowledge of reaction mechanisms; functional\nmaterials for technological applications are characterized on a range\nof length scales, which begins with increasingly accurate information\nabout where exactly the atoms are located relative to one another\nin three-dimensional space.\nResearch progress in structural\nchemistry has largely been driven\nby advances in experimental characterization techniques, from landmark\nstudies in X-ray and neutron crystallography to novel electron microscopy\ntechniques which make it possible to visualize individual atoms directly.\nComplementing these new developments, detailed and realistic structural\ninsight is also increasingly gained from computer simulations. Today,\nchemists (together with materials scientists) are heavy users of large-scale\nsupercomputing fa...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC8391963"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Learning to Make Chemical Predictions: The Interplay of Feature ...",
      "text": "<div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/chem/vol/6/issue/7\"><span><span></span></span></a></p></div><div><div><h2>The Bigger Picture</h2><div><ul><li><span>\u2022</span><span><p>Scarce and sparse chemical datasets. The number of unique small molecules is practically infinite, with number estimates of possible synthesizable small molecules ranging from 10<sup>24</sup>\u201310<sup>60</sup>. Machine learning could expand coverage of chemical space\u2014for instance, in the design, synthesis, and development stages of drugs\u2014that traditionally are resource-intensive tasks.</p></span></li><li><span>\u2022</span><span><p>Machine learning of chemical reactions is a far more difficult task than using <em>ab initio</em> data to train non-reactive potential energy surfaces. It is the next frontier of machine learning in the molecular sciences\u2014to generate a predictive map of chemical reactivity space that can chart all reaction pathways in complex environments.</p></span></li><li><span>\u2022</span><span><p>Physics-informed machine learning: machine learning is designed to determine patterns in high-dimensional data that are otherwise not obvious and thus are perceived to have limited transparency. A worthy goal is to develop physics-inspired features and creation of datasets that model experiments for better understanding of machine learning outcomes.</p></span></li></ul></div></div><div><h2>Summary</h2><div><p>Recently, supervised machine learning has been ascending in providing new predictive approaches for chemical, biological, and materials sciences applications. In this Perspective, we focus on the interplay of machine learning methods with the chemically motivated descriptors and the size and type of datasets needed for molecular property prediction. Using nuclear magnetic resonance chemical shift prediction as an example, we demonstrate that success is predicated on the choice of feature extracted or real-space representations of chemical structures, whether the molecular property data are abundant and/or experimentally or computationally derived, and how these together will influence the correct choice of popular machine learning methods drawn from deep learning, random forests, or kernel methods.</p></div></div></div><ul><li></li><li></li></ul><div><div><h2>UN Sustainable Development Goals</h2><p><span>SDG9: Industry, innovation, and infrastructure</span></p></div><div><h2>Keywords</h2><p><span>machine learning</span></p><p><span>feature representation</span></p><p><span>chemical data</span></p><p><span>chemical property</span></p><p><span>convolutional neural networks</span></p><p><span>random forest regression</span></p></div></div><section><header><h2>Cited by (0)</h2></header></section><div><dl><dt><a href=\"#bfn1\"><span><span><sup>6</sup></span></span></a></dt><dd><p>These authors contributed equally</p></dd></dl></div><p><span>\u00a9 2020 Elsevier Inc.</span></p></article></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2451929420302370"
    },
    {
      "title": "Gaussian Process Regression Networks",
      "text": "Gaussian Process Regression Networks\nAndrew Gordon Wilson agw38@cam.ac.uk\nDavid A. Knowles dak33@cam.ac.uk\nZoubin Ghahramani zoubin@eng.cam.ac.uk\nDepartment of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK\nAbstract\nWe introduce a new regression frame\u0002work, Gaussian process regression networks\n(GPRN), which combines the structural\nproperties of Bayesian neural networks with\nthe nonparametric flexibility of Gaussian pro\u0002cesses. GPRN accommodates input (pre\u0002dictor) dependent signal and noise corre\u0002lations between multiple output (response)\nvariables, input dependent length-scales and\namplitudes, and heavy-tailed predictive dis\u0002tributions. We derive both elliptical slice\nsampling and variational Bayes inference pro\u0002cedures for GPRN. We apply GPRN as a\nmultiple output regression and multivariate\nvolatility model, demonstrating substantially\nimproved performance over eight popular\nmultiple output (multi-task) Gaussian pro\u0002cess models and three multivariate volatility\nmodels on real datasets, including a 1000 di\u0002mensional gene expression dataset.\n1. Introduction\n\u201cLearning representations by back-propagating errors\u201d\nby Rumelhart et al. (1986) is a defining paper in ma\u0002chine learning history. This paper made neural net\u0002works popular for their ability to capture correlations\nbetween multiple outputs, and to discover hidden fea\u0002tures in data, by using adaptive hidden basis functions\nthat were shared across the outputs.\nMacKay (1992) and Neal (1996) later showed that no\nmatter how large or complex the neural network, one\ncould avoid overfitting using a Bayesian formulation.\nNeal (1996) also argued that \u201climiting complexity is\nlikely to conflict with our prior beliefs, and can there\u0002fore only be justified to the extent that it is neces\u0002Appearing in Proceedings of the 29 th International Confer\u0002ence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nsary for computational reasons\u201d. Accordingly, Neal\n(1996) pursued the limit of large models, and found\nthat Bayesian neural networks became Gaussian pro\u0002cesses as the number of hidden units approached infin\u0002ity, and conjectured that \u201cthere may be simpler ways\nto do inference in this case\u201d.\nThese simple inference techniques became the corner\u0002stone of subsequent Gaussian process models (Ras\u0002mussen & Williams, 2006). These models assume a\nprior directly over functions, rather than parameters.\nBy further assuming homoscedastic Gaussian noise,\none can analytically infer a posterior distribution over\nthese functions, given data. The properties of these\nfunctions \u2013 smoothness, periodicity, etc. \u2013 can easily\nbe controlled by a Gaussian process covariance kernel.\nGaussian process models have recently become pop\u0002ular for non-linear regression and classification (Ras\u0002mussen & Williams, 2006), and have impressive em\u0002pirical performances (Rasmussen, 1996).\nHowever, a neural network allowed for correlations be\u0002tween multiple outputs, through sharing adaptive hid\u0002den basis functions across the outputs. In the infinite\nlimit of basis functions, these correlations vanished.\nMoreover, neural networks were envisaged as intelli\u0002gent agents which discovered hidden features and rep\u0002resentations in data, while Gaussian processes, though\neffective at regression and classification, are simply\nsmoothing devices (MacKay, 1998).\nRecently there has been an explosion of interest in ex\u0002tending the Gaussian process regression framework to\naccount for fixed correlations between output variables\n(Alvarez & Lawrence, 2011; Yu et al., 2009; Bonilla\net al., 2008; Teh et al., 2005; Boyle & Frean, 2004).\nThese are often called \u2018multi-task\u2019 learning or \u2018multi\u0002ple output\u2019 regression models. Capturing correlations\nbetween outputs (responses) can be used to make bet\u0002ter predictions. Imagine we wish to predict cadmium\nconcentrations in a region of the Swiss Jura, where ge\u0002ologists are interested in heavy metal concentrations.\nA standard Gaussian process regression model would\nonly be able to use cadmium training measurements.\nGaussian Process Regression Networks\nWith a multi-task method, we can also make use of\ncorrelated heavy metal measurements to enhance cad\u0002mium predictions (Goovaerts, 1997). We could further\nenhance predictions if we could use how these (signal)\ncorrelations change with geographical location.\nThere has similarly been great interest in extending\nGaussian process (GP) regression to account for in\u0002put dependent noise variances (Goldberg et al., 1998;\nKersting et al., 2007; Adams & Stegle, 2008; Turner,\n2010; Wilson & Ghahramani, 2010b;a; L\u00b4azaro-Gredilla\n& Titsias, 2011). Wilson & Ghahramani (2010a; 2011)\nand Fox & Dunson (2011) further extended the GP\nframework to accommodate input dependent noise cor\u0002relations between multiple output (response) variables.\nIn this paper, we introduce a new regression frame\u0002work, Gaussian Process Regression Networks (GPRN),\nwhich combines the structural properties of Bayesian\nneural networks with the nonparametric flexibility of\nGaussian processes. This network is an adaptive mix\u0002ture of Gaussian processes, which naturally accommo\u0002dates input dependent signal and noise correlations\nbetween multiple output variables, input dependent\nlength-scales and amplitudes, and heavy tailed predic\u0002tive distributions, without expensive or numerically\nunstable computations. The GPRN framework ex\u0002tends and unifies the work of Journel & Huijbregts\n(1978), Neal (1996), Gelfand et al. (2004), Teh et al.\n(2005), Adams & Stegle (2008), Turner (2010), and\nWilson & Ghahramani (2010b; 2011).\nThroughout this text we assume we are given a dataset\nof input output pairs, D = {(xi, y(xi)) : i = 1, . . . , N},\nwhere x \u2208 X is an input (predictor) variable belonging\nto an arbitrary set X , and y(x) is the corresponding\np dimensional output; each element of y(x) is a one\ndimensional output (response) variable, for example\nthe concentration of a single heavy metal at a geo\u0002graphical location x. We aim to predict y(x\u2217)|x\u2217, D\nand \u03a3(x\u2217) = cov[y(x\u2217)|x\u2217, D] at a test input x\u2217, while\naccounting for input dependent signal and noise cor\u0002relations between the elements of y(x).\nWe start by introducing the GPRN framework and\ndiscussing inference. We then further discuss related\nwork, before comparing to eight multiple output GP\nmodels, on gene expression and geostatistics datasets,\nand three multivariate volatility models on several\nbenchmark financial datasets. In the supplementary\nmaterial (Wilson & Ghahramani, 2012) we further dis\u0002cuss theoretical aspects of GPRN, and review GP re\u0002gression and notation (Rasmussen & Williams, 2006).\n2. Gaussian Process Networks\nWe wish to model a p dimensional function y(x), with\nsignal and noise correlations that vary with x.\nWe model y(x) as\ny(x) = W(x)[f(x) + \u03c3f \u000f] + \u03c3yz , (1)\nwhere \u000f = \u000f(x) and z = z(x) are respectively N (0, Iq)\nand N (0, Ip) white noise processes. Iq and Ip are q \u00d7q\nand p\u00d7p dimensional identity matrices. W(x) is a p\u00d7q\nmatrix of independent Gaussian processes such that\nW(x)ij \u223c GP(0, kw), and f(x) = (f1(x), . . . , fq(x))>\nis a q \u00d7 1 vector of independent GPs with fi(x) \u223c\nGP(0, kfi). The GPRN prior on y(x) is induced\nthrough GP priors in W(x) and f(x), and the noise\nmodel is induced through \u000f and z.\nWe represent the Gaussian process regression network\n(GPRN)1 of equation (1) in Figure 1. Each of the la\u0002tent Gaussian processes in f(x) has additive Gaussian\nnoise. Changing variables to include the noise \u03c3f \u000f, we\nlet \u02c6fi(x) = fi(x) + \u03c3f \u000f \u223c GP(0, kf\u02c6i\n), where\nkf\u02c6i(xa, xw) = kfi(xa, xw) + \u03c3\n2\nf\n\u03b4aw , (2)\nand \u03b4aw is the Kronecker delta. The latent node func\u0002tions f\u02c6(x) are connected together to form the outputs\ny(x). The strengths of the connections change as a\nfunction of x; the weights themselves \u2013 the entries of\nW(x) \u2013 are functions. Old connections can break and\nnew connections can form. This is an adaptive net\u0002work, where the signal and noise correlations between\nthe components of y(x) vary with x. We label the\nlength-scale hyperparameter...",
      "url": "https://icml.cc/2012/papers/329.pdf"
    },
    {
      "title": "Neural network Gaussian processes as efficient models of potential ...",
      "text": "## We apologize for the inconvenience...\n\nTo ensure we keep this website safe, please can you confirm you are a human by ticking the box below.\n\nIf you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.\n\n[https://ioppublishing.org/contacts/](https://ioppublishing.org/contacts/)\n\n**Incident ID: 9b5137d6-cnvj-4d10-ba8e-2d9df45c1828**",
      "url": "https://iopscience.iop.org/article/10.1088/2632-2153/ad0652/pdf"
    },
    {
      "title": "Uncertainty quantification with graph neural networks for efficient ...",
      "text": "Uncertainty quantification with graph neural networks for efficient molecular design | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-025-58503-0?error=cookies_not_supported&code=abdbb114-11c0-416e-95c2-4d2c24b21a5c)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nUncertainty quantification with graph neural networks for efficient molecular design\n[Download PDF](https://www.nature.com/articles/s41467-025-58503-0.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-025-58503-0.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:05 April 2025# Uncertainty quantification with graph neural networks for efficient molecular design\n* [Lung-Yi Chen](#auth-Lung_Yi-Chen-Aff1)[ORCID:orcid.org/0000-0002-9411-6404](https://orcid.org/0000-0002-9411-6404)[1](#Aff1)&amp;\n* [Yi-Pei Li](#auth-Yi_Pei-Li-Aff1-Aff2)[ORCID:orcid.org/0000-0002-1314-3276](https://orcid.org/0000-0002-1314-3276)[1](#Aff1),[2](#Aff2)\n[*Nature Communications*](https://www.nature.com/ncomms)**volume16**, Article\u00a0number:3262(2025)[Cite this article](#citeas)\n* 19kAccesses\n* 21Citations\n* 11Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-025-58503-0/metrics)\n### Subjects\n* [Chemical engineering](https://www.nature.com/subjects/chemical-engineering)\n* [Cheminformatics](https://www.nature.com/subjects/cheminformatics)\n## Abstract\nOptimizing molecular design across expansive chemical spaces presents unique challenges, especially in maintaining predictive accuracy under domain shifts. This study integrates uncertainty quantification (UQ), directed message passing neural networks (D-MPNNs), and genetic algorithms (GAs) to address these challenges. We systematically evaluate whether UQ-enhanced D-MPNNs can effectively optimize broad, open-ended chemical spaces and identify the most effective implementation strategies. Using benchmarks from the Tartarus and GuacaMol platforms, our results show that UQ integration via probabilistic improvement optimization (PIO) enhances optimization success in most cases, supporting more reliable exploration of chemically diverse regions. In multi-objective tasks, PIO proves especially advantageous, balancing competing objectives and outperforming uncertainty-agnostic approaches. This work provides practical guidelines for integrating UQ in computational-aided molecular design (CAMD).\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-51321-w/MediaObjects/41467_2024_51321_Fig1_HTML.png)\n### [Data-driven quantum chemical property prediction leveraging 3D conformations with Uni-Mol+](https://www.nature.com/articles/s41467-024-51321-w?fromPaywallRec=false)\nArticleOpen access19 August 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41529-023-00391-0/MediaObjects/41529_2023_391_Fig1_HTML.png)\n### [Searching the chemical space for effective magnesium dissolution modulators: a deep learning approach using sparse features](https://www.nature.com/articles/s41529-023-00391-0?fromPaywallRec=false)\nArticleOpen access12 September 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-025-04972-3/MediaObjects/41597_2025_4972_Fig1_HTML.png)\n### [The QD*\u03c0*dataset, training data for drug-like molecules and biopolymer fragments and their interactions](https://www.nature.com/articles/s41597-025-04972-3?fromPaywallRec=false)\nArticleOpen access25 April 2025\n## Introduction\nThe exploration of novel chemical materials is a pivotal scientific endeavor with the potential to significantly advance both the economy and society[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](https://www.nature.com/articles/s41467-025-58503-0#ref-CR4). Historically, the discovery of innovative molecules has led to major breakthroughs in various fields, including the development of enhanced medical therapies[5](https://www.nature.com/articles/s41467-025-58503-0#ref-CR5), innovative catalysts for chemical reactions[6](https://www.nature.com/articles/s41467-025-58503-0#ref-CR6), and more efficient carbon capture technologies[7](https://www.nature.com/articles/s41467-025-58503-0#ref-CR7). These discoveries have traditionally resulted from labor-intensive experimental processes characterized by extensive trial and error.\nIn response to the limitations of these traditional experimental approaches, computational-aided molecular design (CAMD) has emerged as a crucial innovation. By conceptualizing material design as an optimization problem, where molecular structures and their properties are treated as variables and objectives, CAMD harnesses computational power to efficiently predict and identify promising materials. The advent of sophisticated machine learning algorithms has marked a paradigm shift from conventional knowledge-based methods, such as the group contribution method[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41467-025-58503-0#ref-CR10), to advanced learning-based strategies[11](https://www.nature.com/articles/s41467-025-58503-0#ref-CR11). Among these, deep learning has demonstrated exceptional accuracy and flexibility, modeling complex interrelations between chemical structures and properties that challenge traditional theoretical approaches[12](https://www.nature.com/articles/s41467-025-58503-0#ref-CR12). For example, graph neural networks (GNNs) have emerged as powerful tools for representing molecular structures[13](https://www.nature.com/articles/s41467-025-58503-0#ref-CR13). Unlike traditional models that rely on fixed molecular descriptors, GNNs operate directly on molecular graphs, capturing detailed connectivity and spatial relationships between atoms. This graph-based approach enables GNNs to model molecular interactions with high fidelity[14](https://www.nature.com/articles/s41467-025-58503-0#ref-CR14),[15](https://www.nature.com/articles/s41467-025-58503-0#ref-CR15), making them particularly well-suited for applications in molecular design, where accurate structural representation is critical. Furthermore, GNNs offer scalability[16](https://www.nature.com/articles/s41467-025-58503-0#ref-CR16), enabling efficient processing of large datasets, which is essential for exploring the expansive chemical spaces required in CAMD.\nAs CAMD has evolved, it has incorporated various generative models and sophisticated optimization strategies that employ surrogate models as objective functions to enhance molecular design. For example, variational autoencoders (VAEs) have been widely used for molecular generation by encoding molecules into a latent space where new structures can be sampled and decoded, facilitating exploration of chemical space[17](https://www.nature.com/articles/s41467-025-58503-0#ref-CR17),[18](https://www.nature.com/articles/s41467-025-58503-0#ref-CR18). These VAEs are often coordinated with optimization techniques, such as evolutionary algorithms[19](#ref-CR19),[20](#...",
      "url": "https://www.nature.com/articles/s41467-025-58503-0"
    }
  ]
}