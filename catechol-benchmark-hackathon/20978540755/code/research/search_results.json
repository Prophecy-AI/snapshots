{
  "query": "How can I reduce the gap between cross-validation and test performance in chemical reaction yield prediction with small datasets?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Based on recent studies of QSPR/QSAR modelling, probabilistic calibration, and transfer\u2011learning approaches for low\u2011data chemistry, you can narrow the cross\u2011validation\u2011to\u2011test performance gap in reaction\u2011yield prediction by following these focused steps\u202f([Rakhimbekova\u202f2021](https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735); [Aspuru\u2011Guzik\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b); [Shim\u202f2022](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b); [Schindler\u202f2025](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d); [Wiest\u202f2023](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h); [Schwaller\u202f2020](https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101)):\n\n1. **Adopt \u201cout\u2011of\u2011scope\u201d cross\u2011validation schemes** \u2013 use *transformation\u2011out* and *solvent\u2011out* CV (or analogous \u201cleave\u2011group\u2011out\u201d splits) so that each fold mimics truly unseen reaction types or conditions, giving an unbiased estimate of performance on novel chemistry\u202f([Rakhimbekova\u202f2021](https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735)).\n\n2. **Standardize the CV protocol** \u2013 follow the MatFold recommendations (e.g., fixed random seeds, stratified splits by reaction class, reporting both mean and variance) to avoid optimistic bias that inflates CV scores relative to an external test set\u202f([Schindler\u202f2025](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d)).\n\n3. **Calibrate model uncertainty on low\u2011data sets** \u2013 train a probabilistic model such as DIONYSUS and apply post\u2011hoc calibration (e.g., temperature scaling or isotonic regression) to align predicted confidence intervals with observed errors, which reduces over\u2011optimistic CV metrics\u202f([Aspuru\u2011Guzik\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b)).\n\n4. **Enrich the training set via data augmentation** \u2013 generate chemically plausible synthetic reactions (e.g., by perturbing reagents, solvents, or stoichiometry) and augment yields with noise consistent with experimental variability; this expands the effective sample size and stabilizes CV estimates\u202f([Schwaller\u202f2020](https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101)).\n\n5. **Leverage active transfer learning** \u2013 start from a pre\u2011trained model on a related reaction family, then iteratively select the most informative new experiments (high uncertainty or high expected improvement) to add to the training set; keep the model simple (few shallow decision trees) to preserve generalizability\u202f([Shim\u202f2022](https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b)).\n\n6. **Validate on a truly external hold\u2011out set** \u2013 after model selection, evaluate on a separate real\u2011world dataset (e.g., historical ELN data) that was never used in any CV split; this final check reveals any remaining optimism and guides further refinement\u202f([Wiest\u202f2023](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)).\n\n7. **Report both CV and external test metrics with uncertainty bounds** \u2013 include confidence intervals, calibration curves, and error distribution plots; transparent reporting makes the gap evident and helps you iteratively close it\u202f([Thomas\u202f2014](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10)).  \n\nFollowing these steps\u2014especially using out\u2011of\u2011scope CV, calibrated probabilistic models, data augmentation, and active transfer learning\u2014will align cross\u2011validation performance more closely with true test\u2011set results even when only a few hundred reaction examples are available.",
      "url": ""
    },
    {
      "title": "[PDF] Cross-validation strategies in QSPR modelling of chemical reactions | Semantic Scholar",
      "text": "<div><div><div><pre>@article{Rakhimbekova2021CrossvalidationSI,\ntitle={Cross-validation strategies in QSPR modelling of chemical reactions},\nauthor={Assima Rakhimbekova and Tagir Akhmetshin and G I Minibaeva and Ramil I. Nugmanov and Timur R. Gimadiev and Timur I. Madzhidov and Igor I. Baskin and Alexandre Varnek},\njournal={SAR and QSAR in Environmental Research},\nyear={2021},\nvolume={32},\npages={207 - 219},\nurl={https://api.semanticscholar.org/CorpusID:231962595}\n}</pre><div><p><span><span>Two strategies of model cross-validation of the quantitative structure-property relationship models for reactions are suggested, \u2018transformation-out\u2019 CV, and \u2018solvent-out' CV, which provide an unbiased estimation of the predictive performance of the models for novel types of structural transformations in chemical reactions and reactions going under new conditions.</span></span></p></div></div><div><div><div><h2>12 Citations</h2></div><div><a href=\"https://www.semanticscholar.org/paper/National-Institutes-of-Health-(NIH)-Workshop-on-Warr/a1473d56107d2628477218ab45451e93b020591c\"><h3>National Institutes of Health (NIH) Workshop on Reaction Informatics</h3></a><ul><span><span><a href=\"https://www.semanticscholar.org/author/W.-Warr/1710629\"><span><span>W. Warr</span></span></a></span></span><p><span>Chemistry, Computer Science</span></p><li><span><span>2021</span></span></li></ul><div><p><span><span>The themes, in the order used for this report, were reaction representations, file formats, and standards; sources of reaction data; AI and machine learning applications of reaction-related data in de novo drug design, synthetic accessibility, synthesis planning, reaction prediction, and automation and progression toward autonomous synthesis.</span></span></p></div></div></div><div><h2>41 References</h2></div></div></div></div>",
      "url": "https://www.semanticscholar.org/paper/Cross-validation-strategies-in-QSPR-modelling-of-Rakhimbekova-Akhmetshin/2dc4403f1bf5e74cc4fc6e8c549a0be6a385a735"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Predicting reaction conditions from limited data through active transfer learning",
      "text": "Predicting reaction conditions from limited data\nthrough active transfer learning\u2020\nEunjae Shim, a Joshua A. Kammeraad, ab Ziping Xu, b Ambuj Tewari, bc\nTim Cernak *ad and Paul M. Zimmerman *a\nTransfer and active learning have the potential to accelerate the development of new chemical reactions,\nusing prior data and new experiments to inform models that adapt to the target area of interest. This article\nshows how specifically tuned machine learning models, based on random forest classifiers, can expand the\napplicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First,\nmodel transfer is shown to be effective when reaction mechanisms and substrates are closely related, even\nwhen models are trained on relatively small numbers of data points. Then, a model simplification scheme is\ntested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen\nreagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit\nover random selection, an active transfer learning strategy is introduced to improve model predictions.\nSimple models, composed of a small number of decision trees with limited depths, are crucial for\nsecuring generalizability, interpretability, and performance of active transfer learning.\nIntroduction\nComputers are becoming increasingly capable of performing\nhigh-level chemical tasks.1\u20134 Machine learning approaches have\ndemonstrated viable retrosynthetic analyses,5\u20137 product predic\u0002tion,8\u201311 reaction condition suggestion,12\u201316 prediction of ster\u0002eoselectivity,17\u201320 regioselectivity,19,21\u201324 and reaction yield25,26\nand optimization of reaction conditions.27\u201330 These advances\nallow computers to assist synthesis planning for functional\nmolecules using well-established chemistry. For machine\nlearning to aid the development of new reactions, a model\nbased on established chemical knowledge must be able to\ngeneralize its predictions to reactivity that lies outside of the\ndataset. However, because most supervised learning algorithms\nlearn how features (e.g. reaction conditions) within a particular\ndomain relate to an outcome (e.g. yield), the model is not ex\u0002pected to be accurate outside its domain. This situation\nrequires chemists to consider other machine learning methods\nfor navigating new reactivity.\nExpert knowledge based on known reactions plays a central\nrole in the design of new reactions. The assumption that\nsubstrates with chemically similar reaction centers have trans\u0002ferable performance provides a plausible starting point for\nexperimental exploration. This concept of chemical similarity,\ntogether with literature data, guides expert chemists in the\ndevelopment of new reactions. Transfer learning, which\nassumes that data from a nearby domain, called the source\ndomain, can be leveraged to model the problem of interest in\na new domain, called the target domain,31 emulates a tactic\ncommonly employed by human chemists.\nTransfer learning is a promising strategy when limited data\nis available in the domain of interest, but a sizeable dataset is\navailable in a related domain.31,32 Models are \ue103rst created using\nthe source data, then transferred to the target domain using\nvarious algorithms.19,33\u201335 For new chemical targets where no\nlabeled data is available, the head start in predictivity a source\nmodel can provide becomes important. However, when a shi\ue09d\nin distribution of descriptor values occurs (e.g., descriptors\noutside of the original model ranges) in the target data, making\npredictions becomes challenging. For such a situation, the\nobjective of transfer learning becomes training a model that is\nas predictive in the target domain as possible.31,36 Toward this\nend, cross-validation is known to improve generalizability by\nproviding a procedure to avoid over\ue103tting on the training data.37\nThe reduction of generalization error, however, may not be\nsufficient outside the source domain. Accordingly, new\nmethods that enhance the applicability of a transferred model\nto new targets would be bene\ue103cial for reaction condition\nprediction.\nAnother machine learning method that can help tackle data\nscarcity is active learning. By making iterative queries of\na\nDepartment of Chemistry, University of Michigan, Ann Arbor, MI, USA. E-mail:\npaulzim@umich.edu\nb\nDepartment of Statistics, University of Michigan, Ann Arbor, MI, USA\nc\nDepartment of Electrical Engineering and Computer Science, University of Michigan,\nAnn Arbor, MI, USA\nd\nDepartment of Medicinal Chemistry, University of Michigan, Ann Arbor, MI, USA.\nE-mail: tcernak@med.umich.edu\n\u2020 Electronic supplementary information (ESI) available: Additional results. See\nhttps://doi.org/10.1039/d1sc06932b.\nCite this: Chem. Sci., 2022, 13, 6655\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 10th December 2021\nAccepted 10th May 2022\nDOI: 10.1039/d1sc06932b\nrsc.li/chemical-science\n\u00a9 2022 The Author(s). Published by the Royal Society of Chemistry Chem. Sci., 2022, 13, 6655\u20136668 | 6655\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 11 May 2022. Downloaded on 8/27/2025 5:29:13 AM. This article is licensed under a Creative Commons Attribution-NonCommercial 3.0 Unported Licence. View Article Online View Journal | View Issue\nlabeling a small number of datapoints, active learning updates\nmodels with knowledge from newly labeled data. As a result,\nexploration is guided into the most informative areas and\navoids collection of unnecessary data.38,39 Active learning is\ntherefore well-suited for reaction development, which greatly\nbene\ue103ts from efficient exploration and where chemists conduct\nthe next batch of reactions based on previous experimental\nresults. Based on this analogy, reaction optimization27,28 and\nreaction condition identi\ue103cation40 have been demonstrated to\nbene\ue103t from active learning. However, these prior works initiate\nexploration with randomly selected data points (Fig. 1A) which\ndoes not leverage prior knowledge, and therefore does not\nre\ue104ect how expert chemists initiate exploration. Initial search\ndirected by transfer learning could identify productive regions\nearly on, which in turn will help build more useful models for\nsubsequent active learning steps.\nTo align transfer and active learning closer to how expert\nchemists develop new reactions, appropriate chemical reaction\ndata is necessary.41 Available datasets42 that are o\ue09den used for\nmachine learning are overrepresented by positive reactions,\nfailing to re\ue104ect reactions with negative outcomes. On the other\nhand, reaction condition screening data of methodology\nreports\u2014which chemists o\ue09den refer to\u2014only constitute\na sparse subset of possible reagent combinations, making it\nhard for machine learning algorithms to extract meaningful\nknowledge.43\nHigh-throughput experimentation44\u201346 (HTE) data can \ue103ll\nthis gap. HTE provides reaction data16,25,27,47,48 with reduced\nvariations in outcome due to systematic experimentation. Pd\u0002catalyzed coupling data was therefore collected from reported\nwork using nanomole scale HTE in 1536 well plates.49\u201351 In the\ncurrent work, subsets of this data, classi\ue103ed by nucleophile type\nas shown in Fig. 2A, were selected to a dataset size of approxi\u0002mately 100 datapoints, which captured both positive and\nnegative reaction performance.\nReaction condition exploration could be made more efficient\nif algorithmic strategies could leverage prior knowledge. Toward\nthis goal, model transfer and its combination with active\nlearning were evaluated. Taking advantage of diverse campaigns,\nthis study will show that transferred models can be effective in\napplying prior reaction conditions to a new substrate type under\ncertain conditions. Next, the source model's ability to predict\nreaction conditions with new combinations of reagents will also\nbe evaluated. Lastly, challenging scenarios are considered where\nproductive reaction conditions for one class of substrate...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2022/sc/d1sc06932b"
    },
    {
      "title": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols \u2020",
      "text": "MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols - Digital Discovery (RSC Publishing) DOI:10.1039/D4DD00250D\n[![Royal Society of Chemistry](https://pubs.rsc.org/content/NewImages/royal-society-of-chemistry-logo.png)](https://pubs.rsc.org/)\n[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2025/dd/d4dd00250d)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00353e)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00313f)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](https://pubs.rsc.org/content/newimages/open_access_blue.png)Open Access Article\n![](https://pubs.rsc.org/content/newimages/CCBY.svg)This Open Access Article is licensed under a\n[Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\nDOI:[10.1039/D4DD00250D](https://doi.org/10.1039/D4DD00250D)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2025,**4**, 625-635\n# MatFold: systematic insights into materials discovery models' performance through standardized cross-validation protocols[\u2020](#fn1)\nMatthew D. Witman[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0001-6263-5114)\\*aandPeter Schindler[![ORCID logo](https://pubs.rsc.org/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1319-6570)\\*b\naSandia National Laboratories, Livermore, California 94551, USA. E-mail:[mwitman@sandia.gov](mailto:mwitman@sandia.gov)\nbNortheastern University, Boston, Massachusetts 02115, USA. E-mail:[p.schindler@northeastern.edu](mailto:p.schindler@northeastern.edu)\nReceived 7th August 2024, Accepted 7th December 2024\nFirst published on 9th December 2024\n## Abstract\nMachine learning (ML) models in the materials sciences that are validated by overly simplistic cross-validation (CV) protocols can yield biased performance estimates for downstream modeling or materials screening tasks. This can be particularly counterproductive for applications where the time and cost of failed validation efforts (experimental synthesis, characterization, and testing) are consequential. We propose a set of standardized and increasingly difficult splitting protocols for chemically and structurally motivated CV that can be followed to validate any ML model for materials discovery. Among several benefits, this enables systematic insights into model generalizability, improvability, and uncertainty, provides benchmarks for fair comparison between competing models with access to differing quantities of data, and systematically reduces possible data leakage through increasingly strict splitting protocols. Performing thorough CV investigations across increasingly strict chemical/structural splitting criteria, localvs.global property prediction tasks, smallvs.large datasets, and structurevs.compositional model architectures, some common threads are observed; however, several marked differences exist across these exemplars, indicating the need for comprehensive analysis to fully understand each model's generalization accuracy and potential for materials discovery. For this we provide a general-purpose, featurization-agnostic toolkit, MatFold, to automate reproducible construction of these CV splits and encourage further community use in model benchmarking.\n## Introduction\nUnderstanding and quantifying the generalizability, improvability, and uncertainty of machine learning (ML)-based materials discovery models is critical, especially in applications where downstream experimental validation (synthesis, characterization, and testing) is often time- and cost-intensive. Careful, and sometimes extensive, cross-validation (CV) is required to both avoid erroneous conclusions regarding a model's capabilities and to fully understand its limitations.[1](#cit1)Withholding randomly selected test data is often insufficient for quantifying a model's performance as this sub-set is drawn from the same distribution that potentially suffers from data leakage. This in-distribution (ID) generalization error is typically minimized during model training and hyperparameter tuning to avoid over/underfitting. Model prediction uncertainties can be assessed utilizing model ensembling (e.g., for bagged regressor ML models[2,3](#cit2)and deep neural networks[4,5](#cit4)) and/or through nested (\u201cdouble\u201d) CV.[6](#cit6)However, the out-of-distribution (OOD) generalization error constitutes a more useful performance metric for assessing a model's true ability to generalize to unseen data\u2014an especially critical factor when models are used to discover materials with exceptional target properties (i.e., outliers).[7](#cit7)This error originates from either lack of knowledge (e.g., imbalance in data, or poor data representation) or sub-optimal model architecture and is referred to as beingepistemic.[4](#cit4)Evaluating OOD generalization, however, requires more careful considerations during data splitting.\nOne approach to constructing OOD test sets is to utilize unsupervised clustering with a chosen materials featurization and then conduct leave-one-cluster-out CV (LOCO-CV). For example, on compositional models for superconducting transition temperatures, LOCO-CV revealed how generalizability and expected accuracy are drastically overestimated due to data leakage in random train/test splits.[8](#cit8)Omeeet al.have investigated the performance of OOD prediction tasks on MatBench[9](#cit9)datasets (refractive index, shear modulus, and formation energy) utilizing structure-based graph neural network (GNN) models and LOCO-CV (k-means clustering and t-distributed stochastic neighbor embedding).[10](#cit10)Huet al.similarly have utilized LOCO-CV to study the improvement of OOD generalizability of various domain adaptation algorithms during materials property predictions (experimental band gaps and bulk metallic glass formation ability).[11](#cit11)\nQuantifying distribution shifts in materials databases over time and identifying whether specific samples are OOD have been shown critical for developing databases and models that promote greater robustness and generalizability.[12](#cit12)To quantify whether data points are OOD can be assessed based on their distance to training data in feature space (e.g.,viakernel density estimates[2](#cit2)). Data bias arising from uneven coverage of materials families may also be mitigated by entropy-targeted active learning.[13](#cit13)\nAlternative methods for defining OOD splits without relying on the feature space include using (i) target property ranges, (ii) time or date thresholds when data was added, or (iii) general materials information, such as structure, chemistry, or prototype/class. Splits based on target-property-sorted data[14](#cit14)can facilitate the discovery of materials with extraordinary target properties[7](#cit7)and has also been used in \u201ck-fold forward CV\u201d.[15](#cit15)Splitting datasets based on when data points were added mimics acquiring new, unseen data that may be realistically considered OOD.[14,16,17](#cit14)Lastly, the OOD generalization has recently been studied for formation energy models with structural and chemical hold-outs.[18](#cit18)\nTo further encourage standardized reporting of these types of detailed insights into generalization performance and limitations of ML-based models in the materials sciences, here we provide \u201cMatFold\u201d as a featurization-agnostic programmatic tool for automatically generating CV splits for arbitrary materials datasets and model architectures, such as structure-based[19](#cit19)or composition-based[20](#cit20)models. Specifically, we propose a standardized series of CV splits based on increasingly difficult chemical/structural hold-out criteria, dataset size reduction, nestedvs.non-nested splits, and others. By assessing model performance across various combinations of MatFold spli...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2025/dd/d4dd00250d"
    },
    {
      "title": "On the use of real-world datasets for reaction yield prediction",
      "text": "[Jump to main content ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#maincontent) [Jump to site search ![](https://www.rsc-cdn.org/oxygen/assets/icons/arrow-right-o-light.png)](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#SearchText)\n\n[Issue 19, 2023](https://pubs.rsc.org/en/journals/journal/sc?issueid=sc014019&type=current&issnprint=2041-6520)\n\n[![](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=CoverIssue&imageInfo.ImageIdentifier.SerCode=SC&imageInfo.ImageIdentifier.IssueId=SC014019)\\\n\\\nFrom the journal: **Chemical Science**](https://pubs.rsc.org/en/journals/journal/sc)\n\n## On the use of real-world datasets for reaction yield prediction [\u2020](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h\\#fn1)\n\n![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_horizontal.svg)\n\n[Mandana\\\nSaebi](https://pubs.rsc.org/en/results?searchtext=Author%3AMandana%20Saebi), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#fn2) _a_[Bozhao\\\nNan](https://pubs.rsc.org/en/results?searchtext=Author%3ABozhao%20Nan), [\u2021](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#fn2) _b_[John E.\\\nHerr](https://pubs.rsc.org/en/results?searchtext=Author%3AJohn%20E.%20Herr),_b_[Jessica\\\nWahlers](https://pubs.rsc.org/en/results?searchtext=Author%3AJessica%20Wahlers),_b_[Zhichun\\\nGuo](https://pubs.rsc.org/en/results?searchtext=Author%3AZhichun%20Guo),_a_[Andrzej M.\\\nZura\u0144ski](https://pubs.rsc.org/en/results?searchtext=Author%3AAndrzej%20M.%20Zura%C5%84ski), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-8640-7551)_c_[Thierry\\\nKogej](https://pubs.rsc.org/en/results?searchtext=Author%3AThierry%20Kogej),_d_[Per-Ola\\\nNorrby](https://pubs.rsc.org/en/results?searchtext=Author%3APer-Ola%20Norrby), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-2419-0705)_e_[Abigail G.\\\nDoyle](https://pubs.rsc.org/en/results?searchtext=Author%3AAbigail%20G.%20Doyle), [![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0002-6641-0833)_cf_[Nitesh V.\\\nChawla](https://pubs.rsc.org/en/results?searchtext=Author%3ANitesh%20V.%20Chawla)[![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0003-3932-5956)\n\\*_a_\nand\n[Olaf\\\nWiest](https://pubs.rsc.org/en/results?searchtext=Author%3AOlaf%20Wiest)[![ORCID logo](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/orcid_16x16.png)](https://orcid.org/0000-0001-9316-7720)\n\\*_b_\n\n[Author affiliations](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)\n\n\\\\* Corresponding authors\n\naDepartment of Computer Science and Engineering and Lucy Family Institute for Data and Society, University of Notre Dame, Notre Dame, IN 46556, USA\n\n**E-mail:** [nchawla@nd.edu](mailto:nchawla@nd.edu)\n\nbDepartment of Chemistry and Biochemistry, University of Notre Dame, Notre Dame, IN 46556, USA\n\n**E-mail:** [owiest@nd.edu](mailto:owiest@nd.edu)\n\ncDepartment of Chemistry, Princeton University, Princeton, New Jersey 08544, USA\n\ndMolecular AI, Discovery Sciences, R&D, AstraZeneca, Pepparedsleden 1, SE-431 83 M\u00f6lndal, Gothenburg, Sweden\n\neData Science and Modelling, Pharmaceutical Sciences, R&D, AstraZeneca, Pepparedsleden 1, SE-431 83 M\u00f6lndal, Gothenburg, Sweden\n\nfDepartment of Chemistry and Biochemistry, University of California, Los Angeles, California 90095, USA\n\n### Abstract\n\nThe lack of publicly available, large, and unbiased datasets is a key bottleneck for the application of machine learning (ML) methods in synthetic chemistry. Data from electronic laboratory notebooks (ELNs) could provide less biased, large datasets, but no such datasets have been made publicly available. The first real-world dataset from the ELNs of a large pharmaceutical company is disclosed and its relationship to high-throughput experimentation (HTE) datasets is described. For chemical yield predictions, a key task in chemical synthesis, an attributed graph neural network (AGNN) performs as well as or better than the best previous models on two HTE datasets for the Suzuki\u2013Miyaura and Buchwald\u2013Hartwig reactions. However, training the AGNN on an ELN dataset does not lead to a predictive model. The implications of using ELN data for training ML-based models are discussed in the context of yield predictions.\n\n![Graphical abstract: On the use of real-world datasets for reaction yield prediction](https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&imageInfo.ImageIdentifier.ManuscriptID=D2SC06041H&imageInfo.ImageIdentifier.Year=2023)\n\n- This article is part of the themed collection:\n[2023 Chemical Science HOT Article Collection](https://pubs.rsc.org/en/journals/articlecollectionlanding?sercode=sc&themeid=6046ae5e-50e8-4865-8f24-534fb9ec219c)\n\nThis article is Open Access\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/Ajax-GA-Loader.gif)\nPlease wait while we load your content...\nSomething went wrong. [Try again?](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)\n\n[About](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#pnlAbstract)\n\n[Cited by](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#pnlCitation)\n\n[Related](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h#pnlRelatedContent)\n\n[Download options Please wait...](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)\n\n## Supplementary files\n\n- [Supplementary information\\\nPDF (2610K)](https://www.rsc.org/suppdata/d2/sc/d2sc06041h/d2sc06041h1.pdf)\n\n## Article information\n\nDOI[https://doi.org/10.1039/D2SC06041H](https://doi.org/10.1039/D2SC06041H)\n\n**Article type**Edge Article\n\nSubmitted01 Nov 2022\n\nAccepted09 Mar 2023\n\nFirst published13 Mar 2023\n\n![](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/open-access-icon-orange.png)\n\n**This article is Open Access**\n\nAll publication charges for this article have been paid for by the Royal Society of Chemistry[![Creative Commons BY-NC license](https://www.rsc-cdn.org/pubs-core/2022.0.139/content/NewImages/CCBY-NC.svg)](http://creativecommons.org/licenses/by-nc/3.0/)\n\n### Download Citation\n\n_**Chem. Sci.**_, 2023, **14**, 4997-5005\n\nBibTexEndNoteMEDLINEProCiteReferenceManagerRefWorksRIS\n\n### Permissions\n\n[Request permissions](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)\n\n[![](https://www.rsc-cdn.org/oxygen/assets/icons/cross.png)](https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h)\n\n### On the use of real-world datasets for reaction yield prediction\n\nM. Saebi, B. Nan, J. E. Herr, J. Wahlers, Z. Guo, A. M. Zura\u0144ski, T. Kogej, P. Norrby, A. G. Doyle, N. V. Chawla and O. Wiest,\n_Chem. Sci._, 2023,\u00a0**14**, 4997\n**DOI:** 10.1039/D2SC06041H\n\nThis article is licensed under a [Creative Commons Attribution-NonCommercial 3.0 Unported Licence](https://creativecommons.org/licenses/by-nc/3.0/). **You can use material from**\n**this article in other publications, without requesting further permission** from the RSC,\nprovided that the correct acknowledgement is given and it is not used for commercial purposes.\n\nTo request permission **to reproduce material from this article in a commercial publication**,\nplease go to the [Copyright Clearance Center request page](https://marketplace.copyright.com/rs-ui-web/mp/search/all/10.1039%2fD2SC06041H).\n\nIf you are **an author contributing to an RSC publication, you do not need to request permission**\nprovided correct acknowledgement is given.\n\nIf you are **the author of this article, you do not need to request permission to reproduce figures**\n**and diagrams** provided correct acknowledgement is given. If you want to reproduce the whole\narticle in a third-p...",
      "url": "https://pubs.rsc.org/en/content/articlelanding/2023/sc/d2sc06041h"
    },
    {
      "title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty",
      "text": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty | Theoretical and Computational Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toTheoretical and Computational Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470ce)\nSearch within Theoretical and Computational Chemistry\n[](#)\n![RSS feed for Theoretical and Computational Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# Data augmentation strategies to improve reaction yield predictions and estimate uncertainty\n26 November 2020, Version 1\nWorking Paper\n## Authors\n* [Philippe Schwaller](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Philippe%20Schwaller)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-3046-6576),\n* [Alain C. Vaucher](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Alain%20C.%20Vaucher)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0001-7554-0288),\n* [Teodoro Laino](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Teodoro%20Laino),\n* [Jean-Louis Reymond](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Jean-Louis%20Reymond)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-2724-2942)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nChemical reactions describe how precursor molecules react together and transform into products. The reaction yield describes the percentage of the precursors successfully transformed into products relative to the theoretical maximum. The prediction of reaction yields can help chemists navigate reaction space and accelerate the design of more effective routes. Here, we investigate the best-studied high-throughput experiment data set and show how data augmentation on chemical reactions can improve yield predictions' accuracy, even when only small data sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In this manuscript, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the random training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Keywords\n[SMILES](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=SMILES)\n[SMILES-Encoded](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=SMILES-Encoded)\n[chemical reactions](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=chemical%20reactions)\n[reaction yields](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=reaction%20yields)\n[data augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=data%20augmentation)\n[BERT](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=BERT)\n[Transformers](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Transformers)\n[Deep learning](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Deep%20learning)\n[regression](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=regression)\n[test-time augmentation](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=test-time%20augmentation)\n## Comments\nYou are signed in as . Your name will appear\nwith any comment you post.\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\n&#8203;\n300 words allowed\nYou can enter up to 300 words.Post comment\nLog in or register with\nORCID to comment\nComments are not moderated before they are posted, but they can be removed\nby the site moderators if they are found to be in contravention of our[Commenting Policy[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/about-information?show=commenting-policy)- please read this policy before you post. Comments should be used for\nscholarly discussion of the content in question. You can[find more information about how to use the commenting feature here[opens in a new tab]](https://chemrxiv.org/engage/chemrxiv/contact-information?show=faqs).\nThis site is protected by reCAPTCHA and the Google[Privacy Policy[opens in a new tab]](https://policies.google.com/privacy)and[Terms of Service[opens in a new tab]](https://policies.google.com/terms)apply.\n## Version History\nNov 26, 2020 Version 1\n## Version Notes\nAccepted to NeurIPS 2020 Machine Learning for Molecules workshop.\n## Metrics\n15,686\n2,814\n22\nViews\nDownloads\nView article\nCitations\n## License\n![CC logo](https://chemrxiv.org/engage/_nuxt/img/cc.e3defa7.svg)\nCC\n![BY logo](https://chemrxiv.org/engage/_nuxt/img/by.7813b57.svg)\nBY\n![NC logo](https://chemrxiv.org/engage/_nuxt/img/nc.e378f90.svg)\nNC\n![ND logo](https://chemrxiv.org/engage/_nuxt/img/nd.7966b83.svg)\nND\nThe content is available under[CC BY NC ND 4.0[opens in a new tab]](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n## DOI\n[\n10.26434/chemrxiv.13286741.v1\nD O I: 10.26434/chemrxiv.13286741.v1 [opens in a new tab]](https://doi.org/10.26434/chemrxiv.13286741.v1)\n## Author\u2019s competing interest statement\nNo conflict of interest.\n## Share",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/60c75258702a9b726c18c101"
    },
    {
      "title": "Cross-validation pitfalls when selecting and assessing regression and classification models",
      "text": "Cross-validation pitfalls when selecting and assessing regression and classification models | Journal of Cheminformatics\n[Skip to main content](#main)\nAdvertisement\nBMC journals have moved to Springer Nature Link.[Learn more about website changes.](https://support.springernature.com/en/support/solutions/articles/6000281876-springer-nature-brand-websites-are-moving-to-springer-nature-link)\n[![Springer Nature Link](https://jcheminf.biomedcentral.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1186/1758-2946-6-10?)\n# Cross-validation pitfalls when selecting and assessing regression and classification models\n* Methodology\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:29 March 2014\n* Volume\u00a06, article\u00a0number10, (2014)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/13321?as=webp)Journal of Cheminformatics](https://jcheminf.biomedcentral.com/journal/13321)[Aims and scope](https://jcheminf.biomedcentral.com/journal/13321/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/13321/3)\nCross-validation pitfalls when selecting and assessing regression and classification models\n[Download PDF](https://jcheminf.biomedcentral.com/content/pdf/10.1186/1758-2946-6-10.pdf)\n* [Damjan Krstajic](#auth-Damjan-Krstajic-Aff1-Aff2-Aff3)[1](#Aff1),[2](#Aff2),[3](#Aff3),\n* [Ljubomir J Buturovic](#auth-Ljubomir_J-Buturovic-Aff3)[3](#Aff3),\n* [David E Leahy](#auth-David_E-Leahy-Aff4)[4](#Aff4)&amp;\n* \u2026* [Simon Thomas](#auth-Simon-Thomas-Aff5)[5](#Aff5)Show authors\n* 150kAccesses\n* 902Citations\n* 42Altmetric\n* 1Mention\n* [Explore all metrics](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10/metrics)\n## Abstract\n### Background\nWe address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.\n### Methods\nWe describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case.\n### Results\nWe show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models.\n### Conclusions\nWe demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-024-06630-y/MediaObjects/10994_2024_6630_Fig1_HTML.png)\n### [Reducing cross-validation variance through seed blocking in hyperparameter tuning](https://link.springer.com/10.1007/s10994-024-06630-y?fromPaywallRec=false)\nArticle17 February 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-31041-7?as&#x3D;webp)\n### [Enhancement of Cross Validation Using Hybrid Visual and Analytical Means with Shannon Function](https://link.springer.com/10.1007/978-3-030-31041-7_29?fromPaywallRec=false)\nChapter\u00a9 2020\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs42979-022-01051-x/MediaObjects/42979_2022_1051_Figa_HTML.png)\n### [An Efficient Ridge Regression Algorithm with Parameter Estimation for Data Analysis in Machine Learning](https://link.springer.com/10.1007/s42979-022-01051-x?fromPaywallRec=false)\nArticle23 February 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Compound Screening](https://jcheminf.biomedcentral.com/subjects/compound-screening)\n* [Linear Models and Regression](https://jcheminf.biomedcentral.com/subjects/linear-models-and-regression)\n* [Learning algorithms](https://jcheminf.biomedcentral.com/subjects/learning-algorithms)\n* [Machine Learning](https://jcheminf.biomedcentral.com/subjects/machine-learning)\n* [Molecular Target Validation](https://jcheminf.biomedcentral.com/subjects/molecular-target-validation)\n* [Statistical Learning](https://jcheminf.biomedcentral.com/subjects/statistical-learning)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=13321)\nAvoid common mistakes on your manuscript.\n## Background\nAllen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)], Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] and Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)], independently introduced cross-validation as a way of estimating parameters for predictive models in order to improve predictions. Allen [[1](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR1)] proposed the PRESS (Prediction Sum of Squares) criteria, equivalent to leave-one-out cross-validation, for problems with selection of predictors and suggested it for general use. Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] suggested the use of leave-one-out cross-validation for estimating model parameters and for assessing their predictive error. It is important to note that Stone [[2](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR2)] was the first to clearly differentiate between the use of cross-validation to select the model (\u201ccross-validatory choice\u201d) and to assess the model (\u201ccross-validatory assessment\u201d). Geisser [[3](https://jcheminf.biomedcentral.com/article/10.1186/1758-2946-6-10#ref-CR3)] introduced the Predictive Sample Reuse Method, a method equivalent to V-fold cross-validation, arguing that it improves predictive performance of the cross-validatory choice, at a cost of introducing pseudo-randomness in the process. Since then, cross-validation, with its different varieties, has been investigated extensively and, due to its universality, gained popularity in statistical modelling.\nIn an ideal situation we would have enough data to train and validate our models (training samples) and have separate data for assessing the quality of our model (test samples). Both training and test samples would need to be sufficiently large and diverse in order to be represenatitive. However such data rich situations are rare in life sciences, including QSAR. A major problem with selection and assessment of models is that we usually only have information from the training samples, and it is therefore not feasible to calculate a test error. However, even though we cannot calculate the test error, it is possible to estimate the expected test error using training samples. It can be shown that the expected test error is the...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-6-10"
    },
    {
      "title": "A focus on the use of real-world datasets for yield prediction",
      "text": "A focus on the use of real-world datasets for yield\nprediction\nLatimah Bustillo and Tiago Rodrigues *\nThe prediction of reaction yields remains a challenging task for machine learning (ML), given the vast search\nspaces and absence of robust training data. Wiest, Chawla et al. (https://doi.org/10.1039/D2SC06041H)\nshow that a deep learning algorithm performs well on high-throughput experimentation data but\nsurprisingly poorly on real-world, historical data from a pharmaceutical company. The result suggests\nthat there is considerable room for improvement when coupling ML to electronic laboratory notebook data.\nMachine learning (ML) has seen formi\u0002dable applications in diverse \ue103elds of\nscience, including chemistry.1 The\nprediction of retrosynthetic routes,2 the\nde novo design of chemical entities,3 and\nthe prediction of pharmacological\npro\ue103les for small molecules4 are just\na few examples where ML is currently\nmaking an impact and accelerating\nresearch.5,6 These advances are made\npossible chie\ue104y due to improved algo\u0002rithms, methods for describing molec\u0002ular structure and, above all, available\ndatasets. In fact, a corollary in ML\nresearch is that a model can only be as\ngood as its training data.7 To that end,\na considerable amount of time in the ML\ndevelopment process is devoted to col\u0002lecting, curating and harmonizing data.\nWith the emergence of large language\nmodels (LLMs; e.g., ChatGPT) that\n\u2018converse\u2019 with human users one may\nassume that predicting reaction yields is\nonly a simple task. That is not the case.\nPredicting yields remains challenging\ndue to the absence of robust reaction\nprecedents.8 Among the many shortcom\u0002ings, datasets tend to be biased towards\nproductive reactions. This has deep roots\nin the chemistry literature given the\npreference to report positive results and\nomit \u2018failures\u2019.\n9 It is also known that\nobservational uncertainty affects the re\u0002ported yield (usually measured once\nrather than veri\ue103ed through replicates)\nand, in some cases, the values are mis\u0002assigned in databases.5 These errors\npropagate and impact the ML model\naccuracy. Finally, the search spaces are so\nvast that any currently available dataset\nremains sparse in terms of coverage. As\na result, there is a need to develop new\nML methodologies and approaches that\nmitigate the referred limitations.\nActive and reinforcement learning\nhave been previously employed in the\noptimization of reaction conditions using\nyield as a metric to gauge success.10,11\nHigh-throughput experimentation (HTE)\ndata is a viable starting point for reaction\noptimization campaigns and is compat\u0002ible with diverse \ue104avours of molecular\ndescriptors.8,10,12 Those data can however\nbe too focused on certain regions of the\nsearch space and the resulting ML tools\nmight generalize poorly. It has been\nhypothesized that electronic laboratory\nnotebooks (ELNs) \u2013 for example those\nfrom pharmaceutical companies \u2013 can\nprovide less biased dataset alternatives to\nHTE datasets. As an additional selling\npoint, the larger chemical space coverage\nin ELNs may be instrumental for\nimproved model generalizability and the\nidenti\ue103cation of new reactivity patterns.\nStill, the utility of ELNs as data sources\nremained unknown until now partly due\nto their restricted access.\nIn a collaborative study,13 Wiest (Notre\nDame), Chawla (Notre Dame), Doyle\n(University of California), Norrby (Astra\u0002Zeneca) and co-workers delved into ELNs\nto build ML models and predict reaction\nyields. Using pharmaceutically relevant\ntransformations as examples (e.g., Buch\u0002wald\u2013Hartwig and Suzuki couplings) it\nwas found that state-of-the-art represen\u0002tation learning performs unsatisfactorily\nand not much better than other simpler\nmethods, such as random forests.\nThe team started by querying Astra\u0002Zeneca's legacy data and pre-processing\nthe retrieved information. As expected,\nincomplete reactions and those with 0%\nyield were highly prevalent in the ELN.\nStill, a total of 781 Buchwald\u2013Hartwig\nreactions ful\ue103lled the pre-established\nquality criteria, which was a minute\nnumber of reactions in comparison to the\nwhole search space size (\u223c4.7 \u00d7 108\n).\nNotwithstanding the diversity of the ELN\ndata, its size was in stark contrast with\na related HTE dataset which comprised\n3960 Buchwald\u2013Hartwig reactions\ncovering a space of 4140 possibilities for\nonly \ue103ve distinct products. These datasets\nwere then used to train ML models with\nchemically meaningful descriptors. The\nperformance of random forests, BERT, k\u0002nearest neighbours, Lasso, and support\nvector machines was subsequently\ncompared. For a realistic assessment of\nthe expected baseline performance, Y\u0002randomized (shuffled) datasets were\nalso generated and optimized models\nResearch Institute for Medicines (iMed), Faculty of\nPharmacy, University of Lisbon, Av Prof Gama Pinto,\n1649-003 Lisbon, Portugal. E-mail: tiago.rodrigues@ff.\nulisboa.pt\nCite this: Chem. Sci., 2023, 14, 4958\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nDOI: 10.1039/d3sc90069j\nrsc.li/chemical-science\n4958 | Chem. Sci., 2023, 14, 4958\u20134960 \u00a9 2023 The Author(s). Published by the Royal Society of Chemistry\nChemical\nScience\nCOMMENTARY\nOpen Access Article. Published on 27 April 2023. Downloaded on 10/30/2025 3:42:30 AM. This article is licensed under a Creative Commons Attribution 3.0 Unported Licence. View Article Online View Journal | View Issue\nwere trained. From all the surveyed\nmethods, the random forests and BERT\nmodels performed best for HTE datasets,\nproviding r\n2 values > 0.82.\nIn comparison, the ELN data did not\nafford a model with satisfactory perfor\u0002mance. Random forests performed best\nbut with an r\n2 value of only 0.266. While\nthe obtained performance was slightly\nbetter than a randomized baseline, the\nresult suggested that the vast search\nspace was only sparsely populated by the\ntraining data. As a potential solution, the\nauthors hypothesized that the maximum\namount of information could be captured\nby retaining elements of the two best\nperforming methods in HTE datasets \u2013\nrandom forests and BERT. This meant\nthat an optimized model \u2013 herein named\nYieldGNN \u2013 would combine molecular\nproperties (i.e., chemical features or\ndescriptors) from the tabular data used in\nrandom forests with the molecular graph\nstructure in attributed graph neural\nnetworks (GNN; Fig. 1).\nIn short, Weisfeiler\u2013Lehman networks\nwere employed wherein atom and bond\ninformation was aggregated using con\u0002volutional operations. The embedded\nneighbourhood features were then\nconcatenated with atom representations\nto ultimately access attended GNN\nfeatures that were the basis of yield\npredictions. To avoid over\ue103tting of the\nneural network, a random forest was\ntrained in parallel, and its outputs were\nlinearly combined to those of the atten\u0002ded GNN to afford a \ue103nal yield\nprediction. The resulting composite\nmodel outperformed all previous\nmethods on HTE datasets, including the\nrandom forests, as indicated by more\nfavourable r\n2 and mean absolute error\nvalues. However, as the authors correctly\nmention, the obtained improvements are\nmore relevant from a statistical point of\nview than in a real-life laboratory setting.\nThis owes to the fact that the marginally\nhigher r\n2 value absorbs uncertainties and\nerrors in yield measurements. Still, the\nresults show some evidence that connec\u0002tivity data is important for model perfor\u0002mance and that its inclusion is a viable\nstrategy for future ML implementations.\nThe application of YieldGNN to ELN\ndata provided less promising and unex\u0002pected results in comparison. No mean\u0002ingful predictions were obtained (r\n2 # 0)\neven a\ue09der extensively \ue103ne-tuning the base\nmodel. One of the reasons for this\nobservation was the inability of YieldGNN\nto learn key features that govern the\ntransformation, according to weight\nvalues that did not surpass 0.05.\nIn an attempt to solve this issue, the\nresearch team investigated whether pre\u0002training of a GNN with a large corpus of\nmolecules could help improving the ML\nmodel performan...",
      "url": "https://pubs.rsc.org/en/content/articlepdf/2023/sc/d3sc90069j"
    }
  ]
}