{
  "query": "What are the best approaches for out-of-distribution generalization in molecular property prediction when test molecules are completely unseen during training?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Out\u2011of\u2011distribution (OOD) generalization in molecular property prediction is most reliably achieved by combining **robust data splits, inductive\u2011bias\u2011rich models, and training strategies that explicitly bridge the gap between the training and unseen chemical space**.  Empirical studies show that the hardest OOD scenarios arise when test sets are generated by **chemical\u2011similarity clustering (e.g., K\u2011means on ECFP4 fingerprints)** rather than simple scaffold splits, and that performance on such splits correlates only weakly with in\u2011distribution (ID) scores (Pearson\u202fr\u202f\u2248\u202f0.4)\u202f([chemrxiv](https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866)).  Consequently, benchmarking on cluster\u2011based splits (as in the BOOM suite) is essential for realistic assessment\u202f([arXiv\u202f2505.01912](https://arxiv.org/abs/2505.01912);\u202f[arXiv\u202f2505.01912\u202fPDF](https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf)).  Models that incorporate **strong chemical inductive biases**\u2014such as message\u2011passing graph neural networks (MP\u2011GNNs) with scaffold\u2011aware encodings\u2014tend to outperform generic foundation models on OOD tasks, especially when paired with careful hyper\u2011parameter tuning and data\u2011generation strategies\u202f([BOOM benchmark](https://github.com/FLASK-LLNL/BOOM)).\n\nBeyond architecture, **meta\u2011learning and generative approaches that exploit unlabeled molecules** have demonstrated sizable gains on covariate\u2011shifted datasets.  A meta\u2011learning framework that interpolates between ID and OOD distributions using abundant unlabeled data reduces prediction instability and improves accuracy on challenging real\u2011world benchmarks\u202f([arXiv\u202f2506.11877](https://arxiv.org/abs/2506.11877);\u202f[OpenReview PDF](https://openreview.net/pdf/70bee4e7da17329c1a51ab89274f7974d10cc74d.pdf)).  Similarly, the **SCI (Semantic\u2011Component Identifiable) generative model** separates latent variables into semantic\u2011relevant and semantic\u2011irrelevant components, enforcing minimal changes to causal mechanisms and thereby enhancing OOD robustness\u202f([arXiv\u202f2311.04837](https://arxiv.org/abs/2311.04837)).\n\nFinally, **probabilistic calibration and uncertainty\u2011driven active learning** provide practical tools for safely extrapolating to unseen molecules.  Calibrated Bayesian models (e.g., DIONYSUS) maintain reliable confidence estimates even on low\u2011data regimes, while uncertainty\u2011guided acquisition can prioritize the most informative experiments, accelerating the collection of OOD data and further improving model generalization\u202f([RSC\u202fDigital\u202fDiscovery\u202f2023](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b));\u202f[PMCID\u202f10633997](https://pmc.ncbi.nlm.nih.gov/articles/PMC10633997)).  In practice, the most effective OOD pipeline integrates **(i) rigorous cluster\u2011based evaluation, (ii) inductive\u2011bias\u2011rich GNN or generative architectures, (iii) meta\u2011learning with unlabeled data, and (iv) calibrated uncertainty estimates coupled with active learning** to ensure reliable predictions for completely unseen test molecules.",
      "url": ""
    },
    {
      "title": "Evaluating Machine Learning Models for Molecular Property Prediction: Performance and Robustness on Out-of-Distribution Data",
      "text": "<div><div><p></p><h2>Abstract</h2><p></p> <p>Today, machine learning models are employed extensively to predict the physicochemical and biological properties of molecules. Their performance is typically evaluated on in-distribution (ID) data, i.e., data originating from the same distribution as the training data. However, the real-world applications of such models often involve molecules that are more distant from the training data, which necessitates assessing their performance on out-of-distribution (OOD) data. In this work, we investigate and evaluate the performance of twelve machine learning models, including classical approaches like random forests, as well as graph neural network (GNN) methods, such as message-passing graph neural networks, across eight data sets using seven splitting strategies for OOD data generation. First, we investigate what constitutes OOD data in the molecular domain for bioactivity and ADMET prediction tasks. In contrast to the common point of view, we show that both classical machine learning and GNN models work well (not substantially different from random splitting) on data split based on Bemis-Murcko scaffolds. Splitting based on chemical similarity clustering (K-means clustering using ECFP4 fingerprints) poses the hardest challenge for both types of models. Second, we investigate the extent to which ID and OOD performance have a positive linear relationship. If a positive correlation holds, models with the best performance on the ID data can be selected with the promise of having the best performance on OOD data. We show that the strength of this linear relationship is strongly related to how the OOD data is generated, i.e., which splitting strategies are used for generating OOD data. While the correlation between ID and OOD performance for scaffold splitting is strong (Pearson r \u223c 0.9), this correlation decreases significantly for cluster-based splitting (Pearson r \u223c 0.4). Therefore, the relationship can be more nuanced, and a strong positive correlation is not guaranteed for all OOD scenarios. These findings suggest that OOD performance evaluation and model selection should be carefully aligned with the intended application domain.</p> </div></div>",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/67c8a90bfa469535b9148866"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2505.01912] BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2505.01912\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2505.01912**(cs)\n[Submitted on 3 May 2025 ([v1](https://arxiv.org/abs/2505.01912v1)), last revised 19 Dec 2025 (this version, v2)]\n# Title:BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models\nAuthors:[Evan R. Antoniuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Antoniuk,+E+R),[Shehtab Zaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaman,+S),[Tal Ben-Nun](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Nun,+T),[Peggy Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[James Diffenderfer](https://arxiv.org/search/cs?searchtype=author&amp;query=Diffenderfer,+J),[Busra Sahin](https://arxiv.org/search/cs?searchtype=author&amp;query=Sahin,+B),[Obadiah Smolenski](https://arxiv.org/search/cs?searchtype=author&amp;query=Smolenski,+O),[Tim Hsu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hsu,+T),[Anna M. Hiszpanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Hiszpanski,+A+M),[Kenneth Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+K),[Bhavya Kailkhura](https://arxiv.org/search/cs?searchtype=author&amp;query=Kailkhura,+B),[Brian Van Essen](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Essen,+B)\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n[View PDF](https://arxiv.org/pdf/2505.01912)[HTML (experimental)](https://arxiv.org/html/2505.01912v2)> > Abstract:\n> Data-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML) and generative modeling to filter and design novel molecules. Discovering novel molecules requires accurate out-of-distribution (OOD) predictions, but ML models struggle to generalize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks. We present $\\mathbf{BOOM}$, $\\mathbf{b}$enchmarks for $\\mathbf{o}$ut-$\\mathbf{o}$f-distribution $\\mathbf{m}$olecular property predictions: a chemically-informed benchmark for OOD performance on common molecular property prediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning models on OOD performance. Overall, we find that no existing model achieves strong generalization across all tasks: even the top-performing model exhibited an average OOD error 3x higher than in-distribution. Current chemical foundation models do not show strong OOD extrapolation, while models with high inductive bias can perform well on OOD tasks with simple, specific properties. We perform extensive ablation experiments, highlighting how data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation impact OOD performance. Developing models with strong OOD generalization is a new frontier challenge in chemical ML. This open-source benchmark is available at [> this https URL\n](https://github.com/FLASK-LLNL/BOOM)> Subjects:|Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2505.01912](https://arxiv.org/abs/2505.01912)[cs.LG]|\n|(or[arXiv:2505.01912v2](https://arxiv.org/abs/2505.01912v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2505.01912](https://doi.org/10.48550/arXiv.2505.01912)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Evan Antoniuk [[view email](https://arxiv.org/show-email/6abb5e36/2505.01912)]\n**[[v1]](https://arxiv.org/abs/2505.01912v1)**Sat, 3 May 2025 19:51:23 UTC (35,134 KB)\n**[v2]**Fri, 19 Dec 2025 23:00:10 UTC (16,070 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models, by Evan R. Antoniuk and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2505.01912)\n* [HTML (experimental)](https://arxiv.org/html/2505.01912v2)\n* [TeX Source](https://arxiv.org/src/2505.01912)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2505.01912&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2505.01912&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-05](https://arxiv.org/list/cs.LG/2025-05)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/2505.01912?context=cond-mat)\n[cond-mat.mtrl-sci](https://arxiv.org/abs/2505.01912?context=cond-mat.mtrl-sci)\n[cs](https://arxiv.org/abs/2505.01912?context=cs)\n[cs.AI](https://arxiv.org/abs/2505.01912?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2505.01912)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2505.01912)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2505.01912)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2505.01912&amp;description=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2505.01912&amp;title=BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*...",
      "url": "https://arxiv.org/abs/2505.01912"
    },
    {
      "title": "Robust Molecular Property Prediction via Densifying Scarce Labeled Data",
      "text": "[View PDF](https://arxiv.org/pdf/2506.11877) [HTML (experimental)](https://arxiv.org/html/2506.11877v1)\n\n> Abstract:A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.\n\n## Submission history\n\nFrom: Jina Kim \\[ [view email](https://arxiv.org/show-email/96b77896/2506.11877)\\]\n\n**\\[v1\\]**\nFri, 13 Jun 2025 15:27:40 UTC (10,933 KB)",
      "url": "https://arxiv.org/abs/2506.11877"
    },
    {
      "title": "Identifying Semantic Component for Robust Molecular Property Prediction",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2311.04837** (cs)\n\n\\[Submitted on 8 Nov 2023\\]\n\n# Title:Identifying Semantic Component for Robust Molecular Property Prediction\n\nAuthors: [Zijian Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Z), [Zunhong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+Z), [Ruichu Cai](https://arxiv.org/search/cs?searchtype=author&query=Cai,+R), [Zhenhui Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+Z), [Yuguang Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+Y), [Zhifeng Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao,+Z), [Guangyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+G), [Kun Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+K)\n\nView a PDF of the paper titled Identifying Semantic Component for Robust Molecular Property Prediction, by Zijian Li and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2311.04837)\n\n> Abstract:Although graph neural networks have achieved great success in the task of molecular property prediction in recent years, their generalization ability under out-of-distribution (OOD) settings is still under-explored. Different from existing methods that learn discriminative representations for prediction, we propose a generative model with semantic-components identifiability, named SCI. We demonstrate that the latent variables in this generative model can be explicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI) components, which contributes to better OOD generalization by involving minimal change properties of causal mechanisms. Specifically, we first formulate the data generation process from the atom level to the molecular level, where the latent space is split into SI substructures, SR substructures, and SR atom variables. Sequentially, to reduce misidentification, we restrict the minimal changes of the SR atom variables and add a semantic latent substructure regularization to mitigate the variance of the SR substructure under augmented domain changes. Under mild assumptions, we prove the block-wise identifiability of the SR substructure and the comment-wise identifiability of SR atom variables. Experimental studies achieve state-of-the-art performance and show general improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the visualization results of the proposed SCI method provide insightful case studies and explanations for the prediction results. The code is available at: [this https URL](https://github.com/DMIRLAB-Group/SCI).\n\n|     |     |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM) |\n| Cite as: | [arXiv:2311.04837](https://arxiv.org/abs/2311.04837) \\[cs.LG\\] |\n|  | (or [arXiv:2311.04837v1](https://arxiv.org/abs/2311.04837v1) \\[cs.LG\\] for this version) |\n|  | [https://doi.org/10.48550/arXiv.2311.04837](https://doi.org/10.48550/arXiv.2311.04837)<br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zijian Li \\[ [view email](https://arxiv.org/show-email/64c4a2f8/2311.04837)\\]\n\n**\\[v1\\]**\nWed, 8 Nov 2023 17:01:35 UTC (10,755 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Identifying Semantic Component for Robust Molecular Property Prediction, by Zijian Li and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2311.04837)\n- [TeX Source](https://arxiv.org/src/2311.04837)\n- [Other Formats](https://arxiv.org/format/2311.04837)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2311.04837&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2311.04837&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-11](https://arxiv.org/list/cs.LG/2023-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2311.04837?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2311.04837?context=cs.AI)\n\n[q-bio](https://arxiv.org/abs/2311.04837?context=q-bio)\n\n[q-bio.QM](https://arxiv.org/abs/2311.04837?context=q-bio.QM)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.04837)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.04837)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.04837)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2311.04837&description=Identifying Semantic Component for Robust Molecular Property Prediction) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2311.04837&title=Identifying Semantic Component for Robust Molecular Property Prediction)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.04837) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
      "url": "https://arxiv.org/abs/2311.04837"
    },
    {
      "title": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS \u2020",
      "text": "Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS - Digital Discovery (RSC Publishing) DOI:10.1039/D2DD00146B\n[![Royal Society of Chemistry](/content/NewImages/royal-society-of-chemistry-logo.png)](/)\n[View\u00a0PDF\u00a0Version](/en/content/articlepdf/2023/dd/d2dd00146b)[Previous\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00012e)[Next\u00a0Article](/en/content/articlehtml/2023/dd/d3dd00061c)\n[![Check for updates](https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg)](#)\n![](/content/newimages/open_access_blue.png)Open Access Article\n![](/content/newimages/CCBY-NC.svg)This Open Access Article is licensed under a[Creative Commons Attribution-Non Commercial 3.0 Unported Licence](http://creativecommons.org/licenses/by-nc/3.0/)\nDOI:[10.1039/D2DD00146B](https://doi.org/10.1039/D2DD00146B)(Paper)[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023,**2**, 759-774\n# Calibration and generalizability of probabilistic models on low-data chemical datasets with DIONYSUS[\u2020](#fn1)\nGary Tom[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-8470-6515)abc,Riley J. Hickman[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-5762-1006)abc,Aniket Zinzuwadiad,Afshan Mohajeri[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-3858-3024)e,Benjamin Sanchez-Lengeling[![ORCID logo](/content/NewImages/orcid_16x16.png)](http://orcid.org/0000-0002-1116-1745)fandAl\u00e1n Aspuru-Guzik\\*abcghi\naChemical Physics Theory Group, Department of Chemistry, University of Toronto, Toronto, ON, Canada. E-mail:[alan@aspuru.com](mailto:alan@aspuru.com)\nbDepartment of Computer Science, University of Toronto, Toronto, ON, Canada\ncVector Institute for Artificial Intelligence, Toronto, ON, Canada\ndHarvard Medical School, Harvard University, Boston, MA, USA\neDepartment of Chemistry, Shiraz University, Shiraz, Iran\nfGoogle Research, Brain Team, USA\ngDepartment of Chemical Engineering &amp; Applied Chemistry, University of Toronto, Toronto, ON, Canada\nhDepartment of Materials Science &amp; Engineering, University of Toronto, Toronto, ON, Canada\niLebovic Fellow, Canadian Institute for Advanced Research, Toronto, ON, Canada\nReceived 21st December 2022, Accepted 21st April 2023\nFirst published on 2nd May 2023\n## Abstract\nDeep learning models that leverage large datasets are often the state of the art for modelling molecular properties. When the datasets are smaller (&lt;2000 molecules), it is not clear that deep learning approaches are the right modelling tool. In this work we perform an extensive study of the calibration and generalizability of probabilistic machine learning models on small chemical datasets. Using different molecular representations and models, we analyse the quality of their predictions and uncertainties in a variety of tasks (regression or binary classification) and datasets. We also introduce two simulated experiments that evaluate their performance: (1) Bayesian optimization guided molecular design, (2) inference on out-of-distribution dataviaablated cluster splits. We offer practical insights into model and feature choice for modelling small chemical datasets, a common scenario in new chemical experiments. We have packaged our analysis into the DIONYSUS repository, which is open sourced to aid in reproducibility and extension to new datasets.\n## 1. Introduction\nThe design and discovery of molecular materials routinely enables technologies which have crucial societal consequences. Given a library of compounds, prediction of molecular functionality from its structure enables ranking and selection of promising candidates prior to experimental validation or other screening filters. Therefore, building accurate quantitative structure\u2013activity relationship models (QSAR) is key to accelerated chemical design and efficient experimental decision-making.[1](#cit1)Models that leverage statistical patterns in data are now often the state of the art on such tasks. Specifically, data science and machine learning (ML) have played critical roles in modern science in general,[2](#cit2)enabling the utilization of data at unprecedented scales. Deep learning (DL) models are able to extract statistical patterns in dataset features and give accurate QSAR predictions and classifications.[3](#cit3)When compared to traditionalab initiotechniques, such as density functional theory (DFT), ML models are less computationally demanding, and can learn statistical patterns directly from experimental data. However, the quality of such models is determined by the quality of the original datasets they are trained on, and thus the models are still affected by the cost of accurate data generation.\nTo date, many studies consider molecular property prediction tasks where training data is plentiful.[4,5](#cit4)In real-world molecular design campaigns, particularly in the initial stages, only small molecular datasets (&lt;2000 data points) are available due to the expense (monetary, resource, or labour) associated with the design, synthesis, and characterization of chemicals. In addition to the datasets examined in this work, examples of applications in the low-data regime include design of optoelectronic materials (i.e.organic photovoltaics,[6](#cit6)or photoswitching molecules[7](#cit7)), prediction of biochemical properties (i.e.olfactory response,[8,9](#cit8)or mosquito repellency[10](#cit10)), and drug discovery.[11,12](#cit11)Despite the practical importance of this regime, molecular property prediction using ML with limited data instances has been relatively under-explored, and remains a challenging task, especially for deep learning models which often require large amounts of training instances due to large number of model parameters.\nIn the low-data setting, understanding a ML model's performance is important since predictions inform decisions about further research directions, or, in a sequential learning setting, promote molecules to be subject to property measurement. In particular, we place emphasis on (1) the generalizability, the ability of a model to predict accurately on new chemical data, and (2) uncertainty calibration, the ability of a model to estimate the confidence of its predictions ([Fig. 1](#imgfig1)).\n[![image file: d2dd00146b-f1.tif](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1.gif)](/image/article/2023/DD/d2dd00146b/d2dd00146b-f1_hi-res.gif)|\n|**Fig. 1**Schematic of the evaluation of probabilistic model on small molecular datasets with DIONYSUS. We study the performance and calibration of probabilistic models with different molecular representations when applied to small molecular datasets. The models are then evaluated on their performance in a simulated optimization campaign and their ability to generalize to out-of-distribution molecules.||\nAdequate generalizability, the ability for a model to make accurate predictions on out-of-distribution (OOD) data, is paramount for many learning tasks, such as in the hit-to-lead and early lead optimization phases of drug discovery.[12,13](#cit12)After identification of a biological target (usually a protein or nucleic acid), initial molecular hits are optimized in an expensive and time-consuming make-design-test cycle. Using ML to predict molecular properties has indeed been shown to reduce the number of syntheses and measurements required.[14\u201316](#cit14)Commonly, drug discovery project permit the synthesis and measurement of hundreds of candidate molecules due to constraints in expense, and typically involve functionalizations of a common molecular core or scaffold. Model generalization is therefore critical for the reuse of QSAR models for unstudied molecular scaffolds.[17,18](#cit17)\nUncertainty calibration is the ability of a probabilistic model to produce accurate estimates of its confidence, and is also a crucial aspect of the molecular design process and high-risk decision making.[19](#c...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00146b"
    },
    {
      "title": "Evaluating uncertainty-based active learning for accelerating the generalization of molecular property prediction",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Deep learning models have proven to be a powerful tool for the prediction of molecular properties for applications including drug design and the development of energy storage materials. However, in order to learn accurate and robust structure\u2013property mappings, these models require large amounts of data which can be a challenge to collect given the time and resource-intensive nature of experimental material characterization efforts. Additionally, such models fail to generalize to new types of molecular structures that were not included in the model training data. The acceleration of material development through uncertainty-guided experimental design has the promise to significantly reduce the data requirements and enable faster generalization to new types of materials. To evaluate the potential of such approaches for electrolyte design applications, we perform comprehensive evaluation of existing uncertainty quantification methods on the prediction of two relevant molecular properties - aqueous solubility and redox potential. We develop novel evaluation methods to probe the utility of the uncertainty estimates for both in-domain and out-of-domain data sets. Finally, we leverage selected uncertainty estimation methods for active learning to evaluate their capacity to support experimental design.</p>\n<section><p><strong>Keywords:</strong> Molecular property prediction, Deep learning, Uncertainty quantification, Active learning</p></section></section><section><h2>Introduction</h2>\n<p>Quantitative structure\u2013property relations (QSAR) models have become a central component of molecular design protocols across a wide range of application areas including drug design [<a href=\"#CR1\">1</a>] and electrolyte development [<a href=\"#CR2\">2</a>]. The current pipeline for electrolyte development is time- and resource-intensive due\u00a0to the multiple computational and experimental requirements needed to fully characterize electrolyte performance [<a href=\"#CR3\">3</a>\u2013<a href=\"#CR5\">5</a>]. The ability to rapidly and accurately screen vast libraries of potential molecular candidates for performance-relevant properties would significantly accelerate the discovery of novel materials that will be needed to meet the next-generation energy storage requirements.</p>\n<p>While deep learning models have been proven to be a promising tool for the prediction of molecular properties from molecular structure [<a href=\"#CR6\">6</a>\u2013<a href=\"#CR8\">8</a>], the practical utility of such models for the screening and discovery of molecules for targeted applications is still limited in many respects. Such practical applications typically require the ability to transfer models trained on one set of molecules with known target properties to another set of molecules which may differ in significant ways from the original training data. Due to the known tendency of such models towards overfitting [<a href=\"#CR9\">9</a>], such predictions are often unreliable, overconfident, and poorly calibrated. This is a particular challenge for molecular property prediction efforts as relevant training data sets are often biased to certain subsets of molecular space due to the scale, variation, and data availability of this space.</p>\n<p>In application towards high throughput virtual screening (HTVS), it is beneficial for the deep learning models to be accompanied by uncertainty quantification (UQ) capabilities to understand the reliability of the predictions. Such estimates are also crucial for the targeted acquisition of new measurements or computations to support improved model performance on previously uncertain regions of molecular space to best optimize the use of time- and effort-intensive experimental and computational resources [<a href=\"#CR3\">3</a>\u2013<a href=\"#CR5\">5</a>]. Ensuring that UQ methods are informative for out-of-distribution (OOD) molecules which differ significantly from the training data is essential for the effective optimal selection of new molecules for characterization.</p>\n<p>In the context of our study, we consider UQ to be predominantly arising from two sources: data uncertainty and model uncertainty. Data uncertainty can be due to a myriad of factors, including but not limited to data domain, data sampling bias, and data sparsity. Similarly, model uncertainty encompasses uncertainties arising from aspects such as model architecture, model parameters, and training methodology.</p>\n<p>There are several challenges to generating accurate estimates of the uncertainty of deep learning models including the ability to accurately estimate uncertainty for both in-distribution (ID) and OOD samples. Due to the sensitivity of deep learning models to distribution shifts, uncertainty estimates for OOD molecules can suffer from inaccuracy similarly to the inaccuracies observed for molecular properties predictions in these regions of molecular structure space. Prior work [<a href=\"#CR10\">10</a>] discusses the limitations in the data sets typically used for UQ studies, with most studies performed using standard data sets that are specific to particular use cases and few performed on real-world data. Moreover, additional work is needed to understand how UQ methods perform across different deep learning architectures [<a href=\"#CR10\">10</a>]. In particular, there have been relatively few studies investigating UQ for graph neural network (GNN) architectures [<a href=\"#CR11\">11</a>]. Other challenges of UQ for deep learning include method scalability to large data, adaptability to complex model architectures, and interpretability for non-experts [<a href=\"#CR12\">12</a>].</p>\n<p>Previous work [<a href=\"#CR13\">13</a>] demonstrates that no single UQ method has been shown to consistently outperform others across all molecular property prediction tasks. In this study, we expand upon this existing work in several ways. First, we expand the UQ evaluation to a set of larger, more diverse data sets with target properties that are relevant for energy storage applications. Secondly, while most UQ methods are evaluated on data sampled from the same data set as the training data, we specifically target our evaluation approach to probe UQ performance on tasks relevant to <em>generalizing</em> to previously unseen molecule types. To this end, we introduce novel evaluation approaches to probe whether the UQ methods can identify OOD molecules and can successfully quantify changes in model uncertainty due to data set changes. We discover that many standard UQ approaches fail to perform well at these OOD tasks. We admit that it is indeed possible to adversarially construct data populations that pose significant challenges for extrapolation from training subsets, unless there is some a priori knowledge. While the identification of OOD molecules and the quantification of changes in model uncertainty cannot be directly correlated in a trivial manner, our approach attempts to explore these correlations within the constraints of our methodology. We believe this exploration has substantial value in progressing our understanding of model uncertainties in the context of OOD prediction. Finally, we study the relationship between UQ methods and the downstream performance of uncertainty-based active learning (AL) methods. Again, we specifically focus on the capabilities of these methods to accelerate the <em>generalization</em> of the models, which is crucial for the practical usage of these methods for material design and discovery pipelines.</p>\n<p>In this study, we perform a comprehensive analysis of UQ and AL performance for the prediction of aqueous solubility and redox potential. These properties were selected due to their applicability for the design of aqueous organic redox flow batteries (AORFBs), which are a promising next-generation energy storage technology with the potential to address current challenges with implementing grid s...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10633997"
    },
    {
      "title": "Robust Molecular Property Prediction via  Densifying Scarce Labeled Data",
      "text": "Robust Molecular Property Prediction via\nDensifying Scarce Labeled Data\nJina Kim * 1 Jeffrey Willette * 1 Bruno Andreis * 1 Sung Ju Hwang 1 2\nAbstract\nA widely recognized limitation of molecular pre\u0002diction models is their reliance on structures ob\u0002served in the training data, resulting in poor gener\u0002alization to out-of-distribution compounds. Yet in\ndrug discovery, the compounds most critical for\nadvancing research often lie beyond the training\nset, making the bias toward the training data par\u0002ticularly problematic. This mismatch introduces\nsubstantial covariate shift, under which standard\ndeep learning models produce unstable and in\u0002accurate predictions. Furthermore, the scarcity\nof labeled data\u2014stemming from the onerous and\ncostly nature of experimental validation\u2014further\nexacerbates the difficulty of achieving reliable\ngeneralization. To address these limitations, we\npropose a novel meta-learning-based approach\nthat leverages unlabeled data to interpolate be\u0002tween in-distribution (ID) and out-of-distribution\n(OOD) data, enabling the model to meta-learn\nhow to generalize beyond the training distribution.\nWe demonstrate significant performance gains on\nchallenging real-world datasets with substantial\ncovariate shift, supported by t-SNE visualizations\nhighlighting our interpolation method.\n1. Introduction\nMolecular property prediction plays a central role in drug\ndiscovery pipelines, enabling researchers to prioritize com\u0002pounds for costly and time-consuming experimental valida\u0002tion. Accurate computational models have the potential to\ndramatically accelerate early-stage discovery by predicting\ncritical attributes such as bioactivity, toxicity, and solubility\n*Equal contribution 1Korea Advanced Institute of Sci\u0002ence and Technology (KAIST), South Korea 2Deepauto.ai,\nSouth Korea. Correspondence to: Jina Kim <ji\u0002nakim@kaist.ac.kr>, Jeffrey Willette <jwillette@kaist.ac.kr>,\nBruno Andreis <andries@kaist.ac.kr>, Sung Ju Hwang\n<sungju.hwang@kaist.ac.kr>.\nProceedings of the Workshop on Generative AI for Biology at the\n42 nd International Conference on Machine Learning, Vancouver,\nCanada. PMLR 267, 2025. Copyright 2025 by the author(s).\nIn-Distribution Out-Of-Distribution\nGeneralizing OOD\ntrain point\n, \nContext guided Interpolation\ncontext point\nRobust\nPrediction\nCovariate Shift\nOOD data point\nlogP: 1.2 | Tox: low logP: 4.2 | Tox: high\nlogP: 2.5 | Tox: medium\nFigure 1. Concept. We densify the train dataset using external\nunlabeled data (context point) for robust generalization across\ncovariate shift. Notation details are provided in Section 4.\nbefore synthesis (Schneider, 2018; Vamathevan et al., 2019).\nHowever, building reliable predictive models generalizing to\nnovel, unseen compounds remains a fundamental challenge.\nStandard molecular property prediction models tend to rely\nheavily on patterns observed within the training distribution,\nresulting in poor generalization to out-of-distribution com\u0002pounds (Klarner et al., 2023; Ovadia et al., 2019; Koh et al.,\n2021). In drug discovery, this limitation is particularly prob\u0002lematic, since the compounds most crucial for advancing re\u0002search often lie far beyond the chemical spaces represented\nduring training (Lee et al., 2023). The resulting covariate\nshift introduces significant obstacles to reliable prediction,\nwith models frequently producing unstable outputs when\nextrapolating to new regions of chemical space. Further\ncompounding these challenges, experimental validation of\nmolecular properties is both costly and resource-intensive,\nleading to a scarcity of labeled data and increasing reliance\non computational exploration (Altae-Tran et al., 2017). Also,\navailable labeled data is typically concentrated in narrow\nregions of chemical space, introducing bias that hampers\ngeneralization to unseen compounds (Klarner et al., 2023).\nWhile vast collections of unlabeled molecular structures\nare readily available (Sterling & Irwin, 2015; Kim et al.,\n2021), offering rich information about the structure of\nchemical space, existing methods often fail to fully ex\u0002ploit this resource to improve generalization (Klarner et al.,\n1\nRobust Molecular Property Prediction via Densifying Scarce Labeled Data\n2023). Therefore, we propose a novel meta-learning based\nmethod that leverages unlabeled data to densify the scarce\ntrain dataset and guide the model toward sensible behav\u0002ior in unexplored regions of chemical space. Our code\ncan be found at https://github.com/JinA0218/\ndrugood-densify.\n2. Methodology\nPreliminaries. We consider the problem of molecular\nproperty prediction under covariate shift. Given a small\nlabeled dataset Dtrain = {(xi, yi)}\nn\ni=1 and abundant unla\u0002beled molecules Dunlabeled = {xj}\nm\nj=1, the goal is to learn a\npredictive model f : X \u2192 Y that reliably generalizes to a\ndistributionally shifted test set Dtest.\nScarce Data Densification with Unlabeled Data To ad\u0002dress this, we propose a meta-learning based framework that\ninterpolates the training distribution Dtrain with an exoge\u0002nous distribution Dunlabeled. Our objective is to leverage the\ncheaper and more abundant distribution Dunlabeled to densify\nthe scarce labeled distribution Dtrain in a way that encour\u0002ages the model to generalize robustly under covariate shift,\nparticularly in out-of-distribution scenarios where we have\nno label information and therefore high uncertainty. For this,\nwe utilize subsets of Dunlabeled as Dcontext and Dmvalid, where\nDcontext is a domain-informed external task distribution for\ninterpolating to Dtrain, and Dmvalid is a meta-validation set\nused to guide the interpolation function. Inspired by (Lee\net al., 2024), we introduce a permutation invariant learnable\nset function (Zaheer et al., 2017; Lee et al., 2019) \u00b5\u03bb as a\nmixer (interpolator), which learns to mix each point from\nxi \u223c Dtrain with the context points {cij}\nmi\nj=1 in a way that\ndensifies Dtrain, where\n(xi, yi) \u223c Dtrain, {cij}\nmi\nj=1 \u223c Dcontext, i \u2208 {1, . . . , B}\nand B denotes the minibatch size, mi \u223c Uint(0, M) where\nM controls the maximum number of context samples drawn\nfrom Dcontext for each minibatch. Given a feature dimen\u0002sion D, for each i, the input consists of, xi \u2208 R\nB\u00d71\u00d7D\nand {cij}\nmi\nj=1 \u2208 R\nB\u00d71\u00d7D, where the set {cij}\nmi\nj=1 can be\norganized into a tensor Ci \u2208 R\nB\u00d7mi\u00d7D.\nOverall, our model has two main components: (1) a meta\u0002learner f\u03b8l\n, which is a standard MLP at the l\nth layer, that\nmaps input data x\n(l\u22121)\ni \u2208 R\nB\u00d71\u00d7D to the feature space of\nthe (l + 1)th layer, producing x\n(l)\ni = f\u03b8l\n(x\n(l\u22121)\ni\n), and (2) a\nlearnable set function \u00b5\u03bb which mixes x\n(lmix)\ni\nand C\n(lmix)\ni\nas\na set and outputs a single pooled representation x\u02dc\n(lmix)\ni =\n\u00b5\u03bb({x\n(lmix)\ni\n, C(lmix)\ni\n}) \u2208 R\nB\u00d71\u00d7H, where H is the hidden\ndimension and lmix is the layer where the mixing happens.\nThe full model structure with L layers can be expressed as\n\u02c6f\u03b8,\u03bb := f\u03b8L\n\u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03b8lmix+1 \u25e6 \u00b5\u03bb \u25e6 f\u03b8lmix\u22121\n\u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03b81.\nWe utilize bilevel optimization for training meta-learner f\u03b8l,\nand treat the set function parameter \u00b5\u03bb as a hyperparameter\nto be optimized in the outer loop (Lorraine et al., 2019). As\nshown in Table 2 (w/o bilevel optimization), simply opti\u0002mizing the meta-learner parameters \u03b8 and the set function\nparameters \u03bb jointly can lead to overfitting to the task dis\u0002tribution and harms test-time generalization. Following the\nsetting of (Lorraine et al., 2019), during training, we only\nupdate the parameter \u03b8 in the inner loop and we only update\nthe parameter \u03bb in the outer loop (see Figure 9b for the\ndetailed model structure of the bilevel optimization).\nIn the inner loop, the model accepts xi \u2208 R\nB\u00d71\u00d7H\nand Ci \u2208 R\nB\u00d7mi\u00d7H and the set encoder \u00b5\u03bb mixes\n{x\n(lmix)\ni\n, C(lmix)\ni\n} and outputs x\u02dc\n(lmix)\ni \u2208 R\nB\u00d71\u00d7H. Since Ci\nis used to introduce a domain-informed external context to\ndensify Dtrain, we utilize the original label yi from Dtrain to\ntrain the task learner parameters f\u03b8l, with the mixed x\u02dc\n(lmix)\ni\n.\nIn the outer loop, we train the set encoder using hypergra\u0002dient (Lorraine et al., 2019), w...",
      "url": "https://openreview.net/pdf/70bee4e7da17329c1a51ab89274f7974d10cc74d.pdf"
    },
    {
      "title": "",
      "text": "BOOM: Benchmarking Out-Of-distribution Molecular\nProperty Predictions of Machine Learning Models\nEvan R. Antoniuk\u2020\u2217 Shehtab Zaman\u2021\u2217 Tal Ben-Nun\u2020 Peggy Li\u2020\nJames Diffenderfer\u2020 Busra Demirci\u2021 Obadiah Smolenski\u2021 Tim Hsu\u2020\nAnna M. Hiszpanski\u2020 Kenneth Chiu\u2021 Bhavya Kailkhura\u2020\nBrian Van Essen\u2020\n\u2020 Lawrence Livermore National Laboratory, Livermore, CA\n\u2021 Binghamton University, Binghamton, NY\nAbstract\nData-driven molecular discovery leverages artificial intelligence/machine learning (AI/ML)\nand generative modeling to filter and design novel molecules. Discovering novel molecules\nrequires accurate out-of-distribution (OOD) predictions, but ML models struggle to general\u0002ize OOD. Currently, no systematic benchmarks exist for molecular OOD prediction tasks.\nWe present BOOM, benchmarks for out-of-distribution molecular property predictions: a\nchemically-informed benchmark for OOD performance on common molecular property\nprediction tasks. We evaluate over 150 model-task combinations to benchmark deep learning\nmodels on OOD performance. Overall, we find that no existing model achieves strong gener\u0002alization across all tasks: even the top-performing model exhibited an average OOD error 3\u00d7\nhigher than in-distribution. Current chemical foundation models do not show strong OOD ex\u0002trapolation, while models with high inductive bias can perform well on OOD tasks with sim\u0002ple, specific properties. We perform extensive ablation experiments, highlighting how data\ngeneration, pre-training, hyperparameter optimization, model architecture, and molecular\nrepresentation impact OOD performance. Developing models with strong out-of-distribution\n(OOD) generalization is a new frontier challenge in chemical machine learning (ML). This\nopen-source benchmark is available at https://github.com/FLASK-LLNL/BOOM.\n1 Introduction\nMolecular discovery pipelines have increasingly relied upon machine learning (ML) models [Bohacek\net al., 1996, Reymond, 2015, Kailkhura et al., 2019]. These models discover new molecules by\neither screening a list of enumerated molecules or by guiding a generative model towards molecules\nof interest [Wang et al., 2023a]. Molecular discovery is inherently an out-of-distribution (OOD)\nprediction problem, since the molecules need to either (i) exhibit properties that extrapolate beyond\nthe training dataset, or (ii) possess a previously unconsidered chemical substructure. In either case,\nsuccess depends on the learned model\u2019s ability to make accurate predictions on samples that are not\nin the same distribution as the training data.\nDespite the importance of OOD performance to real-world molecular discovery, the OOD per\u0002formance of common ML models for molecular property prediction has yet to be systematically\nexplored. Due to the lack of standardized splits for testing models, especially splits based on the\ndata distribution, we believe that current ML models are optimizing in-distribution performance on\n\u2217Equal Contribution\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\ninsufficiently challenging datasets that do not adequately measure real-world performance. Currently,\nlittle empirical knowledge exists about how choices regarding the pretraining task, model architecture,\nand/or dataset diversity impact the generalization performance of chemistry foundation models that\nare expected to generalize across all chemical systems.\nIn this work, we develop BOOM, benchmarks for out-of-distribution molecular property predictions,\na standardized benchmark for assessing the OOD generalization performance of molecule property\nprediction models. Our work consists of the following main contributions:\n\u2022 We develop a general and robust methodology for evaluating the performance of chemical\nproperty prediction models for property values beyond their training distribution. We intro\u0002duce OOD-specific metrics such as binned R2\nto allow comparisons of OOD performance\nacross all models.\n\u2022 We perform the first large-scale OOD performance benchmarking of state-of-the-art ML\nchemical property prediction models. Across 10 diverse OOD tasks and 15 models, we\ndo not find any existing models that show strong OOD generalization across all tasks. We\ntherefore put forth BOOM OOD property prediction as a frontier challenge for chemical\nfoundation models.\n\u2022 Our work highlights insights into how pretraining strategies, model architecture, molecular\nrepresentation, and data augmentation impact OOD performance. These findings point\ntowards strategies for the chemistry community to achieve chemical foundation models with\nstrong OOD generalization across all chemical systems.\n2 BOOM\nDefining Out-of-distribution. Consider a supervised dataset D with N molecules M \u2208\n{M1,M2, ...,MN } and associated labels or properties y \u2208 {y1, y2, ..., yN }. The problem of\nout-of-distribution prediction can be defined as the mismatch in the probability distribution, P of the\ntraining and test sets, Dtrain and Dtest such that,\nP(M, y|Dtest) \u0338= P(M, y|Dtrain) (1)\nThe key question is defining the density function P(M, y) over a set of molecules and their respective\nproperties. The density can be defined over the chemical structure or molecule features, or over the\nproperties. Formally, we define out-of-distribution as low-density regions over the property space,\nsuch that:\n0 < P(ytest) \u2264 P(ytrain) (2)\nFarquhar and Gal [2022] define this as a complement distribution conditioned on the targets. This is\nknown as concept or label shift as well [Liu et al., 2024]. While we focus on designing splits with a\nconcept shift, it is important to note that depending on the property, this may result in a covariate shit,\nresulting in a structural or chemical imbalance. The probability density over the labels is determined\nusing kernel density estimation (KDE), allowing us to generalize to multimodal distributions. The\nsplit strategy algorithm for each dataset is detailed in Appendix A.1. The lowest probability samples\nfrom the KDE estimated distribution are held-out (see Fig. 1) to evaluate the consistency of ML\nmodels to discover molecules with state-of-the-art properties that extrapolate beyond the training\ndata.\nDatasets. BOOM consists of 10 quantum chemical molecular property datasets derived from\nQM9 [Ramakrishnan et al., 2014] and the 10k Dataset [Antoniuk et al., 2025], derived from the Cam\u0002bridge Structural Database. The 10k Dataset was sourced from 10,206 experimentally synthesized,\nsmall organic molecules and contains the density functional theory calculated values of their molecu\u0002lar density and solid heat of formation (HoF). We collect 8 molecular property datasets from the QM9\nDataset: isotropic polarizability (\u03b1), heat capacity (Cv), highest occupied molecular orbital (HOMO)\nenergy, lowest unoccupied molecular orbital (LUMO) energy, HOMO-LUMO gap, dipole moment\n(\u00b5), electronic spatial extent (\nR2\n\u000b\n), and zero point vibrational energy (ZPVE). We also select a\nrandom subset of the dataset to serve as the ID test set, detailed in Appendix A. To further expand the\napplication space of BOOM, we also perform benchmarking on the Lipophilicity dataset[Wu et al.,\n2018] of 4,200 experimental measurements of the octanol/water distribution coefficient, which is of\nrelevance for drug compounds. The inclusion of the Lipophilicity dataset serves as an exemplary\n2\nFigure 1: (Left) An example OOD dataset included in the BOOM benchmark. To assess OOD\nperformance, we split each chemical property dataset into an out-of-distribution (OOD) Test Set\n(blue), an in-distribution (ID) Test Set (orange) and a Train Set (green), as described in Section 2.\n(Right) Example model predictions on this task exhibiting weak correlation on the OOD samples.\ndataset for performing OOD evaluations on experimentally measured properties, rather than only\ncomputed physicochemical properties (See Table 9).\nMetrics. We also propose standardized metrics over the ID and OOD to compare models. We\nuse root mean square error (RMSE) over respective data spli...",
      "url": "https://openreview.net/pdf/80574b0cbe412a64f9a63884b631eb51505762e6.pdf"
    }
  ]
}