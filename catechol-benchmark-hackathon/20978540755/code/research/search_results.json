{
  "query": "What techniques can reduce the gap between local cross-validation and leaderboard score when the intercept is larger than the target in Kaggle competitions?",
  "results": [
    {
      "title": "ANSWER",
      "text": "The most effective way to shrink the \u201cvalidation\u2011vs\u2011leaderboard\u201d gap when the model\u2019s intercept (or\u202fbase\u2011score) is noticeably larger than the true target is to treat the intercept as a tunable bias and align it with the test\u2011set distribution.\u202fXGBoost lets you override the automatically estimated intercept by fixing\u202f`base_score`\u202fto the empirical mean of the target (or to a value obtained on a small hold\u2011out that mimics the leaderboard data) and, if you already have a first\u2011stage prediction, you can feed it in as a\u202f`base_margin`\u202fso the second\u2011stage model learns only the residual correction\u202f([xgboost.readthedocs.io](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html)).\u202fDoing this removes the systematic over\u2011prediction caused by an oversized intercept.\n\nIn addition, standard Kaggle\u2011style practices that directly address the validation\u2011leaderboard gap should be applied: use a validation split that reproduces the test\u2011set\u2019s temporal or stratified structure, calibrate the final predictions on a private hold\u2011out (e.g., by fitting a simple linear scaling or isotonic regression), and optionally blend several models so that any residual bias is averaged out.\u202fThese steps are repeatedly recommended when the leaderboard score diverges from cross\u2011validation results\u202f([datascience.stackexchange.com](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)). Together, intercept correction, realistic CV, and post\u2011hoc calibration are the key techniques for narrowing the gap.",
      "url": ""
    },
    {
      "title": "Gap leaderboard score and model scoring on a Competition",
      "text": "**Teams**\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and easy to search.\n\n[Learn more about Teams](https://stackoverflow.co/teams/)\n\n# [Gap leaderboard score and model scoring on a Competition](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\n\n[Ask Question](https://datascience.stackexchange.com/questions/ask)\n\nAsked7 years, 4 months ago\n\nModified [6 years, 4 months ago](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?lastactivity)\n\nViewed\n694 times\n\n3\n\n$\\\\begingroup$\n\nI'm working on a Veolia challenge on Ens Data Challenge [ens-data](https://challengedata.ens.fr/en/home) (equivalent to Kaggle) the goal is to classify very rare binary events (the failure of a pipeline ) for 2014 and 2015 (y={2014,2015}). In input we have 5 features 3 categorical features (which I turned into dummy variable) and two continuous. The score is average AUC, $0.6\\*AUC\\_1 + 0.4\\*AUC\\_2$.\n\nMy problem is the following, when I compute each AUC (for 2014 and 2015) with a stratified kfold cross validation and I compute the average AUC I get roughly 0.88 and when I submit on the website I end up with 0.67, I guess there is a problem in my code.\n\nHere is my code for choosing the best model for 2014:\n\nRk: to predict on the test set (2014,2015 unknown), I first predict with all 5 features, 2014. Then I add the prediction of 2014 to my feature to predict 2015\n\n```\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LG', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\n\n# stratifiedkfold is defined by default when there is an integer\nscoring = 'roc_auc'\nnum_folds = 10\n\nfor name, model in models:\n    cv_results = cross_validation.cross_val_score(model, X, Y, cv=num_folds, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n```\n\n- [machine-learning](https://datascience.stackexchange.com/questions/tagged/machine-learning)\n- [python](https://datascience.stackexchange.com/questions/tagged/python)\n\n[Share](https://datascience.stackexchange.com/q/16789)\n\n[Improve this question](https://datascience.stackexchange.com/posts/16789/edit)\n\nFollow\n\n[edited Feb 7, 2017 at 9:17](https://datascience.stackexchange.com/posts/16789/revisions)\n\nbouritosse\n\nasked Feb 6, 2017 at 20:09\n\n[![bouritosse's user avatar](https://www.gravatar.com/avatar/d05cc42fc90ecb44bf614b1f69a90fcc?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/19065/bouritosse)\n\n[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse) bouritosse\n\n9366 bronze badges\n\n$\\\\endgroup$\n\n5\n\n- $\\\\begingroup$You mean you have a model scoring 0.88 while the leaderboard(the score on the website) shows 0.67?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 2:27\n\n- $\\\\begingroup$yes that is correct$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 9:15\n\n- 1\n\n\n\n\n\n$\\\\begingroup$Your test data(the 0.88 one) is different from the test data on the website(the 0.67 one). There must be some difference between their scores. Are you concerning about over-fitting?$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 10:13\n\n- $\\\\begingroup$kfold cross validation is not supposed to downsize the overfitting ?$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 12:21\n\n- $\\\\begingroup$Can you add your score on training set to your question, that's crucial.$\\\\endgroup$\n\n\u2013\u00a0[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade)\n\nCommentedFeb 7, 2017 at 12:49\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## 1 Answer 1\n\nSorted by:\n[Reset to default](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition?answertab=scoredesc#tab-top)\n\nHighest score (default)Date modified (newest first)Date created (oldest first)\n\n1\n\n$\\\\begingroup$\n\nAs you have commented, you are concerning about over-fitting.\n\nIn fact, cross validation will help to weaken over-fitting, but it can't eliminate over-fitting. Model scoring on TRAIN dataset sometimes exceeds scoring on TEST dataset. Here are some examples I can find:\n\n1. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) (the famous ResNet paper). Checkout figure.1\n2. This [kernel](https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/univariate-analysis-regression-lb-0-006) of [Two Sigma Financial Modeling Challenge](https://www.kaggle.com/c/two-sigma-financial-modeling) on [Kaggle](https://www.kaggle.com/) says:\n\n> we are getting a public score of 0.0169 which is slightly better than the previous one.\n> Submitting this model to the LB gave me a score of 0.006\n\nIn my practice, I am more willing to concern the scoring gap between TRAIN, VALIDATION(DEVELOP) and TEST dataset. Personally speaking, TRAIN>VALIDATION=TEST is better than TRAIN=VALIDATION>TEST.\n\nEdit: For class imbalance problem, there are some resources:\n\n1. [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This blog shows a common workflow dealing with imbalanced class issue.\n\n2. [Class Imbalance Problem in Data Mining: Review](http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). This paper compares several algorithms created for solving the class imbalance problem.\n\n\n[Share](https://datascience.stackexchange.com/a/16808)\n\n[Improve this answer](https://datascience.stackexchange.com/posts/16808/edit)\n\nFollow\n\n[edited Feb 8, 2017 at 1:58](https://datascience.stackexchange.com/posts/16808/revisions)\n\nanswered Feb 7, 2017 at 12:48\n\n[![Icyblade's user avatar](https://www.gravatar.com/avatar/0696854959b681c4cefaf494015e8bf4?s=64&d=identicon&r=PG&f=y&so-version=2)](https://datascience.stackexchange.com/users/28628/icyblade)\n\n[Icyblade](https://datascience.stackexchange.com/users/28628/icyblade) Icyblade\n\n4,33611 gold badge2424 silver badges3434 bronze badges\n\n$\\\\endgroup$\n\n2\n\n- $\\\\begingroup$On the training I get 0.847700964724 AUC and on the validation set LG: 0.909566 (0.032773). With LogisticRegression(class\\_weight='balanced')$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 13:09\n\n- 1\n\n\n\n\n\n$\\\\begingroup$I think the problem is that my class are really imbalance I have 0.19% of failure in class 2014$\\\\endgroup$\n\n\u2013\u00a0[bouritosse](https://datascience.stackexchange.com/users/19065/bouritosse)\n\nCommentedFeb 7, 2017 at 20:06\n\n\n[Add a comment](https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition)\u00a0\\|\n\n## Your Answer\n\nDraft saved\n\nDraft discarded\n\n### Sign up or [log in](https://datascience.stackexchange.com/users/login?ssrc=question_page&returnurl=https%3a%2f%2fdatascience.stackexchange.com%2fquestions%2f16789%2fgap-leaderboard-score-and-model-scoring-on-a-competition%23new-answer)\n\nSign up using Google\n\nSign up using Facebook\n\nSign up using Email and Password\n\nSubmit\n\n### Post as a guest\n\nName\n\nEmail\n\nRequired, but never shown\n\nPost Your Answer\n\nDiscard\n\nBy clicking \u201cPost Your Answer\u201d, you agree to our [terms of service](https://stackoverflow.com/legal/terms-of-service/public) and acknowledge you have read our [pri...",
      "url": "https://datascience.stackexchange.com/questions/16789/gap-leaderboard-score-and-model-scoring-on-a-competition"
    },
    {
      "title": "",
      "text": "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/index.html)\n- Intercept\n- [View page source](https://xgboost.readthedocs.io/en/stable/_sources/tutorials/intercept.rst.txt)\n* * *\n# Intercept [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#intercept)\nAdded in version 2.0.0.\nSince 2.0.0, XGBoost supports estimating the model intercept (named `base_score`)\nautomatically based on targets upon training. The behavior can be controlled by setting\n`base_score` to a constant value. The following snippet disables the automatic\nestimation:\n```\nimportxgboostasxgb\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\nIn addition, here 0.5 represents the value after applying the inverse link function. See\nthe end of the document for a description.\nOther than the `base_score`, users can also provide global bias via the data field\n`base_margin`, which is a vector or a matrix depending on the task. With multi-output\nand multi-class, the `base_margin` is a matrix with size `(n_samples, n_targets)` or\n`(n_samples, n_classes)`.\n```\nimportxgboostasxgb\nfromsklearn.datasetsimport make_regression\nX, y = make_regression()\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\nIt specifies the bias for each sample and can be used for stacking an XGBoost model on top\nof other models, see [Demo for boosting from prediction](https://xgboost.readthedocs.io/en/stable/python/examples/boost_from_prediction.html#sphx-glr-python-examples-boost-from-prediction-py) for a worked\nexample. When `base_margin` is specified, it automatically overrides the `base_score`\nparameter. If you are stacking XGBoost models, then the usage should be relatively\nstraightforward, with the previous model providing raw prediction and a new model using\nthe prediction as bias. For more customized inputs, users need to take extra care of the\nlink function. Let \\\\(F\\\\) be the model and \\\\(g\\\\) be the link function, since\n`base_score` is overridden when sample-specific `base_margin` is available, we will\nomit it here:\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i)\\\\\\]\nWhen base margin \\\\(b\\\\) is provided, it\u2019s added to the raw model output \\\\(F\\\\):\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i) + b\\_i\\\\\\]\nand the output of the final model is:\n\\\\\\[g^{-1}(F(x\\_i) + b\\_i)\\\\\\]\nUsing the gamma deviance objective `reg:gamma` as an example, which has a log link\nfunction, hence:\n\\\\\\[\\\\begin{split}\\\\ln{(E\\[y\\_i\\])} = F(x\\_i) + b\\_i \\\\\\\nE\\[y\\_i\\] = \\\\exp{(F(x\\_i) + b\\_i)}\\\\end{split}\\\\\\]\nAs a result, if you are feeding outputs from models like GLM with a corresponding\nobjective function, make sure the outputs are not yet transformed by the inverse link\n(activation).\nIn the case of `base_score` (intercept), it can be accessed through\n[`save_config()`](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster.save_config) after estimation. Unlike the `base_margin`, the\nreturned value represents a value after applying inverse link. With logistic regression\nand the logit link function as an example, given the `base_score` as 0.5,\n\\\\(g(intercept) = logit(0.5) = 0\\\\) is added to the raw model output:\n\\\\\\[E\\[y\\_i\\] = g^{-1}{(F(x\\_i) + g(intercept))}\\\\\\]\nand 0.5 is the same as \\\\(base\\\\\\_score = g^{-1}(0) = 0.5\\\\). This is more intuitive if\nyou remove the model and consider only the intercept, which is estimated before the model\nis fitted:\n\\\\\\[\\\\begin{split}E\\[y\\] = g^{-1}{(g(intercept))} \\\\\\\nE\\[y\\] = intercept\\\\end{split}\\\\\\]\nFor some objectives like MAE, there are close solutions, while for others it\u2019s estimated\nwith one step Newton method.\n## Offset [\uf0c1](https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html\\#offset)\nThe `base_margin` is a form of `offset` in GLM. Using the Poisson objective as an\nexample, we might want to model the rate instead of the count:\n\\\\\\[rate = \\\\frac{count}{exposure}\\\\\\]\nAnd the offset is defined as log link applied to the exposure variable:\n\\\\(\\\\ln{exposure}\\\\). Let \\\\(c\\\\) be the count and \\\\(\\\\gamma\\\\) be the exposure,\nsubstituting the response \\\\(y\\\\) in our previous formulation of base margin:\n\\\\\\[g(\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}) = F(x\\_i)\\\\\\]\nSubstitute \\\\(g\\\\) with \\\\(\\\\ln\\\\) for Poisson regression:\n\\\\\\[\\\\ln{\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}} = F(x\\_i)\\\\\\]\nWe have:\n\\\\\\[\\\\begin{split}E\\[c\\_i\\] &= \\\\exp{(F(x\\_i) + \\\\ln{\\\\gamma\\_i})} \\\\\\\nE\\[c\\_i\\] &= g^{-1}(F(x\\_i) + g(\\\\gamma\\_i))\\\\end{split}\\\\\\]\nAs you can see, we can use the `base_margin` for modeling with offset similar to GLMs",
      "url": "https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html"
    },
    {
      "title": "3rd Place Solution - a little XGBoost, a lot of feature engineering",
      "text": "3rd Place Solution - a little XGBoost, a lot of feature engineering | Kaggle\nKaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic.\n[\nLearn more\n](https://www.kaggle.com/cookies)\nOK, Got it.\n![](https://www.kaggle.com/static/images/error/server-error-illo_light.svg)\n###### We can't find that page.\nYou can search Kaggle above or[visit our homepage](https://www.kaggle.com/).",
      "url": "https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/discussion/563064"
    },
    {
      "title": "1st Place Solution | Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings/writeups/1st-place-solution"
    },
    {
      "title": "How to Score better in Kaggle Competition",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/code/vithal2311/how-to-score-better-in-kaggle-competition"
    },
    {
      "title": "How to improve your kaggle competition leaderboard ranking | by HD",
      "text": "<div><div><div><h2>Tips from a new \u2018Kaggler\u2019 building CNN\u2019s for blindness detection</h2><div><a href=\"https://medium.com/@hdln?source=post_page---byline--bcd16643eddf---------------------------------------\"><div><p></p></div></a></div></div><figure></figure><blockquote><p>After recently competing in the 2019 APTOS Blindness Detection Kaggle Competition and finishing in top 32%, I thought I would share my process for training convolutional neural networks. My only prior deep learning experience was completing the Deeplearning.ai Specialisation, hence this is all you should need to read this article.</p></blockquote><h2><strong>Sections to this article</strong></h2><ol><li>Competition context</li><li>Keeping a logbook</li><li>Get more data</li><li>Leveraging existing kernels</li><li>Preprocessing images</li><li>Training is a very very slow process (but don\u2019t worry)</li><li>Transfer learning</li><li>Model selection</li></ol><h2>Competition context</h2><p>I spent the last 2\u20133 months working on and off on the<a href=\"https://www.kaggle.com/c/aptos2019-blindness-detection\"> APTOS 2019 Blindness Detection Competition</a> on Kaggle, which required you to grade images of people\u2019s eyes to 5 categories of diabetic retinopathy severity. In the remainder of this article I talk about some tips and tricks you can use in <em>any kaggle vision competition, </em>as I feel that the things I learned from this competition are pretty much universally applicable<em>.</em></p><h2>Keep a logbook</h2><p>Like any good scientific experiment, we change one thing at a time and compare our results to our control. Hence when training a CNN (Convolutional Neural Network) we should do likewise and record the change and the results in a logbook. Heres the one I used from the blindness detection challenge.</p><figure><figcaption>My logbook for Kaggle APTOS Blindness Detection Challenge</figcaption></figure><p>I don\u2019t claim that the exact table I use here is ideal (far from it), but I found it useful to be able to at least identify each time I made a change whether the model improved or not cumulatively on the previous changes. Regardless I highly recommend you keep some form of logbook as its very difficult to identify if anything your doing is working otherwise.</p><p>Some ideas I have for my next competition logbook is to:</p><ol><li>Establish a single baseline model to compare all future changes to</li><li>Come up with a bunch of tweaks you want to try and run modified versions of the baseline for each tweak independently rather than in a cumulative fashion.</li><li>Maintain the same (and smallest) CNN Architecture for as long as possible as it will make iteration quicker and with some look many of the hyper-parameters should transfer decently to larger more complex models.</li></ol><h2>Get more data</h2><p>Do some research before you start coding and see if a similar competition has been run before or if there are any databases of similar labelled training sets you can use. More data is never really harmful to your model (assuming the quality of labelling is decent), so get as much of it as you can, but just don\u2019t forget to keep your validation and test sets from the original dataset provided to you or you may end up with a train- test mismatch<em>.</em></p><h2>Leveraging existing kernels</h2><p>If your new to deep learning competitions (like me) you probably don\u2019t want to write your entire notebook from scratch \u2014 especially when someone else has probably already posted a starter kernel for your competition (Why reinvent the wheel right?). This will probably save you a bunch of time on debugging and get you onto learning new stuff faster by just tweaking someone else\u2019s model.</p><p><strong>This was a good starter kernel</strong> that I used and retrofitted for almost all of my further trials.</p><p><strong>A word of warning: </strong>If a kernel suggests a bunch of techniques to use for your model you should check if they state the resultant performance gains, otherwise be skeptical and conduct tests yourself before blindly incorporating them into your own models :)</p><h2>Preprocessing Images</h2><p><strong>Cropping &amp; Other Augmentations: </strong>This step is a must. Training images may be in a very raw state. For example in the blindness detection challenge the images were all cropped at different ratios which meant a dumb algorithm could overfit to the black space around the eye which was more prevalent in one class than another.</p><figure><figcaption>Source: <a href=\"https://www.kaggle.com/taindow/be-careful-what-you-train-on\">https://www.kaggle.com/taindow/be-careful-what-you-train-on</a></figcaption></figure><p>Hence cropping and resizing images in a robust way was a crucial part of this competition. There were also many image augmentation techniques such as random cropping, rotation, contrast and brightness etc, which I had varying degrees of success with.</p><p><strong>Imbalanced classes:</strong> Invariably there are more training examples for some classes than others, so you need to fix this before you start training. A combination of techniques that work ok are <em>over / under-sampling </em>as well as <em>mixup </em>(Zhang et al., 2019) during mini batch gradient descent.</p><p><strong>Preprocessing Computation: </strong>Often the dataset will be quite large and applying rudimentary procedures such as standardising size and cropping of images should be done in a separate kernel t (or offline dep. on the size of the dataset)and re-uploaded as a modified version of the original data \u2014 otherwise you will have to do this computation at every epoch / run of your model (which is a terrible waste of time).</p><h2>Training is a very very slow process</h2><p>Now that you\u2019ve written your first kernel you need to test it out! Kaggle kernels can run for up to 9 hours (the kernel time limit may vary by competition), the site is also running many models and can be slower at some times of the day than others as a result. My best advice is to first quickly run it in browser for 1 or 2 iterations to make sure you haven\u2019t made any errors then get several ideas you want to test out simultaneously and just hit commit on all of them and check back in a few hours. Note that if you hit commit rather than just running the kernel you don\u2019t have to keep your laptop running :).</p><h2>Transfer Learning</h2><p>You won\u2019t be training any model from scratch which is sufficiently large. Typically we will just take a large model pre-trained on imagenet or some other large dataset and fine-tune it for our purposes. In almost all cases you should unfreeze all layers of the model during fine-tuning as the results are likely to be most stable.</p><blockquote><p>This is nicely illustrated by this chart (Yosinski et al. 2014) where two networks are trained on datasets A and B then the network is chopped at layer n and the layers before are either frozen or fine tuned (indicated by +). The conclusion being seen in the second figure with the to line AnB+ with all 7 layers being tuned producing the best top-1 accuracy.</p></blockquote><figure><figcaption>(Yosinski et al. 2014)</figcaption></figure><h2>Model selection</h2><p>Your probably best off starting with a smaller model (like ResNet 50), then trying some larger architectures such as (Resnet-101, InceptionNets, EfficientNets). All of these networks have papers available and are definitely worth a read before you go ahead and use them, typically though you should expect to get better accuracy with newer models than older ones.</p><h2>Closing Remarks</h2><p>With the information i\u2019ve provided above you should be able to get a really decent score on both the public and private leaderboards.</p><p>My intuition from competing in this challenge would suggest that getting into the top 30 on the public leaderboard is sufficient to have a good chance at finishing in the top 10 on the private board due to the uncertainty associated with the remaining held-out...",
      "url": "https://medium.com/data-science/how-to-improve-your-kaggle-competition-leaderboard-ranking-bcd16643eddf"
    },
    {
      "title": "Hyperparameter Tuning\u2026 - Kaggle",
      "text": "Checking your browser - reCAPTCHA\nChecking your browser before accessing www.kaggle.com ...\nClick[here](#)if you are not automatically redirected after 5 seconds.",
      "url": "https://www.kaggle.com/questions-and-answers/570443"
    },
    {
      "title": "Intercept \u2014 xgboost 3.1.0-dev documentation",
      "text": "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/latest/tutorials/index.html)\n- Intercept\n- [View page source](https://xgboost.readthedocs.io/en/latest/_sources/tutorials/intercept.rst.txt)\n* * *\n# Intercept [\uf0c1](https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html\\#intercept)\nAdded in version 2.0.0.\nSince 2.0.0, XGBoost supports estimating the model intercept (named `base_score`)\nautomatically based on targets upon training. The behavior can be controlled by setting\n`base_score` to a constant value. The following snippet disables the automatic\nestimation:\n```\nimportxgboostasxgb\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\nIn addition, here 0.5 represents the value after applying the inverse link function. See\nthe end of the document for a description.\nOther than the `base_score`, users can also provide global bias via the data field\n`base_margin`, which is a vector or a matrix depending on the task. With multi-output\nand multi-class, the `base_margin` is a matrix with size `(n_samples, n_targets)` or\n`(n_samples, n_classes)`.\n```\nimportxgboostasxgb\nfromsklearn.datasetsimport make_regression\nX, y = make_regression()\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\nIt specifies the bias for each sample and can be used for stacking an XGBoost model on top\nof other models, see [Demo for boosting from prediction](https://xgboost.readthedocs.io/en/latest/python/examples/boost_from_prediction.html#sphx-glr-python-examples-boost-from-prediction-py) for a worked\nexample. When `base_margin` is specified, it automatically overrides the `base_score`\nparameter. If you are stacking XGBoost models, then the usage should be relatively\nstraightforward, with the previous model providing raw prediction and a new model using\nthe prediction as bias. For more customized inputs, users need to take extra care of the\nlink function. Let \\\\(F\\\\) be the model and \\\\(g\\\\) be the link function, since\n`base_score` is overridden when sample-specific `base_margin` is available, we will\nomit it here:\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i)\\\\\\]\nWhen base margin \\\\(b\\\\) is provided, it\u2019s added to the raw model output \\\\(F\\\\):\n\\\\\\[g(E\\[y\\_i\\]) = F(x\\_i) + b\\_i\\\\\\]\nand the output of the final model is:\n\\\\\\[g^{-1}(F(x\\_i) + b\\_i)\\\\\\]\nUsing the gamma deviance objective `reg:gamma` as an example, which has a log link\nfunction, hence:\n\\\\\\[\\\\begin{split}\\\\ln{(E\\[y\\_i\\])} = F(x\\_i) + b\\_i \\\\\\\nE\\[y\\_i\\] = \\\\exp{(F(x\\_i) + b\\_i)}\\\\end{split}\\\\\\]\nAs a result, if you are feeding outputs from models like GLM with a corresponding\nobjective function, make sure the outputs are not yet transformed by the inverse link\n(activation).\nIn the case of `base_score` (intercept), it can be accessed through\n[`save_config()`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.Booster.save_config) after estimation. Unlike the `base_margin`, the\nreturned value represents a value after applying inverse link. With logistic regression\nand the logit link function as an example, given the `base_score` as 0.5,\n\\\\(g(intercept) = logit(0.5) = 0\\\\) is added to the raw model output:\n\\\\\\[E\\[y\\_i\\] = g^{-1}{(F(x\\_i) + g(intercept))}\\\\\\]\nand 0.5 is the same as \\\\(base\\\\\\_score = g^{-1}(0) = 0.5\\\\). This is more intuitive if\nyou remove the model and consider only the intercept, which is estimated before the model\nis fitted:\n\\\\\\[\\\\begin{split}E\\[y\\] = g^{-1}{(g(intercept))} \\\\\\\nE\\[y\\] = intercept\\\\end{split}\\\\\\]\nFor some objectives like MAE, there are close solutions, while for others it\u2019s estimated\nwith one step Newton method.\n## Offset [\uf0c1](https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html\\#offset)\nThe `base_margin` is a form of `offset` in GLM. Using the Poisson objective as an\nexample, we might want to model the rate instead of the count:\n\\\\\\[rate = \\\\frac{count}{exposure}\\\\\\]\nAnd the offset is defined as log link applied to the exposure variable:\n\\\\(\\\\ln{exposure}\\\\). Let \\\\(c\\\\) be the count and \\\\(\\\\gamma\\\\) be the exposure,\nsubstituting the response \\\\(y\\\\) in our previous formulation of base margin:\n\\\\\\[g(\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}) = F(x\\_i)\\\\\\]\nSubstitute \\\\(g\\\\) with \\\\(\\\\ln\\\\) for Poisson regression:\n\\\\\\[\\\\ln{\\\\frac{E\\[c\\_i\\]}{\\\\gamma\\_i}} = F(x\\_i)\\\\\\]\nWe have:\n\\\\\\[\\\\begin{split}E\\[c\\_i\\] &= \\\\exp{(F(x\\_i) + \\\\ln{\\\\gamma\\_i})} \\\\\\\nE\\[c\\_i\\] &= g^{-1}(F(x\\_i) + g(\\\\gamma\\_i))\\\\end{split}\\\\\\]\nAs you can see, we can use the `base_margin` for modeling with offset similar to GLMs",
      "url": "https://xgboost.readthedocs.io/en/latest/tutorials/intercept.html"
    }
  ]
}