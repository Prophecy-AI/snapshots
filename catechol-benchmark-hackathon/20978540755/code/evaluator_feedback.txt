## What I Understood

The junior researcher followed my previous suggestion to try per-target models - training separate models for SM vs Products. The hypothesis was sound: SM has a different distribution (mean 0.52, std 0.36) vs Products (mean ~0.13, std ~0.14), and Product 2/3 are highly correlated (0.923). The competition explicitly allows different hyperparameters for different objectives.

The implementation used:
- SM model: MLP[64,32] + LightGBM (larger architecture for higher-variance target)
- Product model: MLP[32,16] + LightGBM (shared model for correlated P2/P3)

**Result**: CV 0.009068 - **4.36% WORSE** than exp_024 baseline (0.008689).

## Technical Execution Assessment

**Validation**: SOUND ✓
- Leave-one-solvent-out CV for single solvents (24 folds) correctly implemented
- Leave-one-ramp-out CV for mixtures (13 folds) correctly implemented
- Template structure followed correctly (last 3 cells match template exactly)
- TTA for mixtures properly implemented

**Leakage Risk**: None detected ✓
- Scalers fitted only on training data per fold
- Feature lookups are static (no target leakage)
- Per-target models trained independently

**Score Integrity**: VERIFIED ✓
- Single Solvent MSE: 0.009249 (n=656)
- Full Data MSE: 0.008971 (n=1227)
- Overall MSE: 0.009068
- Per-target breakdown shows SM is the problem: SM MSE 0.014034 vs P2 0.005917, P3 0.007797

**Code Quality**: GOOD ✓
- Clean implementation of per-target ensemble
- Proper handling of output dimensions
- Submission file generated correctly

Verdict: **TRUSTWORTHY** - The results are reliable, the approach just didn't work.

## Strategic Assessment

**Approach Fit**: REASONABLE BUT FAILED
The hypothesis was sound - different targets have different characteristics. However, the per-target breakdown reveals the problem:
- Product 2 MSE: 0.005917 (BETTER than joint model)
- Product 3 MSE: 0.007797 (BETTER than joint model)
- SM MSE: 0.014034 (MUCH WORSE - this is the culprit)

The SM-specific model is overfitting or undertrained. The larger architecture [64,32] for SM may have been counterproductive given the small dataset.

**Effort Allocation**: APPROPRIATE
This was a reasonable experiment to try - the competition explicitly allows per-target optimization. The negative result is informative: joint training provides useful regularization for SM prediction.

**Key Insight from This Experiment**:
The joint model benefits from **multi-task learning** - predicting all three targets together provides implicit regularization. SM prediction benefits from the shared representation learned for Products. Separating them removes this beneficial coupling.

**Assumptions Challenged**:
1. ❌ "Separate models will be better because targets have different distributions" - FALSE
2. ✓ "Products benefit from shared model" - TRUE (P2/P3 improved)
3. ❌ "SM needs larger architecture" - FALSE (overfits with [64,32])

**What This Tells Us**:
- Multi-task learning is valuable for this problem
- SM is the hardest target to predict (highest MSE in all experiments)
- The correlation between targets provides useful signal

## What's Working

1. **ACS PCA features are valuable**: exp_024 (CV 0.008689, LB 0.0893) is the best result so far
2. **Joint multi-target training**: Provides implicit regularization that helps SM prediction
3. **Simple architectures**: [32,16] MLP works better than larger networks
4. **MLP + LightGBM ensemble**: Diversity helps reduce variance

## Key Concerns

### HIGH PRIORITY: Submission Strategy

**Observation**: Only 1 submission has been made (exp_024, LB 0.0893). 4 submissions remain. The target is 0.01727.

**Why it matters**: The current best LB (0.0893) is 5.17x away from target. The CV-LB relationship suggests the target may require a fundamentally different approach. However, we have 4 submissions left and should use them strategically.

**Suggestion**: Do NOT submit this worse model. Instead, focus on approaches that might break the CV-LB relationship:
1. Try loss weighting for SM (give SM higher weight in joint model)
2. Try different architectures specifically for SM (simpler, more regularization)
3. Try ensemble of joint models with different random seeds

### MEDIUM: SM is the Bottleneck

**Observation**: Per-target MSE breakdown shows SM MSE (0.014034) is 2x worse than Products (~0.006-0.008).

**Why it matters**: SM has the highest variance (std 0.36 vs ~0.14 for Products) and is hardest to predict. Improving SM prediction specifically could yield significant gains.

**Suggestion**: Instead of separate models, try:
1. **Loss weighting**: Weight SM loss higher (e.g., 2x) in joint model
2. **Target-specific regularization**: More dropout for SM output
3. **Auxiliary loss**: Add a consistency loss between SM and Products (they should sum to ~1)

### MEDIUM: Unexplored Ensemble Diversity

**Observation**: All experiments use similar base models (MLP + LightGBM). No exploration of other model families.

**Why it matters**: Ensemble diversity is key to reducing variance. Different model families make different errors.

**Suggestion**: Try adding:
1. **XGBoost** or **CatBoost** (different gradient boosting implementations)
2. **Gaussian Process** (good for small datasets, provides uncertainty)
3. **Random Forest** (different inductive bias)

## Trajectory Assessment

| Experiment | CV Score | LB Score | Notes |
|------------|----------|----------|-------|
| exp_012 | 0.009004 | 0.0913 | First LB submission |
| exp_024 | 0.008689 | 0.0893 | Best LB (ACS PCA) |
| exp_025 | 0.009068 | - | Per-target (WORSE) |

**Key insight**: The CV-LB relationship (LB ≈ 4.19*CV + 0.0537) suggests:
- To reach target 0.01727, we'd need CV < -0.0087 (impossible)
- The intercept (0.0537) is already 3x the target

**This means**: The current approach has a fundamental ceiling. To beat the target, we need either:
1. A fundamentally different approach with a different CV-LB relationship
2. A breakthrough in CV that breaks the linear relationship

## Current State Summary

| Metric | Value |
|--------|-------|
| Best CV Score | 0.008601 (exp_022, non-compliant) |
| Best Compliant CV | 0.008689 (exp_024) |
| Best LB Score | 0.0893 (exp_024) |
| Target | 0.01727 |
| Gap to Target | 5.17x |
| Submissions Remaining | 4 |

## Top Priority for Next Experiment

**DO NOT SUBMIT THE CURRENT MODEL** - it's worse than exp_024.

**INSTEAD, TRY: Loss-Weighted Joint Model with SM Emphasis**

The per-target experiment revealed that SM is the bottleneck. Instead of separate models (which lose multi-task regularization), try:

1. **Weighted MSE Loss**: `loss = 2.0 * SM_loss + 1.0 * P2_loss + 1.0 * P3_loss`
2. **Keep joint architecture**: [32,16] MLP + LightGBM ensemble
3. **Keep ACS PCA features**: They helped in exp_024

This preserves the multi-task learning benefit while focusing optimization on the hardest target.

**Alternative high-leverage experiments**:
1. **Consistency regularization**: Add loss term for `SM + P2 + P3 ≈ 1` (mass balance)
2. **Larger ensemble**: 10+ models with different seeds (reduce variance)
3. **Different model family**: Add XGBoost or CatBoost to ensemble

## Remaining Submissions Strategy

With 4 submissions left:
1. **Next**: Loss-weighted joint model (if CV improves significantly)
2. **Reserve 2-3**: For final refinements based on LB feedback
3. **Final**: Best performing approach

**THE TARGET IS REACHABLE. The per-target experiment was informative - it showed that multi-task learning is valuable. Use this insight to guide the next experiment.**
