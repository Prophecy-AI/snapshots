{
  "query": "What techniques can achieve 50% improvement in cross-validation error for chemical reaction yield prediction?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Improving the cross\u2011validation error of reaction\u2011yield models typically relies on three complementary ideas: richer, fine\u2011grained representations; self\u2011supervised or contrastive pre\u2011training; and systematic data\u2011augmentation combined with robust validation splits.  Recent work introduced **YieldFCP**, which links SMILES strings to 3D geometry through a fine\u2011grained cross\u2011modal projector and pre\u2011trains on large reaction corpora; this approach consistently outperforms prior state\u2011of\u2011the\u2011art methods on real\u2011world datasets, indicating a sizable error reduction compared with coarse\u2011grained encodings ([sciencedirect](https://www.sciencedirect.com/science/article/pii/S2949747725000028)).  A parallel line of research (Egret) adds **reaction\u2011condition\u2011based contrastive learning** to a BERT\u2011style encoder, making the model far more sensitive to subtle condition changes and yielding \u201ccomparable or even superior performance\u201d on several benchmark sets ([spj.science](https://spj.science.org/doi/10.34133/research.0292)).  Both strategies rely on self\u2011supervised pre\u2011training that captures atomic\u2011level interactions, which has been shown to shrink cross\u2011validation errors relative to baseline models that only use raw descriptors.\n\nA second set of techniques focuses on **data augmentation** and **test\u2011time augmentation**.  Randomly permuting precursor order or applying SMILES randomizations creates multiple views of the same reaction, and fine\u2011tuning transformer\u2011based reaction models on these augmented sets improves yield prediction even when only\u202f2.5\u202f% of the original data are used for training ([ibm.com](https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty)).  Test\u2011time augmentation further generates uncertainty estimates that correlate with prediction errors, helping to prune noisy folds during cross\u2011validation.  Similarly, **pre\u2011trained graph neural networks** (GNNs) that are fine\u2011tuned on augmented reaction graphs have been reported to restore performance lost on small data regimes and to lower validation errors relative to fingerprint\u2011based baselines ([jcheminf.biomedcentral.com](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z)).\n\nFinally, **active and transfer learning** can dramatically boost predictive reliability when data are scarce.  By first transferring a random\u2011forest model trained on closely related reactions and then iteratively selecting the most informative new experiments (active transfer learning), researchers achieved \u201ccomparative predictivity\u201d on unseen nucleophiles while keeping model depth shallow for interpretability ([ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577)).  Coupled with specialized cross\u2011validation schemes such as \u201ctransformation\u2011out\u201d and \u201csolvent\u2011out\u201d splits, which mimic novel reaction types or conditions, these strategies provide unbiased error estimates and have been shown to improve model generalization ([semanticscholar](https://api.semanticscholar.org/CorpusID:231962595)).  \n\nWhile the cited studies report **substantial reductions in cross\u2011validation error**\u2014often enough to change the ranking of competing models\u2014none quantify the improvement as a precise 50\u202f% gain.  The combination of fine\u2011grained cross\u2011modal pre\u2011training, contrastive learning on reaction conditions, extensive data\u2011augmentation (including test\u2011time augmentation), pre\u2011trained GNN fine\u2011tuning, and active/transfer learning with tailored CV splits constitutes the current toolkit that can approach or exceed such large error reductions in practice.",
      "url": ""
    },
    {
      "title": "YieldFCP: Enhancing Reaction Yield Prediction via Fine- ...",
      "text": "<div><div><header></header><div><div><ul><li><a><span><span><span>View\u00a0<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article><div><p><a href=\"https://www.sciencedirect.com/journal/artificial-intelligence-chemistry\"><span><span></span></span></a></p><p><a href=\"https://www.sciencedirect.com/journal/artificial-intelligence-chemistry/vol/3/issue/1\"><span><span></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><span><span>license</span></span></a></p><p><span></span>Open access</p></div><div><h2>Abstract</h2><div><p>Predicting chemical reaction yields is a critical yet challenging task in organic chemistry. While integrating multi-modal information has shown promise, existing methods typically encode the entire reaction in different modalities and then align these embeddings for the same reactions. Such a coarse-grained modal fusion strategy may neglect atomic-level interactions crucial for accurate predictions. Recognizing the crucial role of modal fusion in multi-modal learning and the limitations of current methods in real-world scenarios, we propose YieldFCP, a reaction <span></span> prediction model based on <span></span>ine-grained <span></span>ross-modal <span></span>re-training. Its cross-modal projector links the molecular SMILES sequence with 3D geometric data, focusing on the atomic-level interactions to achieve fine-grained modal fusion and enhance yield prediction. YieldFCP is pre-trained on a large-scale dataset leveraging cross-modal self-supervised learning techniques. Experimental results on the high-throughput experiments, real-world electronic laboratory notebook, and real-world organic reaction publication datasets demonstrate the effectiveness of our approach. Particularly, YieldFCP outperforms the state-of-the-art methods in real-world scenarios and successfully recognizes key components that determine reaction yields with valuable interpretability.</p></div></div><ul><li></li><li></li></ul><div><h2>Keywords</h2><p><span>Chemical reaction yield prediction</span></p><p><span>Self-supervised learning</span></p><p><span>Pre-training</span></p><p><span>Deep learning</span></p></div><section><h2>Data availability</h2></section><section><header><h2>Cited by (0)</h2></header></section><p><span>\u00a9 2025 The Authors. Published by Elsevier B.V.</span></p></article></div></div></div></div>",
      "url": "https://www.sciencedirect.com/science/article/pii/S2949747725000028"
    },
    {
      "title": "Enhancing Generic Reaction Yield Prediction through ...",
      "text": "<div><div><section><h2>Abstract</h2><p>Deep learning (DL)-driven efficient synthesis planning may profoundly transform the paradigm for designing novel pharmaceuticals and materials. However, the progress of many DL-assisted synthesis planning (DASP) algorithms has suffered from the lack of reliable automated pathway evaluation tools. As a critical metric for evaluating chemical reactions, accurate prediction of reaction yields helps improve the practicality of DASP algorithms in the real-world scenarios. Currently, accurately predicting yields of interesting reactions still faces numerous challenges, mainly including the absence of high-quality generic reaction yield datasets and robust generic yield predictors. To compensate for the limitations of high-throughput yield datasets, we curated a generic reaction yield dataset containing 12 reaction categories and rich reaction condition information. Subsequently, by utilizing 2 pretraining tasks based on chemical reaction masked language modeling and contrastive learning, we proposed a powerful bidirectional encoder representations from transformers (BERT)-based reaction yield predictor named Egret. It achieved comparable or even superior performance to the best previous models on 4 benchmark datasets and established state-of-the-art performance on the newly curated dataset. We found that reaction-condition-based contrastive learning enhances the model\u2019s sensitivity to reaction conditions, and Egret is capable of capturing subtle differences between reactions involving identical reactants and products but different reaction conditions. Furthermore, we proposed a new scoring function that incorporated Egret into the evaluation of multistep synthesis routes. Test results showed that yield-incorporated scoring facilitated the prioritization of literature-supported high-yield reaction pathways for target molecules. In addition, through meta-learning strategy, we further improved the reliability of the model\u2019s prediction for reaction types with limited data and lower data quality. Our results suggest that Egret holds the potential to become an essential component of the next-generation DASP tools.</p></section><div><section><h2>Introduction</h2><p>Efficient chemical synthesis is crucial to satisfying the future demands for pharmaceuticals, materials, and energy [<a href=\"#B1\">1</a>]. Corey and Wipke [<a href=\"#B2\">2</a>] first proposed the concept of computer-aided synthesis planning (CASP) in the 1960s. CASP programs take the target molecule as input and return a series of single-step reactions that decompose the target molecule into a set of commercially available starting compounds or simple precursors that can be easily synthesized [<a href=\"#B3\">3</a>]. A feasible synthesis plan may dramatically accelerate the synthesis of desired molecules [<a href=\"#B4\">4</a>]. In recent years, with the development of data science, deep learning (DL) algorithms, and computing power, DL-assisted synthesis planning (DASP) has gained considerable interest [<a href=\"#B5\">5</a>\u2013<a href=\"#B17\">17</a>]. Modern DASP programs can quickly plan multiple potential retrosynthetic pathways for a given target molecule according to the constraints set by the user for the retrosynthetic search (such as the overall search time and number of single-step expansion steps) [<a href=\"#B18\">18</a>]. However, these theoretically feasible reaction pathways often become impractical because of such factors as incomplete conversion of reactants, side reactions, or inadequate purification [<a href=\"#B19\">19</a>]. Therefore, retrosynthetic route planning is only a major component of a successful DASP system [<a href=\"#B20\">20</a>]. To provide feasible suggestions that can be implemented by chemists in the laboratory, it is necessary to identify the optimal reaction conditions for the retrosynthetic route [<a href=\"#B21\">21</a>] and evaluate the quality of the overall synthesis route, and reaction yield is one of the most scientific and intuitive metrics for screening reaction conditions and evaluating synthesis pathway [<a href=\"#B22\">22</a>,<a href=\"#B23\">23</a>].</p><p>Reaction yield refers to the percentage of reactants that are successfully converted to the desired product [<a href=\"#B24\">24</a>]. Models that can reliably predict actual yields not only serve as scoring functions of DASP but also help chemists evaluate the overall yield of complex reaction pathways, giving priority to high-yield reactions to save time and cost in wet experiments [<a href=\"#B25\">25</a>]. However, because of the complexity of molecular structures, the multidimensionality of chemical reactions, and the limited availability of data, it is still a great challenge to predict the yields of chemical reactions under specific conditions [<a href=\"#B26\">26</a>]. The current yield prediction models are mainly built on high-throughput experimental (HTE) datasets, and Buchwald\u2013Hartwig reactions [<a href=\"#B26\">26</a>\u2013<a href=\"#B28\">28</a>] and Suzuki\u2013Miyaura reactions [<a href=\"#B29\">29</a>,<a href=\"#B30\">30</a>] are the 2 most well-studied HTE yield datasets (Fig. <a href=\"#F1\">1</a>A and B). Early studies utilized computed physicochemical descriptors [<a href=\"#B26\">26</a>], one-hot encoding of reactions [<a href=\"#B27\">27</a>], or structure-based molecular fingerprints [<a href=\"#B28\">28</a>] to predict the yields for these 2 datasets. Recently, the DL fingerprint rxnfp developed by Schwaller et\u00a0al. [<a href=\"#B31\">31</a>] and the differential reaction fingerprint drfp developed by Probst et\u00a0al. [<a href=\"#B32\">32</a>] have substantially outperformed previous methods. Rxnfp and drfp respectively achieved the best performance on test sets of Buchwald\u2013Hartwig dataset and Suzuki\u2013Miyaura dataset (70:30 random split), with coefficient of determination (<i>R</i><sup>2</sup>) scores of 0.95 and 0.85. However, by studying previous experimental results, we found that most of the aforementioned methods did not achieve the ideal predictive performance on the out-of-sample test sets of Buchwald\u2013Hartwig reactions containing additional additives. This indicates the limitations of using HTE datasets for yield prediction. HTE datasets usually involve specific classes of reactions and focus on a narrow chemical space. When using yield prediction models to explore unknown chemical spaces, this performance degradation problem will be prevalent because the unknown chemical space to be predicted can be very large [<a href=\"#B33\">33</a>]. Yield prediction models trained on HTE datasets cannot be applied in the real-world scenarios aimed at predicting the yields for a broad variety of reactions. Therefore, curating generic reaction yield datasets that are not limited to specific reaction classes is the first step to promote the practical application of yield prediction models.</p><div><figure><figcaption><span>Fig.\u00a01</span>. Overall reaction and variables for the Buchwald\u2013Hartwig (B-H) (A), Suzuki\u2013Miyaura (B), and Reaxys-MultiCondi-Yield (C) datasets.</figcaption></figure></div><p>Another key point to apply yield prediction models to chemical synthesis practice is to adopt effective modeling methods for generic reaction yield datasets. Reaction Simplified Molecular Input Line Entry System (SMILES) is a simplified chemical language for representing chemical reactions [<a href=\"#B34\">34</a>]. Therefore, SMILES-based yield prediction can be viewed as a natural language processing (NLP) problem, extracting molecular features directly from reaction SMILES without relying on any manually generated feature. In 2017, Vaswani et\u00a0al. [<a href=\"#B35\">35</a>] proposed the transformer architecture for handling various NLP tasks, which achieved excellent feature extraction capability through the self-attention mechanism. In recent years, many pretraining language models such as bidirectional encoder representations from transformers (BERT) [<a href=\"#B36\">36</a>] and generative pretrained transformer (GPT)...",
      "url": "https://spj.science.org/doi/10.34133/research.0292"
    },
    {
      "title": "Predicting reaction conditions from limited data through active ...",
      "text": "<div><div>\n \n <main>\n \n <article><section></section><section><section><h2>Abstract</h2>\n<p>Transfer and active learning have the potential to accelerate the development of new chemical reactions, using prior data and new experiments to inform models that adapt to the target area of interest. This article shows how specifically tuned machine learning models, based on random forest classifiers, can expand the applicability of Pd-catalyzed cross-coupling reactions to types of nucleophiles unknown to the model. First, model transfer is shown to be effective when reaction mechanisms and substrates are closely related, even when models are trained on relatively small numbers of data points. Then, a model simplification scheme is tested and found to provide comparative predictivity on reactions of new nucleophiles that include unseen reagent combinations. Lastly, for a challenging target where model transfer only provides a modest benefit over random selection, an active transfer learning strategy is introduced to improve model predictions. Simple models, composed of a small number of decision trees with limited depths, are crucial for securing generalizability, interpretability, and performance of active transfer learning.</p></section><section><hr/>\n<p>Transfer learning is combined with active learning to discover synthetic reaction conditions in a small-data regime. This strategy is tested on cross-coupling reactions from a high-throughput experimentation dataset and shows promising results.<a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-ga.jpg\"></a></p></section><section><h2>Introduction</h2>\n<p>Computers are becoming increasingly capable of performing high-level chemical tasks.<sup><a href=\"#cit1\">1\u20134</a></sup> Machine learning approaches have demonstrated viable retrosynthetic analyses,<sup><a href=\"#cit5\">5\u20137</a></sup> product prediction,<sup><a href=\"#cit8\">8\u201311</a></sup> reaction condition suggestion,<sup><a href=\"#cit12\">12\u201316</a></sup> prediction of stereoselectivity,<sup><a href=\"#cit17\">17\u201320</a></sup> regioselectivity,<sup><a href=\"#cit19\">19,21\u201324</a></sup> and reaction yield<sup><a href=\"#cit25\">25,26</a></sup> and optimization of reaction conditions.<sup><a href=\"#cit27\">27\u201330</a></sup> These advances allow computers to assist synthesis planning for functional molecules using well-established chemistry. For machine learning to aid the development of new reactions, a model based on established chemical knowledge must be able to generalize its predictions to reactivity that lies outside of the dataset. However, because most supervised learning algorithms learn how features (<em>e.g.</em> reaction conditions) within a particular domain relate to an outcome (<em>e.g.</em> yield), the model is not expected to be accurate outside its domain. This situation requires chemists to consider other machine learning methods for navigating new reactivity.</p>\n<p>Expert knowledge based on known reactions plays a central role in the design of new reactions. The assumption that substrates with chemically similar reaction centers have transferable performance provides a plausible starting point for experimental exploration. This concept of chemical similarity, together with literature data, guides expert chemists in the development of new reactions. Transfer learning, which assumes that data from a nearby domain, called the source domain, can be leveraged to model the problem of interest in a new domain, called the target domain,<sup><a href=\"#cit31\">31</a></sup> emulates a tactic commonly employed by human chemists.</p>\n<p>Transfer learning is a promising strategy when limited data is available in the domain of interest, but a sizeable dataset is available in a related domain.<sup><a href=\"#cit31\">31,32</a></sup> Models are first created using the source data, then transferred to the target domain using various algorithms.<sup><a href=\"#cit19\">19,33\u201335</a></sup> For new chemical targets where no labeled data is available, the head start in predictivity a source model can provide becomes important. However, when a shift in distribution of descriptor values occurs (<em>e.g.</em>, descriptors outside of the original model ranges) in the target data, making predictions becomes challenging. For such a situation, the objective of transfer learning becomes training a model that is as predictive in the target domain as possible.<sup><a href=\"#cit31\">31,36</a></sup> Toward this end, cross-validation is known to improve generalizability by providing a procedure to avoid overfitting on the training data.<sup><a href=\"#cit37\">37</a></sup> The reduction of generalization error, however, may not be sufficient outside the source domain. Accordingly, new methods that enhance the applicability of a transferred model to new targets would be beneficial for reaction condition prediction.</p>\n<p>Another machine learning method that can help tackle data scarcity is active learning. By making iterative queries of labeling a small number of datapoints, active learning updates models with knowledge from newly labeled data. As a result, exploration is guided into the most informative areas and avoids collection of unnecessary data.<sup><a href=\"#cit38\">38,39</a></sup> Active learning is therefore well-suited for reaction development, which greatly benefits from efficient exploration and where chemists conduct the next batch of reactions based on previous experimental results. Based on this analogy, reaction optimization<sup><a href=\"#cit27\">27,28</a></sup> and reaction condition identification<sup><a href=\"#cit40\">40</a></sup> have been demonstrated to benefit from active learning. However, these prior works initiate exploration with randomly selected data points (<a href=\"#fig1\">Fig. 1A</a>) which does not leverage prior knowledge, and therefore does not reflect how expert chemists initiate exploration. Initial search directed by transfer learning could identify productive regions early on, which in turn will help build more useful models for subsequent active learning steps.</p>\n<figure><h3>Fig. 1. Workflow of (A) previous active learning studies and (B) this work. Distinctions that arise from the different problem setting and incorporation of transfer learning are highlighted bold in (B).</h3>\n<p><a href=\"https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&amp;p=PMC3&amp;id=9172577_d1sc06932b-f1.jpg\"></a></p>\n</figure><p>To align transfer and active learning closer to how expert chemists develop new reactions, appropriate chemical reaction data is necessary.<sup><a href=\"#cit41\">41</a></sup> Available datasets<sup><a href=\"#cit42\">42</a></sup> that are often used for machine learning are overrepresented by positive reactions, failing to reflect reactions with negative outcomes. On the other hand, reaction condition screening data of methodology reports\u2014which chemists often refer to\u2014only constitute a sparse subset of possible reagent combinations, making it hard for machine learning algorithms to extract meaningful knowledge.<sup><a href=\"#cit43\">43</a></sup></p>\n<p>High-throughput experimentation<sup><a href=\"#cit44\">44\u201346</a></sup> (HTE) data can fill this gap. HTE provides reaction data<sup><a href=\"#cit16\">16,25,27,47,48</a></sup> with reduced variations in outcome due to systematic experimentation. Pd-catalyzed coupling data was therefore collected from reported work using nanomole scale HTE in 1536 well plates.<sup><a href=\"#cit49\">49\u201351</a></sup> In the current work, subsets of this data, classified by nucleophile type as shown in <a href=\"#fig2\">Fig. 2A</a>, were selected to a dataset size of approximately 100 datapoints, which captured both positive and negative reaction performance.</p>\n<figure><h3>Fig. 2. (A) Structure of reactions in the dataset. A total of 1220 reactions across 10 t...",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9172577"
    },
    {
      "title": "Improving chemical reaction yield prediction using pre-trained graph neural networks",
      "text": "Search all BMC articles\n\nSearch\n\nImproving chemical reaction yield prediction using pre-trained graph neural networks\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n[Download PDF](https://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-024-00818-z.pdf)\n\n[Download ePub](https://jcheminf.biomedcentral.com/counter/epub/10.1186/s13321-024-00818-z.epub)\n\n- Research\n- [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n- Published: 01 March 2024\n\n# Improving chemical reaction yield prediction using pre-trained graph neural networks\n\n- [Jongmin Han](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Jongmin-Han-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1),\n- [Youngchun Kwon](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youngchun-Kwon-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2),\n- [Youn-Suk Choi](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Youn_Suk-Choi-Aff2) [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff2) &\n- \u2026\n- [Seokho Kang](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#auth-Seokho-Kang-Aff1) [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#Aff1)\n\nShow authors\n\n[_Journal of Cheminformatics_](https://jcheminf.biomedcentral.com/) **volume\u00a016**, Article\u00a0number:\u00a025 (2024)\n[Cite this article](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#citeas)\n\n- 5588 Accesses\n\n- 9 Citations\n\n- 2 Altmetric\n\n- [Metrics details](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z/metrics)\n\n\n## Abstract\n\nGraph neural networks (GNNs) have proven to be effective in the prediction of chemical reaction yields. However, their performance tends to deteriorate when they are trained using an insufficient training dataset in terms of quantity or diversity. A promising solution to alleviate this issue is to pre-train a GNN on a large-scale molecular database. In this study, we investigate the effectiveness of GNN pre-training in chemical reaction yield prediction. We present a novel GNN pre-training method for performance improvement.Given a molecular database consisting of a large number of molecules, we calculate molecular descriptors for each molecule and reduce the dimensionality of these descriptors by applying principal component analysis. We define a pre-text task by assigning a vector of principal component scores as the pseudo-label to each molecule in the database. A GNN is then pre-trained to perform the pre-text task of predicting the pseudo-label for the input molecule. For chemical reaction yield prediction, a prediction model is initialized using the pre-trained GNN and then fine-tuned with the training dataset containing chemical reactions and their yields. We demonstrate the effectiveness of the proposed method through experimental evaluation on benchmark datasets.\n\n## Introduction\n\nA chemical reaction is a process in which reactants are changed into products through chemical transformations. The percentage of products obtained relative to the reactants consumed is referred to as the chemical reaction yield. The prediction of the chemical reaction yields provides clues for exploring high-yield chemical reactions without the need for conducting direct experiments. This is crucial for accelerating synthesis planning in organic chemistry by significantly reducing time and cost. Machine learning has been actively utilized for the fast and accurate prediction of chemical reaction yields in a data-driven manner \\[ [1](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR1), [2](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR2), [3](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR3), [4](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR4), [5](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR5), [6](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR6), [7](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR7), [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\].\n\nRecently, deep learning has shown remarkable performance in predicting chemical reaction yields by effectively modeling the intricate relationships between chemical reactions and their yields using neural networks. Schwaller et al. \\[ [6](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR6), [7](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR7)\\] represented a chemical reaction as a series of simplified molecular-input line-entry system (SMILES) strings and built a bidirectional encoder representations from transformers (BERT) as the prediction model. Kwon et al. \\[ [8](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR8)\\] represented a chemical reaction as a set of molecular graphs and built a graph neural network (GNN) that operates directly on the molecular graphs as the prediction model. The use of GNNs led to a significant improvement in the predictive performance owing to their high expressive power on molecular graphs \\[ [9](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR9), [10](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR10)\\].\n\nDespite its effectiveness, the predictive performance of a GNN can suffer when it is trained on an insufficient training dataset in terms of quantity or diversity. For example, a GNN may not generalize well to query reactions involving substances that are not considered in the training dataset. Although the performance can be significantly improved by securing a large-scale training dataset, this is difficult in practice because of the high cost associated with conducting direct experiments to acquire the yields for a large number of chemical reactions.\n\nTo alleviate this issue, a promising solution is to pre-train a GNN on a large-scale molecular database and use it to adapt to chemical reaction yield prediction. Various pre-training methods have been studied in the literature, which can be categorized into contrastive learning and pre-text task approaches \\[ [11](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR11), [12](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR12)\\]. The contrastive learning approach pre-trains a GNN by learning molecular representations such that different views of the same molecule are mapped close together, and views of different molecules are mapped far apart \\[ [13](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR13), [14](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR14), [15](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR15), [16](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR16), [17](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR17), [18](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR18)\\]. Most existing methods based on this approach have utilized data augmentation techniques to generate different views of each molecule. Data augmentation may potentially alter the properties of the molecules being represented \\[ [19](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR19), [20](https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z#ref-CR20)\\]. The pre-text task approach acquires the pseudo-labels of molecules and pre-trains a GNN to predict them \\[ [21](https://jcheminf.biomedcentral.com/jcheminf.biomedcentral.com#ref-CR21), [22](https://jcheminf.biomedcentral.com/jcheminf.biome...",
      "url": "https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00818-z"
    },
    {
      "title": "Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020",
      "text": "Authors Data augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020 https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty\nData augmentation strategies to improve reaction yield predictions and estimate uncertainty for NeurIPS 2020\nAuthors\n2020-12-06T00:00:00Z\n## Abstract\nChemical reactions describe how precursor molecules react together and transform into products. The reaction yield describes the percentage of the precursors successfully transformed into products relative to the theoretical maximum. The prediction of reaction yields can help chemists navigate reaction space and accelerate the design of more effective routes. Here, we investigate the best-studied high-throughput experiment data set and show how data augmentation on chemical reactions can improve yield predictions' accuracy, even when only small data sets are available. Previous work used molecular fingerprints, physics-based or categorical descriptors of the precursors. In this manuscript, we fine-tune natural language processing-inspired reaction transformer models on different augmented data sets to predict yields solely using a text-based representation of chemical reactions. When the random training sets contain 2.5% or more of the data, our models outperform previous models, including those using physics-based descriptors as inputs. Moreover, we demonstrate the use of test-time augmentation to generate uncertainty estimates, which correlate with the prediction errors.\n## Related\nWorkshop paper\n### [Monitoring the Impact of Wildfires on Tree Species with Deep Learning](https://research.ibm.com/publications/monitoring-the-impact-of-wildfires-on-tree-species-with-deep-learning)\nWang Zhou, Levente Klein\nNeurIPS 2020\nWorkshop paper\n### [Structure Discovery in (Causal) Proximal Graphical Event Models](https://research.ibm.com/publications/structure-discovery-in-causal-proximal-graphical-event-models)\nDebarun Bhattacharjya, Karthikeyan Shanmugam, et al.\nNeurIPS 2020\nWorkshop paper\n### [Differentially Private Stochastic Coordinate Descent](https://research.ibm.com/publications/differentially-private-stochastic-coordinate-descent--1)\nGeorgios Damaskinos, Celestine Mendler-D\u00fcnner, et al.\nNeurIPS 2020\nWorkshop paper\n### [Long-Range Seasonal Forecasting of 2m Temperature with Machine Learning](https://research.ibm.com/publications/long-range-seasonal-forecasting-of-2m-temperature-with-machine-learning)\nEtienne Eben Vos, Ashley Daniel Gritzman, et al.\nNeurIPS 2020\n[View all publications](https://research.ibm.com/publications)",
      "url": "https://research.ibm.com/publications/data-augmentation-strategies-to-improve-reaction-yield-predictions-and-estimate-uncertainty"
    },
    {
      "title": "Virtual data augmentation method for reaction prediction",
      "text": "<div><div>\n \n <div><h2>Introduction</h2><div><p>Today, organic synthesis occupies a core position in the organic chemistry field and supports the research and development of other fields, such as material science, environmental science, and drug discovery. With increasing advancements in artificial intelligence, several successful applications have been devised in the fields of integrated organic chemistry and artificial intelligence, such as reaction prediction<sup><a href=\"#ref-CR1\">1</a>,<a href=\"#ref-CR2\">2</a>,<a href=\"#ref-CR3\">3</a>,<a href=\"#ref-CR4\">4</a>,<a href=\"#ref-CR5\">5</a>,<a href=\"#ref-CR6\">6</a>,<a href=\"#ref-CR7\">7</a>,<a href=\"#ref-CR8\">8</a>,<a href=\"#ref-CR9\">9</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR10\">10</a></sup>. One of the most compelling approaches to predicting reactions is Nam and Kim's proposal to view reaction prediction as a translation task; implemented based on a neural machine translation (NMT) model<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR5\">5</a></sup>. Ahneman et al. successfully used machine learning to predict the synthetic reaction performance of Buchwald\u2013Hartwig cross-coupling<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR7\">7</a></sup>. Schwaller et al. creatively used sequence-to-sequence models to aid predictions in organic chemistry<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR10\">10</a></sup>.</p><p>Deep learning, a popular branch of artificial intelligence has considerably progressed in areas such as speech recognition, visual object recognition, and other fields such as organic reaction prediction<sup><a href=\"#ref-CR11\">11</a>,<a href=\"#ref-CR12\">12</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR13\">13</a></sup>. However, deep learning methods are generally determined using large datasets. In addition, previous research has demonstrated that focusing on massive reaction datasets requires considerable effort<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR14\">14</a></sup>. Moreover, in all these studies, the amount of data considered for a particular reaction type was insufficient to support related applications because of high costs and time-consuming experiments. Therefore, deep learning methods must be able to comprehensively deal with small datasets to solve project-tailored tasks in the cross-domain with chemistry.</p><p>As such, many strategies have been designed for improving the poor performance in the small datasets of deep learning methods<sup><a href=\"#ref-CR15\">15</a>,<a href=\"#ref-CR16\">16</a>,<a href=\"#ref-CR17\">17</a>,<a href=\"#ref-CR18\">18</a>,<a href=\"#ref-CR19\">19</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR20\">20</a></sup>. An effective method is transfer learning, which transfers prior knowledge, learned from abundant data, to another domain task; this can subsequently be used in situations with less data but similar task scenarios<sup><a href=\"#ref-CR21\">21</a>,<a href=\"#ref-CR22\">22</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR23\">23</a></sup>. Reymond et al. performed transfer learning on carbohydrate reactions and showed better performance than a model trained only on carbohydrate reactions<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR23\">23</a></sup>. Apart from the transfer learning method, data augmentation strategies are crucial for deep learning pipelines aiming at reaction prediction tasks, as the performance of the model increases with the amount of training data. Data augmentation is the process of modifying or \u201caugmenting\u201d a dataset with additional data; this is a powerful strategy used in image processing<sup><a href=\"#ref-CR24\">24</a>,<a href=\"#ref-CR25\">25</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR26\">26</a></sup>. Tetko et al. proved that augmenting input and target data simultaneously can improve the performance of predicting new sequences<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR27\">27</a></sup>. In general, augmenting training-set sequences allows deep learning methods to achieve better accuracy according to the characteristic of the simplified molecular-input line-entry system (SMILES)<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR28\">28</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR29\">29</a></sup>. Notably, all the augmented SMILESs are valid structures without changing their chemical meaning. Inspired by the work of Maimaiti et al., we propose an intelligent strategy for data augmentation; they manually created a batch of fake data to increase the target training set by deleting words, randomly sampling words, or replacing some words during text generation<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR30\">30</a></sup>. In this manner, synthetic data augmentation was realized by transforming the text into low-resource language scenarios. Based on the similarity of the SMILES representation to the text, we added fake data instead of \u201crandom SMILES\u201d to the training dataset to improve model accuracy, terming it as virtual data augmentation. The fake data was generated by replacing substituents with equivalent functional groups in the reactants, which do not change the reaction sites and atom valences of the reactant molecules.</p><p>In this study, we applied and scrutinized virtual data augmentation. In addition, we show that the fake data can lead to better performance on the transformer model, which is state-of-the-art in natural language processing<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR27\">27</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR31\">31</a>,<a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR32\">32</a></sup>. Although the transformer model shows excellent performance in various reaction tasks, the data-driven model remains inefficient in the case of insufficient data resources. Our study was aimed at predicting the outcomes of reactions, as detailed in Fig.\u00a0<a href=\"https://www.nature.com/articles/s41598-022-21524-6#Fig1\">1</a>. The datasets used in this study are coupling reactions; an organic chemical reaction in which two chemical entities (or units) combine to form one molecule. When the virtual data augmentation method is trained on the transformer-baseline model, the accuracy of reaction prediction compared with the raw data is improved from 2.74 to 25.8%. Furthermore, combined with the transfer learning method, the performance of the transformer model increased from 1 to 53%, proving that this virtual data augmentation can improve the model performance. Overall, this virtual data augmentation aims to expand the density of sample data points in the chemical space already covered by the existing literature datasets. Moreover, we believe that this method can be a useful tool for solving tasks with small datasets using deep learning methods in low-resource scenarios.</p><div><figure><figcaption><b>Figure 1</b></figcaption><div><div><a href=\"https://www.nature.com/articles/s41598-022-21524-6/figures/1\"></a></div><p>Schematic illustration of the virtual data augmentation method.</p></div><p><a href=\"https://www.nature.com/articles/s41598-022-21524-6/figures/1\"><span>Full size image</span></a></p></figure></div></div></div><div><h2>Methods</h2><div><h3>Dataset preparation</h3><p>In this study, we exported five coupling reaction datasets, i.e., those of Buchwald\u2013Hartwig, Chan\u2013Lam, Kumada, Hiyama, and Suzuki\u2019s, based on the name and structure search from the \u2018Reaxys\u2019 database<sup><a href=\"https://www.nature.com/articles/s41598-022-21524-6#ref-CR33\">33</a></sup>. Each dataset was preprocessed as follows. First, irrelevant information (e.g., pressure, temperature, yield, etc.) was omitted from these datasets, retaining only reactio...",
      "url": "https://www.nature.com/articles/s41598-022-21524-6"
    },
    {
      "title": "Predicting reaction performance in C\u2013N cross-coupling using ...",
      "text": "REPORT \u25e5\nORGANIC CHEMISTRY\nPredicting reaction performance\nin C\u2013N cross-coupling using\nmachine learning\nDerek T. Ahneman,1 Jes\u00fas G. Estrada,1 Shishi Lin,2\nSpencer D. Dreher,2* Abigail G. Doyle1*\nMachine learning methods are becoming integral to scientific inquiry in numerous\ndisciplines.We demonstrated that machine learning can be used to predict the performance\nof a synthetic reaction in multidimensional chemical space using data obtained via high\u0002throughput experimentation. We created scripts to compute and extract atomic, molecular,\nand vibrational descriptors for the components of a palladium-catalyzed Buchwald-Hartwig\ncross-coupling of aryl halides with 4-methylaniline in the presence of various potentially\ninhibitory additives. Using these descriptors as inputs and reaction yield as output, we\nshowed that a random forest algorithm provides significantly improved predictive\nperformance over linear regression analysis.The random forest model was also successfully\napplied to sparse training sets and out-of-sample prediction, suggesting its value in\nfacilitating adoption of synthetic methodology.\nM\nachine learning (ML) is the study and\nconstruction of computer algorithms that\ncan learn from data (1). The ability of\nthese algorithms to detect meaningful\npatterns has led to their adoption across\na wide range of applications in science and tech\u0002nology, from autonomous vehicle control to\nrecommender systems (2). ML has also been\nsuccessfully applied in the biomedical sciences\nto enhance the virtual screening of libraries of\ndruglike molecules for biological function (3\u20135).\nHowever, its application to the chemical sciences,\nand synthetic organic chemistry in particular,\nhas been limited (6, 7). Prior efforts have focused\nprimarily on using ML to assist with synthetic\nplanning via retrosynthetic pathways or to predict\nthe products of chemical reactions given a set of\nreactants and conditions (8\u201311). Applications of\nML to predict the performance of a given reaction,\nhowever, are rare. Studies in the area of heteroge\u0002neous catalysis have used ML to predict reaction\nperformance when only a single component is\nvaried (12,13). Two recent studies have advanced\nthe field by evaluating predictions in multi\u0002dimensional chemical space, although these studies\nperformed a binary classification of reaction\nsuccess (14, 15). The use of regression-based ML\nto predict reaction yields in multidimensional\nchemical space could provide chemists with a\npowerful tool to navigate the adoption of syn\u0002thetic methodology.\nThe many challenges in applying ML to re\u0002action performance have previously hindered\nits use in the field of chemical synthesis. Imple\u0002mentation of these algorithms has historically\nbeen complicated for nonspecialists. Further.\nthe amount of data required to obtain statisti\u0002cally meaningful results grows exponentially\nwith the number of dimensions under study, a\nproblem known as the \u201ccurse of dimensionality\u201d\n(1). Given the multidimensionality of chemical\nstructure and reactivity, it has been difficult to gen\u0002erate enough data or to get access to sufficiently\ncomplete and consistent data from databases to\nwarrant implementation of these algorithms (14).\nFortunately, over the past decade, high-throughput\nexperimentation (HTE) has emerged as a powerful\ntool in industry and academia for reaction op\u0002timization and discovery (16, 17). We sought to\nevaluate whether ML could be applied to the scale\nof data available to modern HTE and enable yield\nprediction in multidimensional chemical space.\nLinear regression is the traditional tool for re\u0002action prediction and analysis in both industry\nand academia (18). In this approach, the user\nassumes a linear relationship between reaction\ninput (e.g., catalyst descriptors) and output (e.g.,\nproduct selectivity) and hand-selects input varia\u0002bles on the basis of specific mechanistic hypotheses\n(19, 20). A strength of linear regression is its inter\u0002pretability: A good fit between reagent descriptors\nand output supports mechanistic inferences, such\nas in the seminal Hammett linear free-energy\nrelationship (21).\nThe models obtained from linear regression\nanalysis have also been used for prediction. Re\u0002cently, Sigman and co-workers have applied multi\u0002variate linear and polynomial regression analyses\nto optimize reaction selectivity by predicting\ncatalyst, ligand, and substrate effects (22\u201324). Pre\u0002dicting yield tends to be more difficult; whereas\nproduct selectivity is determined by a small num\u0002ber of elementary steps, many on- and off-cycle\nevents can substantially alter reaction yield. ML\napproaches accept numerous input descriptors\nwithout recourse to a mechanistic hypothesis\nand evaluate functions with greater flexibility to\nmatch patterns in data. We postulated that ML\nmight outperform regression analysis for yield\nprediction and circumvent the challenge of select\u0002ing mechanistically relevant descriptors for large\nand multidimensional data sets. Here, we report\nthat a random forest ML model trained on multi\u0002dimensional chemical data can be used to predict\nthe performance of a Buchwald-Hartwig amina\u0002tion reaction conducted in the presence of poten\u0002tially inhibitory additives and to infer underlying\nreactivity. We have taken steps to automate re\u0002action parameterization and modeling with the\naim of making this tool accessible to the synthetic\nchemistry community.\nWe selected the Pd-catalyzed Buchwald-Hartwig\nreaction as our test reaction for model develop\u0002ment because of its broad value in pharmaceu\u0002tical synthesis (Fig. 1A) (25). Nevertheless, the\napplication of this reaction to complex drug-like\nmolecules remains challenging (26). One limita\u0002tion is the poor performance of substrates pos\u0002sessing five-membered heterocycles that contain\nheteroatom-heteroatom bonds, such as isoxa\u0002zoles. These heterocycles have drug-like charac\u0002teristics but are underrepresented in successful\ndrug candidates (27). Thus, we sought to use ML\nto predict the performance of the Buchwald\u0002Hartwig reaction in the presence of isoxazoles.\nRather than evaluate the coupling of a collection\nof substrates directly bearing the heterocycle\nfunctionality, we pursued a Glorius fragment\nadditive screening approach (28) wherein we\nevaluated the effects of isoxazole fragment ad\u0002ditives on the amination of different aryl and\nheteroaryl halides. This method cannot always\naccount for the full impact of a structural motif\nembedded within a substrate. However, the\nGlorius approach allowed us to test 345 diverse\nstructural interactions between isoxazoles and\naryl and heteroaryl halides. This large array\nwould not be possible with whole molecules\nbecause of the necessity of synthesizing and\nisolating all possible products for quantifica\u0002tion in this study. We conducted the coupling\nreactions using the ultra\u2013high-throughput setup\nrecently developed in the Merck Research Lab\u0002oratories for nanomole-scale experimentation\nin 1536-well plates (16). Use of the Mosquito\nrobot enabled simultaneous evaluation of more\nreaction dimensions than could previously be\nexamined by classical statistical analysis. Three\n1536-well plates consisting of a full matrix of\n15 aryl and heteroaryl halides, 4 Buchwald ligands,\n3 bases, and 23 isoxazole additives generated a\ntotal of 4608 reactions (including controls).\nThe yields of these reactions were used as the\nmodel output. Approximately 30% of the re\u0002actions failed to deliver any product, with the\nRESEARCH\nAhneman et al., Science 360, 186\u2013190 (2018) 13 April 2018 1 of 5\n1\nDepartment of Chemistry, Princeton University, Princeton, NJ\n08544, USA. 2\nChemistry Capabilities and Screening, Merck\nSharp & Dohme Corporation, Kenilworth, NJ 07033, USA.\n*Corresponding author. Email: spencer_dreher@merck.com\n(S.D.D.); agdoyle@princeton.edu (A.G.D.)\nremainder quite evenly spread over the range of\nyields (fig. S7).\nNext we turned to the selection of appropriate\ndescriptors. In linear regression analysis, this\nselection is typically done by hand according to\na mecha...",
      "url": "https://doyle.princeton.edu/wp-content/uploads/2020/07/43-Predicting-Reaction-Performance-in-C-N-Cross-Coupling-Using-Machine-Learning.pdf"
    },
    {
      "title": "Data augmentation strategies to improve reaction yield ...",
      "text": "Data augmentation strategies to improve reaction\nyield predictions and estimate uncertainty\nPhilippe Schwaller1,2\nphs@zurich.ibm.com\nAlain C. Vaucher1\nava@zurich.ibm.com\nTeodoro Laino1\nteo@zurich.ibm.com\nJean-Louis Reymond2\njean-louis.reymond@dcb.unibe.ch\n1\nIBM Research \u2013 Europe, S\u00e4umerstrasse 4, 8803 R\u00fcschlikon, Switzerland\n2Department of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012 Bern,\nSwitzerland\nAbstract\nChemical reactions describe how precursor molecules react together and trans\u0002form into products. The reaction yield describes the percentage of the precursors\nsuccessfully transformed into products relative to the theoretical maximum. The\nprediction of reaction yields can help chemists navigate reaction space and accel\u0002erate the design of more effective routes. Here, we investigate the best-studied\nhigh-throughput experiment data set and show how data augmentation on chemical\nreactions can improve yield predictions\u2019 accuracy, even when only small data\nsets are available. Previous work used molecular fingerprints, physics-based or\ncategorical descriptors of the precursors. In this manuscript, we fine-tune natural\nlanguage processing-inspired reaction transformer models on different augmented\ndata sets to predict yields solely using a text-based representation of chemical reac\u0002tions. When the random training sets contain 2.5% or more of the data, our models\noutperform previous models, including those using physics-based descriptors as\ninputs. Moreover, we demonstrate the use of test-time augmentation to generate\nuncertainty estimates, which correlate with the prediction errors.\n1 Introduction\nThe synthesis of new chemicals affects numerous aspects of our life, ranging from food and medicine\nto novel materials for technological applications. The current machine learning revolution in auto\u0002mated synthesis can significantly accelerate novel materials and molecules\u2019 development. In the last\nyears, natural language processing methods emerged as robust and effective approaches in the field\nof organic chemistry, showing promising results in reaction prediction (1; 2; 3; 4), retrosynthesis\nplanning (5; 6; 7; 8), data curation (9) and synthesis action generation (10; 11). In those studies the\nencoder-decoder transformer models introduced by Vaswani et al. (12) excel among all other neural\nnetwork architectures. More recently, the use of encoder-only transformers such as BERT (13; 14)\nled to advances in reaction classification and fingerprints (15), as well as in unsupervised reaction\natom-to-atom mapping (16) and reaction yield predictions (17).\nReaction yields describe the percentage of the reactant molecules converted into the desired product\nmolecule during a chemical reaction. The prediction of reaction yields can guide chemists in selecting\nthe next experiments to perform, and retrosynthetic planning tools in aiming for routes that maximize\nthe overall yield, thus minimizing waste. Extensive chemical reaction yield data sets exist for high\u0002throughput experiments (HTE). Examples are the Suzuki\u2013Miyaura coupling reactions by Perera et\nMachine Learning for Molecules Workshop at NeurIPS 2020. https://ml4molecules.github.io\nal. (18) and the palladium-catalyzed Buchwald\u2013Hartwig reactions by Ahneman et al. (19), to date the\nbest-studied HTE yield data set. In this work, we study reactions yield prediction using the latter data\nset (19), containing a total of 3955 Buchwald\u2013Hartwig reactions with measured yields. Figure 1 a)\nprovides an overview of the data set.\nIn a recent manuscript, Schwaller et al. (17) introduced a BERT (13) model with a regression head\nto predict reactions\u2019 yields given as input a reaction SMILES (20; 21), a text-based molecule and\nreaction representation. We show in Figure 1 a) and c) the task description, together with an example\nof a reaction SMILES. Here, we investigate how different data augmentation techniques (Figure 1 b),\nmolecule permutations and SMILES randomizations (22; 23; 24; 8)) improve the performance of the\nyield prediction models. Moreover, we demonstrate the use of test-time augmentation (Figure 1 d)) to\nprovide uncertainty estimates (25) on the reaction yields, that correlate with the predictions\u2019 errors.\n[F,Cl,Br,I]\nC:1\n*\n*\nN:4\nC:5\n*\nC:1\n*\nN:4\n+ C:5\na) Buchwald-Hartwig reaction template\n Reaction example\nCanonical reaction SMILES\nb) Data augmentations:\nMolecule permutations Molecule SMILES randomizations\nc) Task\nd) Test time augmentation\nDifferent variants of same reaction.\nMolecule permutations and/or SMILES randomizations.\nChemical reaction Yield\n15 aryl halides methylaniline\nYield-BERT\nClc1ccccn1.Cc1ccc(N)cc1.O=S(=O)(O[Pd]1c2ccccc2-c2ccccc2N~1)C(F)\n(F)F.COc1ccc(OC)c(P([C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)[C@]23C[C@H]4C[C@H](C[C@H](C4)C2)C3)c1-\nc1c(C(C)C)cc(C(C)C)cc1C(C)C.CCN=P(N=P(N(C)C)(N(C)C)N(C)C)(N(C)C)N(C)C.Cc1cc(C)on1>>Cc1ccc(Nc2ccccn2)cc1\nc1c(N)ccc(C)c1\nc1cc(C)ccc1N\nc1c(C)ccc(N)c1\nc1(C)ccc(N)cc1\nCc1ccc(N)cc1\nc1(N)ccc(C)cc1\nNc1ccc(C)cc1\nc1cc(N)ccc1C\n=\nCl\nN\nH2N H\nN\nN\n+\nH2N\n{aryl_halide}.{methylaniline}.{pd_catalyst}.{ligand}.{base}.{additive}>>{product}\n{ligand}.{base}.{methylaniline}.{additive}.{pd_catalyst}.{aryl_halide}>>{product}\n{base}.{methylaniline}.{pd_catalyst}.{aryl_halide}.{additive}.{ligand}>>{product}\n{additive}.{base}.{aryl_halide}.{ligand}.{methylaniline}.{pd_catalyst}>>{product}\n{aryl_halide}.{pd_catalyst}.{base}.{ligand}.{methylaniline}.{additive}>>{product}\n{aryl_halide}.{methylaniline}.{pd_catalyst}.{ligand}.{base}.{additive}>>{product}\n{ligand}.{base}.{methylaniline}.{additive}.{pd_catalyst}.{aryl_halide}>>{product}\n{base}.{methylaniline}.{pd_catalyst}.{aryl_halide}.{additive}.{ligand}>>{product}\n{additive}.{base}.{aryl_halide}.{ligand}.{methylaniline}.{pd_catalyst}>>{product}\n{aryl_halide}.{pd_catalyst}.{base}.{ligand}.{methylaniline}.{additive}>>{product} }\n4 Buchwald ligands\n1 Pd catalyst, \n3 bases, 23 additives\nligand_1, base_2\nadditive_5, catalyst\nCl\nN\nH2N H\nN\nN\n+\nligand_1, base_2\nadditive_5 Reaction encoder\n& regression head\nTrained\nYield-BERT\n70.4%\n70.4 \u00b1 0.2 %\nAverage Yield \u00b1 Std\nuncertainty estimate\nTotal:\n3955 reactions\nFigure 1: Training/evaluation pipeline and task description.\n2 Results & Discussion\nOur models were trained using Simpletransformers (26), huggingface transformers (27), PyTorch\n(28) and scripts adapted from the RXN yields github repository (17; 29). Canonicalizations and\naugmentations were done using RDKit (30). As described in the work of Schwaller et al. (17),\nfine-tuning a pretrained reaction BERT model (15) for a specific task provides the advantage of\nhaving most of the hyperparameters already optimized and fixed. Schwaller et al. (17) tuned only the\ndropout probability and the learning rate on the training data of the first random split, further split\ninto a smaller training and validation set. Here, we initialized the dropout and learning rate using\nthe values reported in (17) and we determined the optimal numbers of data augmentations using\nthe same training/validation set. We investigated the two data augmentation techniques: molecule\npermutations, where we randomly shuffle the order of the precursors, SMILES randomizations, where\n2\nwe generated multiple randomized SMILES for a given molecule (24), and the combination of the\ntwo. Examples of augmented reactions and molecules are shown in Figure 1 b).\n2.1 Yield prediction\nMost of the results in the literature were published on 70%/30% (training/testing) random splits. In\nTable 1, we compared the results of the canonical order, the permuted precursors, the randomized\nSMILES and the combination of both permutation plus randomization to previous studies (19; 31;\n32; 17). While the use of the canonical order SMILES representation plus BERT with a regression\nhead (17) already outperforms one-hot encodings (31), physics-based descriptors (19) and multi\u0002fingerprint features (32) plus a random forest regressor, here we significantly improve the R2\nscore\nusing randomization. The same numbe...",
      "url": "https://ml4molecules.github.io/papers2020/ML4Molecules_2020_paper_21.pdf"
    }
  ]
}