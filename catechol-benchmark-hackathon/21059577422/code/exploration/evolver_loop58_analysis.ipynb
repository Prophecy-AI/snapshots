{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0883a612",
   "metadata": {},
   "source": [
    "# Loop 58 Analysis: Post-Simpler Model Assessment\n",
    "\n",
    "**Situation:**\n",
    "- 58 experiments completed, 27 consecutive failures since exp_030\n",
    "- Best LB: 0.0877 (exp_030), Target: 0.0707\n",
    "- Gap: 1.24x (0.0877 / 0.0707) = 19.4% improvement needed\n",
    "- 5 submissions remaining\n",
    "- exp_057 (Simpler Model with Spange Only) FAILED - CV 0.023017 (177.4% worse)\n",
    "\n",
    "**Critical Evaluator Insight:**\n",
    "- The target IS reachable! Intercept (0.0525) < Target (0.0707)\n",
    "- Required CV to hit target: 0.00422 (49% improvement from current best 0.008298)\n",
    "- The 'mixall' kernel uses GroupKFold(5) instead of Leave-One-Out(24)\n",
    "\n",
    "**Questions:**\n",
    "1. What approaches haven't been tried?\n",
    "2. How can we reduce CV by 49%?\n",
    "3. What's the path to LB 0.0707?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b1cbb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:42.962837Z",
     "iopub.status.busy": "2026-01-16T05:35:42.962159Z",
     "iopub.status.idle": "2026-01-16T05:35:44.211576Z",
     "shell.execute_reply": "2026-01-16T05:35:44.211040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission History:\n",
      "    exp     cv     lb\n",
      "exp_000 0.0111 0.0982\n",
      "exp_001 0.0123 0.1065\n",
      "exp_003 0.0105 0.0972\n",
      "exp_005 0.0104 0.0969\n",
      "exp_006 0.0097 0.0946\n",
      "exp_007 0.0093 0.0932\n",
      "exp_009 0.0092 0.0936\n",
      "exp_012 0.0090 0.0913\n",
      "exp_024 0.0087 0.0893\n",
      "exp_026 0.0085 0.0887\n",
      "exp_030 0.0083 0.0877\n",
      "exp_035 0.0098 0.0970\n",
      "\n",
      "Target LB: 0.0707\n",
      "Best LB: 0.0877 (exp_030)\n",
      "Gap to target: 1.24x (24.0% improvement needed)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Submission history\n",
    "submissions = [\n",
    "    {'exp': 'exp_000', 'cv': 0.0111, 'lb': 0.0982},\n",
    "    {'exp': 'exp_001', 'cv': 0.0123, 'lb': 0.1065},\n",
    "    {'exp': 'exp_003', 'cv': 0.0105, 'lb': 0.0972},\n",
    "    {'exp': 'exp_005', 'cv': 0.0104, 'lb': 0.0969},\n",
    "    {'exp': 'exp_006', 'cv': 0.0097, 'lb': 0.0946},\n",
    "    {'exp': 'exp_007', 'cv': 0.0093, 'lb': 0.0932},\n",
    "    {'exp': 'exp_009', 'cv': 0.0092, 'lb': 0.0936},\n",
    "    {'exp': 'exp_012', 'cv': 0.0090, 'lb': 0.0913},\n",
    "    {'exp': 'exp_024', 'cv': 0.0087, 'lb': 0.0893},\n",
    "    {'exp': 'exp_026', 'cv': 0.0085, 'lb': 0.0887},\n",
    "    {'exp': 'exp_030', 'cv': 0.0083, 'lb': 0.0877},\n",
    "    {'exp': 'exp_035', 'cv': 0.0098, 'lb': 0.0970},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(submissions)\n",
    "print(\"Submission History:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nTarget LB: 0.0707\")\n",
    "print(f\"Best LB: {df['lb'].min():.4f} ({df.loc[df['lb'].idxmin(), 'exp']})\")\n",
    "print(f\"Gap to target: {df['lb'].min() / 0.0707:.2f}x ({(df['lb'].min() - 0.0707) / 0.0707 * 100:.1f}% improvement needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272e381d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.213971Z",
     "iopub.status.busy": "2026-01-16T05:35:44.213438Z",
     "iopub.status.idle": "2026-01-16T05:35:44.220512Z",
     "shell.execute_reply": "2026-01-16T05:35:44.219998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-LB Linear Relationship:\n",
      "  LB = 4.31 * CV + 0.0525\n",
      "  R² = 0.9505\n",
      "  Intercept = 0.0525\n",
      "  Target LB = 0.0707\n",
      "\n",
      "CRITICAL INSIGHT:\n",
      "  Intercept (0.0525) < Target (0.0707)\n",
      "  This means the target IS REACHABLE!\n",
      "\n",
      "Required CV to hit target:\n",
      "  CV = (0.0707 - 0.0525) / 4.31 = 0.004213\n",
      "  Current best CV: 0.008298\n",
      "  Required improvement: 49.2%\n"
     ]
    }
   ],
   "source": [
    "# CV-LB relationship analysis\n",
    "cv = df['cv'].values\n",
    "lb = df['lb'].values\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(cv, lb)\n",
    "\n",
    "print(f\"CV-LB Linear Relationship:\")\n",
    "print(f\"  LB = {slope:.2f} * CV + {intercept:.4f}\")\n",
    "print(f\"  R² = {r_value**2:.4f}\")\n",
    "print(f\"  Intercept = {intercept:.4f}\")\n",
    "print(f\"  Target LB = 0.0707\")\n",
    "print(f\"\")\n",
    "print(f\"CRITICAL INSIGHT:\")\n",
    "print(f\"  Intercept ({intercept:.4f}) < Target ({0.0707})\")\n",
    "print(f\"  This means the target IS REACHABLE!\")\n",
    "print(f\"\")\n",
    "print(f\"Required CV to hit target:\")\n",
    "required_cv = (0.0707 - intercept) / slope\n",
    "print(f\"  CV = (0.0707 - {intercept:.4f}) / {slope:.2f} = {required_cv:.6f}\")\n",
    "print(f\"  Current best CV: 0.008298\")\n",
    "print(f\"  Required improvement: {(0.008298 - required_cv) / 0.008298 * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734120b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.222326Z",
     "iopub.status.busy": "2026-01-16T05:35:44.222164Z",
     "iopub.status.idle": "2026-01-16T05:35:44.232387Z",
     "shell.execute_reply": "2026-01-16T05:35:44.231891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV-LB Residual Analysis:\n",
      "(Negative residual = performed BETTER on LB than expected from CV)\n",
      "\n",
      "  exp_000: CV=0.0111, LB=0.0982, Predicted=0.1004, Residual=-0.0022\n",
      "  exp_024: CV=0.0087, LB=0.0893, Predicted=0.0901, Residual=-0.0008\n",
      "  exp_030: CV=0.0083, LB=0.0877, Predicted=0.0883, Residual=-0.0006\n",
      "  exp_003: CV=0.0105, LB=0.0972, Predicted=0.0978, Residual=-0.0006\n",
      "  exp_026: CV=0.0085, LB=0.0887, Predicted=0.0892, Residual=-0.0005\n",
      "  exp_005: CV=0.0104, LB=0.0969, Predicted=0.0974, Residual=-0.0005\n",
      "  exp_012: CV=0.0090, LB=0.0913, Predicted=0.0914, Residual=-0.0001\n",
      "  exp_006: CV=0.0097, LB=0.0946, Predicted=0.0944, Residual=+0.0002\n",
      "  exp_007: CV=0.0093, LB=0.0932, Predicted=0.0926, Residual=+0.0006\n",
      "  exp_001: CV=0.0123, LB=0.1065, Predicted=0.1056, Residual=+0.0009\n",
      "  exp_009: CV=0.0092, LB=0.0936, Predicted=0.0922, Residual=+0.0014\n",
      "  exp_035: CV=0.0098, LB=0.0970, Predicted=0.0948, Residual=+0.0022\n",
      "\n",
      "Best residual: exp_000 (-0.0022)\n",
      "Worst residual: exp_035 (0.0022)\n"
     ]
    }
   ],
   "source": [
    "# Analyze residuals - which experiments performed better/worse than expected?\n",
    "df['predicted_lb'] = slope * df['cv'] + intercept\n",
    "df['residual'] = df['lb'] - df['predicted_lb']\n",
    "\n",
    "print(\"CV-LB Residual Analysis:\")\n",
    "print(\"(Negative residual = performed BETTER on LB than expected from CV)\")\n",
    "print(\"\")\n",
    "for _, row in df.sort_values('residual').iterrows():\n",
    "    print(f\"  {row['exp']}: CV={row['cv']:.4f}, LB={row['lb']:.4f}, Predicted={row['predicted_lb']:.4f}, Residual={row['residual']:+.4f}\")\n",
    "\n",
    "print(f\"\\nBest residual: {df.loc[df['residual'].idxmin(), 'exp']} ({df['residual'].min():.4f})\")\n",
    "print(f\"Worst residual: {df.loc[df['residual'].idxmax(), 'exp']} ({df['residual'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f924a10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.234041Z",
     "iopub.status.busy": "2026-01-16T05:35:44.233881Z",
     "iopub.status.idle": "2026-01-16T05:35:44.239194Z",
     "shell.execute_reply": "2026-01-16T05:35:44.238757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "APPROACHES TRIED (58 experiments)\n",
      "============================================================\n",
      "  1. MLP with Arrhenius kinetics (exp_000, exp_006, exp_007)\n",
      "  2. LightGBM (exp_001)\n",
      "  3. DRFP features with PCA (exp_002)\n",
      "  4. Combined Spange + DRFP (exp_003, exp_005)\n",
      "  5. Deep Residual MLP (exp_004) - FAILED\n",
      "  6. Large Ensemble 15 models (exp_005)\n",
      "  7. Simpler models [64,32] (exp_006, exp_007, exp_008)\n",
      "  8. Ridge Regression (exp_009, exp_033)\n",
      "  9. Single layer 16 (exp_010)\n",
      "  10. Diverse Ensemble (exp_011, exp_047)\n",
      "  11. Simple Ensemble (exp_012)\n",
      "  12. Compliant Ensemble (exp_013)\n",
      "  13. Ensemble weight tuning (exp_014, exp_031, exp_035, exp_036)\n",
      "  14. Three model ensemble (exp_015)\n",
      "  15. Attention model (exp_017)\n",
      "  16. Fragprints (exp_018)\n",
      "  17. ACS PCA features (exp_019, exp_023, exp_024)\n",
      "  18. Per-target models (exp_025)\n",
      "  19. Weighted loss (exp_026)\n",
      "  20. Simple features (exp_027)\n",
      "  21. Four model ensemble (exp_028)\n",
      "  22. Normalization (exp_029)\n",
      "  23. GP Ensemble (exp_030) - BEST\n",
      "  24. Higher GP weight (exp_031, exp_035)\n",
      "  25. Pure GP (exp_032)\n",
      "  26. Kernel Ridge (exp_034)\n",
      "  27. Similarity weighting (exp_037)\n",
      "  28. Minimal features (exp_038)\n",
      "  29. Learned embeddings (exp_039)\n",
      "  30. GNN architectures (exp_040, exp_052)\n",
      "  31. ChemBERTa (exp_041)\n",
      "  32. Calibration (exp_042)\n",
      "  33. Nonlinear mixture (exp_043)\n",
      "  34. Hybrid model (exp_044)\n",
      "  35. Mean reversion (exp_045)\n",
      "  36. Adaptive weighting (exp_046)\n",
      "  37. Hybrid features (exp_048)\n",
      "  38. Manual OOD handling (exp_049)\n",
      "  39. LISA/REX (exp_050)\n",
      "  40. Simpler model (exp_051, exp_054)\n",
      "  41. mixall full features (exp_053)\n",
      "  42. Chemical constraints (exp_055)\n",
      "  43. XGBoost + RF Ensemble (exp_056)\n",
      "  44. Simpler Spange Only (exp_057)\n"
     ]
    }
   ],
   "source": [
    "# What approaches have been tried?\n",
    "print(\"=\"*60)\n",
    "print(\"APPROACHES TRIED (58 experiments)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "approaches = [\n",
    "    \"MLP with Arrhenius kinetics (exp_000, exp_006, exp_007)\",\n",
    "    \"LightGBM (exp_001)\",\n",
    "    \"DRFP features with PCA (exp_002)\",\n",
    "    \"Combined Spange + DRFP (exp_003, exp_005)\",\n",
    "    \"Deep Residual MLP (exp_004) - FAILED\",\n",
    "    \"Large Ensemble 15 models (exp_005)\",\n",
    "    \"Simpler models [64,32] (exp_006, exp_007, exp_008)\",\n",
    "    \"Ridge Regression (exp_009, exp_033)\",\n",
    "    \"Single layer 16 (exp_010)\",\n",
    "    \"Diverse Ensemble (exp_011, exp_047)\",\n",
    "    \"Simple Ensemble (exp_012)\",\n",
    "    \"Compliant Ensemble (exp_013)\",\n",
    "    \"Ensemble weight tuning (exp_014, exp_031, exp_035, exp_036)\",\n",
    "    \"Three model ensemble (exp_015)\",\n",
    "    \"Attention model (exp_017)\",\n",
    "    \"Fragprints (exp_018)\",\n",
    "    \"ACS PCA features (exp_019, exp_023, exp_024)\",\n",
    "    \"Per-target models (exp_025)\",\n",
    "    \"Weighted loss (exp_026)\",\n",
    "    \"Simple features (exp_027)\",\n",
    "    \"Four model ensemble (exp_028)\",\n",
    "    \"Normalization (exp_029)\",\n",
    "    \"GP Ensemble (exp_030) - BEST\",\n",
    "    \"Higher GP weight (exp_031, exp_035)\",\n",
    "    \"Pure GP (exp_032)\",\n",
    "    \"Kernel Ridge (exp_034)\",\n",
    "    \"Similarity weighting (exp_037)\",\n",
    "    \"Minimal features (exp_038)\",\n",
    "    \"Learned embeddings (exp_039)\",\n",
    "    \"GNN architectures (exp_040, exp_052)\",\n",
    "    \"ChemBERTa (exp_041)\",\n",
    "    \"Calibration (exp_042)\",\n",
    "    \"Nonlinear mixture (exp_043)\",\n",
    "    \"Hybrid model (exp_044)\",\n",
    "    \"Mean reversion (exp_045)\",\n",
    "    \"Adaptive weighting (exp_046)\",\n",
    "    \"Hybrid features (exp_048)\",\n",
    "    \"Manual OOD handling (exp_049)\",\n",
    "    \"LISA/REX (exp_050)\",\n",
    "    \"Simpler model (exp_051, exp_054)\",\n",
    "    \"mixall full features (exp_053)\",\n",
    "    \"Chemical constraints (exp_055)\",\n",
    "    \"XGBoost + RF Ensemble (exp_056)\",\n",
    "    \"Simpler Spange Only (exp_057)\",\n",
    "]\n",
    "\n",
    "for i, approach in enumerate(approaches, 1):\n",
    "    print(f\"  {i}. {approach}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e69409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.240962Z",
     "iopub.status.busy": "2026-01-16T05:35:44.240805Z",
     "iopub.status.idle": "2026-01-16T05:35:44.247495Z",
     "shell.execute_reply": "2026-01-16T05:35:44.247025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "APPROACHES NOT YET TRIED\n",
      "============================================================\n",
      "1. PREDICTION CALIBRATION (Isotonic Regression)\n",
      "   - Train best model (exp_030)\n",
      "   - Use CV predictions to fit isotonic regression\n",
      "   - Apply calibration to test predictions\n",
      "   - Explicitly corrects systematic bias\n",
      "\n",
      "2. IMPORTANCE WEIGHTING\n",
      "   - Weight training samples by similarity to test distribution\n",
      "   - Use adversarial validation to identify drifting features\n",
      "   - Down-weight samples that are far from test distribution\n",
      "\n",
      "3. DOMAIN ADAPTATION\n",
      "   - Adapt model to test distribution at inference time\n",
      "   - Use test-time training (TTT) or transductive learning\n",
      "   - Fine-tune on test data without labels\n",
      "\n",
      "4. CATBOOST\n",
      "   - Different gradient boosting implementation\n",
      "   - Handles categorical features natively\n",
      "   - May have different inductive biases\n",
      "\n",
      "5. NEURAL NETWORK ENSEMBLES WITH DIFFERENT ARCHITECTURES\n",
      "   - Train multiple MLPs with different architectures\n",
      "   - Use different activation functions (GELU, SiLU)\n",
      "   - Use different regularization (LayerNorm, GroupNorm)\n",
      "\n",
      "6. QUANTILE REGRESSION\n",
      "   - Train model with quantile loss (median)\n",
      "   - May produce more robust predictions\n",
      "   - Different loss function could change CV-LB relationship\n"
     ]
    }
   ],
   "source": [
    "# What approaches HAVEN'T been tried?\n",
    "print(\"=\"*60)\n",
    "print(\"APPROACHES NOT YET TRIED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "untried = [\n",
    "    \"1. PREDICTION CALIBRATION (Isotonic Regression)\",\n",
    "    \"   - Train best model (exp_030)\",\n",
    "    \"   - Use CV predictions to fit isotonic regression\",\n",
    "    \"   - Apply calibration to test predictions\",\n",
    "    \"   - Explicitly corrects systematic bias\",\n",
    "    \"\",\n",
    "    \"2. IMPORTANCE WEIGHTING\",\n",
    "    \"   - Weight training samples by similarity to test distribution\",\n",
    "    \"   - Use adversarial validation to identify drifting features\",\n",
    "    \"   - Down-weight samples that are far from test distribution\",\n",
    "    \"\",\n",
    "    \"3. DOMAIN ADAPTATION\",\n",
    "    \"   - Adapt model to test distribution at inference time\",\n",
    "    \"   - Use test-time training (TTT) or transductive learning\",\n",
    "    \"   - Fine-tune on test data without labels\",\n",
    "    \"\",\n",
    "    \"4. CATBOOST\",\n",
    "    \"   - Different gradient boosting implementation\",\n",
    "    \"   - Handles categorical features natively\",\n",
    "    \"   - May have different inductive biases\",\n",
    "    \"\",\n",
    "    \"5. NEURAL NETWORK ENSEMBLES WITH DIFFERENT ARCHITECTURES\",\n",
    "    \"   - Train multiple MLPs with different architectures\",\n",
    "    \"   - Use different activation functions (GELU, SiLU)\",\n",
    "    \"   - Use different regularization (LayerNorm, GroupNorm)\",\n",
    "    \"\",\n",
    "    \"6. QUANTILE REGRESSION\",\n",
    "    \"   - Train model with quantile loss (median)\",\n",
    "    \"   - May produce more robust predictions\",\n",
    "    \"   - Different loss function could change CV-LB relationship\",\n",
    "]\n",
    "\n",
    "for line in untried:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de48209c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.249546Z",
     "iopub.status.busy": "2026-01-16T05:35:44.249049Z",
     "iopub.status.idle": "2026-01-16T05:35:44.255300Z",
     "shell.execute_reply": "2026-01-16T05:35:44.254826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSIS: The 'mixall' Kernel Approach\n",
      "============================================================\n",
      "\n",
      "The 'mixall' kernel achieves good LB scores using:\n",
      "\n",
      "1. ENSEMBLE: MLP (0.4) + XGBoost (0.2) + RandomForest (0.2) + LightGBM (0.2)\n",
      "   - Our exp_056 tried XGBoost + RF but FAILED\n",
      "   - Key difference: mixall uses different weights and architecture\n",
      "\n",
      "2. FEATURES: Spange descriptors + Residence Time + Temperature\n",
      "   - Simple features, no DRFP\n",
      "   - Our exp_057 tried this but FAILED\n",
      "\n",
      "3. CV SCHEME: GroupKFold(5) instead of Leave-One-Out(24)\n",
      "   - This is a GRAY AREA in competition rules\n",
      "   - Their local CV is not comparable to ours\n",
      "   - But their model may still generalize better\n",
      "\n",
      "4. ARCHITECTURE: MLP [128, 64, 32] with dropout 0.1\n",
      "   - Similar to our exp_006, exp_007\n",
      "   - Not fundamentally different\n",
      "\n",
      "KEY INSIGHT:\n",
      "The mixall kernel's success is NOT due to a fundamentally different approach.\n",
      "It's likely due to:\n",
      "  a) Different hyperparameters\n",
      "  b) Different random seeds\n",
      "  c) Different training dynamics\n",
      "  d) Luck in the CV-LB relationship\n",
      "\n",
      "\n",
      "Our best model (exp_030) uses:\n",
      "  - GP (0.15) + MLP (0.55) + LGBM (0.30)\n",
      "  - Spange + DRFP + Arrhenius features\n",
      "  - CV: 0.008298, LB: 0.0877\n",
      "\n",
      "To reach target LB 0.0707, we need:\n",
      "  - CV: 0.004213 (49% improvement)\n",
      "  - Or change the CV-LB relationship (reduce intercept)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the mixall kernel approach\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS: The 'mixall' Kernel Approach\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "The 'mixall' kernel achieves good LB scores using:\n",
    "\n",
    "1. ENSEMBLE: MLP (0.4) + XGBoost (0.2) + RandomForest (0.2) + LightGBM (0.2)\n",
    "   - Our exp_056 tried XGBoost + RF but FAILED\n",
    "   - Key difference: mixall uses different weights and architecture\n",
    "\n",
    "2. FEATURES: Spange descriptors + Residence Time + Temperature\n",
    "   - Simple features, no DRFP\n",
    "   - Our exp_057 tried this but FAILED\n",
    "\n",
    "3. CV SCHEME: GroupKFold(5) instead of Leave-One-Out(24)\n",
    "   - This is a GRAY AREA in competition rules\n",
    "   - Their local CV is not comparable to ours\n",
    "   - But their model may still generalize better\n",
    "\n",
    "4. ARCHITECTURE: MLP [128, 64, 32] with dropout 0.1\n",
    "   - Similar to our exp_006, exp_007\n",
    "   - Not fundamentally different\n",
    "\n",
    "KEY INSIGHT:\n",
    "The mixall kernel's success is NOT due to a fundamentally different approach.\n",
    "It's likely due to:\n",
    "  a) Different hyperparameters\n",
    "  b) Different random seeds\n",
    "  c) Different training dynamics\n",
    "  d) Luck in the CV-LB relationship\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nOur best model (exp_030) uses:\")\n",
    "print(\"  - GP (0.15) + MLP (0.55) + LGBM (0.30)\")\n",
    "print(\"  - Spange + DRFP + Arrhenius features\")\n",
    "print(\"  - CV: 0.008298, LB: 0.0877\")\n",
    "print(\"\\nTo reach target LB 0.0707, we need:\")\n",
    "print(f\"  - CV: {required_cv:.6f} (49% improvement)\")\n",
    "print(f\"  - Or change the CV-LB relationship (reduce intercept)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ff5518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.257060Z",
     "iopub.status.busy": "2026-01-16T05:35:44.256902Z",
     "iopub.status.idle": "2026-01-16T05:35:44.262895Z",
     "shell.execute_reply": "2026-01-16T05:35:44.262403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRATEGIC ANALYSIS\n",
      "============================================================\n",
      "\n",
      "CURRENT SITUATION:\n",
      "- 27 consecutive failures since exp_030\n",
      "- Best LB: 0.0877 (exp_030)\n",
      "- Target: 0.0707 (19.4% improvement needed)\n",
      "- 5 submissions remaining\n",
      "\n",
      "THE PROBLEM:\n",
      "- We've tried many approaches but none beat exp_030\n",
      "- The CV-LB relationship is: LB = 4.31*CV + 0.0525\n",
      "- To reach target, we need CV = 0.00422 (49% improvement)\n",
      "\n",
      "THE PATH FORWARD:\n",
      "\n",
      "1. FOCUS ON CV IMPROVEMENT\n",
      "   - Current best CV: 0.008298\n",
      "   - Required CV: 0.00422\n",
      "   - This is a 49% improvement - very aggressive\n",
      "   - Need fundamentally better features or models\n",
      "\n",
      "2. FOCUS ON CV-LB RELATIONSHIP\n",
      "   - The intercept (0.0525) is the systematic bias\n",
      "   - Prediction calibration could reduce this\n",
      "   - Importance weighting could reduce this\n",
      "\n",
      "3. SUBMISSION STRATEGY\n",
      "   - 5 submissions remaining\n",
      "   - Use 2-3 for experiments that might change CV-LB relationship\n",
      "   - Save 2 for final attempts\n",
      "\n",
      "RECOMMENDED PRIORITIES:\n",
      "1. Prediction Calibration (Isotonic Regression) - directly addresses intercept\n",
      "2. Importance Weighting - addresses distribution shift\n",
      "3. CatBoost - different inductive biases\n",
      "4. Quantile Regression - different loss function\n",
      "\n",
      "\n",
      "NOTE: The evaluator says the target IS reachable.\n",
      "The intercept (0.0525) < Target (0.0707) means we CAN reach it.\n",
      "We just need to improve CV by 49% or reduce the intercept.\n"
     ]
    }
   ],
   "source": [
    "# Strategic analysis\n",
    "print(\"=\"*60)\n",
    "print(\"STRATEGIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "CURRENT SITUATION:\n",
    "- 27 consecutive failures since exp_030\n",
    "- Best LB: 0.0877 (exp_030)\n",
    "- Target: 0.0707 (19.4% improvement needed)\n",
    "- 5 submissions remaining\n",
    "\n",
    "THE PROBLEM:\n",
    "- We've tried many approaches but none beat exp_030\n",
    "- The CV-LB relationship is: LB = 4.31*CV + 0.0525\n",
    "- To reach target, we need CV = 0.00422 (49% improvement)\n",
    "\n",
    "THE PATH FORWARD:\n",
    "\n",
    "1. FOCUS ON CV IMPROVEMENT\n",
    "   - Current best CV: 0.008298\n",
    "   - Required CV: 0.00422\n",
    "   - This is a 49% improvement - very aggressive\n",
    "   - Need fundamentally better features or models\n",
    "\n",
    "2. FOCUS ON CV-LB RELATIONSHIP\n",
    "   - The intercept (0.0525) is the systematic bias\n",
    "   - Prediction calibration could reduce this\n",
    "   - Importance weighting could reduce this\n",
    "\n",
    "3. SUBMISSION STRATEGY\n",
    "   - 5 submissions remaining\n",
    "   - Use 2-3 for experiments that might change CV-LB relationship\n",
    "   - Save 2 for final attempts\n",
    "\n",
    "RECOMMENDED PRIORITIES:\n",
    "1. Prediction Calibration (Isotonic Regression) - directly addresses intercept\n",
    "2. Importance Weighting - addresses distribution shift\n",
    "3. CatBoost - different inductive biases\n",
    "4. Quantile Regression - different loss function\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nNOTE: The evaluator says the target IS reachable.\")\n",
    "print(\"The intercept (0.0525) < Target (0.0707) means we CAN reach it.\")\n",
    "print(\"We just need to improve CV by 49% or reduce the intercept.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5eadec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T05:35:44.264312Z",
     "iopub.status.busy": "2026-01-16T05:35:44.264151Z",
     "iopub.status.idle": "2026-01-16T05:35:44.270092Z",
     "shell.execute_reply": "2026-01-16T05:35:44.269629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOOP 58 SUMMARY\n",
      "============================================================\n",
      "\n",
      "Current Status:\n",
      "  - Best CV: 0.008298 (exp_030)\n",
      "  - Best LB: 0.0877 (exp_030)\n",
      "  - Target LB: 0.0707\n",
      "  - Gap: 19.4% improvement needed\n",
      "  - Submissions remaining: 5\n",
      "  - Consecutive failures: 27\n",
      "\n",
      "Key Findings:\n",
      "  1. CV-LB relationship: LB = 4.31*CV + 0.0525 (R²=0.95)\n",
      "  2. Intercept (0.0525) < Target (0.0707) - target IS reachable\n",
      "  3. Required CV to hit target: 0.00422 (49% improvement)\n",
      "  4. exp_057 (Simpler Spange Only) FAILED - CV 0.023017 (177.4% worse)\n",
      "  5. The 'mixall' kernel uses GroupKFold(5) - not directly comparable\n",
      "\n",
      "Recommended Next Steps:\n",
      "  1. Prediction Calibration (Isotonic Regression) - directly addresses intercept\n",
      "  2. Importance Weighting - addresses distribution shift\n",
      "  3. CatBoost - different inductive biases\n",
      "  4. Quantile Regression - different loss function\n",
      "\n",
      "Submission Strategy:\n",
      "  - Use 2-3 submissions for experiments that might change CV-LB relationship\n",
      "  - Save 2 submissions for final attempts\n",
      "  - Focus on approaches that reduce the intercept, not just CV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"LOOP 58 SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Current Status:\n",
    "  - Best CV: 0.008298 (exp_030)\n",
    "  - Best LB: 0.0877 (exp_030)\n",
    "  - Target LB: 0.0707\n",
    "  - Gap: 19.4% improvement needed\n",
    "  - Submissions remaining: 5\n",
    "  - Consecutive failures: 27\n",
    "\n",
    "Key Findings:\n",
    "  1. CV-LB relationship: LB = 4.31*CV + 0.0525 (R²=0.95)\n",
    "  2. Intercept (0.0525) < Target (0.0707) - target IS reachable\n",
    "  3. Required CV to hit target: 0.00422 (49% improvement)\n",
    "  4. exp_057 (Simpler Spange Only) FAILED - CV 0.023017 (177.4% worse)\n",
    "  5. The 'mixall' kernel uses GroupKFold(5) - not directly comparable\n",
    "\n",
    "Recommended Next Steps:\n",
    "  1. Prediction Calibration (Isotonic Regression) - directly addresses intercept\n",
    "  2. Importance Weighting - addresses distribution shift\n",
    "  3. CatBoost - different inductive biases\n",
    "  4. Quantile Regression - different loss function\n",
    "\n",
    "Submission Strategy:\n",
    "  - Use 2-3 submissions for experiments that might change CV-LB relationship\n",
    "  - Save 2 submissions for final attempts\n",
    "  - Focus on approaches that reduce the intercept, not just CV\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
