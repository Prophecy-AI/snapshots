{
  "workspace_dir": "/home/code",
  "competition_id": "catechol-benchmark-hackathon",
  "metric_direction": true,
  "start_time": "2026-01-14T06:47:06.574262",
  "time_limit_minutes": 2100,
  "experiments": [
    {
      "id": "exp_000",
      "name": "Physics-Informed MLP Baseline with Arrhenius Kinetics + TTA",
      "model_type": "MLP",
      "score": 0.08819,
      "notes": "Implemented physics-informed MLP with: (1) Arrhenius kinetics features (1/T, ln(t), 1/T*ln(t)), (2) Spange descriptors for solvent features, (3) Robust MLP with BatchNorm, Dropout(0.2), Sigmoid output, (4) HuberLoss for robustness, (5) TTA for mixed solvent symmetry, (6) Ensemble of 5 models per fold. Single Solvent CV RMSE: 0.08719 \u00b1 0.04350, Full Data CV RMSE: 0.08920 \u00b1 0.03871. This beats the reference score of 0.09831 from the Arrhenius kinetics notebook. Target is 0.047400 - need ~1.9x improvement.",
      "experiment_folder": "/home/code/experiments/001_baseline",
      "timestamp": "2026-01-14T07:57:26.226994"
    }
  ],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.08819,
      "model_name": "Physics-Informed MLP Baseline with Arrhenius Kinetics + TTA",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-14T07:57:26.227927"
    }
  ],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Catechol Reaction Yield Prediction - Seed Prompt\n\n## Competition Overview\nThis is a chemistry/reaction yield prediction problem. The goal is to predict yields (SM, Product 2, Product 3) for the allyl substituted catechol reaction under different solvent conditions and process parameters (temperature, residence time).\n\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: data shapes, feature distributions, target distributions, solvent counts\n\n## CRITICAL SUBMISSION CONSTRAINTS\n\n**MANDATORY STRUCTURE:**\n- The submission MUST follow the template notebook structure\n- Only the last 3 cells can be modified, and ONLY the `model = MLPModel()` line can be changed\n- Model must implement: `train_model(X_train, y_train)` and `predict(X_test)` methods\n- Two tasks: single_solvent (leave-one-solvent-out CV, 24 folds) and full (leave-one-ramp-out CV, 13 folds)\n- Target order in Y: [\"Product 2\", \"Product 3\", \"SM\"]\n\n**DATA CONTAMINATION RULES:**\n- Pre-training on solvent mixture data to predict full solvent data = NOT ALLOWED\n- Different hyperparameters for different tasks (full vs single) ARE allowed\n- Different hyperparameters for different objectives (SM vs Product 1) ARE allowed\n\n## Key Data Characteristics\n\n**Single Solvent Data:**\n- 656 samples, 24 unique solvents\n- Leave-one-solvent-out CV (24 folds, ~27 samples per solvent on average)\n- Temperature: 175-225\u00b0C, Residence Time: 2-15 minutes\n\n**Full Data (Mixtures):**\n- 1227 samples, 13 unique solvent pair ramps\n- Leave-one-ramp-out CV (13 folds)\n- SolventB% ranges from 0.0 to 1.0 (continuous mixing ratio)\n\n**Targets:**\n- SM (starting material), Product 2, Product 3 - all yields in [0, 1]\n- Targets are NOT constrained to sum to 1 (can have side products)\n\n**Feature Lookup Tables Available:**\n- Spange descriptors (13 features) - physical/chemical properties - MOST COMMONLY USED\n- ACS PCA descriptors (5 features) - PCA of green chemistry properties\n- DRFP (2048 features) - differential reaction fingerprints\n- Fragprints (2133 features) - molecular fingerprints\n\n## Winning Approaches from Top Kernels\n\n### 1. Physics-Informed Features (Arrhenius Kinetics) - Score: 0.09831\n**Key insight:** Chemical reactions follow Arrhenius kinetics where rate depends on temperature exponentially.\n\n**Feature Engineering:**\n```python\ntemp_k = temp_c + 273.15  # Convert to Kelvin\ninv_temp = 1000.0 / temp_k  # Inverse temperature (Arrhenius)\nlog_time = np.log(time + 1e-6)  # Log residence time\ninteraction = inv_temp * log_time  # Kinetic interaction term\n```\n\n**Test Time Augmentation (TTA) for Symmetry:**\n- For mixed solvents, (A, B) is chemically identical to (B, A)\n- Predict with both orderings and average: `final = (pred_AB + pred_BA) / 2`\n- Also augment training data with both orderings\n\n### 2. Robust MLP Architecture\n**Architecture:**\n- BatchNorm1d at input\n- Hidden layers: [128, 128, 64] with BatchNorm, ReLU, Dropout(0.2)\n- Sigmoid output (constrains predictions to [0, 1])\n\n**Training:**\n- HuberLoss (robust to outliers) instead of MSE\n- Adam optimizer with lr=5e-4, weight_decay=1e-5\n- ReduceLROnPlateau scheduler (factor=0.5, patience=20)\n- Gradient clipping: clip_grad_norm_(1.0)\n- 300 epochs\n\n### 3. Ensemble/Bagging\n- Train 5-7 models with different seeds\n- Average predictions for final output\n- Reduces variance significantly\n\n### 4. Additional Feature Engineering\nFrom other kernels:\n- `rt^2`, `temp^2` - polynomial features\n- `log1p(rt)`, `log1p(temp)` - log transforms\n- `rt * temp` - interaction term\n- StandardScaler for all numeric features\n\n## Model Recommendations\n\n### Primary Approach: Physics-Informed MLP with TTA\n1. **Feature Engineering:**\n   - Use Spange descriptors (13 features) as base\n   - Add Arrhenius-inspired features: 1/T, ln(t), 1/T * ln(t)\n   - StandardScaler normalization\n\n2. **Architecture:**\n   - Input BatchNorm\n   - 3-4 hidden layers [128, 128, 64] or [256, 128, 64]\n   - BatchNorm + ReLU + Dropout(0.2) after each hidden layer\n   - Sigmoid output layer\n\n3. **Training:**\n   - HuberLoss or SmoothL1Loss\n   - AdamW optimizer (lr=5e-4, weight_decay=1e-5)\n   - CosineAnnealingLR or ReduceLROnPlateau\n   - Early stopping with patience ~30\n   - 200-300 epochs\n\n4. **Inference:**\n   - For full data: TTA with symmetric solvent swapping\n   - Ensemble 5-7 models with different seeds\n\n### Alternative Approaches to Try\n\n**TabPFN (Tabular Foundation Model):**\n- Transformer pre-trained on millions of synthetic datasets\n- Excels on small tabular datasets (<10K samples)\n- No hyperparameter tuning needed\n- Particularly strong for regression on chemistry data\n- May work well given our small dataset size\n\n**Gradient Boosting (XGBoost/LightGBM):**\n- MultiOutputRegressor wrapper\n- Feature engineering same as MLP\n- Post-processing: clip to [0, 1]\n- May work well for single solvent task\n\n**Gaussian Process:**\n- Good for small data with uncertainty quantification\n- Use Spange descriptors as kernel features\n- Iterative refinement possible\n\n## Solvent Featurization for Mixed Solvents\n\n**Linear Mixing (Standard Approach):**\n```python\nmixed_features = A_features * (1 - pct_B) + B_features * pct_B\n```\n\n**Considerations:**\n- This assumes linear interpolation of solvent properties\n- May not capture non-linear mixing effects\n- Alternative: concatenate both solvent features + mixing ratio\n\n## Validation Strategy\n\n**Single Solvent Task:**\n- Leave-one-solvent-out CV (24 folds)\n- Each fold leaves out all data for one solvent\n- Tests generalization to unseen solvents\n\n**Full Data Task:**\n- Leave-one-ramp-out CV (13 folds)\n- Each fold leaves out one solvent pair combination\n- Tests generalization to unseen solvent mixtures\n\n## Post-Processing\n\n1. **Clip predictions to [0, 1]:** `predictions = np.clip(predictions, 0, 1)`\n2. **Optional normalization:** Some approaches normalize rows to sum to 1, but this may not be physically correct for this reaction\n\n## Key Insights from Research\n\n1. **Solvent descriptors matter:** Spange descriptors (physical/chemical properties) work better than fingerprints for this task\n2. **Physics-informed features help:** Arrhenius kinetics features significantly improve predictions\n3. **Symmetry exploitation:** TTA for mixed solvents provides free performance boost\n4. **Robust loss functions:** HuberLoss handles outliers better than MSE\n5. **Ensemble reduces variance:** 5-7 model ensemble with different seeds\n6. **Small data considerations:** TabPFN or other foundation models may help with limited samples\n\n## Advanced Techniques from Literature\n\n**Physics-Informed Neural Networks (PINNs):**\n- Embed Arrhenius law directly into loss function\n- Can learn rate constants while respecting physics\n- Three-stage training: fit data \u2192 learn physics \u2192 fine-tune\n\n**Chemical Reaction Neural Networks (CRNNs):**\n- Encode mass action and Arrhenius laws in architecture\n- Interpretable kinetic parameters\n- Good for uncertainty quantification\n\n**Data Augmentation for Chemistry:**\n- SMILES randomization (different valid SMILES for same molecule)\n- Reaction SMILES permutation\n- Symmetric solvent swapping (already implemented in TTA)\n\n## Implementation Checklist\n\n1. \u2705 Model class with `train_model(X_train, y_train)` and `predict(X_test)` methods\n2. \u2705 Featurizer that handles both single and mixed solvents\n3. \u2705 Physics-informed features (Arrhenius kinetics)\n4. \u2705 TTA for mixed solvent symmetry\n5. \u2705 Ensemble of multiple seeds\n6. \u2705 Robust loss function (Huber/SmoothL1)\n7. \u2705 Proper normalization (StandardScaler)\n8. \u2705 Output clipping to [0, 1]\n\n## Target Score\nBeat **0.047400** (lower is better - likely RMSE or similar metric)\n\nCurrent best public kernel: 0.09831 with Arrhenius kinetics + TTA approach\n\n**Gap to close:** Need ~2x improvement from best public approach to beat target.\nConsider: deeper ensembles, better feature engineering, alternative architectures (TabPFN), or combining multiple approaches.\n",
      "agent": "explorer",
      "timestamp": "2026-01-14T06:51:41.049781"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Arrhenius Kinetics approach achieves 0.09831: Uses 1/Temperature (inverse Kelvin), ln(Time), and their interaction as physics-informed features. Also uses Test Time Augmentation (TTA) for symmetric solvent mixtures - predicting with both (A,B) and (B,A) and averaging.",
      "source": "../research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/arrhenius-kinetics-tta-0-09831.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Template structure: Must use last 3 cells unchanged. Only allowed to change `model = MLPModel()` line. Model needs train_model(X_train, y_train) and predict(X_test) methods. Two tasks: single_solvent (leave-one-solvent-out CV) and full (leave-one-ramp-out CV for mixtures).",
      "source": "../research/kernels/josepablofolch_catechol-benchmark-hackathon-template/catechol-benchmark-hackathon-template.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Alchemy baseline uses: StandardScaler for features, numeric feature engineering (rt^2, temp^2, log1p(rt), log1p(temp), rt*temp), Huber loss (SmoothL1Loss), AdamW optimizer with CosineAnnealingLR, BatchNorm + Dropout, early stopping, and ensemble of 3 seeds per fold.",
      "source": "../research/kernels/paritoshtripathi5_alchemy-baseline/alchemy-baseline.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "XGBoost approach: Uses MultiOutputRegressor with XGBRegressor, feature engineering (Reaction_Energy = Temp*Time, B_Conc_Temp = SolventB%*Temp), post-processing with clipping to [0,1] and row normalization to sum to 1.",
      "source": "../research/kernels/mr0106_catechol/catechol.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data structure: Single solvent data has 656 samples with 24 unique solvents (leave-one-solvent-out CV = 24 folds). Full data has 1227 samples with 13 unique solvent pair ramps (leave-one-ramp-out CV = 13 folds). Targets are SM, Product 2, Product 3 (yields in [0,1]).",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Feature lookup tables: Spange descriptors (13 features - physical/chemical properties), ACS PCA (5 features), DRFP (2048 features - reaction fingerprints), Fragprints (2133 features). Spange is most commonly used in top solutions.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "TabPFN (Tabular Prior-data Fitted Network) is a transformer-based foundation model that excels on small tabular datasets (<10K samples). It requires no hyperparameter tuning and can make predictions in a single forward pass. Particularly strong for regression tasks on small chemistry datasets.",
      "source": "Web search: neural network architectures for small chemistry datasets",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best machine learning approaches for predicting chemical reaction yields using solvent descriptors?",
      "finding": "Predicting reaction yields when the solvent is a key variable is most successful when the model combines **rich solvent descriptors** (e.g., \u03c3\u2011profiles, COSMO\u2011RS\u2010derived polarity or hydrogen\u2011bonding parameters) with **machine\u2011learning architectures that can exploit both categorical reaction information and continuous physicochemical features**.  Early work showed that simple one\u2011hot encodings of reactants together with computed molecular descriptors already give useful yield estimates, but performance improves markedly when the solvent descriptors are treated as explicit numerical inputs to regression models such as Random Forests or Gradient\u2011Boosted Trees\u202f([Nature\u202f2026](https://www.nature.com/articles/s42004-025-01866-8)).  In asymmetric catalysis, Gaussian\u2011process surrogate models built on a small set of solvent \u03c3\u2011profiles were able to rank solvents for conversion and enantioselectivity, and iterative retraining on a handful of newly tested solvents drove the prediction error from \u201cp",
      "agent": "explorer"
    },
    {
      "query": "How do physics-informed neural networks incorporate Arrhenius kinetics for chemical reaction modeling?",
      "finding": "Physics\u2011informed neural networks (PINNs) embed the Arrhenius law directly into the learning problem so that the network respects the temperature\u2011dependence of reaction rates while fitting data\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2023/cp/d2cp05083h); [arxiv.org](https://arxiv.org/html/2511.07686v1)).\n\n**How to incorporate Arrhenius kinetics in a PINN**\n\n1. **Formulate the governing ODEs**  \n   Write the species\u2011balance equations for the reacting system and express each reaction rate \\(k(T)\\) with the Arrhenius expression \\(k(T)=A\\exp(-E_a/RT)\\). These equations become the physics constraints for the PINN.\u202f([pubs.rsc.org](https://pubs.rsc.org/en/content/articlehtml/2023/cp/d2cp05083h))\n\n2. **Design the neural network architecture**  \n   - Use a standard feed\u2011forward network to approximate the concentration trajectories \\(c_i(t)\\).  \n   - Optionally add a small sub\u2011network that predicts the Arrhenius parameters \\(A\\) and \\(E_a\\) (or treat them as learnable scalar v",
      "agent": "explorer"
    },
    {
      "query": "What neural network architectures work best for small tabular chemistry datasets with less than 2000 samples?",
      "finding": "Neural\u2011network models that are explicitly designed to exploit structure or pre\u2011learned priors tend to outperform generic multilayer perceptrons on chemistry tabular sets with\u202f<\u202f2\u202f000 samples.  The most successful recent approach is **TabPFN**, a transformer that has been trained on millions of synthetic tabular datasets and can make accurate predictions for a new small\u2011data problem in a single forward pass.  Because the model\u2019s weights already encode a rich \u201cmeta\u2011learning\u201d prior over feature interactions, it requires virtually no hyper\u2011parameter tuning and consistently beats standard MLPs and even gradient\u2011boosted trees on low\u2011sample drug\u2011discovery benchmarks\u202f([Nature\u202f2025](https://www.nature.com/articles/s41586-024-08328-6);\u202f[ChemRxiv\u202f2025](https://chemrxiv.org/engage/chemrxiv/article-details/68d29b1cf2aff1677025b18f);\u202f[arXiv\u202f2022/2023](https://arxiv.org/abs/2207.01848)).  \n\nA complementary strategy is **GCondNet**, which builds a graph for each feature dimension (connecting samples t",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}