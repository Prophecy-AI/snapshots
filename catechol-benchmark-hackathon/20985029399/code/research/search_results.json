{
  "query": "What neural network architectures work best for small tabular chemistry datasets with less than 2000 samples?",
  "results": [
    {
      "title": "ANSWER",
      "text": "Neural\u2011network models that are explicitly designed to exploit structure or pre\u2011learned priors tend to outperform generic multilayer perceptrons on chemistry tabular sets with\u202f<\u202f2\u202f000 samples.  The most successful recent approach is **TabPFN**, a transformer that has been trained on millions of synthetic tabular datasets and can make accurate predictions for a new small\u2011data problem in a single forward pass.  Because the model\u2019s weights already encode a rich \u201cmeta\u2011learning\u201d prior over feature interactions, it requires virtually no hyper\u2011parameter tuning and consistently beats standard MLPs and even gradient\u2011boosted trees on low\u2011sample drug\u2011discovery benchmarks\u202f([Nature\u202f2025](https://www.nature.com/articles/s41586-024-08328-6);\u202f[ChemRxiv\u202f2025](https://chemrxiv.org/engage/chemrxiv/article-details/68d29b1cf2aff1677025b18f);\u202f[arXiv\u202f2022/2023](https://arxiv.org/abs/2207.01848)).  \n\nA complementary strategy is **GCondNet**, which builds a graph for each feature dimension (connecting samples that are similar in that dimension) and feeds these graphs to a Graph Neural Network that conditions the first layer of a downstream predictor (MLP or tabular transformer).  By injecting graph\u2011regularisation the method leverages the high\u2011dimensional correlations that are otherwise invisible to a plain MLP, and it has been shown to improve performance on 12 real\u2011world chemistry\u2011related tabular tasks, many of which have fewer than a few thousand records\u202f([arXiv\u202f2024](https://arxiv.org/html/2211.06302v4)).  \n\nOverall, the literature suggests that **meta\u2011trained transformers (TabPFN) and graph\u2011conditioned architectures (GCondNet)** are currently the best neural\u2011network choices for very small chemistry tabular datasets.  They provide strong inductive biases that compensate for limited samples, while more conventional NNs still lag behind unless combined with such priors\u202f([arXiv\u202f2023 \u201cWhen Do Neural Nets Outperform Boosted Trees?\u201d](https://arxiv.org/abs/2305.02997)).",
      "url": ""
    },
    {
      "title": "Accurate predictions on small data with a tabular foundation model",
      "text": "Accurate predictions on small data with a tabular foundation model | Nature\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature](https://media.springernature.com/full/nature-cms/uploads/product/nature/header-86f1267ea01eccd46b530284be10585e.svg)](https://www.nature.com/)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41586-024-08328-6?error=cookies_not_supported&code=a88591bc-0496-4f30-a97b-549655340e8f)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41586)\n* [RSS feed](https://www.nature.com/nature.rss)\nAccurate predictions on small data with a tabular foundation model\n[Download PDF](https://www.nature.com/articles/s41586-024-08328-6.pdf)\n[Download PDF](https://www.nature.com/articles/s41586-024-08328-6.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:08 January 2025# Accurate predictions on small data with a tabular foundation model\n* [Noah Hollmann](#auth-Noah-Hollmann-Aff1-Aff2-Aff3)[ORCID:orcid.org/0000-0001-8556-518X](https://orcid.org/0000-0001-8556-518X)[1](#Aff1),[2](#Aff2),[3](#Aff3)[na1](#na1),\n* [Samuel M\u00fcller](#auth-Samuel-M_ller-Aff1)[ORCID:orcid.org/0009-0000-0795-6097](https://orcid.org/0009-0000-0795-6097)[1](#Aff1)[na1](#na1),\n* [Lennart Purucker](#auth-Lennart-Purucker-Aff1)[ORCID:orcid.org/0009-0001-1181-0549](https://orcid.org/0009-0001-1181-0549)[1](#Aff1),\n* [Arjun Krishnakumar](#auth-Arjun-Krishnakumar-Aff1)[ORCID:orcid.org/0009-0000-0688-9435](https://orcid.org/0009-0000-0688-9435)[1](#Aff1),\n* [Max K\u00f6rfer](#auth-Max-K_rfer-Aff1)[1](#Aff1),\n* [Shi Bin Hoo](#auth-Shi_Bin-Hoo-Aff1)[1](#Aff1),\n* [Robin Tibor Schirrmeister](#auth-Robin_Tibor-Schirrmeister-Aff4-Aff5)[ORCID:orcid.org/0000-0002-5518-7445](https://orcid.org/0000-0002-5518-7445)[4](#Aff4),[5](#Aff5)&amp;\n* \u2026* [Frank Hutter](#auth-Frank-Hutter-Aff1-Aff3-Aff6)[ORCID:orcid.org/0000-0002-2037-3694](https://orcid.org/0000-0002-2037-3694)[1](#Aff1),[3](#Aff3),[6](#Aff6)Show authors\n[*Nature*](https://www.nature.com/)**volume637**,pages319\u2013326 (2025)[Cite this article](#citeas)\n* 406kAccesses\n* 374Citations\n* 515Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41586-024-08328-6/metrics)\n### Subjects\n* [Computational science](https://www.nature.com/subjects/computational-science)\n* [Computer science](https://www.nature.com/subjects/computer-science)\n* [Scientific data](https://www.nature.com/subjects/scientific-data)\n* [Software](https://www.nature.com/subjects/software)\n* [Statistics](https://www.nature.com/subjects/statistics)\n## Abstract\nTabular data, spreadsheets organized in rows and columns, are ubiquitous across scientific fields, from biomedicine to particle physics to economics and climate science[1](https://www.nature.com/articles/s41586-024-08328-6#ref-CR1),[2](https://www.nature.com/articles/s41586-024-08328-6#ref-CR2). The fundamental prediction task of filling in missing values of a label column based on the rest of the columns is essential for various applications as diverse as biomedical risk models, drug discovery and materials science. Although deep learning has revolutionized learning from raw data and led to numerous high-profile success stories[3](#ref-CR3),[4](#ref-CR4),[5](https://www.nature.com/articles/s41586-024-08328-6#ref-CR5), gradient-boosted decision trees[6](#ref-CR6),[7](#ref-CR7),[8](#ref-CR8),[9](https://www.nature.com/articles/s41586-024-08328-6#ref-CR9)have dominated tabular data for the past 20\u2009years. Here we present the Tabular Prior-data Fitted Network (TabPFN), a tabular foundation model that outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time. In 2.8\u2009s, TabPFN outperforms an ensemble of the strongest baselines tuned for 4\u2009h in a classification setting. As a generative transformer-based foundation model, this model also allows fine-tuning, data generation, density estimation and learning reusable embeddings. TabPFN is a\u00a0learning algorithm that is itself learned across millions of synthetic datasets, demonstrating the power of this approach for algorithm development. By improving modelling abilities across diverse fields, TabPFN has the potential to accelerate scientific discovery and enhance important decision-making in various domains.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41551-024-01268-6/MediaObjects/41551_2024_1268_Fig1_HTML.png)\n### [Interpretable discovery of patterns in tabular data via spatially semantic topographic maps](https://www.nature.com/articles/s41551-024-01268-6?fromPaywallRec=false)\nArticle15 October 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-15019-3/MediaObjects/41598_2025_15019_Fig1_HTML.png)\n### [An enhancement of machine learning model performance in disease prediction with synthetic data generation](https://www.nature.com/articles/s41598-025-15019-3?fromPaywallRec=false)\nArticleOpen access29 September 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41597-024-03605-5/MediaObjects/41597_2024_3605_Fig1_HTML.png)\n### [ENTRANT: A Large Financial Dataset for Table Understanding](https://www.nature.com/articles/s41597-024-03605-5?fromPaywallRec=false)\nArticleOpen access13 August 2024\n## Main\nThroughout the history of artificial intelligence, manually created algorithmic components have been replaced with better-performing end-to-end learned ones. Hand-designed features in computer vision, such as SIFT (Scale Invariant Feature Transform)[10](https://www.nature.com/articles/s41586-024-08328-6#ref-CR10)and HOG (Histogram of Oriented Gradients)[11](https://www.nature.com/articles/s41586-024-08328-6#ref-CR11), have been replaced by learned convolutions; grammar-based approaches in natural language processing have been replaced by learned transformers[12](https://www.nature.com/articles/s41586-024-08328-6#ref-CR12); and the design of customized opening and end-game libraries in game playing has been superseded by end-to-end learned strategies[3](https://www.nature.com/articles/s41586-024-08328-6#ref-CR3),[13](https://www.nature.com/articles/s41586-024-08328-6#ref-CR13). Here we extend this end-to-end learning to the ubiquitous domain of tabular data.\nThe diversity of tabular datasets them apart from unprocessed modalities such as text and images. While in language modellingfor example the meaning of a word is consistent across documents, in tabular datasets the same value can mean fundamentally different things. A drug discovery dataset, for example, might record chemical properties, whereas another dataset in materials science might document thermal and electric properties. This specialization leads to a proliferation of smaller, independent datasets and associated models. To illustrate, on the popular tabular benchmarking website openml.org, 76% of the datasets contain less than 10,000 rows at the time of writing.\nDeep learning methods have traditionally struggled with tabular data, because of the heterogeneity between datasets and the heterogeneity of the raw data itself: Tables contain columns, also called features, with various scales and types (Boolean, categorical, ordinal, integer, floating point), imbalanced or miss...",
      "url": "https://www.nature.com/articles/s41586-024-08328-6"
    },
    {
      "title": "GCondNet: A Novel Method for Improving Neural Networks on Small ...",
      "text": "\\\\doparttoc\\\\faketableofcontents\n\nAndrei Margeloiu am2770@cam.ac.uk\n\nDepartment of Computer Science and Technology\n\nUniversity of Cambridge, UK\nNikola Simidjievski ns779@cam.ac.uk\n\nPrecision Breast Cancer Institute, Department of Oncology, University of Cambridge, UK\n\nDepartment of Computer Science and Technology, University of Cambridge, UK\nPietro Li\u00f2 pl219@cam.ac.uk\n\nDepartment of Computer Science and Technology\n\nUniversity of Cambridge, UK\nMateja Jamnik mj201@cam.ac.uk\n\nDepartment of Computer Science and Technology\n\nUniversity of Cambridge, UK\n\n###### Abstract\n\nNeural networks often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model\u2019s parameters accurately. In such small data scenarios, leveraging additional structures can improve the model\u2019s performance and training stability. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) to extract this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor network. By creating many small graphs, GCondNet exploits the data\u2019s high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate GCondNet\u2019s effectiveness on 12 real-world datasets, where it outperforms 14 standard and state-of-the-art methods. The results show that GCondNet is a versatile framework for injecting graph-regularisation into various types of neural networks, including MLPs and tabular Transformers. Code is available at [https://github.com/andreimargeloiu/GCondNet](https://github.com/andreimargeloiu/GCondNet).\n\n### 1 Introduction\n\nTabular datasets are ubiquitous in scientific fields such as medicine (Meira et\u00a0al., [2001](https://arxiv.org/html/2211.06302v4#bib.bib54); Balendra & Isaacs, [2018](https://arxiv.org/html/2211.06302v4#bib.bib6); Kelly & Semsarian, [2009](https://arxiv.org/html/2211.06302v4#bib.bib40)), physics (Baldi et\u00a0al., [2014](https://arxiv.org/html/2211.06302v4#bib.bib5); Kasieczka et\u00a0al., [2021](https://arxiv.org/html/2211.06302v4#bib.bib35)), and chemistry (Zhai et\u00a0al., [2021](https://arxiv.org/html/2211.06302v4#bib.bib86); Keith et\u00a0al., [2021](https://arxiv.org/html/2211.06302v4#bib.bib39)). These datasets often have a limited number of samples but a large number of features for each sample. This is because collecting many samples is often costly or infeasible, but collecting many features for each sample is relatively easy. For example, in medicine (Schaefer et\u00a0al., [2020](https://arxiv.org/html/2211.06302v4#bib.bib65); Yang et\u00a0al., [2012](https://arxiv.org/html/2211.06302v4#bib.bib81); Gao et\u00a0al., [2015](https://arxiv.org/html/2211.06302v4#bib.bib22); Iorio et\u00a0al., [2016](https://arxiv.org/html/2211.06302v4#bib.bib33); Garnett et\u00a0al., [2012](https://arxiv.org/html/2211.06302v4#bib.bib23); Bajwa et\u00a0al., [2016](https://arxiv.org/html/2211.06302v4#bib.bib4); Curtis et\u00a0al., [2012](https://arxiv.org/html/2211.06302v4#bib.bib15); Tomczak et\u00a0al., [2015](https://arxiv.org/html/2211.06302v4#bib.bib74)), clinical trials targeting rare diseases often enrol only a few hundred patients at most. Despite the small number of participants, it is common to gather extensive data on each individual, such as measuring thousands of gene expression patterns. This practice results in small-size datasets that are high-dimensional, with the number of features () greatly exceeding the number of samples (). Making effective inferences from such datasets is vital for advancing research in scientific fields.\n\nWhen faced with high-dimensional tabular data, neural network models struggle to achieve strong performance (Liu et\u00a0al., [2017](https://arxiv.org/html/2211.06302v4#bib.bib49); Feng & Simon, [2017](https://arxiv.org/html/2211.06302v4#bib.bib19)), partly because they encounter increased degrees of freedom, which results in overfitting, particularly in scenarios involving small datasets. Despite transfer learning\u2019s success in image and language tasks (Tan et\u00a0al., [2018](https://arxiv.org/html/2211.06302v4#bib.bib71)), a general transfer learning protocol is lacking for tabular data (Borisov et\u00a0al., [2022](https://arxiv.org/html/2211.06302v4#bib.bib11)), and current methods assume shared features (Levin et\u00a0al., [2023](https://arxiv.org/html/2211.06302v4#bib.bib45)) or large upstream datasets (Wang & Sun, [2022](https://arxiv.org/html/2211.06302v4#bib.bib75); Nam et\u00a0al., [2022](https://arxiv.org/html/2211.06302v4#bib.bib56)), which is unsuitable for our scenarios. Consequently, we focus on improving training neural networks from scratch.\n\nPrevious approaches for training models on small sample-size and high-dimensional data constrained the model\u2019s parameters to ensure that similar features have similar coefficients, as initially proposed in\u00a0(Li & Li, [2008](https://arxiv.org/html/2211.06302v4#bib.bib46)) for linear regression and later extended to neural networks (Ruiz et\u00a0al., [2023](https://arxiv.org/html/2211.06302v4#bib.bib63)). For applications in biomedical domains, such constraints can lead to more interpretable identification of genes (features) that are biologically relevant (Li & Li, [2008](https://arxiv.org/html/2211.06302v4#bib.bib46)). However, these methods require access to external application-specific knowledge graphs (e.g., gene regulatory networks) to obtain feature similarities, which provide \u201cexplicit relationships\u201d between features. But numerous tasks do not have access to such application-specific graphs. We aim to integrate a similar inductive bias, posing that performance is enhanced when similar features have similar coefficients. We accomplish this without relying on \u201cexplicit relationships\u201d defined in external application-specific graphs.\n\nWe propose a novel method (Graph-Conditioned Networks) to enhance the performance of various neural network predictors, such as Multi-layer Perceptrons (MLPs). The key innovation of lies in leveraging the \u201cimplicit relationships\u201d between samples by performing \u201csoft parameter-sharing\u201d to constrain the model\u2019s parameters in a principled manner, thereby reducing overfitting. Prior work has shown that such relationships between samples can be beneficial (Fatemi et\u00a0al., [2021](https://arxiv.org/html/2211.06302v4#bib.bib18); Kazi et\u00a0al., [2022](https://arxiv.org/html/2211.06302v4#bib.bib37); Zhou et\u00a0al., [2022](https://arxiv.org/html/2211.06302v4#bib.bib87)). These methods, however, typically generate and operate with one graph between samples while relying on additional dataset-specific assumptions such as the smoothness assumption (for extended discussion see Section\u00a0[4](https://arxiv.org/html/2211.06302v4#S4)). In contrast, we leverage _sample-wise multiplex graphs_, a novel and general approach to identify and use these potential relationships between samples by constructing many graphs between samples, one for each feature. We then use Graph Neural Networks (GNNs) to extract any implicit structure and condition the parameters of the first layer of an underlying predictor MLP network. Note that still considers the samples as independent and identically distributed (IID) at both train-time and test-time because the information from the graphs is encapsulated within the model parameters and is not used directly for prediction (see Section\u00a0[2.2](https://arxiv.org/html/2211.06302v4#S2.SS2)).\n\nWe introduce two similarity-based approaches for constructing the sample-wise multiplex graphs from any tabular dataset. Both approaches generate a graph for each feature in the dataset (resulting in graphs), with each node representing a sample (totalling nodes per graph). For instance, in a gene expression dataset, we create a un...",
      "url": "https://arxiv.org/html/2211.06302v4"
    },
    {
      "title": "TabPFN Opens New Avenues for Small-Data Tabular Learning in Drug Discovery",
      "text": "TabPFN Opens New Avenues for Small-Data Tabular Learning in Drug Discovery | Biological and Medicinal Chemistry | ChemRxiv | Cambridge Open Engage\n[![Cambridge Open Engage home](https://chemrxiv.org/engage/_nuxt/img/OpenEngageWhiteLogoWithText.0047d13.svg)](https://chemrxiv.org/engage/coe/public-dashboard)\n[What is Cambridge Open Engage?](https://chemrxiv.org/engage/coe/contact-information?show=faqs)\n[![ChemRxiv Home](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/chemrxiv/rgb.svg)](https://chemrxiv.org/engage/chemrxiv/public-dashboard)\n[**How to Submit**](https://chemrxiv.org/engage/chemrxiv/submission-information)\n[**Browse**](https://chemrxiv.org/engage/chemrxiv/browse-dashboard)\n[**About**](https://chemrxiv.org/engage/chemrxiv/about-information)\n[\n**News**[opens in a new tab]\n](https://connect.acspubs.org/chemrxiv)\nLog in\n[Back toBiological and Medicinal Chemistry](https://chemrxiv.org/engage/chemrxiv/category-dashboard/605c72ef153207001f6470d0)\nSearch within Biological and Medicinal Chemistry\n[](#)\n![RSS feed for Biological and Medicinal Chemistry](https://chemrxiv.org/engage/assets/public/chemrxiv/social/rss.svg)\n# TabPFN Opens New Avenues for Small-Data Tabular Learning in Drug Discovery\n29 September 2025, Version 1\nWorking Paper\n## Authors\n* [Woruo Chen](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Woruo%20Chen),\n* [Yao Tian](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Yao%20Tian),\n* [Youchao Deng](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Youchao%20Deng),\n* [Dejun Jiang](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Dejun%20Jiang),\n* [Dongsheng Cao](https://chemrxiv.org/engage/chemrxiv/search-dashboard?authors=Dongsheng%20Cao)[![Author ORCID: We display the ORCID iD icon alongside authors names on our website to acknowledge that the ORCiD has been authenticated when entered by the user. To view the users ORCiD record click the icon. [opens in a new tab]](https://chemrxiv.org/engage/assets/public/chemrxiv/images/logos/orcid.png)](https://orcid.org/0000-0003-3604-3785)\n[Show author details](#)\n![](https://chemrxiv.org/engage/_nuxt/img/NonPeerReviewed.5753084.svg)This content is a preprint and has not undergone peer review at the time of posting.\nDownload\nCite\nComment\n## Abstract\nEarly-stage drug discovery often suffers from data scarcity and out-of-distribution (OOD) shifts, which constrain the reliability of predictive models. While deep learning has advanced representation learning from molecular and biological data, tabular modeling remains indispensable, particularly in small-sample and OOD scenarios. For over a decade, gradient-boosted decision trees (GBDTs) such as XGBoost have been the dominant choice, yet their robustness is limited under such conditions. TabPFN, a recently introduced transformer-based tabular foundation model, enables accurate predictions on small datasets without task-specific retraining. Applying TabPFN to a variety of molecular data sets, we find that TabPFN performs on par with XGBoost in classification, but demonstrates clear and stable advantages in regression, with its strongest gains on small and medium datasets and under OOD evaluations. Feature and data ablations (10\u201390%) further highlight its robustness, as performance degrades gracefully and exhibits minimal sensitivity compared with tree ensembles. On quantum tasks, TabPFN shows competitive accuracy on QM7 but is challenged by the larger QM8 dataset, where tree ensembles regain strength. Beyond metrics, embedding analyses indicate smoother structure\u2013property relationships of TabPFN and enhanced class separability, reflecting beneficial inductive biases rather than overfitting. Collectively, these findings demonstrate that TabPFN offers a robust and data-efficient alternative for tabular learning in drug discovery, shedding new light on predictive modeling under \u001f\u001fsmall-data and OOD challenges.\n## Keywords\n[Tabular Prior-Fitted Network (TabPFN)](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Tabular%20Prior-Fitted%20Network%20(TabPFN))\n[Molecular property prediction](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Molecular%20property%20prediction)\n[Out-of-distribution generalization](https://chemrxiv.org/engage/chemrxiv/search-dashboard?keywords=Out-of-distribution%20generalization)\n## Supplementary materials\n**Title**\n**Description**\n**Actions**\n**Title**\nFigure 1. Benchmarking TabPFN against conventional ML models across ADMET tasks.\n**Description**\n(A) Pairwise AUROC comparison with XGBoost on classification datasets stratified by sample size (&lt;2,000; 2,000\u20134,000; 4,000\u20138,000). Green markers indicate datasets where TabPFN performs significantly better. (B) Normalized AUROC aggregated across all classification datasets, showing TabPFN\u2019s consistently high performance relative to SVM, RF, and XGBoost. (C) Normalized R\u00b2 scores across regression datasets, highlighting TabPFN\u2019s superiority. (D) Pairwise regression comparison with XGBoost, where TabPFN shows significant improvements on smaller datasets (Wilcoxon p = 0.0020). denotes the per-dataset metric difference between models (e.g., , ). We use an equivalence margin : gray = tie, green = (TabPFN better), blue = (XGBoost better)\n**Actions**\n**Download(10 MB)**\n**Title**\nFigure 2. Robustness of TabPFN across different molecular representations.\n**Description**\n(A) Average R\u00b2 on 10 regression datasets using RDKit 2D descriptors (\u25b2) or RDKit 2D+MACCS fingerprints (\u25cf). (B) Average AUROC on 24 classification datasets under the same feature settings. Bars represent model means; stars indicate the best-performing model for each dataset.\n**Actions**\n**Download(34 MB)**\n**Title**\nFigure 3. Out-of-distribution generalization of TabPFN under cluster-based splits.\n**Description**\n(A) Pairwise regression performance (R\u00b2) under random vs. cluster-based splits. Triangles denote random splits, stars denote OOD splits. (B) Aggregated normalized MCC scores for classification tasks under cluster-based splits. (C) Pairwise MCC comparison between TabPFN and XGBoost across classification datasets, showing significant gains for TabPFN (Wilcoxon p = 0.0149). (D) Pairwise regression comparison (R\u00b2) under cluster-based splits, confirming TabPFN\u2019s significant advantage (Wilcoxon p = 0.0020).\n**Actions**\n**Download(21 MB)**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\nFigure 4. Model performance under systematic feature ablation across classification and regression tasks. Comparative analysis of TabPFN, XGBoost, RF, and SVM under progressive feature removal (10-90%) across pharmaceutical datasets.\n**Description**\n(A) Circular heatmap visualization of classification performance (AUROC) across drug property datasets. Concentric rings represent increasing feature ablation rates from inner (10%) to outer rings (90%), with color intensity indicating performance level (red: high, blue: low). (B) Best model distribution stratified by sample size (top) and class imbalance ratio (bottom), showing frequency of optimal performance across ablation scenarios. Bar patterns indicate performance metric. (C) Performance density plot mapping model superiority (colored points) across feature dimensionality (y-axis) and dataset size (x-axis). Point size indicates performance quality, with larger points representing better performance for that metric. (D) Regression performance metrics (R\u00b2, RMSE, MAE, Pearson's R) across ablation rates for four representative datasets of increasing size. (E) Best model distribution for regression tasks stratified by sample size ranges, showing frequency of optimal performance across evaluation metrics and ablation rates, analogous to panel B but for regression tasks.\n**Actions**\n**Download(2 MB)**\n**Title**\n![](https://chemrxiv.org/engage/_nuxt/img/pdfIcon.1fd0100.svg)\nFigure 5. Model performance under systematic data ablation across classification and regression tasks. ...",
      "url": "https://chemrxiv.org/engage/chemrxiv/article-details/68d29b1cf2aff1677025b18f"
    },
    {
      "title": "Computer Science > Machine Learning",
      "text": "[2207.01848] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2207.01848\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2207.01848**(cs)\n[Submitted on 5 Jul 2022 ([v1](https://arxiv.org/abs/2207.01848v1)), last revised 16 Sep 2023 (this version, v6)]\n# Title:TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\nAuthors:[Noah Hollmann](https://arxiv.org/search/cs?searchtype=author&amp;query=Hollmann,+N),[Samuel M\u00fcller](https://arxiv.org/search/cs?searchtype=author&amp;query=M\u00fcller,+S),[Katharina Eggensperger](https://arxiv.org/search/cs?searchtype=author&amp;query=Eggensperger,+K),[Frank Hutter](https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter,+F)\nView a PDF of the paper titled TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second, by Noah Hollmann and 3 other authors\n[View PDF](https://arxiv.org/pdf/2207.01848)> > Abstract:\n> We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230$\\times$ speedup. This increases to a 5 700$\\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at [> this https URL\n](https://github.com/automl/TabPFN)> . Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2207.01848](https://arxiv.org/abs/2207.01848)[cs.LG]|\n|(or[arXiv:2207.01848v6](https://arxiv.org/abs/2207.01848v6)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2207.01848](https://doi.org/10.48550/arXiv.2207.01848)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Noah Hollmann [[view email](https://arxiv.org/show-email/a3a3339c/2207.01848)]\n**[[v1]](https://arxiv.org/abs/2207.01848v1)**Tue, 5 Jul 2022 07:17:43 UTC (1,473 KB)\n**[[v2]](https://arxiv.org/abs/2207.01848v2)**Sat, 1 Oct 2022 08:53:17 UTC (2,132 KB)\n**[[v3]](https://arxiv.org/abs/2207.01848v3)**Wed, 12 Oct 2022 07:08:27 UTC (2,132 KB)\n**[[v4]](https://arxiv.org/abs/2207.01848v4)**Tue, 29 Nov 2022 18:31:44 UTC (2,228 KB)\n**[[v5]](https://arxiv.org/abs/2207.01848v5)**Sun, 7 May 2023 19:43:41 UTC (4,339 KB)\n**[v6]**Sat, 16 Sep 2023 09:33:32 UTC (4,340 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second, by Noah Hollmann and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2207.01848)\n* [TeX Source](https://arxiv.org/src/2207.01848)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2207.01848&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2207.01848&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-07](https://arxiv.org/list/cs.LG/2022-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2207.01848?context=cs)\n[stat](https://arxiv.org/abs/2207.01848?context=stat)\n[stat.ML](https://arxiv.org/abs/2207.01848?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.01848)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.01848)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.01848)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](<http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2207.01848&amp;description=TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second>)[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](<https://reddit.com/submit?url=https://arxiv.org/abs/2207.01848&amp;title=TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second>)\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv reco...",
      "url": "https://arxiv.org/abs/2207.01848"
    },
    {
      "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
      "text": "# Computer Science > Machine Learning\n\n**arXiv:2305.02997** (cs)\n\n\\[Submitted on 4 May 2023 ( [v1](https://arxiv.org/abs/2305.02997v1)), last revised 15 Jul 2024 (this version, v4)\\]\n\n# Title:When Do Neural Nets Outperform Boosted Trees on Tabular Data?\n\nAuthors: [Duncan McElfresh](https://arxiv.org/search/cs?searchtype=author&query=McElfresh,+D), [Sujay Khandagale](https://arxiv.org/search/cs?searchtype=author&query=Khandagale,+S), [Jonathan Valverde](https://arxiv.org/search/cs?searchtype=author&query=Valverde,+J), [Vishak Prasad C](https://arxiv.org/search/cs?searchtype=author&query=C,+V+P), [Benjamin Feuer](https://arxiv.org/search/cs?searchtype=author&query=Feuer,+B), [Chinmay Hegde](https://arxiv.org/search/cs?searchtype=author&query=Hegde,+C), [Ganesh Ramakrishnan](https://arxiv.org/search/cs?searchtype=author&query=Ramakrishnan,+G), [Micah Goldblum](https://arxiv.org/search/cs?searchtype=author&query=Goldblum,+M), [Colin White](https://arxiv.org/search/cs?searchtype=author&query=White,+C)\n\nView a PDF of the paper titled When Do Neural Nets Outperform Boosted Trees on Tabular Data?, by Duncan McElfresh and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2305.02997) [HTML (experimental)](https://arxiv.org/html/2305.02997v4)\n\n> Abstract:Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. A remarkable exception is the recently-proposed prior-data fitted network, TabPFN: although it is effectively limited to training sets of size 3000, we find that it outperforms all other algorithms on average, even when randomly sampling 3000 training datapoints. Next, we analyze dozens of metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at [this https URL](https://github.com/naszilla/tabzilla).\n\n|     |     |\n| --- | --- |\n| Comments: | NeurIPS Datasets and Benchmarks Track 2023 |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2305.02997](https://arxiv.org/abs/2305.02997) \\[cs.LG\\] |\n| (or [arXiv:2305.02997v4](https://arxiv.org/abs/2305.02997v4) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2305.02997](https://doi.org/10.48550/arXiv.2305.02997) <br>Focus to learn more<br>arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Colin White \\[ [view email](https://arxiv.org/show-email/8e86a57a/2305.02997)\\] **[\\[v1\\]](https://arxiv.org/abs/2305.02997v1)**\nThu, 4 May 2023 17:04:41 UTC (245 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2305.02997v2)**\nTue, 17 Oct 2023 21:51:42 UTC (396 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2305.02997v3)**\nMon, 30 Oct 2023 21:24:22 UTC (419 KB)\n**\\[v4\\]**\nMon, 15 Jul 2024 19:00:47 UTC (419 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled When Do Neural Nets Outperform Boosted Trees on Tabular Data?, by Duncan McElfresh and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2305.02997)\n- [HTML (experimental)](https://arxiv.org/html/2305.02997v4)\n- [TeX Source](https://arxiv.org/src/2305.02997)\n- [Other Formats](https://arxiv.org/format/2305.02997)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2305.02997&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2305.02997&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-05](https://arxiv.org/list/cs.LG/2023-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2305.02997?context=cs) [cs.AI](https://arxiv.org/abs/2305.02997?context=cs.AI) [stat](https://arxiv.org/abs/2305.02997?context=stat) [stat.ML](https://arxiv.org/abs/2305.02997?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2305.02997)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2305.02997)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2305.02997)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2305.02997) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax...",
      "url": "https://arxiv.org/abs/2305.02997"
    },
    {
      "title": "Top 7 Main Models to Know for Tabular Data on Kaggle",
      "text": "[All Posts](https://apxml.com/posts)\n\n\u2192\n\n[Top 7 Main Models to Know for Tabular Data on Kaggle](https://apxml.com/posts/main-models-to-know-tabular-data-kaggle)\n\nBy Wei Ming T. on Oct 18, 2024\n\nRecommended Posts\n\n- [5 Essential Machine Learning Models You Should Know](https://apxml.com/posts/essential-machine-learning-models-you-should-know)\n- [What is Feature Engineering? Tips and Tricks for Data Scientists](https://apxml.com/posts/what-is-feature-engineering-tips-tricks)\n- [Guide to All 70+ Scikit-Learn Models and When to Use Them](https://apxml.com/posts/scikit-learn-models-guide)\n- [Top 7 Best Kaggle Datasets for Beginner Data Scientists](https://apxml.com/posts/top-kaggle-datasets-for-beginner-data-scientists)\n- [7 Steps Guide To Kaggle Competition For Machine Learning Beginners](https://apxml.com/posts/getting-started-with-kaggle)\n\nRecommended Courses\n\nRelated to this post\n\n[Introduction to Machine Learning](https://apxml.com/courses/introduction-to-machine-learning) [Fundamentals of Model Evaluation and Metrics](https://apxml.com/courses/basics-model-evaluation-metrics) [Introduction to Data Engineering](https://apxml.com/courses/intro-data-engineering) [Introduction to Feature Engineering](https://apxml.com/courses/intro-feature-engineering) [Mastering Gradient Boosting Algorithms](https://apxml.com/courses/mastering-gradient-boosting-algorithms) [View all courses](https://apxml.com/courses/)\n\nConnect With Us\n\nFollow for updates on AI/ML research and practical tips.\n\n[LinkedIn](https://www.linkedin.com/company/apxml/)",
      "url": "https://apxml.com/posts/main-models-to-know-tabular-data-kaggle"
    },
    {
      "title": "Neural network potentials for chemistry: concepts, applications and ...",
      "text": "[View\u00a0PDF\u00a0Version](https://pubs.rsc.org/en/content/articlepdf/2023/dd/d2dd00102k)[Previous\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00061j)[Next\u00a0Article](https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00088a)\n\nOpen Access Article\nThis Open Access Article is licensed under a [Creative Commons Attribution 3.0 Unported Licence](http://creativecommons.org/licenses/by/3.0/)\n\nDOI:\u00a0[10.1039/D2DD00102K](https://doi.org/10.1039/D2DD00102K)\n(Perspective)\n[Digital Discovery](https://doi.org/10.1039/2635-098X/2022), 2023, **2**, 28-58\n\n# Neural network potentials for chemistry: concepts, applications and prospects\n\nSilvan\nK\u00e4ser\n,\nLuis Itza\nVazquez-Salazar\n,\nMarkus\nMeuwly\n\\* and Kai\nT\u00f6pfer\n\\*\nDepartment of Chemistry, University of Basel, Klingelbergstrasse 80, CH-4056 Basel, Switzerland. E-mail: [m.meuwly@unibas.ch](mailto:m.meuwly@unibas.ch); [kai.toepfer@unibas.ch](mailto:kai.toepfer@unibas.ch)\n\nReceived\n23rd September 2022\n, Accepted 20th December 2022\n\nFirst published on 21st December 2022\n\n## Abstract\n\nArtificial Neural Networks (NN) are already heavily involved in methods and applications for frequent tasks in the field of computational chemistry such as representation of potential energy surfaces (PES) and spectroscopic predictions. This perspective provides an overview of the foundations of neural network-based full-dimensional potential energy surfaces, their architectures, underlying concepts, their representation and applications to chemical systems. Methods for data generation and training procedures for PES construction are discussed and means for error assessment and refinement through transfer learning are presented. A selection of recent results illustrates the latest improvements regarding accuracy of PES representations and system size limitations in dynamics simulations, but also NN application enabling direct prediction of physical results without dynamics simulations. The aim is to provide an overview for the current state-of-the-art NN approaches in computational chemistry and also to point out the current challenges in enhancing reliability and applicability of NN methods on a larger scale.\n\n## 1 Introduction\n\nThe in silico modeling of chemical and biological processes at a molecular level is of central importance in today's research and will be crucial for future challenges of mankind. [1](https://pubs.rsc.org/pubs.rsc.org#cit1) The modeling often requires a trade-off between accuracy and computational cost: quantum chemical calculations (e.g. ab initio molecular dynamics), at a high level of theory, can be very accurate but also come at a high computational cost rendering the approach impractical except for rather small molecules. Empirical force fields, on the other hand, provide a computationally advantageous approach that scales well with system size but the possibility to carry out quantitative studies is limited due to the assumptions underlying their formulation. Thus, computationally efficient and accurate modelling techniques are required for quantitative molecular simulations. [2](https://pubs.rsc.org/pubs.rsc.org#cit2)\n\nIn this regard, Machine Learning (ML) techniques have emerged as a powerful tool to satisfy such demands for force field models which are limited, in principle, by the accuracy of ab initio methods and allow an efficiency approaching that of empirical force fields. [3](https://pubs.rsc.org/pubs.rsc.org#cit3) Motivated by the advances in computational chemistry techniques and the continuous growth of the performance of computer hardware (Moore's law [4](https://pubs.rsc.org/pubs.rsc.org#cit4)), ML is becoming a daily tool for modeling molecules and materials. By definition, ML methods are data-driven algorithms based on statistical learning theory with the aim of generating numerical methods that generalize to new data, not used in the learning process. [5,6](https://pubs.rsc.org/pubs.rsc.org#cit5) This capability renders ML methods highly appealing for modelling molecular systems. It even reaches levels where some authors believe that the use of ML techniques will constitute the \u201cfourth paradigm of science\u201d, [7](https://pubs.rsc.org/pubs.rsc.org#cit7) bridging the gap from atomic-scale molecular properties towards macroscopic properties of materials [8,9](https://pubs.rsc.org/pubs.rsc.org#cit8) and one of the drivers for a revolution of the simulation techniques of matter. [10](https://pubs.rsc.org/pubs.rsc.org#cit10) The enthusiasm is reflected in the appearance of an extensive number of ML models and their application in computational chemistry.\n\nSome of the most important publications have focused on the study of potential energy surfaces (PESs), which contain all the information about the many-body interactions of a molecular system including stable and metastable structures. [11](https://pubs.rsc.org/pubs.rsc.org#cit11) At the same time, it is possible to extract a considerable amount of information from PESs including the atomic forces driving the dynamics of molecular systems, reactions and structural transitions, and atomic vibrations. [12](https://pubs.rsc.org/pubs.rsc.org#cit12) Additionally, it has been proposed that the chemical information contained in a chemical bond, therefore in the PES, can help in the exploration of chemical space. [13](https://pubs.rsc.org/pubs.rsc.org#cit13) In a recent work, [14](https://pubs.rsc.org/pubs.rsc.org#cit14) it was found that the exploration of chemical space can be improved by adding adequate information from the configurational space represented by the PES.\n\nOver the past several decades several ML-based methods have been used to represent continuous PESs. [3,15\u201317](https://pubs.rsc.org/pubs.rsc.org#cit3) While a number of those are briefly mentioned below, the focus of the present work is on NN-based approaches. Kernel-based methods provide an efficient solution to highly non-linear optimization problems [17](https://pubs.rsc.org/pubs.rsc.org#cit17) by finding a representation of the problem which encodes the distribution of the data in a complete, unique and efficient way. [18](https://pubs.rsc.org/pubs.rsc.org#cit18) There is a large number of possible representations of chemical space that can be used in kernel methods. Examples include Coulomb Matrices, [19](https://pubs.rsc.org/pubs.rsc.org#cit19) Bag of Bonds (BoB), [20](https://pubs.rsc.org/pubs.rsc.org#cit20) Histograms of Distance, Angles and Dihedrals (HDAD), [21](https://pubs.rsc.org/pubs.rsc.org#cit21) Spectrum of London and Axilrod\u2013Teller\u2013Muto (SLATM), [22](https://pubs.rsc.org/pubs.rsc.org#cit22) Faber\u2013Christensen\u2013Huang\u2013von Lilienfeld (FCHL) [23](https://pubs.rsc.org/pubs.rsc.org#cit23) and Smooth Overlap of Atomic Positions (SOAP). [24](https://pubs.rsc.org/pubs.rsc.org#cit24) A comprehensive review of representations for kernel and non-kernel methods can be found in ref. [25](https://pubs.rsc.org/pubs.rsc.org#cit25). It should be noted that variations of kernel methods, such as for Gaussian processes [26](https://pubs.rsc.org/pubs.rsc.org#cit26) which assume a Bayesian/probabilistic point of view for the solution of the problem or the reproducing kernel Hilbert space (RKHS) method [27,28](https://pubs.rsc.org/pubs.rsc.org#cit27) which uses polynomials as support functions have been extensively discussed in the literature. While the remainder of the perspective is mainly dedicated to NN-based approaches, many alternative interpolation and representation methods for PES construction exist. These include, e.g. modified Shepard interpolation, [29](https://pubs.rsc.org/pubs.rsc.org#cit29) (interpolative) moving least-squares, [30\u201332](https://pubs.rsc.org/pubs.rsc.org#cit30) permutationally invariant polynomial (PIP) PESs by least-squares fitting, [33](https://pubs.rsc.org/pubs.rsc.org#cit33) or least absolute shrinkage and selection operator (LASSO) constrained least-squares. [34](https://pubs.rsc.org/pubs.rsc.org#cit34) Several of these approaches have been recentl...",
      "url": "https://pubs.rsc.org/en/content/articlehtml/2023/dd/d2dd00102k"
    },
    {
      "title": "10. Neural Networks - Nature of Code",
      "text": "10. Neural Networks / Nature of Code\n[THE NATURE OF CODE](https://natureofcode.com/)BY DANIEL SHIFFMAN\n* [SUPPORT](https://github.com/sponsors/CodingTrain)\n* [GITHUB](https://github.com/nature-of-code/noc-book-2)\n* [![Coding Train&#x27;s logo](https://natureofcode.com/static/codingtrain_logo-53b0a841be45c7eac7a12f88b7bea596.png)CODING TRAIN](https://thecodingtrain.com/)\nOther Options\n# Chapter 10: Neural Networks\n> > The human brain has 100 billion neurons,\n> > each neuron connected to 10 thousand\n> > other neurons. Sitting on your shoulders\n> > is the most complicated object\n> > in the known universe.\n> > > \u2014Michio Kaku\n> > ![]()\n[*Khipu*on display at the Machu Picchu Museum, Cusco, Peru (photo by Pi3.124)](#khipu-on-display-at-the-machu-picchu-museum-cusco-peru-photo-by-pi3124)\nThe*khipu*(or*quipu*) is an ancient Incan device used for recordkeeping and communication. It comprised a complex system of knotted cords to encode and transmit information. Each colored string and knot type and pattern represented specific data, such as census records or calendrical information. Interpreters, known as*quipucamayocs*, acted as a kind of accountant and decoded the stringed narrative into understandable information.\nI began with inanimate objects living in a world of forces, and I gave them desires, autonomy, and the ability to take action according to a system of rules. Next, I allowed those objects, now called*creatures*, to live in a population and evolve over time. Now I\u2019d like to ask, What is each creature\u2019s decision-making process? How can it adjust its choices by learning over time? Can a computational entity process its environment and generate a decision?\nTo answer these questions, I\u2019ll once again look to nature for inspiration\u2014specifically, the human brain. A brain can be described as a biological**neural network**, an interconnected web of neurons transmitting elaborate patterns of electrical signals. Within each neuron, dendrites receive input signals, and based on those inputs, the neuron fires an output signal via an axon (see Figure 10.1). Or something like that. How the human brain actually works is an elaborate and complex mystery, one that I\u2019m certainly not going to attempt to unravel in rigorous detail in this chapter.\n![Figure 10.1: A neuron with dendrites and an axon connected to another neuron]()\nFigure 10.1: A neuron with dendrites and an axon connected to another neuron\nFortunately, as you\u2019ve seen throughout this book, developing engaging animated systems with code doesn\u2019t require scientific rigor or accuracy. Designing a smart rocket isn\u2019t rocket science, and neither is designing an artificial neural network brain science. It\u2019s enough to simply be inspired by the*idea*of brain function.\nIn this chapter, I\u2019ll begin with a conceptual overview of the properties and features of neural networks and build the simplest possible example of one, a network that consists of a single neuron. I\u2019ll then introduce you to more complex neural networks by using the ml5.js library. This will serve as a foundation for[Chapter 11](https://natureofcode.com/neuroevolution#section-neuroevolution), the grand finale of this book, where I\u2019ll combine GAs with neural networks for physics simulation.\n## [Introducing Artificial Neural Networks](#introducing-artificial-neural-networks)\nComputer scientists have long been inspired by the human brain. In 1943, Warren S. McCulloch, a neuroscientist, and Walter Pitts, a logician, developed the first conceptual model of an artificial neural network. In their paper \u201cA Logical Calculus of the Ideas Immanent in Nervous Activity,\u201d they describe a**neuron**as a single computational cell living in a network of cells that receives inputs, processes those inputs, and generates an output.\nTheir work, and the work of many scientists and researchers who followed, wasn\u2019t meant to accurately describe how the biological brain works. Rather, an*artificial*neural network (hereafter referred to as just a*neural network*) was intended as a computational model based on the brain, designed to solve certain kinds of problems that were traditionally difficult for computers.\nSome problems are incredibly simple for a computer to solve but difficult for humans like you and me. Finding the square root of 964,324 is an example. A quick line of code produces the value 982, a number my computer can compute in less than a millisecond, but if you asked me to calculate that number myself, you\u2019d be in for quite a wait. On the other hand, certain problems are incredibly simple for you or me to solve, but not so easy for a computer. Show any toddler a picture of a kitten or puppy, and they\u2019ll quickly be able to tell you which one is which. Listen to a conversation in a noisy caf\u00e9 and focus on just one person\u2019s voice, and you can effortlessly comprehend their words. But need a machine to perform one of these tasks? Scientists have spent entire careers researching and implementing complex solutions, and neural networks are one of them.\nHere are some of the easy-for-a-human, difficult-for-a-machine applications of neural networks in software today:\n* **Pattern recognition:**Neural networks are well suited to problems when the aim is to detect, interpret, and classify features or patterns within a dataset. This includes everything from identifying objects (like faces) in images, to optical character recognition, to more complex tasks like gesture recognition.\n* **Time-series prediction and anomaly detection:**Neural networks are utilized both in forecasting, such as predicting stock market trends or weather patterns, and in recognizing anomalies, which can be applied to areas like cyberattack detection and fraud prevention.\n* **Control and adaptive decision-making systems:**These applications range from autonomous vehicles like self-driving cars and drones to adaptive decision-making used in game playing, pricing models, and recommendation systems on media platforms.\n* **Signal processing and soft sensors:**Neural networks play a crucial role in devices like cochlear implants and hearing aids by filtering noise and amplifying essential sounds. They\u2019re also involved in*soft sensors*, software systems that process data from multiple sources to give a comprehensive analysis of the environment.\n* **Natural language processing (NLP):**One of the biggest developments in recent years has been the use of neural networks for processing and understanding human language. They\u2019re used in various tasks including machine translation, sentiment analysis, and text summarization, and are the underlying technology behind many digital assistants and chatbots.\n* **Generative models:**The rise of novel neural network architectures has made it possible to generate new content. These systems can synthesize images, enhance image resolution, transfer style between images, and even generate music and video.\nCovering the full gamut of applications for neural networks would merit an entire book (or series of books), and by the time that book was printed, it would probably be out of date. Hopefully, this list gives you an overall sense of the features and possibilities.\n### [How Neural Networks Work](#how-neural-networks-work)\nIn some ways, neural networks are quite different from other computer programs. The computational systems I\u2019ve been writing so far in this book are**procedural**: a program starts at the first line of code, executes it, and goes on to the next, following instructions in a linear fashion. By contrast, a true neural network doesn\u2019t follow a linear path. Instead, information is processed collectively, in parallel, throughout a network of nodes, with each node representing a neuron. In this sense, a neural network is considered a**connectionist**system.\nIn other ways, neural networks aren\u2019t so different from some of the programs you\u2019ve seen. A neural network exhibits all the hallmarks of a complex system, much like a cellular automaton or a flock of boids. Remember how...",
      "url": "https://natureofcode.com/neural-networks"
    }
  ]
}