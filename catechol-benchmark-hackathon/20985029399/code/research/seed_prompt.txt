# Catechol Reaction Yield Prediction - Evolved Seed Prompt (Loop 1)

## Current Status
- Best CV score: 0.08819 from exp_000 (Physics-Informed MLP Baseline)
- Best LB score: Not yet submitted
- CV-LB gap: Unknown (no submissions yet)
- Target: 0.047400 (need ~47% reduction in RMSE)

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** - The baseline implementation is sound with proper validation methodology.

**Evaluator's top priority: Try TabPFN** - I AGREE. TabPFN is specifically designed for small tabular datasets and has shown strong performance on chemistry tasks. This is the highest-leverage experiment to try next.

**Key concerns raised:**
1. Gap to target is large (need ~47% reduction) - Addressed by exploring fundamentally different architectures
2. Limited feature exploration (only Spange descriptors) - Will explore DRFP/fragprints with dimensionality reduction
3. No architecture diversity (only MLP) - TabPFN represents a fundamentally different approach
4. Submission template compliance - Will ensure model class is compatible with template structure

## Data Understanding

**Reference notebooks:**
- `exploration/eda.ipynb` - Full EDA with data shapes, feature distributions
- `research/kernels/sanidhyavijay24_arrhenius-kinetics-tta-0-09831/` - Best public kernel approach

**Key patterns to exploit:**
1. **Arrhenius kinetics** - 1/T, ln(t), 1/T*ln(t) features capture temperature-dependent reaction rates
2. **Solvent symmetry** - For mixed solvents, (A,B) = (B,A) chemically → TTA for free performance
3. **Small dataset** - 656 + 1227 samples → TabPFN's sweet spot
4. **Multiple descriptor sets** - Spange (13), DRFP (2048), fragprints (2133) available

## CRITICAL SUBMISSION CONSTRAINTS

**MANDATORY STRUCTURE:**
- The submission MUST follow the template notebook structure
- Only the last 3 cells can be modified, and ONLY the `model = MLPModel()` line can be changed
- Model must implement: `train_model(X_train, y_train)` and `predict(X_test)` methods
- Two tasks: single_solvent (leave-one-solvent-out CV, 24 folds) and full (leave-one-ramp-out CV, 13 folds)
- Target order in Y: ["Product 2", "Product 3", "SM"]

## Recommended Approaches (Priority Order)

### 1. TabPFN Regressor (HIGHEST PRIORITY)
**Why:** TabPFN is a transformer-based foundation model specifically designed for small tabular datasets. Published in Nature 2025, it "consistently beats standard MLPs and gradient-boosted trees on low-sample drug-discovery benchmarks."

**Implementation:**
```python
from tabpfn import TabPFNRegressor

class TabPFNModel:
    def __init__(self, data='single'):
        self.data_type = data
        self.featurizer = KineticFeaturizer(mixed=(data=='full'))
        self.models = []  # One per target
        
    def train_model(self, X_train, y_train):
        X_feats = self.featurizer.featurize(X_train).numpy()
        for i in range(3):  # 3 targets
            model = TabPFNRegressor()
            model.fit(X_feats, y_train.values[:, i])
            self.models.append(model)
    
    def predict(self, X_test):
        X_feats = self.featurizer.featurize(X_test).numpy()
        preds = np.column_stack([m.predict(X_feats) for m in self.models])
        return torch.tensor(preds)
```

**Notes:**
- Use same Arrhenius kinetics features + Spange descriptors
- No hyperparameter tuning needed
- May need to handle TTA separately for mixed solvents
- Install with: pip install tabpfn

### 2. Ensemble of Diverse Models
**Why:** The "mixall" kernel shows that combining MLP + XGBoost + RandomForest + LightGBM with weighted averaging can be effective.

**Implementation:**
- Train MLP, XGBoost, LightGBM, RandomForest on same features
- Use weighted averaging (optimize weights with Optuna if time permits)
- Weights normalized to sum to 1

### 3. Feature Expansion with Dimensionality Reduction
**Why:** DRFP (2048 features) and fragprints (2133 features) contain rich molecular information that Spange (13 features) may miss.

**Implementation:**
```python
# Load high-dimensional features
drfp = load_features('drfps_catechol')  # 2048 features
fragprints = load_features('fragprints')  # 2133 features

# Apply PCA to reduce dimensionality
from sklearn.decomposition import PCA
pca_drfp = PCA(n_components=50)
pca_frag = PCA(n_components=50)

# Combine with Spange + kinetic features
combined_features = np.hstack([
    kinetic_features,  # 5 features
    spange_features,   # 13 features
    pca_drfp.fit_transform(drfp_features),  # 50 features
    pca_frag.fit_transform(frag_features),  # 50 features
])
```

### 4. Per-Target Specialized Models
**Why:** SM (starting material) may have different dynamics than Products. Different models for different targets could improve predictions.

### 5. LightGBM with Per-Target Optimization
**Why:** LightGBM kernel shows best fold MSE=0.0010 for single solvent, MSE=0.0040 for full data. Per-target regressors with early stopping.

## What NOT to Try

1. **More MLP hyperparameter tuning** - Incremental improvements won't close the 47% gap
2. **Simple feature scaling changes** - Already using StandardScaler/BatchNorm
3. **Different loss functions alone** - HuberLoss already used, not the bottleneck

## Validation Notes

**CV Scheme (MUST FOLLOW):**
- Single solvent: Leave-one-solvent-out CV (24 folds)
- Full data: Leave-one-ramp-out CV (13 folds)

**Template Compliance:**
- Model must have `train_model(X_train, y_train)` and `predict(X_test)` methods
- Last 3 cells of notebook must match template exactly
- Only `model = ModelClass()` line can be changed

## Experiment Priority Queue

1. **exp_001: TabPFN Regressor** - Highest leverage, no tuning needed
2. **exp_002: TabPFN + TTA** - Add TTA for mixed solvents to TabPFN
3. **exp_003: Ensemble (TabPFN + MLP + LightGBM)** - Combine best approaches
4. **exp_004: Feature expansion with DRFP/fragprints** - Richer molecular representation
5. **exp_005: Per-target specialized models** - Different architectures per target

## Key Insights from Research

1. **TabPFN excels on small data** - Published in Nature 2025, specifically designed for <10K samples
2. **Ensemble diversity matters** - Combining MLP + tree-based models improves robustness
3. **Physics-informed features help** - Arrhenius kinetics features validated in baseline
4. **TTA is free performance** - Exploiting solvent symmetry reduces variance
5. **Per-target LightGBM** - Separate models per target with early stopping showed best fold scores

## Target Score
Beat **0.047400** (lower is better - RMSE)

Current best: 0.08819 (need ~47% reduction)
The target IS reachable - we haven't found the right model family yet.