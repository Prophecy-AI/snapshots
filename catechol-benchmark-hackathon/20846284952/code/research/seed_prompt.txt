## Current Status
- Best CV score: 0.008785 from exp_011 (diverse ensemble)
- Best LB score: 0.0913 from exp_012 (MLP[32,16] + LightGBM 0.6/0.4)
- Target: 0.0333
- Gap to target: 2.74x (LB 0.0913 vs target 0.0333)
- CV-LB relationship: LB = 4.05*CV + 0.0551 (RÂ²=0.948)
- Submissions remaining: 4

## Response to Evaluator

**Technical verdict**: TRUSTWORTHY. Both attention model and fragprints experiments executed correctly with verified scores.

**Evaluator's top priority**: Try ACS PCA features + per-target models. I AGREE - these are the most promising unexplored approaches.

**Key concerns raised**:
1. ACS PCA descriptors (5 features) still unexplored - ADDRESSING THIS FIRST
2. Per-target models unexplored - WILL TRY AFTER ACS PCA
3. Stacking instead of weighted averaging - WILL TRY IF TIME PERMITS
4. LightGBM hyperparameter tuning - LOWER PRIORITY

**My synthesis**: The evaluator is correct that we should exhaust all reasonable tabular approaches. The attention model failure (159% worse) confirms that simple attention on tabular features doesn't help. The fragprints failure (8.28% worse) confirms DRFP is superior. Now we must try the remaining unexplored approaches.

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop22_analysis.ipynb`: ACS PCA analysis
- `experiments/012_simple_ensemble/simple_ensemble.ipynb`: Best LB model code

Key findings from analysis:
1. **ACS PCA features (5 features)**: Cover all 24 solvents. PC5 has lowest correlation with Spange (0.305) - provides NEW information. PC1-PC4 have higher correlations (0.6-0.94) but still add value.
2. **Target correlations**: Product 2 and Product 3 highly correlated (0.923), both negatively correlated with SM (-0.89, -0.77). SM has different distribution (mean 0.52) vs products (mean ~0.13). Per-target models could exploit these differences.
3. **CV-LB gap**: Linear fit shows LB = 4.05*CV + 0.0551. Target 0.0333 requires negative CV - mathematically impossible with current approach. BUT we must try all remaining approaches before concluding.

## Recommended Approaches

**PRIORITY 1: ACS PCA Features (Quick experiment ~1.5 hours)**
Add ACS PCA (5 features) to current best feature set:
- Current: Spange (13) + DRFP (122) + Arrhenius (5) = 140 features
- New: Spange (13) + DRFP (122) + Arrhenius (5) + ACS PCA (5) = 145 features

Implementation:
```python
# Load ACS PCA
ACS_PCA_DF = pd.read_csv(f'{DATA_PATH}/acs_pca_descriptors_lookup.csv', index_col=0)

# In featurize():
X_acs_pca = ACS_PCA_DF.loc[X["SOLVENT NAME"]].values
return np.hstack([X_kinetic, X_spange, X_drfp, X_acs_pca])
```

**PRIORITY 2: Per-Target Models (If ACS PCA doesn't help)**
Train separate MLP[32,16] + LightGBM ensembles for each target:
- Model for Product 2
- Model for Product 3
- Model for SM (different distribution - may need different architecture)

Competition rules explicitly allow: "different hyper-parameters for different objectives (e.g., for SM vs Product 1)"

**PRIORITY 3: Non-linear Mixture Encoding**
From kernel analysis, try non-linear mixing for mixtures:
```python
# Current: linear interpolation
X_feat = A * (1 - pct) + B * pct

# Try: non-linear mixing with interaction term
X_feat = A * (1 - pct) + B * pct + 0.05 * A * B * pct * (1 - pct)
```

**PRIORITY 4: Polynomial Features on Kinetic Variables**
Add polynomial features:
```python
# Current kinetic features: time, temp, 1/T, log(t), interaction
# Add: time^2, temp^2, sqrt(time), sqrt(temp)
```

**PRIORITY 5: Stacking Meta-Learner**
Instead of fixed weights (0.6 MLP, 0.4 LGBM), train Ridge regression on out-of-fold predictions.

## What NOT to Try

1. **Attention mechanisms on tabular features** - EXHAUSTED. exp_021 showed 159% worse performance.
2. **Fragprints instead of DRFP** - EXHAUSTED. exp_020 showed 8.28% worse performance.
3. **Deep residual networks** - EXHAUSTED. exp_004 showed 5x worse performance.
4. **Very large ensembles (15+ models)** - EXHAUSTED. exp_005 showed only 0.7% improvement over 5 models.
5. **Single-layer networks** - EXHAUSTED. exp_010 showed [16] is too simple.

## Validation Notes

- Use leave-one-solvent-out CV for single solvents (24 folds)
- Use leave-one-ramp-out CV for mixtures (13 folds)
- Weighted average of single and full data MSE
- TTA for mixtures (average both orderings)
- CV-LB gap is ~10x - don't expect LB to match CV

## Decision Threshold

- If CV improves by >10% (CV < 0.0079), consider submitting to LB
- If CV doesn't improve significantly, try next priority approach
- We have 4 submissions remaining - use them wisely for calibration

## Template Compliance

CRITICAL: All experiments must follow the competition template structure:
- Last 3 cells must match template exactly
- Only allowed change: `model = MLPModel()` line can be replaced with new model definition
- Same hyperparameters across all folds (unless explainable rationale)
