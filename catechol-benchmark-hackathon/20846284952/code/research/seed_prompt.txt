## Current Status
- Best CV score: 0.009004 from exp_012 (MLP[32,16] + LightGBM ensemble)
- Best LB score: 0.0913 from exp_012
- CV-LB gap: ~10x (LB = 4.05*CV + 0.0551)
- Target: 0.0333
- Submissions remaining: 4

## Response to Evaluator

**Technical verdict was TRUSTWORTHY** for completed experiments. The evaluator correctly noted:
1. exp_017 (attention model) was NOT EXECUTED - this is a loose end
2. Per-target models haven't been tried
3. The "unreachable" conclusion may be premature

**Evaluator's top priority**: Execute the attention model and explore per-target models.

**My response**: I AGREE with the evaluator. The mathematical analysis showing "unreachable" is based on:
- Only 8 data points from similar tabular approaches
- 95% CI for intercept is [-3.22, 3.33] - HUGE uncertainty
- A fundamentally different approach might have a different CV-LB relationship

**Key insight**: ALL 8 submissions used similar approaches (MLP/LightGBM with Spange+DRFP+Arrhenius). We have NOT tried:
- Fragprints features (144 non-zero variance features)
- ACS PCA features (5 features)
- Per-target models
- Attention mechanisms

## Data Understanding

Reference notebooks:
- `exploration/evolver_loop20_analysis.ipynb`: Latest analysis showing unexplored approaches
- `experiments/012_simple_ensemble/simple_ensemble.ipynb`: Best submission (LB 0.0913)
- `experiments/017_attention_model/attention_model.ipynb`: Attention model (NOT EXECUTED)

Available features NOT yet tried:
- **fragprints_lookup.csv**: 2133 features, 144 with non-zero variance (different from DRFP!)
- **acs_pca_descriptors_lookup.csv**: 5 PCA features from ACS Green Chemistry

Key patterns:
1. Simpler models generalize better ([32,16] > [256,128,64])
2. 2-model ensemble (MLP+LightGBM) is optimal
3. Combined features (Spange + DRFP + Arrhenius) work best
4. CV-LB gap is ~10x but based on limited data

## Recommended Approaches

**PRIORITY 1: Try Fragprints Features (HIGH POTENTIAL)**

Fragprints capture DIFFERENT structural information than DRFP:
- DRFP: Differential Reaction Fingerprints (reaction-level, 122 high-variance features)
- Fragprints: Fragment-based fingerprints (substructure-level, 144 high-variance features)

Experiment:
1. Load fragprints features
2. Apply variance-based feature selection (keep features with variance > 0)
3. Combine with Spange + Arrhenius kinetics (like exp_012)
4. Train MLP[32,16] + LightGBM ensemble (same as exp_012)
5. Compare CV to exp_012 (0.009004)

**PRIORITY 2: Per-Target Models (MEDIUM POTENTIAL)**

The competition explicitly allows different hyperparameters for different objectives:
- Product 2, Product 3, SM might have different optimal features/architecture
- Train separate models for each target
- This is allowed per competition rules

Experiment:
1. Train separate MLP[32,16] + LightGBM ensembles for each target
2. Use same features (Spange + DRFP + Arrhenius)
3. Compare CV to exp_012

**PRIORITY 3: Execute Attention Model (CLOSES LOOSE END)**

The attention model (exp_017) is set up but NOT executed:
- Code is complete in `/home/code/experiments/017_attention_model/attention_model.ipynb`
- Just need to run cells 7-9
- Expected outcome: CV similar to or worse than exp_012 (but we should know definitively)

**PRIORITY 4: Combined Fragprints + DRFP Features**

If fragprints alone doesn't help, try combining:
- Spange (13) + DRFP high-variance (122) + Fragprints high-variance (144) + Arrhenius (5)
- Total: ~284 features
- May capture complementary structural information

## What NOT to Try

1. **More ensemble variations** - Diminishing returns (3-model was worse than 2-model)
2. **Deeper architectures** - Already proven worse (exp_004 failed)
3. **More hyperparameter tuning** - Weight optimization showed 0.07% difference (noise)
4. **PCA on features** - Already proven worse than variance-based selection

## Validation Notes

- CV scheme: Leave-one-solvent-out (24 folds) + Leave-one-ramp-out (13 folds)
- CV-LB relationship: LB = 4.05*CV + 0.0551 (RÂ²=0.95) BUT based on only 8 points
- The 95% CI for intercept is [-3.22, 3.33] - there is significant uncertainty

## Submission Strategy

**Threshold for submission**: Only submit if CV improves by >10% (CV < 0.0081)

If fragprints or per-target models show significant CV improvement, submit to verify LB.

## Template Compliance

CRITICAL: All experiments must follow the competition template structure:
- Last 3 cells must match template exactly
- Only the model definition line can change
- Use the provided CV evaluation framework

## Key Achievement Summary

| Metric | Value | Context |
|--------|-------|---------|
| Our Best LB | 0.0913 | exp_012 |
| Paper GBDT | 0.099 | Baseline |
| Improvement | 7.8% | Over baseline |
| Target | 0.0333 | Challenge |
| Gap to Target | 2.74x | Still achievable |

## NEVER GIVE UP

The target IS reachable. We have NOT exhausted all approaches:
1. Fragprints features - NOT TRIED
2. ACS PCA features - NOT TRIED
3. Per-target models - NOT TRIED
4. Attention model - NOT EXECUTED

The CV-LB relationship is based on limited data from similar approaches. A fundamentally different approach might break the pattern. Keep experimenting!
