{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9f9669",
   "metadata": {},
   "source": [
    "# Experiment 001: Extended Temporal and Physics-Informed Features\n",
    "\n",
    "**Goal**: Improve baseline by adding longer temporal features and physics-informed features.\n",
    "**Target**: 0.35-0.40 MAE (20-30% improvement over baseline 0.4589)\n",
    "\n",
    "**Key improvements**:\n",
    "1. Extended lags: u_in_lag_5, u_in_lag_10, u_out_lag_5, u_out_lag_10\n",
    "2. Pressure lags (target encoding): pressure_lag_1, pressure_lag_2, pressure_lag_5\n",
    "3. Rate of change: u_in_diff_2, u_in_diff_5\n",
    "4. EMAs: u_in_ewm_5, u_in_ewm_10\n",
    "5. Physics features: RC = R*C, time_normalized = time_step/(R*C), stiffness = 1/C\n",
    "6. GroupKFold CV to prevent breath leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfeac44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T22:18:29.240469Z",
     "iopub.status.busy": "2026-01-13T22:18:29.239591Z",
     "iopub.status.idle": "2026-01-13T22:18:30.310942Z",
     "shell.execute_reply": "2026-01-13T22:18:30.310389Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838c3ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T22:18:30.314754Z",
     "iopub.status.busy": "2026-01-13T22:18:30.314104Z",
     "iopub.status.idle": "2026-01-13T22:18:34.148657Z",
     "shell.execute_reply": "2026-01-13T22:18:34.148002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5432400, 8)\n",
      "Test data shape: (603600, 7)\n",
      "\n",
      "Columns: ['id', 'breath_id', 'R', 'C', 'time_step', 'u_in', 'u_out', 'pressure']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"\\nColumns: {list(train.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fa28d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T22:18:34.151387Z",
     "iopub.status.busy": "2026-01-13T22:18:34.150836Z",
     "iopub.status.idle": "2026-01-13T22:19:35.089200Z",
     "shell.execute_reply": "2026-01-13T22:19:35.088587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating extended features for training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating extended features for test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data with extended features: (5432400, 50)\n",
      "Test data with extended features: (603600, 46)\n"
     ]
    }
   ],
   "source": [
    "# Create extended temporal features\n",
    "def create_extended_features(df):\n",
    "    \"\"\"Create extended temporal and physics-informed features\"\"\"\n",
    "    \n",
    "    # Sort by breath_id and time_step to ensure proper ordering\n",
    "    df = df.sort_values(['breath_id', 'time_step']).reset_index(drop=True)\n",
    "    \n",
    "    # ----- Extended Lags -----\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df[f'u_in_lag_{lag}'] = df.groupby('breath_id')['u_in'].shift(lag)\n",
    "        df[f'u_out_lag_{lag}'] = df.groupby('breath_id')['u_out'].shift(lag)\n",
    "    \n",
    "    # ----- Pressure Lags (Target Encoding) -----\n",
    "    # Only for training data\n",
    "    if 'pressure' in df.columns:\n",
    "        for lag in [1, 2, 5]:\n",
    "            df[f'pressure_lag_{lag}'] = df.groupby('breath_id')['pressure'].shift(lag)\n",
    "    \n",
    "    # ----- Rate of Change Features -----\n",
    "    df['u_in_diff_1'] = df.groupby('breath_id')['u_in'].diff(1)\n",
    "    df['u_in_diff_2'] = df.groupby('breath_id')['u_in'].diff(2)\n",
    "    df['u_in_diff_5'] = df.groupby('breath_id')['u_in'].diff(5)\n",
    "    \n",
    "    # ----- Rolling Statistics with Different Windows -----\n",
    "    for window in [5, 10, 20]:\n",
    "        df[f'u_in_rolling_mean_{window}'] = df.groupby('breath_id')['u_in'].rolling(window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "        df[f'u_in_rolling_std_{window}'] = df.groupby('breath_id')['u_in'].rolling(window, min_periods=1).std().reset_index(0, drop=True)\n",
    "        df[f'u_in_rolling_max_{window}'] = df.groupby('breath_id')['u_in'].rolling(window, min_periods=1).max().reset_index(0, drop=True)\n",
    "        df[f'u_in_rolling_min_{window}'] = df.groupby('breath_id')['u_in'].rolling(window, min_periods=1).min().reset_index(0, drop=True)\n",
    "    \n",
    "    # ----- Exponential Moving Averages -----\n",
    "    for span in [5, 10, 20]:\n",
    "        df[f'u_in_ewm_{span}'] = df.groupby('breath_id')['u_in'].ewm(span=span, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # ----- Cumulative Features -----\n",
    "    df['u_in_cumsum'] = df.groupby('breath_id')['u_in'].cumsum()\n",
    "    df['time_cumsum'] = df.groupby('breath_id')['time_step'].cumsum()\n",
    "    \n",
    "    # ----- Physics-Informed Features -----\n",
    "    # RC time constant (lung mechanics)\n",
    "    df['RC'] = df['R'] * df['C']\n",
    "    \n",
    "    # Normalized time (time relative to lung time constant)\n",
    "    df['time_normalized'] = df['time_step'] / (df['R'] * df['C'])\n",
    "    \n",
    "    # Lung stiffness (inverse of compliance)\n",
    "    df['stiffness'] = 1.0 / df['C']\n",
    "    \n",
    "    # Work done (integral of pressure * flow) - approximate with u_in\n",
    "    df['u_in_integral'] = df.groupby('breath_id')['u_in'].cumsum() * df['time_step']\n",
    "    \n",
    "    # ----- Interaction Features -----\n",
    "    df['u_in_times_R'] = df['u_in'] * df['R']\n",
    "    df['u_in_times_C'] = df['u_in'] * df['C']\n",
    "    df['u_in_times_RC'] = df['u_in'] * df['RC']\n",
    "    \n",
    "    # Time since start of breath\n",
    "    df['time_since_start'] = df['time_step'] - df.groupby('breath_id')['time_step'].transform('first')\n",
    "    \n",
    "    # Breath position (0 to 1 within breath)\n",
    "    df['breath_position'] = df.groupby('breath_id').cumcount() / df.groupby('breath_id').size()\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Creating extended features for training data...\")\n",
    "train_extended = create_extended_features(train.copy())\n",
    "\n",
    "print(\"Creating extended features for test data...\")\n",
    "test_extended = create_extended_features(test.copy())\n",
    "\n",
    "print(f\"\\nTraining data with extended features: {train_extended.shape}\")\n",
    "print(f\"Test data with extended features: {test_extended.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601fc6de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T22:19:35.091758Z",
     "iopub.status.busy": "2026-01-13T22:19:35.091139Z",
     "iopub.status.idle": "2026-01-13T22:19:35.095660Z",
     "shell.execute_reply": "2026-01-13T22:19:35.095122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 45\n",
      "\n",
      "Sample features: ['R', 'C', 'time_step', 'u_in', 'u_out', 'u_in_lag_1', 'u_out_lag_1', 'u_in_lag_2', 'u_out_lag_2', 'u_in_lag_3', 'u_out_lag_3', 'u_in_lag_5', 'u_out_lag_5', 'pressure_lag_1', 'pressure_lag_2', 'pressure_lag_5', 'u_in_diff_1', 'u_in_diff_2', 'u_in_diff_5', 'u_in_rolling_mean_5']\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (exclude IDs, target, and any columns with too many NaNs)\n",
    "exclude_cols = ['id', 'breath_id', 'pressure', 'u_in_lag_10', 'u_out_lag_10']\n",
    "\n",
    "feature_cols = [col for col in train_extended.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nSample features: {feature_cols[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f27849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = train_extended[feature_cols].copy()\n",
    "y = train_extended['pressure'].copy()\n",
    "\n",
    "# Create groups for GroupKFold (one group per breath)\n",
    "groups = train_extended['breath_id']\n",
    "\n",
    "print(f\"Training data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Number of unique breaths: {groups.nunique()}\")\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = X.isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"\\nColumns with NaN values:\")\n",
    "    print(nan_counts[nan_counts > 0].sort_values(ascending=False).head())\n",
    "    \n",
    "    # Fill NaN values with 0 (common approach for lag features)\n",
    "    X = X.fillna(0)\n",
    "    print(\"Filled NaN values with 0\")\n",
    "else:\n",
    "    print(\"\\nNo NaN values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GroupKFold cross-validation\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Create splits\n",
    "folds = list(gkf.split(X, y, groups))\n",
    "\n",
    "print(\"Cross-validation splits created:\")\n",
    "for i, (train_idx, val_idx) in enumerate(folds):\n",
    "    train_breaths = groups.iloc[train_idx].nunique()\n",
    "    val_breaths = groups.iloc[val_idx].nunique()\n",
    "    print(f\"Fold {i+1}: Train={train_breaths} breaths, Val={val_breaths} breaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b673f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model with cross-validation\n",
    "fold_scores = []\n",
    "predictions = np.zeros(len(train_extended))\n",
    "feature_importance_list = []\n",
    "\n",
    "print(\"Training LightGBM model with 5-fold GroupKFold...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\nFold {fold + 1}/5\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Create LightGBM datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Calculate MAE (focusing on inspiratory phase where u_out=0)\n",
    "    val_data_fold = train_extended.iloc[val_idx]\n",
    "    inspiratory_mask = val_data_fold['u_out'] == 0\n",
    "    \n",
    "    if inspiratory_mask.sum() > 0:\n",
    "        fold_score = mean_absolute_error(y_val[inspiratory_mask], val_pred[inspiratory_mask])\n",
    "        print(f\"Fold {fold + 1} MAE (inspiratory): {fold_score:.4f}\")\n",
    "        fold_scores.append(fold_score)\n",
    "    else:\n",
    "        print(f\"Fold {fold + 1}: No inspiratory samples in validation set\")\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions[val_idx] = val_pred\n",
    "    \n",
    "    # Store feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importance(importance_type='gain'),\n",
    "        'fold': fold + 1\n",
    "    })\n",
    "    feature_importance_list.append(importance)\n",
    "\n",
    "# Calculate overall CV score\n",
    "if len(fold_scores) > 0:\n",
    "    cv_score = np.mean(fold_scores)\n",
    "    cv_std = np.std(fold_scores)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Overall CV MAE: {cv_score:.4f} Â± {cv_std:.4f}\")\n",
    "    print(f\"Individual folds: {[f'{s:.4f}' for s in fold_scores]}\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    baseline_score = 0.4589\n",
    "    improvement = (baseline_score - cv_score) / baseline_score * 100\n",
    "    print(f\"Improvement over baseline: {improvement:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nNo valid folds with inspiratory samples found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "feature_importance = pd.concat(feature_importance_list, ignore_index=True)\n",
    "\n",
    "# Calculate mean importance across folds\n",
    "mean_importance = feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 most important features:\")\n",
    "print(mean_importance.head(20))\n",
    "\n",
    "# Save feature importance\n",
    "mean_importance.to_csv('/home/code/exp_001_feature_importance.csv')\n",
    "print(f\"\\nFeature importance saved to: /home/code/exp_001_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data for prediction\n",
    "X_test = test_extended[feature_cols].copy()\n",
    "\n",
    "# Fill NaN values\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Train final model on full training data\n",
    "print(\"\\nTraining final model on full training data...\")\n",
    "\n",
    "final_train_data = lgb.Dataset(X, label=y)\n",
    "\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    final_train_data,\n",
    "    num_boost_round=10000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(50),\n",
    "        lgb.log_evaluation(100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = final_model.predict(X_test, num_iteration=final_model.best_iteration)\n",
    "\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Prediction range: [{test_predictions.min():.4f}, {test_predictions.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_extended['id'],\n",
    "    'pressure': test_predictions\n",
    "})\n",
    "\n",
    "# Ensure correct format\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nLast 5 rows:\")\n",
    "print(submission.tail())\n",
    "\n",
    "# Save submission\n",
    "submission_path = '/home/code/submission_candidates/exp_001_submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
