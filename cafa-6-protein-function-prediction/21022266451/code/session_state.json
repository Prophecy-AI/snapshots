{
  "workspace_dir": "/home/code",
  "competition_id": "cafa-6-protein-function-prediction",
  "metric_direction": false,
  "start_time": "2026-01-15T06:47:30.028703",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Key approach: Use protein language model embeddings (ESM2, ProtBERT, T5) as features, train separate MLP models for each GO aspect (BP, MF, CC), ensemble multiple models, apply GO term propagation to ensure parent terms have scores >= child terms",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Post-processing is critical: 1) GO term propagation (parent >= child scores), 2) GOA corrections (apply known positive annotations with score=1.0, remove negative annotations), 3) Rank normalization to boost weak predictions",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "MLP architecture: Input (embedding dim + features) -> Linear -> BatchNorm -> ReLU -> Dropout -> Linear -> BatchNorm -> ReLU -> Dropout -> Linear (num_classes). Use BCEWithLogitsLoss, AdamW optimizer, CosineAnnealingLR scheduler",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    }
  ],
  "web_research": [],
  "max_submissions": 5,
  "remaining_submissions": 5
}