{
  "workspace_dir": "/home/code",
  "competition_id": "cafa-6-protein-function-prediction",
  "metric_direction": false,
  "start_time": "2026-01-13T10:08:38.546583",
  "time_limit_minutes": 2100,
  "experiments": [
    {
      "id": "exp_000",
      "name": "001_baseline_handcrafted_features",
      "model_type": "MLP",
      "score": 0.1065,
      "notes": "Baseline model using hand-crafted features only (25 dimensions: AA composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity). MLP with hidden dims [512, 256], dropout 0.3. Trained for 15 epochs with BCEWithLogitsLoss and CosineAnnealingLR. Top 500 GO terms per aspect (1500 total). Simple submission without GO hierarchy propagation (too slow). Temperature scaling T=1.5 applied. Validation F1=0.1065 (micro F1 at threshold 0.5). This is a weak baseline - need protein embeddings (ESM2/ProtBERT) for competitive performance.",
      "experiment_folder": "/home/code/experiments/001_baseline",
      "timestamp": "2026-01-13T11:52:18.733747"
    }
  ],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.1065,
      "model_name": "001_baseline_handcrafted_features",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-13T11:52:18.781656"
    }
  ],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\n- Test: 224,309 proteins to predict\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\n\n## Evaluation Metric\n- Weighted F1-measure using Information Accretion (IA) weights\n- Evaluated separately for MF, BP, CC subontologies, then averaged\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\n- Must predict GO terms with confidence scores (0-1)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\n   - State-of-the-art protein language model from Meta\n   - Best performance for protein function prediction\n   - Use mean pooling over sequence positions\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\n\n2. **ProtBERT**: 1024-dimensional embeddings\n   - Good alternative, well-tested on previous competitions\n   - Slightly faster than ESM2\n\n3. **T5 embeddings**: 1024-dimensional\n   - Another viable option\n\n## Model Architecture\n**Multi-Layer Perceptron on top of embeddings:**\n```\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> Output (num_classes)\n```\n\n**Key hyperparameters from top solutions:**\n- Hidden dimensions: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.3\n- Batch size: 64-128\n- Learning rate: 1e-3 to 1e-2\n- Epochs: 10-15\n- Use BCEWithLogitsLoss for multi-label classification\n- CosineAnnealingLR scheduler works well\n\n## Feature Engineering\n**Hand-crafted features to combine with embeddings:**\n- Amino acid composition (20 features)\n- Sequence length\n- Molecular weight\n- Isoelectric point\n- Aromaticity\n- Instability index\n- Gravy (hydropathicity)\n\n## Training Strategy\n1. **Train separate models per aspect (MF, BP, CC)**\n   - Each aspect has different GO term distributions\n   - Use aspect-specific thresholds for predictions\n\n2. **Multi-label classification setup:**\n   - Use top N most frequent GO terms (500-1500)\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\n   - BCEWithLogitsLoss or BCELoss\n\n3. **Validation:**\n   - 90/10 train/val split\n   - Monitor MultilabelF1Score\n\n## Ensembling (IMPORTANT)\n**Ensemble multiple approaches for best results:**\n\n1. **Model ensemble:**\n   - Train 3+ MLPs with different hyperparameters\n   - Average predictions from all models\n   - Different batch sizes, hidden dims, dropout rates\n\n2. **Embedding ensemble:**\n   - Combine predictions from ESM2-based and ProtBERT-based models\n\n3. **BLAST/Sequence similarity predictions:**\n   - Use sequence similarity to transfer annotations from known proteins\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\n   - Diamond BLAST is faster than standard BLAST\n\n## Post-Processing (CRITICAL FOR SCORE)\n**GO Hierarchy Propagation:**\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\n- Parse go-basic.obo for is_a and part_of relationships\n\n**Rank Normalization:**\n- If max prediction score is weak (< 0.95), scale up all scores\n- Target: make best non-root prediction at least 0.95\n- scale_factor = 0.95 / max_val\n\n**GOA Corrections (if available):**\n- Apply known positive annotations with confidence 1.0\n- Remove predictions conflicting with 'NOT' annotations\n- Propagate negative annotations to descendants\n\n## Temperature Scaling\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\n- T=1.5 often improves calibration\n- Helps with confidence score distribution\n\n## Prediction Thresholds\n- Use aspect-specific thresholds:\n  - MF: 0.1-0.2\n  - BP: 0.1-0.2  \n  - CC: 0.1-0.2\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\n- Maximum predictions per protein: 1000-1500\n\n## Advanced Techniques (from literature)\n1. **GO-aware architectures:**\n   - Graph attention networks (GATs) to model GO hierarchy\n   - Multimodal transformers for sequence + GO term embeddings\n   - Contrastive learning for GO term relationships\n\n2. **Semantic entailment:**\n   - DeepGO-SE approach: predict truth values for GO statements\n   - Exploit GO axioms and hierarchical constraints\n\n3. **Zero-shot prediction:**\n   - Learn GO term embeddings from textual definitions\n   - Enables prediction of unseen GO terms\n\n## Key Implementation Notes\n1. Parse GO ontology from go-basic.obo using obonet library\n2. Build ancestor map for efficient propagation\n3. Handle variable-length sequences with mean pooling\n4. Use sparse matrices for large label spaces\n5. Memory management: process in batches, use gc.collect()\n6. Gradient clipping (max_norm=1.0) for stable training\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence\n- No header\n- Confidence scores between 0 and 1\n- Sort by protein_id, then by confidence (descending)\n- Drop extremely low scores (< 0.001) to reduce file size\n\n## Reference Kernels\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T10:20:26.464389"
    }
  ],
  "feedback_history": [
    {
      "feedback": "## What I Understood\n\nThe junior researcher implemented a baseline model for protein function prediction using hand-crafted features (amino acid composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity - 25 dimensions total) with an MLP classifier. The goal was to establish a working pipeline and baseline score before moving to more sophisticated approaches like protein language model embeddings (ESM2/ProtBERT). The validation F1 of 0.1065 was achieved, and a submission candidate was generated.\n\n## Technical Execution Assessment\n\n**Validation**: The validation methodology is sound - a 90/10 train/val split with fixed random_state=42 for reproducibility. However, this is a single split, not cross-validation, so we have no variance estimate. The validation F1 of 0.1065 is reported, but this is micro-F1 at threshold 0.5, which is NOT the competition metric (weighted F1 with Information Accretion weights). This makes the validation score difficult to interpret relative to the target.\n\n**Leakage Risk**: None detected. The MultiLabelBinarizer is fit only on training data. Features are computed per-sequence without any global statistics that could leak. The train/val split is done correctly before any model training.\n\n**Score Integrity**: The validation F1 of 0.1065 is verified in the training logs (Epoch 7 achieved best). The submission file was generated correctly with 3.5M rows covering 224K test proteins. Root GO terms are correctly set to confidence 1.0.\n\n**Code Quality**: The code executed successfully. Seeds are set (random_state=42). The notebook is well-structured. One concern: GO hierarchy propagation was attempted but then abandoned due to being \"too slow\" - the final submission uses direct predictions only without propagation, which is known to be critical for this competition.\n\nVerdict: **TRUSTWORTHY** (but with important caveats about metric mismatch)\n\n## Strategic Assessment\n\n**Approach Fit**: This is a reasonable baseline approach, but the researcher correctly identified in their notes that \"need protein embeddings (ESM2/ProtBERT) for competitive performance.\" The 25-dimensional hand-crafted features are extremely limited compared to the 1280-dimensional ESM2 embeddings that top solutions use. The gap between 0.1065 validation F1 and the 0.441 target is enormous (4x improvement needed).\n\n**Effort Allocation**: The baseline was a necessary first step to establish a working pipeline. However, the effort allocation going forward is critical:\n1. **Protein embeddings are the bottleneck** - The data findings clearly show ESM2/ProtBERT embeddings are essential. This should be the immediate next priority.\n2. **GO hierarchy propagation was skipped** - This is documented as \"critical for score\" in the strategy notes but was abandoned. This is a significant gap.\n3. **Separate models per aspect** - Top solutions train separate models for MF, BP, CC. The baseline trains a single model.\n\n**Assumptions**: \n- The baseline assumes hand-crafted features can capture protein function - this is known to be insufficient.\n- Using micro-F1 at threshold 0.5 as validation metric doesn't match the competition's weighted F1 with IA weights.\n- The threshold of 0.1 for predictions may not be optimal.\n\n**Blind Spots**:\n1. **No protein embeddings** - This is the single biggest gap. The reference kernels all use ESM2 (1280-dim) or ProtBERT (1024-dim).\n2. **No GO hierarchy propagation** - Parent terms must have scores >= child terms. This was attempted but abandoned.\n3. **No GOA corrections** - Known positive annotations should be applied with confidence 1.0.\n4. **No ensemble** - Top solutions use 3+ models with different hyperparameters.\n5. **Single model for all aspects** - Should train separate models per aspect (MF, BP, CC).\n\n**Trajectory**: This is experiment 0 - a baseline. The trajectory is appropriate for establishing a working pipeline. However, the gap to target (0.1065 \u2192 0.441) is substantial. The researcher correctly identified the path forward (protein embeddings), but execution is critical.\n\n## What's Working\n\n1. **Clean, reproducible code** - The notebook is well-structured with clear sections.\n2. **Correct data handling** - FASTA parsing, GO term filtering, train/val split all done correctly.\n3. **Working submission pipeline** - Generated a valid submission file with correct format.\n4. **Good documentation** - The researcher clearly documented limitations and next steps.\n5. **Appropriate model architecture** - MLP with BatchNorm, ReLU, Dropout is the right choice for this task.\n\n## Key Concerns\n\n1. **Observation**: No protein language model embeddings used.\n   **Why it matters**: ESM2/ProtBERT embeddings are the foundation of all competitive solutions. Hand-crafted features (25-dim) vs embeddings (1280-dim) is a 50x difference in representation power. This alone likely accounts for most of the gap to target.\n   **Suggestion**: Prioritize generating ESM2 embeddings for train and test proteins. Pre-computed embeddings may be available in Kaggle datasets.\n\n2. **Observation**: GO hierarchy propagation was abandoned.\n   **Why it matters**: The competition metric uses IA weights that favor deeper GO terms. Propagation ensures parent terms have scores >= child terms, which is required by GO semantics. The strategy notes explicitly call this \"CRITICAL FOR SCORE.\"\n   **Suggestion**: Implement efficient propagation using pre-computed ancestor maps. Process in batches if memory is an issue.\n\n3. **Observation**: Validation metric doesn't match competition metric.\n   **Why it matters**: Micro-F1 at threshold 0.5 is not the same as weighted F1 with IA weights. This makes it hard to estimate how the model will perform on the leaderboard.\n   **Suggestion**: Implement the competition metric for local validation, or at least use IA-weighted F1.\n\n4. **Observation**: Single model for all aspects instead of per-aspect models.\n   **Why it matters**: MF, BP, and CC have different GO term distributions and characteristics. Top solutions train separate models and use aspect-specific thresholds.\n   **Suggestion**: Train separate models for each aspect in future experiments.\n\n## Top Priority for Next Experiment\n\n**Implement protein language model embeddings (ESM2 or ProtBERT).** This is the single highest-leverage change that will close the gap to the target. The reference kernels show that ESM2 embeddings (1280-dim) combined with hand-crafted features, trained with an MLP, can achieve competitive scores. Check if pre-computed embeddings are available in Kaggle datasets to save computation time. If not, use the esm2_t33_650M_UR50D model with mean pooling over sequence positions.\n\nSecondary priorities (in order):\n1. Implement GO hierarchy propagation\n2. Train separate models per aspect\n3. Add ensemble of 3 MLPs with different hyperparameters\n4. Apply GOA corrections for known annotations\n\nThe baseline has established a working pipeline - now it's time to add the components that actually drive performance.\n",
      "experiment_id": "exp_000",
      "agent": "evaluator",
      "timestamp": "2026-01-13T11:54:33.452138"
    }
  ],
  "data_findings": [
    {
      "finding": "ProtBERT embeddings (1024-dim) are effective for protein function prediction. MLP with hidden layers (864, 712) on top of embeddings works well.",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble approach: combine deep learning predictions with BLAST-based predictions using weighted averaging (DL_WEIGHT=0.6, BLAST_WEIGHT=0.4)",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Temperature scaling (T=1.5) applied to logits before sigmoid improves calibration. Use top 500 most frequent GO terms as prediction targets.",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO hierarchy propagation is critical: ensure parent terms have scores >= child terms. Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0. Apply rank normalization to boost weak predictions to 0.95.",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "ESM2 embeddings (1280-dim) combined with hand-crafted features (amino acid composition, molecular weight, etc.) work well. MLP with BatchNorm, ReLU, Dropout. Train separate models per aspect (MF, BP, CC).",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble 3 MLPs with different hyperparameters (batch size, hidden dims, dropout). Use CosineAnnealingLR scheduler. Apply aspect-specific thresholds for predictions.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA (Gene Ontology Annotation) corrections: Apply known positive annotations with confidence 1.0. Remove predictions that conflict with 'NOT' annotations (negative annotations) and their descendants.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Training data: 82,404 proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Test set: 224,309 proteins. Aspect distribution: P (Biological Process) 250K, C (Cellular Component) 158K, F (Molecular Function) 128K.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO term frequency is highly skewed: only 33 terms appear in >1000 proteins, 663 terms in >100 proteins. Most common term GO:0005515 appears in 33,713 proteins. Median term frequency is only 4.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Protein sequence lengths: Train mean 526, median 409. Test mean 429. Range from 2-35,213 amino acids. Need to handle variable length sequences.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced techniques: DeepGO-SE uses semantic entailment with GO axioms. GOProFormer uses multimodal transformer for sequence + GO term embeddings. Graph attention networks (GATs) can model GO hierarchy. STAR-GO integrates GO textual definitions with ontology graph structure.",
      "source": "WebSearch: protein function prediction techniques",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best techniques for protein function prediction using deep learning and Gene Ontology?",
      "finding": "Protein function prediction relies on mapping an amino\u2011acid sequence to the structured terms of the Gene Ontology (GO), which encodes molecular functions, biological processes and cellular components.  Deep\u2011learning models are now the dominant approach because they can learn rich sequence representations while simultaneously exploiting the hierarchical constraints of GO, leading to more consistent and accurate annotations ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).\n\nThe most effective recent techniques combine large\u2011scale protein language models with GO\u2011aware architectures.  **DeepGO\u2011SE** uses a pretrained language model to generate multiple approximate GO models and a neural network that predicts truth values for GO statements, effectively performing semantic entailment and achieving state\u2011of\u2011the\u2011art performance on benchmark sets ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).  **POSA\u2011GO** fuses hierarchical GO information with protein langu",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 4
}