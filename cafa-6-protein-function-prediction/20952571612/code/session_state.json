{
  "workspace_dir": "/home/code",
  "competition_id": "cafa-6-protein-function-prediction",
  "metric_direction": false,
  "start_time": "2026-01-13T10:08:38.546583",
  "time_limit_minutes": 2100,
  "experiments": [
    {
      "id": "exp_000",
      "name": "001_baseline_handcrafted_features",
      "model_type": "MLP",
      "score": 0.1065,
      "notes": "Baseline model using hand-crafted features only (25 dimensions: AA composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity). MLP with hidden dims [512, 256], dropout 0.3. Trained for 15 epochs with BCEWithLogitsLoss and CosineAnnealingLR. Top 500 GO terms per aspect (1500 total). Simple submission without GO hierarchy propagation (too slow). Temperature scaling T=1.5 applied. Validation F1=0.1065 (micro F1 at threshold 0.5). This is a weak baseline - need protein embeddings (ESM2/ProtBERT) for competitive performance.",
      "experiment_folder": "/home/code/experiments/001_baseline",
      "timestamp": "2026-01-13T11:52:18.733747"
    }
  ],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.1065,
      "model_name": "001_baseline_handcrafted_features",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-13T11:52:18.781656"
    }
  ],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\n- Test: 224,309 proteins to predict\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\n\n## Evaluation Metric\n- Weighted F1-measure using Information Accretion (IA) weights\n- Evaluated separately for MF, BP, CC subontologies, then averaged\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\n- Must predict GO terms with confidence scores (0-1)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\n   - State-of-the-art protein language model from Meta\n   - Best performance for protein function prediction\n   - Use mean pooling over sequence positions\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\n\n2. **ProtBERT**: 1024-dimensional embeddings\n   - Good alternative, well-tested on previous competitions\n   - Slightly faster than ESM2\n\n3. **T5 embeddings**: 1024-dimensional\n   - Another viable option\n\n## Model Architecture\n**Multi-Layer Perceptron on top of embeddings:**\n```\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> Output (num_classes)\n```\n\n**Key hyperparameters from top solutions:**\n- Hidden dimensions: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.3\n- Batch size: 64-128\n- Learning rate: 1e-3 to 1e-2\n- Epochs: 10-15\n- Use BCEWithLogitsLoss for multi-label classification\n- CosineAnnealingLR scheduler works well\n\n## Feature Engineering\n**Hand-crafted features to combine with embeddings:**\n- Amino acid composition (20 features)\n- Sequence length\n- Molecular weight\n- Isoelectric point\n- Aromaticity\n- Instability index\n- Gravy (hydropathicity)\n\n## Training Strategy\n1. **Train separate models per aspect (MF, BP, CC)**\n   - Each aspect has different GO term distributions\n   - Use aspect-specific thresholds for predictions\n\n2. **Multi-label classification setup:**\n   - Use top N most frequent GO terms (500-1500)\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\n   - BCEWithLogitsLoss or BCELoss\n\n3. **Validation:**\n   - 90/10 train/val split\n   - Monitor MultilabelF1Score\n\n## Ensembling (IMPORTANT)\n**Ensemble multiple approaches for best results:**\n\n1. **Model ensemble:**\n   - Train 3+ MLPs with different hyperparameters\n   - Average predictions from all models\n   - Different batch sizes, hidden dims, dropout rates\n\n2. **Embedding ensemble:**\n   - Combine predictions from ESM2-based and ProtBERT-based models\n\n3. **BLAST/Sequence similarity predictions:**\n   - Use sequence similarity to transfer annotations from known proteins\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\n   - Diamond BLAST is faster than standard BLAST\n\n## Post-Processing (CRITICAL FOR SCORE)\n**GO Hierarchy Propagation:**\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\n- Parse go-basic.obo for is_a and part_of relationships\n\n**Rank Normalization:**\n- If max prediction score is weak (< 0.95), scale up all scores\n- Target: make best non-root prediction at least 0.95\n- scale_factor = 0.95 / max_val\n\n**GOA Corrections (if available):**\n- Apply known positive annotations with confidence 1.0\n- Remove predictions conflicting with 'NOT' annotations\n- Propagate negative annotations to descendants\n\n## Temperature Scaling\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\n- T=1.5 often improves calibration\n- Helps with confidence score distribution\n\n## Prediction Thresholds\n- Use aspect-specific thresholds:\n  - MF: 0.1-0.2\n  - BP: 0.1-0.2  \n  - CC: 0.1-0.2\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\n- Maximum predictions per protein: 1000-1500\n\n## Advanced Techniques (from literature)\n1. **GO-aware architectures:**\n   - Graph attention networks (GATs) to model GO hierarchy\n   - Multimodal transformers for sequence + GO term embeddings\n   - Contrastive learning for GO term relationships\n\n2. **Semantic entailment:**\n   - DeepGO-SE approach: predict truth values for GO statements\n   - Exploit GO axioms and hierarchical constraints\n\n3. **Zero-shot prediction:**\n   - Learn GO term embeddings from textual definitions\n   - Enables prediction of unseen GO terms\n\n## Key Implementation Notes\n1. Parse GO ontology from go-basic.obo using obonet library\n2. Build ancestor map for efficient propagation\n3. Handle variable-length sequences with mean pooling\n4. Use sparse matrices for large label spaces\n5. Memory management: process in batches, use gc.collect()\n6. Gradient clipping (max_norm=1.0) for stable training\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence\n- No header\n- Confidence scores between 0 and 1\n- Sort by protein_id, then by confidence (descending)\n- Drop extremely low scores (< 0.001) to reduce file size\n\n## Reference Kernels\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T10:20:26.464389"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "ProtBERT embeddings (1024-dim) are effective for protein function prediction. MLP with hidden layers (864, 712) on top of embeddings works well.",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble approach: combine deep learning predictions with BLAST-based predictions using weighted averaging (DL_WEIGHT=0.6, BLAST_WEIGHT=0.4)",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Temperature scaling (T=1.5) applied to logits before sigmoid improves calibration. Use top 500 most frequent GO terms as prediction targets.",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO hierarchy propagation is critical: ensure parent terms have scores >= child terms. Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0. Apply rank normalization to boost weak predictions to 0.95.",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "ESM2 embeddings (1280-dim) combined with hand-crafted features (amino acid composition, molecular weight, etc.) work well. MLP with BatchNorm, ReLU, Dropout. Train separate models per aspect (MF, BP, CC).",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble 3 MLPs with different hyperparameters (batch size, hidden dims, dropout). Use CosineAnnealingLR scheduler. Apply aspect-specific thresholds for predictions.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA (Gene Ontology Annotation) corrections: Apply known positive annotations with confidence 1.0. Remove predictions that conflict with 'NOT' annotations (negative annotations) and their descendants.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Training data: 82,404 proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Test set: 224,309 proteins. Aspect distribution: P (Biological Process) 250K, C (Cellular Component) 158K, F (Molecular Function) 128K.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO term frequency is highly skewed: only 33 terms appear in >1000 proteins, 663 terms in >100 proteins. Most common term GO:0005515 appears in 33,713 proteins. Median term frequency is only 4.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Protein sequence lengths: Train mean 526, median 409. Test mean 429. Range from 2-35,213 amino acids. Need to handle variable length sequences.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced techniques: DeepGO-SE uses semantic entailment with GO axioms. GOProFormer uses multimodal transformer for sequence + GO term embeddings. Graph attention networks (GATs) can model GO hierarchy. STAR-GO integrates GO textual definitions with ontology graph structure.",
      "source": "WebSearch: protein function prediction techniques",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best techniques for protein function prediction using deep learning and Gene Ontology?",
      "finding": "Protein function prediction relies on mapping an amino\u2011acid sequence to the structured terms of the Gene Ontology (GO), which encodes molecular functions, biological processes and cellular components.  Deep\u2011learning models are now the dominant approach because they can learn rich sequence representations while simultaneously exploiting the hierarchical constraints of GO, leading to more consistent and accurate annotations ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).\n\nThe most effective recent techniques combine large\u2011scale protein language models with GO\u2011aware architectures.  **DeepGO\u2011SE** uses a pretrained language model to generate multiple approximate GO models and a neural network that predicts truth values for GO statements, effectively performing semantic entailment and achieving state\u2011of\u2011the\u2011art performance on benchmark sets ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).  **POSA\u2011GO** fuses hierarchical GO information with protein langu",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 4
}