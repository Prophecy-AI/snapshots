{
  "phase": "evolver",
  "loop_count": 1,
  "last_submission": null,
  "competition_id": "cafa-6-protein-function-prediction",
  "seed_prompt": "# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\n- Test: 224,309 proteins to predict\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\n\n## Evaluation Metric\n- Weighted F1-measure using Information Accretion (IA) weights\n- Evaluated separately for MF, BP, CC subontologies, then averaged\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\n- Must predict GO terms with confidence scores (0-1)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\n   - State-of-the-art protein language model from Meta\n   - Best performance for protein function prediction\n   - Use mean pooling over sequence positions\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\n\n2. **ProtBERT**: 1024-dimensional embeddings\n   - Good alternative, well-tested on previous competitions\n   - Slightly faster than ESM2\n\n3. **T5 embeddings**: 1024-dimensional\n   - Another viable option\n\n## Model Architecture\n**Multi-Layer Perceptron on top of embeddings:**\n```\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> Output (num_classes)\n```\n\n**Key hyperparameters from top solutions:**\n- Hidden dimensions: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.3\n- Batch size: 64-128\n- Learning rate: 1e-3 to 1e-2\n- Epochs: 10-15\n- Use BCEWithLogitsLoss for multi-label classification\n- CosineAnnealingLR scheduler works well\n\n## Feature Engineering\n**Hand-crafted features to combine with embeddings:**\n- Amino acid composition (20 features)\n- Sequence length\n- Molecular weight\n- Isoelectric point\n- Aromaticity\n- Instability index\n- Gravy (hydropathicity)\n\n## Training Strategy\n1. **Train separate models per aspect (MF, BP, CC)**\n   - Each aspect has different GO term distributions\n   - Use aspect-specific thresholds for predictions\n\n2. **Multi-label classification setup:**\n   - Use top N most frequent GO terms (500-1500)\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\n   - BCEWithLogitsLoss or BCELoss\n\n3. **Validation:**\n   - 90/10 train/val split\n   - Monitor MultilabelF1Score\n\n## Ensembling (IMPORTANT)\n**Ensemble multiple approaches for best results:**\n\n1. **Model ensemble:**\n   - Train 3+ MLPs with different hyperparameters\n   - Average predictions from all models\n   - Different batch sizes, hidden dims, dropout rates\n\n2. **Embedding ensemble:**\n   - Combine predictions from ESM2-based and ProtBERT-based models\n\n3. **BLAST/Sequence similarity predictions:**\n   - Use sequence similarity to transfer annotations from known proteins\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\n   - Diamond BLAST is faster than standard BLAST\n\n## Post-Processing (CRITICAL FOR SCORE)\n**GO Hierarchy Propagation:**\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\n- Parse go-basic.obo for is_a and part_of relationships\n\n**Rank Normalization:**\n- If max prediction score is weak (< 0.95), scale up all scores\n- Target: make best non-root prediction at least 0.95\n- scale_factor = 0.95 / max_val\n\n**GOA Corrections (if available):**\n- Apply known positive annotations with confidence 1.0\n- Remove predictions conflicting with 'NOT' annotations\n- Propagate negative annotations to descendants\n\n## Temperature Scaling\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\n- T=1.5 often improves calibration\n- Helps with confidence score distribution\n\n## Prediction Thresholds\n- Use aspect-specific thresholds:\n  - MF: 0.1-0.2\n  - BP: 0.1-0.2  \n  - CC: 0.1-0.2\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\n- Maximum predictions per protein: 1000-1500\n\n## Advanced Techniques (from literature)\n1. **GO-aware architectures:**\n   - Graph attention networks (GATs) to model GO hierarchy\n   - Multimodal transformers for sequence + GO term embeddings\n   - Contrastive learning for GO term relationships\n\n2. **Semantic entailment:**\n   - DeepGO-SE approach: predict truth values for GO statements\n   - Exploit GO axioms and hierarchical constraints\n\n3. **Zero-shot prediction:**\n   - Learn GO term embeddings from textual definitions\n   - Enables prediction of unseen GO terms\n\n## Key Implementation Notes\n1. Parse GO ontology from go-basic.obo using obonet library\n2. Build ancestor map for efficient propagation\n3. Handle variable-length sequences with mean pooling\n4. Use sparse matrices for large label spaces\n5. Memory management: process in batches, use gc.collect()\n6. Gradient clipping (max_norm=1.0) for stable training\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence\n- No header\n- Confidence scores between 0 and 1\n- Sort by protein_id, then by confidence (descending)\n- Drop extremely low scores (< 0.001) to reduce file size\n\n## Reference Kernels\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\n",
  "evaluator_feedback": "## What I Understood\n\nThe junior researcher implemented a baseline model for protein function prediction using hand-crafted features (amino acid composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity - 25 dimensions total) with an MLP classifier. The goal was to establish a working pipeline and baseline score before moving to more sophisticated approaches like protein language model embeddings (ESM2/ProtBERT). The validation F1 of 0.1065 was achieved, and a submission candidate was generated.\n\n## Technical Execution Assessment\n\n**Validation**: The validation methodology is sound - a 90/10 train/val split with fixed random_state=42 for reproducibility. However, this is a single split, not cross-validation, so we have no variance estimate. The validation F1 of 0.1065 is reported, but this is micro-F1 at threshold 0.5, which is NOT the competition metric (weighted F1 with Information Accretion weights). This makes the validation score difficult to interpret relative to the target.\n\n**Leakage Risk**: None detected. The MultiLabelBinarizer is fit only on training data. Features are computed per-sequence without any global statistics that could leak. The train/val split is done correctly before any model training.\n\n**Score Integrity**: The validation F1 of 0.1065 is verified in the training logs (Epoch 7 achieved best). The submission file was generated correctly with 3.5M rows covering 224K test proteins. Root GO terms are correctly set to confidence 1.0.\n\n**Code Quality**: The code executed successfully. Seeds are set (random_state=42). The notebook is well-structured. One concern: GO hierarchy propagation was attempted but then abandoned due to being \"too slow\" - the final submission uses direct predictions only without propagation, which is known to be critical for this competition.\n\nVerdict: **TRUSTWORTHY** (but with important caveats about metric mismatch)\n\n## Strategic Assessment\n\n**Approach Fit**: This is a reasonable baseline approach, but the researcher correctly identified in their notes that \"need protein embeddings (ESM2/ProtBERT) for competitive performance.\" The 25-dimensional hand-crafted features are extremely limited compared to the 1280-dimensional ESM2 embeddings that top solutions use. The gap between 0.1065 validation F1 and the 0.441 target is enormous (4x improvement needed).\n\n**Effort Allocation**: The baseline was a necessary first step to establish a working pipeline. However, the effort allocation going forward is critical:\n1. **Protein embeddings are the bottleneck** - The data findings clearly show ESM2/ProtBERT embeddings are essential. This should be the immediate next priority.\n2. **GO hierarchy propagation was skipped** - This is documented as \"critical for score\" in the strategy notes but was abandoned. This is a significant gap.\n3. **Separate models per aspect** - Top solutions train separate models for MF, BP, CC. The baseline trains a single model.\n\n**Assumptions**: \n- The baseline assumes hand-crafted features can capture protein function - this is known to be insufficient.\n- Using micro-F1 at threshold 0.5 as validation metric doesn't match the competition's weighted F1 with IA weights.\n- The threshold of 0.1 for predictions may not be optimal.\n\n**Blind Spots**:\n1. **No protein embeddings** - This is the single biggest gap. The reference kernels all use ESM2 (1280-dim) or ProtBERT (1024-dim).\n2. **No GO hierarchy propagation** - Parent terms must have scores >= child terms. This was attempted but abandoned.\n3. **No GOA corrections** - Known positive annotations should be applied with confidence 1.0.\n4. **No ensemble** - Top solutions use 3+ models with different hyperparameters.\n5. **Single model for all aspects** - Should train separate models per aspect (MF, BP, CC).\n\n**Trajectory**: This is experiment 0 - a baseline. The trajectory is appropriate for establishing a working pipeline. However, the gap to target (0.1065 \u2192 0.441) is substantial. The researcher correctly identified the path forward (protein embeddings), but execution is critical.\n\n## What's Working\n\n1. **Clean, reproducible code** - The notebook is well-structured with clear sections.\n2. **Correct data handling** - FASTA parsing, GO term filtering, train/val split all done correctly.\n3. **Working submission pipeline** - Generated a valid submission file with correct format.\n4. **Good documentation** - The researcher clearly documented limitations and next steps.\n5. **Appropriate model architecture** - MLP with BatchNorm, ReLU, Dropout is the right choice for this task.\n\n## Key Concerns\n\n1. **Observation**: No protein language model embeddings used.\n   **Why it matters**: ESM2/ProtBERT embeddings are the foundation of all competitive solutions. Hand-crafted features (25-dim) vs embeddings (1280-dim) is a 50x difference in representation power. This alone likely accounts for most of the gap to target.\n   **Suggestion**: Prioritize generating ESM2 embeddings for train and test proteins. Pre-computed embeddings may be available in Kaggle datasets.\n\n2. **Observation**: GO hierarchy propagation was abandoned.\n   **Why it matters**: The competition metric uses IA weights that favor deeper GO terms. Propagation ensures parent terms have scores >= child terms, which is required by GO semantics. The strategy notes explicitly call this \"CRITICAL FOR SCORE.\"\n   **Suggestion**: Implement efficient propagation using pre-computed ancestor maps. Process in batches if memory is an issue.\n\n3. **Observation**: Validation metric doesn't match competition metric.\n   **Why it matters**: Micro-F1 at threshold 0.5 is not the same as weighted F1 with IA weights. This makes it hard to estimate how the model will perform on the leaderboard.\n   **Suggestion**: Implement the competition metric for local validation, or at least use IA-weighted F1.\n\n4. **Observation**: Single model for all aspects instead of per-aspect models.\n   **Why it matters**: MF, BP, and CC have different GO term distributions and characteristics. Top solutions train separate models and use aspect-specific thresholds.\n   **Suggestion**: Train separate models for each aspect in future experiments.\n\n## Top Priority for Next Experiment\n\n**Implement protein language model embeddings (ESM2 or ProtBERT).** This is the single highest-leverage change that will close the gap to the target. The reference kernels show that ESM2 embeddings (1280-dim) combined with hand-crafted features, trained with an MLP, can achieve competitive scores. Check if pre-computed embeddings are available in Kaggle datasets to save computation time. If not, use the esm2_t33_650M_UR50D model with mean pooling over sequence positions.\n\nSecondary priorities (in order):\n1. Implement GO hierarchy propagation\n2. Train separate models per aspect\n3. Add ensemble of 3 MLPs with different hyperparameters\n4. Apply GOA corrections for known annotations\n\nThe baseline has established a working pipeline - now it's time to add the components that actually drive performance.\n",
  "latest_experiment": {
    "name": "001_baseline_handcrafted_features",
    "model_type": "MLP",
    "score": 0.1065,
    "notes": "Baseline model using hand-crafted features only (25 dimensions: AA composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity). MLP with hidden dims [512, 256], dropout 0.3. Trained for 15 epochs with BCEWithLogitsLoss and CosineAnnealingLR. Top 500 GO terms per aspect (1500 total). Simple submission without GO hierarchy propagation (too slow). Temperature scaling T=1.5 applied. Validation F1=0.1065 (micro F1 at threshold 0.5). This is a weak baseline - need protein embeddings (ESM2/ProtBERT) for competitive performance.",
    "timestamp": "2026-01-13T11:52:18.733747"
  },
  "competitive_intel": "## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`",
  "target_score": 0.441,
  "messages": [
    {
      "role": "user",
      "content": "Begin experimenting. Follow the seed prompt strategy and create your first baseline.\n\n**IMPORTANT:** Before starting, read `session_state.json` to see existing data findings and notebook references. The strategy below will mention which notebooks to reference for data understanding.\n\n\n## EXISTING DATA FINDINGS\nThe following insights have already been discovered. Reference these instead of re-doing EDA:\n\n- **../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb**: Ensemble approach: combine deep learning predictions with BLAST-based predictions using weighted averaging (DL_WEIGHT=0.6, BLAST_WEIGHT=0.4)\n- **../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb**: Temperature scaling (T=1.5) applied to logits before sigmoid improves calibration. Use top 500 most frequent GO terms as prediction targets.\n- **../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb**: GO hierarchy propagation is critical: ensure parent terms have scores >= child terms. Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0. Apply rank normalization to boost weak predictions t...\n- **../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb**: ESM2 embeddings (1280-dim) combined with hand-crafted features (amino acid composition, molecular weight, etc.) work well. MLP with BatchNorm, ReLU, Dropout. Train separate models per aspect (MF, BP, ...\n- **../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb**: Ensemble 3 MLPs with different hyperparameters (batch size, hidden dims, dropout). Use CosineAnnealingLR scheduler. Apply aspect-specific thresholds for predictions.\n- **../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb**: GOA (Gene Ontology Annotation) corrections: Apply known positive annotations with confidence 1.0. Remove predictions that conflict with 'NOT' annotations (negative annotations) and their descendants.\n- **exploration/eda.ipynb**: Training data: 82,404 proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Test set: 224,309 proteins. Aspect distribution: P (Biological Process) 250K, C (Cellular Component) 158K, F (Molecu...\n- **exploration/eda.ipynb**: GO term frequency is highly skewed: only 33 terms appear in >1000 proteins, 663 terms in >100 proteins. Most common term GO:0005515 appears in 33,713 proteins. Median term frequency is only 4.\n- **exploration/eda.ipynb**: Protein sequence lengths: Train mean 526, median 409. Test mean 429. Range from 2-35,213 amino acids. Need to handle variable length sequences.\n- **WebSearch: protein function prediction techniques**: Advanced techniques: DeepGO-SE uses semantic entailment with GO axioms. GOProFormer uses multimodal transformer for sequence + GO term embeddings. Graph attention networks (GATs) can model GO hierarch...\n\nRead `session_state.json` for the complete list and all notebook references.\n\n## SEED PROMPT STRATEGY\n# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\n- Test: 224,309 proteins to predict\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\n\n## Evaluation Metric\n- Weighted F1-measure using Information Accretion (IA) weights\n- Evaluated separately for MF, BP, CC subontologies, then averaged\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\n- Must predict GO terms with confidence scores (0-1)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\n   - State-of-the-art protein language model from Meta\n   - Best performance for protein function prediction\n   - Use mean pooling over sequence positions\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\n\n2. **ProtBERT**: 1024-dimensional embeddings\n   - Good alternative, well-tested on previous competitions\n   - Slightly faster than ESM2\n\n3. **T5 embeddings**: 1024-dimensional\n   - Another viable option\n\n## Model Architecture\n**Multi-Layer Perceptron on top of embeddings:**\n```\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> Output (num_classes)\n```\n\n**Key hyperparameters from top solutions:**\n- Hidden dimensions: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.3\n- Batch size: 64-128\n- Learning rate: 1e-3 to 1e-2\n- Epochs: 10-15\n- Use BCEWithLogitsLoss for multi-label classification\n- CosineAnnealingLR scheduler works well\n\n## Feature Engineering\n**Hand-crafted features to combine with embeddings:**\n- Amino acid composition (20 features)\n- Sequence length\n- Molecular weight\n- Isoelectric point\n- Aromaticity\n- Instability index\n- Gravy (hydropathicity)\n\n## Training Strategy\n1. **Train separate models per aspect (MF, BP, CC)**\n   - Each aspect has different GO term distributions\n   - Use aspect-specific thresholds for predictions\n\n2. **Multi-label classification setup:**\n   - Use top N most frequent GO terms (500-1500)\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\n   - BCEWithLogitsLoss or BCELoss\n\n3. **Validation:**\n   - 90/10 train/val split\n   - Monitor MultilabelF1Score\n\n## Ensembling (IMPORTANT)\n**Ensemble multiple approaches for best results:**\n\n1. **Model ensemble:**\n   - Train 3+ MLPs with different hyperparameters\n   - Average predictions from all models\n   - Different batch sizes, hidden dims, dropout rates\n\n2. **Embedding ensemble:**\n   - Combine predictions from ESM2-based and ProtBERT-based models\n\n3. **BLAST/Sequence similarity predictions:**\n   - Use sequence similarity to transfer annotations from known proteins\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\n   - Diamond BLAST is faster than standard BLAST\n\n## Post-Processing (CRITICAL FOR SCORE)\n**GO Hierarchy Propagation:**\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\n- Parse go-basic.obo for is_a and part_of relationships\n\n**Rank Normalization:**\n- If max prediction score is weak (< 0.95), scale up all scores\n- Target: make best non-root prediction at least 0.95\n- scale_factor = 0.95 / max_val\n\n**GOA Corrections (if available):**\n- Apply known positive annotations with confidence 1.0\n- Remove predictions conflicting with 'NOT' annotations\n- Propagate negative annotations to descendants\n\n## Temperature Scaling\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\n- T=1.5 often improves calibration\n- Helps with confidence score distribution\n\n## Prediction Thresholds\n- Use aspect-specific thresholds:\n  - MF: 0.1-0.2\n  - BP: 0.1-0.2  \n  - CC: 0.1-0.2\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\n- Maximum predictions per protein: 1000-1500\n\n## Advanced Techniques (from literature)\n1. **GO-aware architectures:**\n   - Graph attention networks (GATs) to model GO hierarchy\n   - Multimodal transformers for sequence + GO term embeddings\n   - Contrastive learning for GO term relationships\n\n2. **Semantic entailment:**\n   - DeepGO-SE approach: predict truth values for GO statements\n   - Exploit GO axioms and hierarchical constraints\n\n3. **Zero-shot prediction:**\n   - Learn GO term embeddings from textual definitions\n   - Enables prediction of unseen GO terms\n\n## Key Implementation Notes\n1. Parse GO ontology from go-basic.obo using obonet library\n2. Build ancestor map for efficient propagation\n3. Handle variable-length sequences with mean pooling\n4. Use sparse matrices for large label spaces\n5. Memory management: process in batches, use gc.collect()\n6. Gradient clipping (max_norm=1.0) for stable training\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence\n- No header\n- Confidence scores between 0 and 1\n- Sort by protein_id, then by confidence (descending)\n- Drop extremely low scores (< 0.001) to reduce file size\n\n## Reference Kernels\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\n\n\nStart with a simple baseline following this strategy. Reference the notebooks mentioned in the strategy for data understanding."
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: I need to first read the session_state.json to understand what has already been discovered, then check the data structure and existing notebooks to build a proper baseline model."
        },
        {
          "type": "tool_use",
          "id": "toolu_01XKMNW3SuHePLqB7pB3xghg",
          "name": "Read",
          "input": {
            "file_path": "/home/code/session_state.json"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_019eFcPBdXakAgHu2XGJX7TV",
          "name": "ListFiles",
          "input": {
            "directory_path": "/home/data"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01XKMNW3SuHePLqB7pB3xghg",
          "content": "{\n  \"workspace_dir\": \"/home/code\",\n  \"competition_id\": \"cafa-6-protein-function-prediction\",\n  \"metric_direction\": false,\n  \"start_time\": \"2026-01-13T10:08:38.546583\",\n  \"time_limit_minutes\": 2100,\n  \"experiments\": [],\n  \"candidates\": [],\n  \"submissions\": [],\n  \"strategy_history\": [\n    {\n      \"strategy\": \"# Protein Function Prediction - Techniques Guide\\n\\n## Data Understanding\\n**Reference notebooks for data characteristics:**\\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\\n- Test: 224,309 proteins to predict\\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\\n\\n## Evaluation Metric\\n- Weighted F1-measure using Information Accretion (IA) weights\\n- Evaluated separately for MF, BP, CC subontologies, then averaged\\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\\n- Must predict GO terms with confidence scores (0-1)\\n\\n## Protein Embeddings (CRITICAL)\\nPre-trained protein language models are essential for this task:\\n\\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\\n   - State-of-the-art protein language model from Meta\\n   - Best performance for protein function prediction\\n   - Use mean pooling over sequence positions\\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\\n\\n2. **ProtBERT**: 1024-dimensional embeddings\\n   - Good alternative, well-tested on previous competitions\\n   - Slightly faster than ESM2\\n\\n3. **T5 embeddings**: 1024-dimensional\\n   - Another viable option\\n\\n## Model Architecture\\n**Multi-Layer Perceptron on top of embeddings:**\\n```\\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\\n                              -> Linear -> Output (num_classes)\\n```\\n\\n**Key hyperparameters from top solutions:**\\n- Hidden dimensions: [1024, 512] or [864, 712]\\n- Dropout: 0.2-0.3\\n- Batch size: 64-128\\n- Learning rate: 1e-3 to 1e-2\\n- Epochs: 10-15\\n- Use BCEWithLogitsLoss for multi-label classification\\n- CosineAnnealingLR scheduler works well\\n\\n## Feature Engineering\\n**Hand-crafted features to combine with embeddings:**\\n- Amino acid composition (20 features)\\n- Sequence length\\n- Molecular weight\\n- Isoelectric point\\n- Aromaticity\\n- Instability index\\n- Gravy (hydropathicity)\\n\\n## Training Strategy\\n1. **Train separate models per aspect (MF, BP, CC)**\\n   - Each aspect has different GO term distributions\\n   - Use aspect-specific thresholds for predictions\\n\\n2. **Multi-label classification setup:**\\n   - Use top N most frequent GO terms (500-1500)\\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\\n   - BCEWithLogitsLoss or BCELoss\\n\\n3. **Validation:**\\n   - 90/10 train/val split\\n   - Monitor MultilabelF1Score\\n\\n## Ensembling (IMPORTANT)\\n**Ensemble multiple approaches for best results:**\\n\\n1. **Model ensemble:**\\n   - Train 3+ MLPs with different hyperparameters\\n   - Average predictions from all models\\n   - Different batch sizes, hidden dims, dropout rates\\n\\n2. **Embedding ensemble:**\\n   - Combine predictions from ESM2-based and ProtBERT-based models\\n\\n3. **BLAST/Sequence similarity predictions:**\\n   - Use sequence similarity to transfer annotations from known proteins\\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\\n   - Diamond BLAST is faster than standard BLAST\\n\\n## Post-Processing (CRITICAL FOR SCORE)\\n**GO Hierarchy Propagation:**\\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\\n- Parse go-basic.obo for is_a and part_of relationships\\n\\n**Rank Normalization:**\\n- If max prediction score is weak (< 0.95), scale up all scores\\n- Target: make best non-root prediction at least 0.95\\n- scale_factor = 0.95 / max_val\\n\\n**GOA Corrections (if available):**\\n- Apply known positive annotations with confidence 1.0\\n- Remove predictions conflicting with 'NOT' annotations\\n- Propagate negative annotations to descendants\\n\\n## Temperature Scaling\\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\\n- T=1.5 often improves calibration\\n- Helps with confidence score distribution\\n\\n## Prediction Thresholds\\n- Use aspect-specific thresholds:\\n  - MF: 0.1-0.2\\n  - BP: 0.1-0.2  \\n  - CC: 0.1-0.2\\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\\n- Maximum predictions per protein: 1000-1500\\n\\n## Advanced Techniques (from literature)\\n1. **GO-aware architectures:**\\n   - Graph attention networks (GATs) to model GO hierarchy\\n   - Multimodal transformers for sequence + GO term embeddings\\n   - Contrastive learning for GO term relationships\\n\\n2. **Semantic entailment:**\\n   - DeepGO-SE approach: predict truth values for GO statements\\n   - Exploit GO axioms and hierarchical constraints\\n\\n3. **Zero-shot prediction:**\\n   - Learn GO term embeddings from textual definitions\\n   - Enables prediction of unseen GO terms\\n\\n## Key Implementation Notes\\n1. Parse GO ontology from go-basic.obo using obonet library\\n2. Build ancestor map for efficient propagation\\n3. Handle variable-length sequences with mean pooling\\n4. Use sparse matrices for large label spaces\\n5. Memory management: process in batches, use gc.collect()\\n6. Gradient clipping (max_norm=1.0) for stable training\\n\\n## Submission Format\\n- TSV file: protein_id, GO_term, confidence\\n- No header\\n- Confidence scores between 0 and 1\\n- Sort by protein_id, then by confidence (descending)\\n- Drop extremely low scores (< 0.001) to reduce file size\\n\\n## Reference Kernels\\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\\n\",\n      \"agent\": \"explorer\",\n      \"timestamp\": \"2026-01-13T10:20:26.464389\"\n    }\n  ],\n  \"feedback_history\": [],\n  \"data_findings\": [\n    {\n      \"finding\": \"ProtBERT embeddings (1024-dim) are effective for protein function prediction. MLP with hidden layers (864, 712) on top of embeddings works well.\",\n      \"source\": \"../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Ensemble approach: combine deep learning predictions with BLAST-based predictions using weighted averaging (DL_WEIGHT=0.6, BLAST_WEIGHT=0.4)\",\n      \"source\": \"../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Temperature scaling (T=1.5) applied to logits before sigmoid improves calibration. Use top 500 most frequent GO terms as prediction targets.\",\n      \"source\": \"../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"GO hierarchy propagation is critical: ensure parent terms have scores >= child terms. Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0. Apply rank normalization to boost weak predictions to 0.95.\",\n      \"source\": \"../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"ESM2 embeddings (1280-dim) combined with hand-crafted features (amino acid composition, molecular weight, etc.) work well. MLP with BatchNorm, ReLU, Dropout. Train separate models per aspect (MF, BP, CC).\",\n      \"source\": \"../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Ensemble 3 MLPs with different hyperparameters (batch size, hidden dims, dropout). Use CosineAnnealingLR scheduler. Apply aspect-specific thresholds for predictions.\",\n      \"source\": \"../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"GOA (Gene Ontology Annotation) corrections: Apply known positive annotations with confidence 1.0. Remove predictions that conflict with 'NOT' annotations (negative annotations) and their descendants.\",\n      \"source\": \"../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Training data: 82,404 proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Test set: 224,309 proteins. Aspect distribution: P (Biological Process) 250K, C (Cellular Component) 158K, F (Molecular Function) 128K.\",\n      \"source\": \"exploration/eda.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"GO term frequency is highly skewed: only 33 terms appear in >1000 proteins, 663 terms in >100 proteins. Most common term GO:0005515 appears in 33,713 proteins. Median term frequency is only 4.\",\n      \"source\": \"exploration/eda.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Protein sequence lengths: Train mean 526, median 409. Test mean 429. Range from 2-35,213 amino acids. Need to handle variable length sequences.\",\n      \"source\": \"exploration/eda.ipynb\",\n      \"agent\": \"explorer\"\n    },\n    {\n      \"finding\": \"Advanced techniques: DeepGO-SE uses semantic entailment with GO axioms. GOProFormer uses multimodal transformer for sequence + GO term embeddings. Graph attention networks (GATs) can model GO hierarchy. STAR-GO integrates GO textual definitions with ontology graph structure.\",\n      \"source\": \"WebSearch: protein function prediction techniques\",\n      \"agent\": \"explorer\"\n    }\n  ],\n  \"web_research\": [\n    {\n      \"query\": \"What are the best techniques for protein function prediction using deep learning and Gene Ontology?\",\n      \"finding\": \"Protein function prediction relies on mapping an amino\\u2011acid sequence to the structured terms of the Gene Ontology (GO), which encodes molecular functions, biological processes and cellular components.  Deep\\u2011learning models are now the dominant approach because they can learn rich sequence representations while simultaneously exploiting the hierarchical constraints of GO, leading to more consistent and accurate annotations ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).\\n\\nThe most effective recent techniques combine large\\u2011scale protein language models with GO\\u2011aware architectures.  **DeepGO\\u2011SE** uses a pretrained language model to generate multiple approximate GO models and a neural network that predicts truth values for GO statements, effectively performing semantic entailment and achieving state\\u2011of\\u2011the\\u2011art performance on benchmark sets ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).  **POSA\\u2011GO** fuses hierarchical GO information with protein langu\",\n      \"agent\": \"explorer\"\n    }\n  ],\n  \"max_submissions\": 5,\n  \"remaining_submissions\": 4\n}",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_019eFcPBdXakAgHu2XGJX7TV",
          "content": "/home/data:\n[FILE] IA.tsv (969.3KB)\n[DIR]  Test/\n[DIR]  Train/\n[FILE] description.md (121.0B)\n[FILE] sample_submission.tsv (469.3KB)",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Let me explore the data structure more and check the sample submission format to understand what we need to produce."
        },
        {
          "type": "tool_use",
          "id": "toolu_01R4Gzb4EEDLNV8GwLqXCfKH",
          "name": "Read",
          "input": {
            "file_path": "/home/data/description.md"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_01SJcYCThc9WUAvdwLkN9dpD",
          "name": "Read",
          "input": {
            "file_path": "/home/data/sample_submission.tsv"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_01BG7x2wKCTkJYiLJCvf2gHi",
          "name": "ListFiles",
          "input": {
            "directory_path": "/home/data/Train"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_01EBy9XgCsWEcQXu7oruCWDY",
          "name": "ListFiles",
          "input": {
            "directory_path": "/home/data/Test"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01R4Gzb4EEDLNV8GwLqXCfKH",
          "content": "# cafa-6-protein-function-prediction\n\nCompetition: https://www.kaggle.com/competitions/cafa-6-protein-function-prediction",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01SJcYCThc9WUAvdwLkN9dpD",
          "content": "A0A0C5B5G6\tGO:0000001\t0.123\nA0A0C5B5G6\tGO:0000002\t0.456\nA0A0C5B5G6\tText\t0.123\tRegulates insulin sensitivity and metabolic homeostasis\nA0A0C5B5G6\tText\t0.456\tInhibits the folate cycle, thereby reducing de novo purine biosynthesis which leads to the accumulation of the de novo purine synthesis intermediate 5-aminoimidazole-4-carboxamide (AICAR)\nA0A0C5B5G6\tText\t0.456\tand the activation of the metabolic regulator 5-AMP-activated protein kinase (AMPK)\nA0A1B0GTW7\tGO:0000001\t0.123\nA0A1B0GTW7\tGO:0000002\t0.456\nA0A1B0GTW7\tText\t0.123\tPutative metalloproteinase that plays a role in left-right patterning process\nA0JNW5\tGO:0000001\t0.123\nA0JNW5\tGO:0000002\t0.456\nA0JP26\tGO:0000001\t0.123\nA0JP26\tGO:0000002\t0.456\nA0PK11\tGO:0000001\t0.123\nA0PK11\tGO:0000002\t0.456\nA1A4S6\tGO:0000001\t0.123\nA1A4S6\tGO:0000002\t0.456\nA1A519\tGO:0000001\t0.123\nA1A519\tGO:0000002\t0.456\nA1L190\tGO:0000001\t0.123\nA1L190\tGO:0000002\t0.456\nA1L3X0\tGO:0000001\t0.123\nA1L3X0\tGO:0000002\t0.456\nA1X283\tGO:0000001\t0.123\nA1X283\tGO:0000002\t0.456\nA2A2Y4\tGO:0000001\t0.123\nA2A2Y4\tGO:0000002\t0.456\nA2RU14\tGO:0000001\t0.123\nA2RU14\tGO:0000002\t0.456\nA2RUB6\tGO:0000001\t0.123\nA2RUB6\tGO:0000002\t0.456\nA2RUC4\tGO:0000001\t0.123\nA2RUC4\tGO:0000002\t0.456\nA4D1B5\tGO:0000001\t0.123\nA4D1B5\tGO:0000002\t0.456\nA4GXA9\tGO:0000001\t0.123\nA4GXA9\tGO:0000002\t0.456\nA5D8V7\tGO:0000001\t0.123\nA5D8V7\tGO:0000002\t0.456\nA5PLL7\tGO:0000001\t0.123\nA5PLL7\tGO:0000002\t0.456\nA6BM72\tGO:0000001\t0.123\nA6BM72\tGO:0000002\t0.456\nA6H8Y1\tGO:0000001\t0.123\nA6H8Y1\tGO:0000002\t0.456\nA6NCS4\tGO:0000001\t0.123\nA6NCS4\tGO:0000002\t0.456\nA6NFY7\tGO:0000001\t0.123\nA6NFY7\tGO:0000002\t0.456\nA6NGG8\tGO:0000001\t0.123\nA6NGG8\tGO:0000002\t0.456\nA6NI61\tGO:0000001\t0.123\nA6NI61\tGO:0000002\t0.456\nA6NKB5\tGO:0000001\t0.123\nA6NKB5\tGO:0000002\t0.456\nA6NNB3\tGO:0000001\t0.123\nA6NNB3\tGO:0000002\t0.456\nA7E2V4\tGO:0000001\t0.123\nA7E2V4\tGO:0000002\t0.456\nA7MCY6\tGO:0000001\t0.123\nA7MCY6\tGO:0000002\t0.456\nA7MD48\tGO:0000001\t0.123\nA7MD48\tGO:0000002\t0.456\nA7XYQ1\tGO:0000001\t0.123\nA7XYQ1\tGO:0000002\t0.456\nA8MQ03\tGO:0000001\t0.123\nA8MQ03\tGO:0000002\t0.456\nA8MW99\tGO:0000001\t0.123\nA8MW99\tGO:0000002\t0.456\nA9UHW6\tGO:0000001\t0.123\nA9UHW6\tGO:0000002\t0.456\nB1AK53\tGO:0000001\t0.123\nB1AK53\tGO:0000002\t0.456\nB2RUY7\tGO:0000001\t0.123\nB2RUY7\tGO:0000002\t0.456\nB3KU38\tGO:0000001\t0.123\nB3KU38\tGO:0000002\t0.456\nB6A8C7\tGO:0000001\t0.123\nB6A8C7\tGO:0000002\t0.456\nB7U540\tGO:0000001\t0.123\nB7U540\tGO:0000002\t0.456\nC9JLW8\tGO:0000001\t0.123\nC9JLW8\tGO:0000002\t0.456\nC9JRZ8\tGO:0000001\t0.123\nC9JRZ8\tGO:0000002\t0.456\nD3W0D1\tGO:0000001\t0.123\nD3W0D1\tGO:0000002\t0.456\nE0CX11\tGO:0000001\t0.123\nE0CX11\tGO:0000002\t0.456\nO00115\tGO:0000001\t0.123\nO00115\tGO:0000002\t0.456\nO00116\tGO:0000001\t0.123\nO00116\tGO:0000002\t0.456\nO00159\tGO:0000001\t0.123\nO00159\tGO:0000002\t0.456\nO00161\tGO:0000001\t0.123\nO00161\tGO:0000002\t0.456\nO00165\tGO:0000001\t0.123\nO00165\tGO:0000002\t0.456\nO00168\tGO:0000001\t0.123\nO00168\tGO:0000002\t0.456\nO00214\tGO:0000001\t0.123\nO00214\tGO:0000002\t0.456\nO00237\tGO:0000001\t0.123\nO00237\tGO:0000002\t0.456\nO00254\tGO:0000001\t0.123\nO00254\tGO:0000002\t0.456\nO00268\tGO:0000001\t0.123\nO00268\tGO:0000002\t0.456\nO00291\tGO:0000001\t0.123\nO00291\tGO:0000002\t0.456\nO00300\tGO:0000001\t0.123\nO00300\tGO:0000002\t0.456\nO00322\tGO:0000001\t0.123\nO00322\tGO:0000002\t0.456\nO00329\tGO:0000001\t0.123\nO00329\tGO:0000002\t0.456\nO00330\tGO:0000001\t0.123\nO00330\tGO:0000002\t0.456\nO00337\tGO:0000001\t0.123\nO00337\tGO:0000002\t0.456\nO00400\tGO:0000001\t0.123\nO00400\tGO:0000002\t0.456\nO00409\tGO:0000001\t0.123\nO00409\tGO:0000002\t0.456\nO00422\tGO:0000001\t0.123\nO00422\tGO:0000002\t0.456\nO00444\tGO:0000001\t0.123\nO00444\tGO:0000002\t0.456\nO00453\tGO:0000001\t0.123\nO00453\tGO:0000002\t0.456\nO00462\tGO:0000001\t0.123\nO00462\tGO:0000002\t0.456\nO00478\tGO:0000001\t0.123\nO00478\tGO:0000002\t0.456\nO00487\tGO:0000001\t0.123\nO00487\tGO:0000002\t0.456\nO00505\tGO:0000001\t0.123\nO00505\tGO:0000002\t0.456\nO00506\tGO:0000001\t0.123\nO00506\tGO:0000002\t0.456\nO00507\tGO:0000001\t0.123\nO00507\tGO:0000002\t0.456\nO00560\tGO:0000001\t0.123\nO00560\tGO:0000002\t0.456\nO00591\tGO:0000001\t0.123\nO00591\tGO:0000002\t0.456\nO00622\tGO:0000001\t0.123\nO00622\tGO:0000002\t0.456\nO00624\tGO:0000001\t0.123\nO00624\tGO:0000002\t0.456\nO00635\tGO:0000001\t0.123\nO00635\tGO:0000002\t0.456\nO00712\tGO:0000001\t0.123\nO00712\tGO:0000002\t0.456\nO00746\tGO:0000001\t0.123\nO00746\tGO:0000002\t0.456\nO00748\tGO:0000001\t0.123\nO00748\tGO:0000002\t0.456\nO00762\tGO:0000001\t0.123\nO00762\tGO:0000002\t0.456\nO14497\tGO:0000001\t0.123\nO14497\tGO:0000002\t0.456\nO14503\tGO:0000001\t0.123\nO14503\tGO:0000002\t0.456\nO14508\tGO:0000001\t0.123\nO14508\tGO:0000002\t0.456\nO14519\tGO:0000001\t0.123\nO14519\tGO:0000002\t0.456\nO14524\tGO:0000001\t0.123\nO14524\tGO:0000002\t0.456\nO14548\tGO:0000001\t0.123\nO14548\tGO:0000002\t0.456\nO14561\tGO:0000001\t0.123\nO14561\tGO:0000002\t0.456\nO14613\tGO:0000001\t0.123\nO14613\tGO:0000002\t0.456\nO14618\tGO:0000001\t0.123\nO14618\tGO:0000002\t0.456\nO14628\tGO:0000001\t0.123\nO14628\tGO:0000002\t0.456\nO14638\tGO:0000001\t0.123\nO14638\tGO:0000002\t0.456\nO14640\tGO:0000001\t0.123\nO14640\tGO:0000002\t0.456\nO14654\tGO:0000001\t0.123\nO14654\tGO:0000002\t0.456\nO14657\tGO:0000001\t0.123\nO14657\tGO:0000002\t0.456\nO14744\tGO:0000001\t0.123\nO14744\tGO:0000002\t0.456\nO14756\tGO:0000001\t0.123\nO14756\tGO:0000002\t0.456\nO14757\tGO:0000001\t0.123\nO14757\tGO:0000002\t0.456\nO14763\tGO:0000001\t0.123\nO14763\tGO:0000002\t0.456\nO14788\tGO:0000001\t0.123\nO14788\tGO:0000002\t0.456\nO14804\tGO:0000001\t0.123\nO14804\tGO:0000002\t0.456\nO14813\tGO:0000001\t0.123\nO14813\tGO:0000002\t0.456\nO14843\tGO:0000001\t0.123\nO14843\tGO:0000002\t0.456\nO14907\tGO:0000001\t0.123\nO14907\tGO:0000002\t0.456\nO14908\tGO:0000001\t0.123\nO14908\tGO:0000002\t0.456\nO14910\tGO:0000001\t0.123\nO14910\tGO:0000002\t0.456\nO14925\tGO:0000001\t0.123\nO14925\tGO:0000002\t0.456\nO14926\tGO:0000001\t0.123\nO14926\tGO:0000002\t0.456\nO14933\tGO:0000001\t0.123\nO14933\tGO:0000002\t0.456\nO14939\tGO:0000001\t0.123\nO14939\tGO:0000002\t0.456\nO14949\tGO:0000001\t0.123\nO14949\tGO:0000002\t0.456\nO14958\tGO:0000001\t0.123\nO14958\tGO:0000002\t0.456\nO14960\tGO:0000001\t0.123\nO14960\tGO:0000002\t0.456\nO14967\tGO:0000001\t0.123\nO14967\tGO:0000002\t0.456\nO14975\tGO:0000001\t0.123\nO14975\tGO:0000002\t0.456\nO14979\tGO:0000001\t0.123\nO14979\tGO:0000002\t0.456\nO14994\tGO:0000001\t0.123\nO14994\tGO:0000002\t0.456\nO15067\tGO:0000001\t0.123\nO15067\tGO:0000002\t0.456\nO15078\tGO:0000001\t0.123\nO15078\tGO:0000002\t0.456\nO15111\tGO:0000001\t0.123\nO15111\tGO:0000002\t0.456\nO15116\tGO:0000001\t0.123\nO15116\tGO:0000002\t0.456\nO15119\tGO:0000001\t0.123\nO15119\tGO:0000002\t0.456\nO15120\tGO:0000001\t0.123\nO15120\tGO:0000002\t0.456\nO15127\tGO:0000001\t0.123\nO15127\tGO:0000002\t0.456\nO15156\tGO:0000001\t0.123\nO15156\tGO:0000002\t0.456\nO15162\tGO:0000001\t0.123\nO15162\tGO:0000002\t0.456\nO15169\tGO:0000001\t0.123\nO15169\tGO:0000002\t0.456\nO15230\tGO:0000001\t0.123\nO15230\tGO:0000002\t0.456\nO15232\tGO:0000001\t0.123\nO15232\tGO:0000002\t0.456\nO15245\tGO:0000001\t0.123\nO15245\tGO:0000002\t0.456\nO15247\tGO:0000001\t0.123\nO15247\tGO:0000002\t0.456\nO15259\tGO:0000001\t0.123\nO15259\tGO:0000002\t0.456\nO15260\tGO:0000001\t0.123\nO15260\tGO:0000002\t0.456\nO15263\tGO:0000001\t0.123\nO15263\tGO:0000002\t0.456\nO15265\tGO:0000001\t0.123\nO15265\tGO:0000002\t0.456\nO15266\tGO:0000001\t0.123\nO15266\tGO:0000002\t0.456\nO15269\tGO:0000001\t0.123\nO15269\tGO:0000002\t0.456\nO15297\tGO:0000001\t0.123\nO15297\tGO:0000002\t0.456\nO15318\tGO:0000001\t0.123\nO15318\tGO:0000002\t0.456\nO15370\tGO:0000001\t0.123\nO15370\tGO:0000002\t0.456\nO15389\tGO:0000001\t0.123\nO15389\tGO:0000002\t0.456\nO15399\tGO:0000001\t0.123\nO15399\tGO:0000002\t0.456\nO15400\tGO:0000001\t0.123\nO15400\tGO:0000002\t0.456\nO15446\tGO:0000001\t0.123\nO15446\tGO:0000002\t0.456\nO15484\tGO:0000001\t0.123\nO15484\tGO:0000002\t0.456\nO15488\tGO:0000001\t0.123\nO15488\tGO:0000002\t0.456\nO15511\tGO:0000001\t0.123\nO15511\tGO:0000002\t0.456\nO15520\tGO:0000001\t0.123\nO15520\tGO:0000002\t0.456\nO15533\tGO:0000001\t0.123\nO15533\tGO:0000002\t0.456\nO15540\tGO:0000001\t0.123\nO15540\tGO:0000002\t0.456\nO15541\tGO:0000001\t0.123\nO15541\tGO:0000002\t0.456\nO15551\tGO:0000001\t0.123\nO15551\tGO:0000002\t0.456\nO15554\tGO:0000001\t0.123\nO15554\tGO:0000002\t0.456\nO42043\tGO:0000001\t0.123\nO42043\tGO:0000002\t0.456\nO43143\tGO:0000001\t0.123\nO43143\tGO:0000002\t0.456\nO43155\tGO:0000001\t0.123\nO43155\tGO:0000002\t0.456\nO43166\tGO:0000001\t0.123\nO43166\tGO:0000002\t0.456\nO43172\tGO:0000001\t0.123\nO43172\tGO:0000002\t0.456\nO43181\tGO:0000001\t0.123\nO43181\tGO:0000002\t0.456\nO43196\tGO:0000001\t0.123\nO43196\tGO:0000002\t0.456\nO43237\tGO:0000001\t0.123\nO43237\tGO:0000002\t0.456\nO43264\tGO:0000001\t0.123\nO43264\tGO:0000002\t0.456\nO43294\tGO:0000001\t0.123\nO43294\tGO:0000002\t0.456\nO43310\tGO:0000001\t0.123\nO43310\tGO:0000002\t0.456\nO43323\tGO:0000001\t0.123\nO43323\tGO:0000002\t0.456\nO43395\tGO:0000001\t0.123\nO43395\tGO:0000002\t0.456\nO43439\tGO:0000001\t0.123\nO43439\tGO:0000002\t0.456\nO43464\tGO:0000001\t0.123\nO43464\tGO:0000002\t0.456\nO43474\tGO:0000001\t0.123\nO43474\tGO:0000002\t0.456\nO43521\tGO:0000001\t0.123\nO43521\tGO:0000002\t0.456\nO43525\tGO:0000001\t0.123\nO43525\tGO:0000002\t0.456\nO43526\tGO:0000001\t0.123\nO43526\tGO:0000002\t0.456\nO43529\tGO:0000001\t0.123\nO43529\tGO:0000002\t0.456\nO43543\tGO:0000001\t0.123\nO43543\tGO:0000002\t0.456\nO43548\tGO:0000001\t0.123\nO43548\tGO:0000002\t0.456\nO43556\tGO:0000001\t0.123\nO43556\tGO:0000002\t0.456\nO43566\tGO:0000001\t0.123\nO43566\tGO:0000002\t0.456\nO43581\tGO:0000001\t0.123\nO43581\tGO:0000002\t0.456\nO43593\tGO:0000001\t0.123\nO43593\tGO:0000002\t0.456\nO43598\tGO:0000001\t0.123\nO43598\tGO:0000002\t0.456\nO43603\tGO:0000001\t0.123\nO43603\tGO:0000002\t0.456\nO43609\tGO:0000001\t0.123\nO43609\tGO:0000002\t0.456\nO43715\tGO:0000001\t0.123\nO43715\tGO:0000002\t0.456\nO43719\tGO:0000001\t0.123\nO43719\tGO:0000002\t0.456\nO43761\tGO:0000001\t0.123\nO43761\tGO:0000002\t0.456\nO43763\tGO:0000001\t0.123\nO43763\tGO:0000002\t0.456\nO43808\tGO:0000001\t0.123\nO43808\tGO:0000002\t0.456\nO43818\tGO:0000001\t0.123\nO43818\tGO:0000002\t0.456\nO43823\tGO:0000001\t0.123\nO43823\tGO:0000002\t0.456\nO43825\tGO:0000001\t0.123\nO43825\tGO:0000002\t0.456\nO43826\tGO:0000001\t0.123\nO43826\tGO:0000002\t0.456\nO43827\tGO:0000001\t0.123\nO43827\tGO:0000002\t0.456\nO43854\tGO:0000001\t0.123\nO43854\tGO:0000002\t0.456\nO43865\tGO:0000001\t0.123\nO43865\tGO:0000002\t0.456\nO43866\tGO:0000001\t0.123\nO43866\tGO:0000002\t0.456\nO43868\tGO:0000001\t0.123\nO43868\tGO:0000002\t0.456\nO43897\tGO:0000001\t0.123\nO43897\tGO:0000002\t0.456\nO43903\tGO:0000001\t0.123\nO43903\tGO:0000002\t0.456\nO43913\tGO:0000001\t0.123\nO43913\tGO:0000002\t0.456\nO43914\tGO:0000001\t0.123\nO43914\tGO:0000002\t0.456\nO60235\tGO:0000001\t0.123\nO60235\tGO:\n...(truncated)...\nO:0000002\t0.456\nQ04917\tGO:0000001\t0.123\nQ04917\tGO:0000002\t0.456\nQ05193\tGO:0000001\t0.123\nQ05193\tGO:0000002\t0.456\nQ05195\tGO:0000001\t0.123\nQ05195\tGO:0000002\t0.456\nQ05315\tGO:0000001\t0.123\nQ05315\tGO:0000002\t0.456\nQ05481\tGO:0000001\t0.123\nQ05481\tGO:0000002\t0.456\nQ05513\tGO:0000001\t0.123\nQ05513\tGO:0000002\t0.456\nQ05707\tGO:0000001\t0.123\nQ05707\tGO:0000002\t0.456\nQ05901\tGO:0000001\t0.123\nQ05901\tGO:0000002\t0.456\nQ05925\tGO:0000001\t0.123\nQ05925\tGO:0000002\t0.456\nQ06136\tGO:0000001\t0.123\nQ06136\tGO:0000002\t0.456\nQ06265\tGO:0000001\t0.123\nQ06265\tGO:0000002\t0.456\nQ06330\tGO:0000001\t0.123\nQ06330\tGO:0000002\t0.456\nQ06416\tGO:0000001\t0.123\nQ06416\tGO:0000002\t0.456\nQ06481\tGO:0000001\t0.123\nQ06481\tGO:0000002\t0.456\nQ06787\tGO:0000001\t0.123\nQ06787\tGO:0000002\t0.456\nQ07092\tGO:0000001\t0.123\nQ07092\tGO:0000002\t0.456\nQ07654\tGO:0000001\t0.123\nQ07654\tGO:0000002\t0.456\nQ07820\tGO:0000001\t0.123\nQ07820\tGO:0000002\t0.456\nQ07837\tGO:0000001\t0.123\nQ07837\tGO:0000002\t0.456\nQ07864\tGO:0000001\t0.123\nQ07864\tGO:0000002\t0.456\nQ07889\tGO:0000001\t0.123\nQ07889\tGO:0000002\t0.456\nQ07890\tGO:0000001\t0.123\nQ07890\tGO:0000002\t0.456\nQ07912\tGO:0000001\t0.123\nQ07912\tGO:0000002\t0.456\nQ07955\tGO:0000001\t0.123\nQ07955\tGO:0000002\t0.456\nQ07960\tGO:0000001\t0.123\nQ07960\tGO:0000002\t0.456\nQ08117\tGO:0000001\t0.123\nQ08117\tGO:0000002\t0.456\nQ08211\tGO:0000001\t0.123\nQ08211\tGO:0000002\t0.456\nQ08289\tGO:0000001\t0.123\nQ08289\tGO:0000002\t0.456\nQ08334\tGO:0000001\t0.123\nQ08334\tGO:0000002\t0.456\nQ08357\tGO:0000001\t0.123\nQ08357\tGO:0000002\t0.456\nQ08378\tGO:0000001\t0.123\nQ08378\tGO:0000002\t0.456\nQ08462\tGO:0000001\t0.123\nQ08462\tGO:0000002\t0.456\nQ08499\tGO:0000001\t0.123\nQ08499\tGO:0000002\t0.456\nQ08722\tGO:0000001\t0.123\nQ08722\tGO:0000002\t0.456\nQ08AE8\tGO:0000001\t0.123\nQ08AE8\tGO:0000002\t0.456\nQ08AH3\tGO:0000001\t0.123\nQ08AH3\tGO:0000002\t0.456\nQ09470\tGO:0000001\t0.123\nQ09470\tGO:0000002\t0.456\nQ0D2H9\tGO:0000001\t0.123\nQ0D2H9\tGO:0000002\t0.456\nQ0VD83\tGO:0000001\t0.123\nQ0VD83\tGO:0000002\t0.456\nQ0VF96\tGO:0000001\t0.123\nQ0VF96\tGO:0000002\t0.456\nQ0ZLH3\tGO:0000001\t0.123\nQ0ZLH3\tGO:0000002\t0.456\nQ10567\tGO:0000001\t0.123\nQ10567\tGO:0000002\t0.456\nQ10587\tGO:0000001\t0.123\nQ10587\tGO:0000002\t0.456\nQ10589\tGO:0000001\t0.123\nQ10589\tGO:0000002\t0.456\nQ11130\tGO:0000001\t0.123\nQ11130\tGO:0000002\t0.456\nQ11206\tGO:0000001\t0.123\nQ11206\tGO:0000002\t0.456\nQ12756\tGO:0000001\t0.123\nQ12756\tGO:0000002\t0.456\nQ12765\tGO:0000001\t0.123\nQ12765\tGO:0000002\t0.456\nQ12767\tGO:0000001\t0.123\nQ12767\tGO:0000002\t0.456\nQ12772\tGO:0000001\t0.123\nQ12772\tGO:0000002\t0.456\nQ12778\tGO:0000001\t0.123\nQ12778\tGO:0000002\t0.456\nQ12802\tGO:0000001\t0.123\nQ12802\tGO:0000002\t0.456\nQ12809\tGO:0000001\t0.123\nQ12809\tGO:0000002\t0.456\nQ12824\tGO:0000001\t0.123\nQ12824\tGO:0000002\t0.456\nQ12829\tGO:0000001\t0.123\nQ12829\tGO:0000002\t0.456\nQ12846\tGO:0000001\t0.123\nQ12846\tGO:0000002\t0.456\nQ12852\tGO:0000001\t0.123\nQ12852\tGO:0000002\t0.456\nQ12857\tGO:0000001\t0.123\nQ12857\tGO:0000002\t0.456\nQ12873\tGO:0000001\t0.123\nQ12873\tGO:0000002\t0.456\nQ12874\tGO:0000001\t0.123\nQ12874\tGO:0000002\t0.456\nQ12908\tGO:0000001\t0.123\nQ12908\tGO:0000002\t0.456\nQ12918\tGO:0000001\t0.123\nQ12918\tGO:0000002\t0.456\nQ12931\tGO:0000001\t0.123\nQ12931\tGO:0000002\t0.456\nQ12980\tGO:0000001\t0.123\nQ12980\tGO:0000002\t0.456\nQ12981\tGO:0000001\t0.123\nQ12981\tGO:0000002\t0.456\nQ13011\tGO:0000001\t0.123\nQ13011\tGO:0000002\t0.456\nQ13033\tGO:0000001\t0.123\nQ13033\tGO:0000002\t0.456\nQ13077\tGO:0000001\t0.123\nQ13077\tGO:0000002\t0.456\nQ13094\tGO:0000001\t0.123\nQ13094\tGO:0000002\t0.456\nQ13099\tGO:0000001\t0.123\nQ13099\tGO:0000002\t0.456\nQ13112\tGO:0000001\t0.123\nQ13112\tGO:0000002\t0.456\nQ13114\tGO:0000001\t0.123\nQ13114\tGO:0000002\t0.456\nQ13129\tGO:0000001\t0.123\nQ13129\tGO:0000002\t0.456\nQ13151\tGO:0000001\t0.123\nQ13151\tGO:0000002\t0.456\nQ13191\tGO:0000001\t0.123\nQ13191\tGO:0000002\t0.456\nQ13206\tGO:0000001\t0.123\nQ13206\tGO:0000002\t0.456\nQ13207\tGO:0000001\t0.123\nQ13207\tGO:0000002\t0.456\nQ13214\tGO:0000001\t0.123\nQ13214\tGO:0000002\t0.456\nQ13219\tGO:0000001\t0.123\nQ13219\tGO:0000002\t0.456\nQ13224\tGO:0000001\t0.123\nQ13224\tGO:0000002\t0.456\nQ13227\tGO:0000001\t0.123\nQ13227\tGO:0000002\t0.456\nQ13232\tGO:0000001\t0.123\nQ13232\tGO:0000002\t0.456\nQ13239\tGO:0000001\t0.123\nQ13239\tGO:0000002\t0.456\nQ13242\tGO:0000001\t0.123\nQ13242\tGO:0000002\t0.456\nQ13243\tGO:0000001\t0.123\nQ13243\tGO:0000002\t0.456\nQ13286\tGO:0000001\t0.123\nQ13286\tGO:0000002\t0.456\nQ13310\tGO:0000001\t0.123\nQ13310\tGO:0000002\t0.456\nQ13315\tGO:0000001\t0.123\nQ13315\tGO:0000002\t0.456\nQ13316\tGO:0000001\t0.123\nQ13316\tGO:0000002\t0.456\nQ13323\tGO:0000001\t0.123\nQ13323\tGO:0000002\t0.456\nQ13347\tGO:0000001\t0.123\nQ13347\tGO:0000002\t0.456\nQ13349\tGO:0000001\t0.123\nQ13349\tGO:0000002\t0.456\nQ13351\tGO:0000001\t0.123\nQ13351\tGO:0000002\t0.456\nQ13360\tGO:0000001\t0.123\nQ13360\tGO:0000002\t0.456\nQ13362\tGO:0000001\t0.123\nQ13362\tGO:0000002\t0.456\nQ13367\tGO:0000001\t0.123\nQ13367\tGO:0000002\t0.456\nQ13370\tGO:0000001\t0.123\nQ13370\tGO:0000002\t0.456\nQ13393\tGO:0000001\t0.123\nQ13393\tGO:0000002\t0.456\nQ13402\tGO:0000001\t0.123\nQ13402\tGO:0000002\t0.456\nQ13404\tGO:0000001\t0.123\nQ13404\tGO:0000002\t0.456\nQ13405\tGO:0000001\t0.123\nQ13405\tGO:0000002\t0.456\nQ13415\tGO:0000001\t0.123\nQ13415\tGO:0000002\t0.456\nQ13424\tGO:0000001\t0.123\nQ13424\tGO:0000002\t0.456\nQ13444\tGO:0000001\t0.123\nQ13444\tGO:0000002\t0.456\nQ13461\tGO:0000001\t0.123\nQ13461\tGO:0000002\t0.456\nQ13472\tGO:0000001\t0.123\nQ13472\tGO:0000002\t0.456\nQ13474\tGO:0000001\t0.123\nQ13474\tGO:0000002\t0.456\nQ13495\tGO:0000001\t0.123\nQ13495\tGO:0000002\t0.456\nQ13505\tGO:0000001\t0.123\nQ13505\tGO:0000002\t0.456\nQ13507\tGO:0000001\t0.123\nQ13507\tGO:0000002\t0.456\nQ13520\tGO:0000001\t0.123\nQ13520\tGO:0000002\t0.456\nQ13523\tGO:0000001\t0.123\nQ13523\tGO:0000002\t0.456\nQ13546\tGO:0000001\t0.123\nQ13546\tGO:0000002\t0.456\nQ13563\tGO:0000001\t0.123\nQ13563\tGO:0000002\t0.456\nQ13595\tGO:0000001\t0.123\nQ13595\tGO:0000002\t0.456\nQ13596\tGO:0000001\t0.123\nQ13596\tGO:0000002\t0.456\nQ13609\tGO:0000001\t0.123\nQ13609\tGO:0000002\t0.456\nQ13610\tGO:0000001\t0.123\nQ13610\tGO:0000002\t0.456\nQ13614\tGO:0000001\t0.123\nQ13614\tGO:0000002\t0.456\nQ13615\tGO:0000001\t0.123\nQ13615\tGO:0000002\t0.456\nQ13617\tGO:0000001\t0.123\nQ13617\tGO:0000002\t0.456\nQ13618\tGO:0000001\t0.123\nQ13618\tGO:0000002\t0.456\nQ13619\tGO:0000001\t0.123\nQ13619\tGO:0000002\t0.456\nQ13620\tGO:0000001\t0.123\nQ13620\tGO:0000002\t0.456\nQ13627\tGO:0000001\t0.123\nQ13627\tGO:0000002\t0.456\nQ13671\tGO:0000001\t0.123\nQ13671\tGO:0000002\t0.456\nQ13733\tGO:0000001\t0.123\nQ13733\tGO:0000002\t0.456\nQ13753\tGO:0000001\t0.123\nQ13753\tGO:0000002\t0.456\nQ13797\tGO:0000001\t0.123\nQ13797\tGO:0000002\t0.456\nQ13823\tGO:0000001\t0.123\nQ13823\tGO:0000002\t0.456\nQ13825\tGO:0000001\t0.123\nQ13825\tGO:0000002\t0.456\nQ13868\tGO:0000001\t0.123\nQ13868\tGO:0000002\t0.456\nQ13885\tGO:0000001\t0.123\nQ13885\tGO:0000002\t0.456\nQ13887\tGO:0000001\t0.123\nQ13887\tGO:0000002\t0.456\nQ13889\tGO:0000001\t0.123\nQ13889\tGO:0000002\t0.456\nQ13895\tGO:0000001\t0.123\nQ13895\tGO:0000002\t0.456\nQ13936\tGO:0000001\t0.123\nQ13936\tGO:0000002\t0.456\nQ13938\tGO:0000001\t0.123\nQ13938\tGO:0000002\t0.456\nQ13946\tGO:0000001\t0.123\nQ13946\tGO:0000002\t0.456\nQ14002\tGO:0000001\t0.123\nQ14002\tGO:0000002\t0.456\nQ14008\tGO:0000001\t0.123\nQ14008\tGO:0000002\t0.456\nQ14011\tGO:0000001\t0.123\nQ14011\tGO:0000002\t0.456\nQ14012\tGO:0000001\t0.123\nQ14012\tGO:0000002\t0.456\nQ14129\tGO:0000001\t0.123\nQ14129\tGO:0000002\t0.456\nQ14135\tGO:0000001\t0.123\nQ14135\tGO:0000002\t0.456\nQ14141\tGO:0000001\t0.123\nQ14141\tGO:0000002\t0.456\nQ14142\tGO:0000001\t0.123\nQ14142\tGO:0000002\t0.456\nQ14153\tGO:0000001\t0.123\nQ14153\tGO:0000002\t0.456\nQ14154\tGO:0000001\t0.123\nQ14154\tGO:0000002\t0.456\nQ14155\tGO:0000001\t0.123\nQ14155\tGO:0000002\t0.456\nQ14157\tGO:0000001\t0.123\nQ14157\tGO:0000002\t0.456\nQ14159\tGO:0000001\t0.123\nQ14159\tGO:0000002\t0.456\nQ14160\tGO:0000001\t0.123\nQ14160\tGO:0000002\t0.456\nQ14165\tGO:0000001\t0.123\nQ14165\tGO:0000002\t0.456\nQ14186\tGO:0000001\t0.123\nQ14186\tGO:0000002\t0.456\nQ14191\tGO:0000001\t0.123\nQ14191\tGO:0000002\t0.456\nQ14207\tGO:0000001\t0.123\nQ14207\tGO:0000002\t0.456\nQ14240\tGO:0000001\t0.123\nQ14240\tGO:0000002\t0.456\nQ14244\tGO:0000001\t0.123\nQ14244\tGO:0000002\t0.456\nQ14247\tGO:0000001\t0.123\nQ14247\tGO:0000002\t0.456\nQ14254\tGO:0000001\t0.123\nQ14254\tGO:0000002\t0.456\nQ14257\tGO:0000001\t0.123\nQ14257\tGO:0000002\t0.456\nQ14296\tGO:0000001\t0.123\nQ14296\tGO:0000002\t0.456\nQ14314\tGO:0000001\t0.123\nQ14314\tGO:0000002\t0.456\nQ14324\tGO:0000001\t0.123\nQ14324\tGO:0000002\t0.456\nQ14393\tGO:0000001\t0.123\nQ14393\tGO:0000002\t0.456\nQ14435\tGO:0000001\t0.123\nQ14435\tGO:0000002\t0.456\nQ14444\tGO:0000001\t0.123\nQ14444\tGO:0000002\t0.456\nQ14457\tGO:0000001\t0.123\nQ14457\tGO:0000002\t0.456\nQ14498\tGO:0000001\t0.123\nQ14498\tGO:0000002\t0.456\nQ14508\tGO:0000001\t0.123\nQ14508\tGO:0000002\t0.456\nQ14527\tGO:0000001\t0.123\nQ14527\tGO:0000002\t0.456\nQ14558\tGO:0000001\t0.123\nQ14558\tGO:0000002\t0.456\nQ14562\tGO:0000001\t0.123\nQ14562\tGO:0000002\t0.456\nQ14565\tGO:0000001\t0.123\nQ14565\tGO:0000002\t0.456\nQ14573\tGO:0000001\t0.123\nQ14573\tGO:0000002\t0.456\nQ14587\tGO:0000001\t0.123\nQ14587\tGO:0000002\t0.456\nQ14624\tGO:0000001\t0.123\nQ14624\tGO:0000002\t0.456\nQ14642\tGO:0000001\t0.123\nQ14642\tGO:0000002\t0.456\nQ14654\tGO:0000001\t0.123\nQ14654\tGO:0000002\t0.456\nQ14656\tGO:0000001\t0.123\nQ14656\tGO:0000002\t0.456\nQ14674\tGO:0000001\t0.123\nQ14674\tGO:0000002\t0.456\nQ14677\tGO:0000001\t0.123\nQ14677\tGO:0000002\t0.456\nQ14679\tGO:0000001\t0.123\nQ14679\tGO:0000002\t0.456\nQ14683\tGO:0000001\t0.123\nQ14683\tGO:0000002\t0.456\nQ14690\tGO:0000001\t0.123\nQ14690\tGO:0000002\t0.456\nQ14691\tGO:0000001\t0.123\nQ14691\tGO:0000002\t0.456\nQ14693\tGO:0000001\t0.123\nQ14693\tGO:0000002\t0.456\nQ14694\tGO:0000001\t0.123\nQ14694\tGO:0000002\t0.456\nQ14714\tGO:0000001\t0.123\nQ14714\tGO:0000002\t0.456\nQ14721\tGO:0000001\t0.123\nQ14721\tGO:0000002\t0.456\nQ14773\tGO:0000001\t0.123\nQ14773\tGO:0000002\t0.456\nQ14831\tGO:0000001\t0.123\nQ14831\tGO:0000002\t0.456\nQ14872\tGO:0000001\t0.123\nQ14872\tGO:0000002\t0.456\nQ14934\tGO:0000001\t0.123\nQ14934\tGO:0000002\t0.456\nQ14956\tGO:0000001\t0.123\nQ14956\tGO:0000002\t0.456\nQ14973\tGO:0000001\t0.123\nQ14973\tGO:0000002\t0.456\nQ14980\tGO:0000001\t0.123\nQ14980\tGO:0000002\t0.456\nQ149M9\tGO:0000001\t0.123\nQ149M9\tGO:0000002\t0.456\nQ149N8\tGO:0000001\t0.123\nQ149N8\tGO:0000002\t0.456\nQ14BN4\tGO:0000001\t0.123\nQ14BN4\tGO:0000002\t0.456\nQ14CN2\tGO:0000001\t0.123\nQ14CN2\tGO:0000002\t0.456\nQ14CZ8\tGO:0000001\t0.123\nQ14CZ8\tGO:0000002\t0.456\nQ14D04\tGO:0000001\t0.123\nQ14D04\tGO:0000002\t0.456\nQ14D33\tGO:0000001\t0.123\nQ14D33\tGO:0000002\t0.456\nQ15005\tGO:0000001\t0.123\nQ15005\tGO:0000002\t0.456\nQ15008\tGO:0000001\t0.123\nQ15008\tGO:0000002\t0.456\nQ15018\tGO:0000001\t0.123\nQ15018\tGO:0000002\t0.456\n",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01BG7x2wKCTkJYiLJCvf2gHi",
          "content": "/home/data/Train:\n[FILE] go-basic.obo (29.9MB)\n[FILE] train_sequences.fasta (50.8MB)\n[FILE] train_taxonomy.tsv (1023.3KB)\n[FILE] train_terms.tsv (10.3MB)",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01EBy9XgCsWEcQXu7oruCWDY",
          "content": "/home/data/Test:\n[FILE] testsuperset-taxon-list.tsv (228.0KB)\n[FILE] testsuperset.fasta (96.4MB)",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Now I understand the data structure. The submission format is TSV with protein_id, GO_term, confidence (tab-separated). Let me look at the train data files and check if there are pre-computed embeddings available, or if I need to generate them. Let me also check the reference kernels to see if there are pre-computed embeddings."
        },
        {
          "type": "tool_use",
          "id": "toolu_01F8z3eeoWQAbjvdHwN5DvLG",
          "name": "Bash",
          "input": {
            "command": "head -50 /home/data/Train/train_sequences.fasta"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_01JBXGjUvnxuTAjPkaW4QqTY",
          "name": "Bash",
          "input": {
            "command": "head -50 /home/data/Train/train_terms.tsv"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_01DQmzo6MVs2HwZrC25f9MRA",
          "name": "Bash",
          "input": {
            "command": "head -20 /home/data/IA.tsv"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01F8z3eeoWQAbjvdHwN5DvLG",
          "content": ">sp|A0A0C5B5G6|MOTSC_HUMAN Mitochondrial-derived peptide MOTS-c OS=Homo sapiens OX=9606 GN=MT-RNR1 PE=1 SV=1\nMRWQEMGYIFYPRKLR\n>sp|A0JNW5|BLT3B_HUMAN Bridge-like lipid transfer protein family member 3B OS=Homo sapiens OX=9606 GN=BLTP3B PE=1 SV=2\nMAGIIKKQILKHLSRFTKNLSPDKINLSTLKGEGELKNLELDEEVLQNMLDLPTWLAINK\nVFCNKASIRIPWTKLKTHPICLSLDKVIMEMSTCEEPRSPNGPSPIATASGQSEYGFAEK\nVVEGISVSVNSIVIRIGAKAFNASFELSQLRIYSVNAHWEHGDLRFTRIQDPQRGEVLTF\nKEINWQMIRIEADATQSSHLEIMCAPVRLITNQSKIRVTLKRRLKDCNVIATKLVLILDD\nLLWVLTDSQLKAMVQYAKSLSEAIEKSTEQRKSMAPEPTQSSTVVASAQQVKTTQTSNAP\nDVNDAIVKLFNDFDVKETSHHLVISHLDLHICDDIHAKEKESNRRITGGAMQLSFTQLTI\nDYYPYHKAGDSCNHWMYFSDATKTKNGWANELLHEFECNVEMLKQAVKDHNVGSPPKSPT\nHASPQHTQTEKDYPLKGTCRTPSVLSQQSKAKLMSSSVVVRLADFNIYQVSTAEQCRSSP\nKSMICCNKKSLYLPQEMSAVYIEFTEYYYPDGKDFPIPSPNLYSQLNALQFTVDERSILW\nLNQFLLDLKQSLNQFMAVYKLNDNSKSDEHVDVRVDGLMLKFVIPSEVKSECHQDQPRAI\nSIQSSEMIATNTRHCPNCRHSDLEALFQDFKDCDFFSKTYTSFPKSCDNFNLLHPIFQRH\nAHEQDTKMHEIYKGNITPQLNKNTLKTSAATDVWAVYFSQFWIDYEGMKSGKGRPISFVD\nSFPLSIWICQPTRYAESQKEPQTCNQVSLNTSQSESSDLAGRLKRKKLLKEYYSTESEPL\nTNGGQKPSSSDTFFRFSPSSSEADIHLLVHVHKHVSMQINHYQYLLLLFLHESLILLSEN\nLRKDVEAVTGSPASQTSICIGILLRSAELALLLHPVDQANTLKSPVSESVSPVVPDYLPT\nENGDFLSSKRKQISRDINRIRSVTVNHMSDNRSMSVDLSHIPLKDPLLFKSASDTNLQKG\nISFMDYLSDKHLGKISEDESSGLVYKSGSGEIGSETSDKKDSFYTDSSSILNYREDSNIL\nSFDSDGNQNILSSTLTSKGNETIESIFKAEDLLPEAASLSENLDISKEETPPVRTLKSQS\nSLSGKPKERCPPNLAPLCVSYKNMKRSSSQMSLDTISLDSMILEEQLLESDGSDSHMFLE\nKGNKKNSTTNYRGTAESVNAGANLQNYGETSPDAISTNSEGAQENHDDLMSVVVFKITGV\nNGEIDIRGEDTEICLQVNQVTPDQLGNISLRHYLCNRPVGSDQKAVIHSKSSPEISLRFE\nSGPGAVIHSLLAEKNGFLQCHIENFSTEFLTSSLMNIQHFLEDETVATVMPMKIQVSNTK\nINLKDDSPRSSTVSLEPAPVTVHIDHLVVERSDDGSFHIRDSHMLNTGNDLKENVKSDSV\nLLTSGKYDLKKQRSVTQATQTSPGVPWPSQSANFPEFSFDFTREQLMEENESLKQELAKA\nKMALAEAHLEKDALLHHIKKMTVE\n>sp|A0JP26|POTB3_HUMAN POTE ankyrin domain family member B3 OS=Homo sapiens OX=9606 GN=POTEB3 PE=1 SV=2\nMVAEVCSMPAASAVKKPFDLRSKMGKWCHHRFPCCRGSGKSNMGTSGDHDDSFMKTLRSK\nMGKCCHHCFPCCRGSGTSNVGTSGDHDNSFMKTLRSKMGKWCCHCFPCCRGSGKSNVGTW\nGDYDDSAFMEPRYHVRREDLDKLHRAAWWGKVPRKDLIVMLRDTDMNKRDKQKRTALHLA\nSANGNSEVVQLLLDRRCQLNVLDNKKRTALIKAVQCQEDECVLMLLEHGADGNIQDEYGN\nTALHYAIYNEDKLMAKALLLYGADIESKNKCGLTPLLLGVHEQKQQVVKFLIKKKANLNA\nLDRYGRTALILAVCCGSASIVNLLLEQNVDVSSQDLSGQTAREYAVSSHHHVICELLSDY\nKEKQMLKISSENSNPEQDLKLTSEEESQRLKVSENSQPEKMSQEPEINKDCDREVEEEIK\nKHGSNPVGLPENLTNGASAGNGDDGLIPQRKSRKPENQQFPDTENEEYHSDEQNDTQKQL\nSEEQNTGISQDEILTNKQKQIEVAEKEMNSKLSLSHKKEEDLLRENSMLREEIAMLRLEL\nDETKHQNQLRENKILEEIESVKEKLLKAIQLNEEALTKTSI\n>sp|A0PK11|CLRN2_HUMAN Clarin-2 OS=Homo sapiens OX=9606 GN=CLRN2 PE=1 SV=1\nMPGWFKKAWYGLASLLSFSSFILIIVALVVPHWLSGKILCQTGVDLVNATDRELVKFIGD\nIYYGLFRGCKVRQCGLGGRQSQFTIFPHLVKELNAGLHVMILLLLFLALALALVSMGFAI\nLNMIQVPYRAVSGPGGICLWNVLAGGVVALAIASFVAAVKFHDLTERIANFQEKLFQFVV\nVEEQYEESFWICVASASAHAANLVVVAISQIPLPEIKTKIEEATVTAEDILY\n>sp|A1A4S6|RHG10_HUMAN Rho GTPase-activating protein 10 OS=Homo sapiens OX=9606 GN=ARHGAP10 PE=1 SV=1\nMGLQPLEFSDCYLDSPWFRERIRAHEAELERTNKFIKELIKDGKNLIAATKSLSVAQRKF\nAHSLRDFKFEFIGDAVTDDERCIDASLREFSNFLKNLEEQREIMALSVTETLIKPLEKFR\nKEQLGAVKEEKKKFDKETEKNYSLIDKHLNLSAKKKDSHLQEADIQVEQNRQHFYELSLE\nYVCKLQEIQERKKFEFVEPMLSFFQGMFTFYHQGHELAKDFNHYKMELQINIQNTRNRFE\nGTRSEVEELMNKIRQNPKDHKRASQFTAEGYLYVQEKRPAPFGSSWVKHYCMYRKAAKKF\n",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01JBXGjUvnxuTAjPkaW4QqTY",
          "content": "EntryID\tterm\taspect\nQ5W0B1\tGO:0000785\tC\nQ5W0B1\tGO:0004842\tF\nQ5W0B1\tGO:0051865\tP\nQ5W0B1\tGO:0006275\tP\nQ5W0B1\tGO:0006513\tP\nQ5W0B1\tGO:0003682\tF\nQ5W0B1\tGO:0005515\tF\nQ3EC77\tGO:0000138\tC\nQ3EC77\tGO:0005794\tC\nQ8IZR5\tGO:0005515\tF\nQ8R2Z3\tGO:0140900\tF\nQ8R2Z3\tGO:0005254\tF\nQ8R2Z3\tGO:0035429\tP\nQ8R2Z3\tGO:0016323\tC\nQ8R2Z3\tGO:1902358\tP\nQ8R2Z3\tGO:0015701\tP\nQ8R2Z3\tGO:0015705\tP\nQ8R2Z3\tGO:0015706\tP\nQ8R2Z3\tGO:0019532\tP\nQ8R2Z3\tGO:0006590\tP\nQ8R2Z3\tGO:0006821\tP\nQ8R2Z3\tGO:0016020\tC\nQ8R2Z3\tGO:0005253\tF\nQ8R2Z3\tGO:0006820\tP\nQ8R2Z3\tGO:0001696\tP\nQ8R2Z3\tGO:0015106\tF\nP63027\tGO:0008021\tC\nP63027\tGO:0016079\tP\nP63027\tGO:0031201\tC\nP63027\tGO:0006887\tP\nP63027\tGO:0015031\tP\nP63027\tGO:0016020\tC\nP63027\tGO:0031982\tC\nP63027\tGO:0030136\tC\nP63027\tGO:0006906\tP\nP63027\tGO:0005515\tF\nP63027\tGO:0005886\tC\nP63027\tGO:0060203\tC\nP63027\tGO:0061202\tC\nP63027\tGO:0070083\tC\nP63027\tGO:0005829\tC\nP63027\tGO:0030667\tC\nP63027\tGO:0030669\tC\nP63027\tGO:0030672\tC\nP63027\tGO:0043308\tP\nP63027\tGO:0019905\tF\nQ6RSH7\tGO:0005515\tF\nP55850\tGO:0005576\tC\nP55850\tGO:0005737\tC\n",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01DQmzo6MVs2HwZrC25f9MRA",
          "content": "GO:0000001\t0.0\nGO:0000002\t2.8496657269155685\nGO:0000011\t0.13750352374993496\nGO:0000012\t6.038630248164372\nGO:0000017\t0.5145731728297583\nGO:0000018\t1.8796369862750815\nGO:0000019\t3.584962500721156\nGO:0000022\t0.0\nGO:0000023\t3.3115861506659034\nGO:0000024\t0.0\nGO:0000025\t0.0\nGO:0000027\t0.011874100288291778\nGO:0000028\t0.05343925896146044\nGO:0000032\t0.0\nGO:0000038\t3.679289883269309\nGO:0000041\t1.7961101648899815\nGO:0000045\t0.009277813588010752\nGO:0000050\t0.0\nGO:0000052\t4.142957953842043\nGO:0000053\t0.0\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Now I understand the data structure. Let me check if there are pre-computed embeddings available in the research kernels, as computing ESM2 embeddings for 82K training + 224K test proteins would be very time-consuming."
        },
        {
          "type": "tool_use",
          "id": "toolu_01NPwghjeLBuzcdixA4oKtFE",
          "name": "ListFiles",
          "input": {
            "directory_path": "/home/code/../research/kernels"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01NPwghjeLBuzcdixA4oKtFE",
          "content": "Not a directory: /home/code/../research/kernels",
          "is_error": true
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01ByRmwwZmgxF8Dqmh5X4Qmt",
          "name": "Bash",
          "input": {
            "command": "find /home -name \"*.pt\" -o -name \"*.npy\" -o -name \"*embedding*\" 2>/dev/null | head -50"
          }
        },
        {
          "type": "tool_use",
          "id": "toolu_0186RdsoMYuPE4ECJEmKin5F",
          "name": "Bash",
          "input": {
            "command": "ls -la /home/"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01ByRmwwZmgxF8Dqmh5X4Qmt",
          "content": "(no output)",
          "is_error": false
        },
        {
          "type": "tool_result",
          "tool_use_id": "toolu_0186RdsoMYuPE4ECJEmKin5F",
          "content": "total 136\ndrwxrwxrwx 1 root    root     4096 Jan 13 10:08 .\ndrwxr-xr-x 1 root    root     4096 Jan 13 10:08 ..\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =0.12.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =0.8.0\n-rw-rw-rw- 1 root    root    21477 Jan 13 10:07 =1.0.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =1.11.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =1.2.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =1.24.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =1.3.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =2.0.0\n-rw-rw-rw- 1 root    root     5320 Jan 13 10:07 =2.13.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =3.7.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =4.0.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =5.9.0\n-rw-rw-rw- 1 root    root        0 Jan 13 10:07 =6.25.0\ndrwxrwxrwx 1 root    root     4096 Jan 13 10:07 agent\ndrwxr-xr-x 6 nonroot nonroot  4096 Jan 13 10:20 code\ndrwxr-xr-x 4 nonroot nonroot  4096 Jan 13 10:08 data\n-rw-rw-rw- 1 root    root      990 Jan 12 06:00 instructions.txt\n-rw-rw-rw- 1 root    root     2586 Jan 12 06:00 instructions_obfuscated.txt\n-rw-rw-rw- 1 root    root    32427 Jan 12 06:00 kaggle_competition_strategy.txt\ndrwxr-xr-x 2 nonroot nonroot  4096 Jan 13 10:08 logs\ndrwxrwxrwx 1 nonroot nonroot  4096 Jan 13 10:09 nonroot\ndrwxr-xr-x 2 nonroot nonroot  4096 Jan 13 10:08 submission\n-rw-rw-rw- 1 root    root    19913 Jan 12 06:00 training_hints.txt\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: No pre-computed embeddings available. I need to create a baseline that can work efficiently. Given the large dataset (82K train + 224K test proteins), computing ESM2 embeddings for all proteins would take too long. Let me create a simpler baseline using hand-crafted features first, then work on embeddings incrementally. I'll start with amino acid composition features and a simple MLP model."
        },
        {
          "type": "tool_use",
          "id": "toolu_01AHn2WVXKkeZWCwBPgCUhuR",
          "name": "Bash",
          "input": {
            "command": "mkdir -p /home/code/experiments/001_baseline"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01AHn2WVXKkeZWCwBPgCUhuR",
          "content": "(no output)",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_016Ke1tTZYcSNi8h7E3kt4tr",
          "name": "CreateNotebook",
          "input": {
            "notebook_path": "/home/code/experiments/001_baseline/baseline.ipynb",
            "cells": [
              {
                "cell_type": "markdown",
                "content": "# Baseline Model for Protein Function Prediction\n\nThis baseline uses:\n1. Hand-crafted features (amino acid composition, sequence length, etc.)\n2. MLP classifier for multi-label classification\n3. Top 500 most frequent GO terms\n4. GO hierarchy propagation for post-processing"
              },
              {
                "cell_type": "code",
                "content": "import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport gc\nimport os\nfrom tqdm import tqdm\n\n# Check GPU\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")"
              },
              {
                "cell_type": "code",
                "content": "# Load data\ndef parse_fasta(fasta_file):\n    \"\"\"Parse FASTA file and return dict of protein_id -> sequence\"\"\"\n    sequences = {}\n    current_id = None\n    current_seq = []\n    \n    with open(fasta_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith('>'):\n                if current_id is not None:\n                    sequences[current_id] = ''.join(current_seq)\n                # Extract protein ID (first part after >)\n                header = line[1:]\n                # Handle different FASTA header formats\n                if '|' in header:\n                    parts = header.split('|')\n                    current_id = parts[1] if len(parts) > 1 else parts[0]\n                else:\n                    current_id = header.split()[0]\n                current_seq = []\n            else:\n                current_seq.append(line)\n        \n        # Don't forget the last sequence\n        if current_id is not None:\n            sequences[current_id] = ''.join(current_seq)\n    \n    return sequences\n\nprint(\"Loading training sequences...\")\ntrain_sequences = parse_fasta('/home/data/Train/train_sequences.fasta')\nprint(f\"Loaded {len(train_sequences)} training sequences\")\n\nprint(\"Loading test sequences...\")\ntest_sequences = parse_fasta('/home/data/Test/testsuperset.fasta')\nprint(f\"Loaded {len(test_sequences)} test sequences\")"
              },
              {
                "cell_type": "code",
                "content": "# Load GO terms\ntrain_terms = pd.read_csv('/home/data/Train/train_terms.tsv', sep='\\t')\nprint(f\"Train terms shape: {train_terms.shape}\")\nprint(train_terms.head())\n\n# Count term frequencies\nterm_counts = train_terms['term'].value_counts()\nprint(f\"\\nTotal unique GO terms: {len(term_counts)}\")\nprint(f\"\\nTop 10 most frequent terms:\")\nprint(term_counts.head(10))"
              },
              {
                "cell_type": "code",
                "content": "# Select top N most frequent GO terms for each aspect\nN_TERMS = 500  # Top 500 terms per aspect\n\n# Group by aspect\naspect_terms = train_terms.groupby('aspect')['term'].value_counts()\n\n# Get top N terms per aspect\nselected_terms = {}\nfor aspect in ['F', 'P', 'C']:\n    aspect_counts = aspect_terms[aspect]\n    selected_terms[aspect] = aspect_counts.head(N_TERMS).index.tolist()\n    print(f\"Aspect {aspect}: {len(selected_terms[aspect])} terms selected\")\n\n# Combine all selected terms\nall_selected_terms = []\nfor aspect in ['F', 'P', 'C']:\n    all_selected_terms.extend(selected_terms[aspect])\nall_selected_terms = list(set(all_selected_terms))\nprint(f\"\\nTotal selected terms: {len(all_selected_terms)}\")"
              },
              {
                "cell_type": "code",
                "content": "# Create protein -> terms mapping (only for selected terms)\nprotein_terms = defaultdict(list)\nfor _, row in train_terms.iterrows():\n    if row['term'] in all_selected_terms:\n        protein_terms[row['EntryID']].append(row['term'])\n\nprint(f\"Proteins with at least one selected term: {len(protein_terms)}\")"
              },
              {
                "cell_type": "code",
                "content": "# Feature extraction functions\nAMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n\ndef compute_aa_composition(sequence):\n    \"\"\"Compute amino acid composition (20 features)\"\"\"\n    seq_len = len(sequence)\n    if seq_len == 0:\n        return np.zeros(20)\n    \n    composition = np.zeros(20)\n    for i, aa in enumerate(AMINO_ACIDS):\n        composition[i] = sequence.count(aa) / seq_len\n    return composition\n\ndef compute_sequence_features(sequence):\n    \"\"\"Compute hand-crafted features from protein sequence\"\"\"\n    seq_len = len(sequence)\n    \n    # Amino acid composition (20 features)\n    aa_comp = compute_aa_composition(sequence)\n    \n    # Sequence length (log-transformed)\n    log_len = np.log1p(seq_len)\n    \n    # Molecular weight approximation (average AA weight ~110 Da)\n    aa_weights = {\n        'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75, 'H': 155,\n        'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132, 'P': 115, 'Q': 146,\n        'R': 174, 'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181\n    }\n    mol_weight = sum(aa_weights.get(aa, 110) for aa in sequence) / 1000  # in kDa\n    \n    # Hydrophobicity (Kyte-Doolittle scale)\n    hydro_scale = {\n        'A': 1.8, 'C': 2.5, 'D': -3.5, 'E': -3.5, 'F': 2.8, 'G': -0.4, 'H': -3.2,\n        'I': 4.5, 'K': -3.9, 'L': 3.8, 'M': 1.9, 'N': -3.5, 'P': -1.6, 'Q': -3.5,\n        'R': -4.5, 'S': -0.8, 'T': -0.7, 'V': 4.2, 'W': -0.9, 'Y': -1.3\n    }\n    gravy = np.mean([hydro_scale.get(aa, 0) for aa in sequence]) if seq_len > 0 else 0\n    \n    # Charge at pH 7 (approximate)\n    pos_charge = sequence.count('K') + sequence.count('R') + sequence.count('H') * 0.1\n    neg_charge = sequence.count('D') + sequence.count('E')\n    net_charge = (pos_charge - neg_charge) / max(seq_len, 1)\n    \n    # Aromaticity\n    aromatic = (sequence.count('F') + sequence.count('W') + sequence.count('Y')) / max(seq_len, 1)\n    \n    # Combine all features\n    features = np.concatenate([\n        aa_comp,  # 20 features\n        [log_len, mol_weight, gravy, net_charge, aromatic]  # 5 features\n    ])\n    \n    return features\n\n# Test feature extraction\ntest_seq = list(train_sequences.values())[0]\ntest_features = compute_sequence_features(test_seq)\nprint(f\"Feature dimension: {len(test_features)}\")"
              },
              {
                "cell_type": "code",
                "content": "# Extract features for all training proteins\nprint(\"Extracting features for training proteins...\")\ntrain_protein_ids = list(protein_terms.keys())\ntrain_features = []\n\nfor pid in tqdm(train_protein_ids):\n    if pid in train_sequences:\n        features = compute_sequence_features(train_sequences[pid])\n    else:\n        features = np.zeros(25)  # Default features if sequence not found\n    train_features.append(features)\n\ntrain_features = np.array(train_features, dtype=np.float32)\nprint(f\"Training features shape: {train_features.shape}\")"
              },
              {
                "cell_type": "code",
                "content": "# Create multi-label targets\nmlb = MultiLabelBinarizer(classes=all_selected_terms)\ntrain_labels = mlb.fit_transform([protein_terms[pid] for pid in train_protein_ids])\nprint(f\"Training labels shape: {train_labels.shape}\")\nprint(f\"Average labels per protein: {train_labels.sum(axis=1).mean():.2f}\")"
              },
              {
                "cell_type": "code",
                "content": "# Train/validation split\nX_train, X_val, y_train, y_val, train_ids, val_ids = train_test_split(\n    train_features, train_labels, train_protein_ids,\n    test_size=0.1, random_state=42\n)\nprint(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}\")"
              },
              {
                "cell_type": "code",
                "content": "# PyTorch Dataset\nclass ProteinDataset(Dataset):\n    def __init__(self, features, labels=None):\n        self.features = torch.FloatTensor(features)\n        self.labels = torch.FloatTensor(labels) if labels is not None else None\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return self.features[idx], self.labels[idx]\n        return self.features[idx]\n\n# Create datasets\ntrain_dataset = ProteinDataset(X_train, y_train)\nval_dataset = ProteinDataset(X_val, y_val)\n\n# DataLoaders\nBATCH_SIZE = 128\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
              },
              {
                "cell_type": "code",
                "content": "# MLP Model\nclass ProteinMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.3):\n        super().__init__()\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.append(nn.Linear(prev_dim, output_dim))\n        \n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Create model\nINPUT_DIM = train_features.shape[1]\nOUTPUT_DIM = len(all_selected_terms)\nHIDDEN_DIMS = [512, 256]\n\nmodel = ProteinMLP(INPUT_DIM, HIDDEN_DIMS, OUTPUT_DIM, dropout=0.3)\nmodel = model.to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
              },
              {
                "cell_type": "code",
                "content": "# Training setup\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for features, labels in loader:\n        features, labels = features.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for features, labels in loader:\n            features, labels = features.to(device), labels.to(device)\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            # Apply sigmoid for predictions\n            preds = torch.sigmoid(outputs)\n            all_preds.append(preds.cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n    \n    all_preds = np.vstack(all_preds)\n    all_labels = np.vstack(all_labels)\n    \n    # Calculate F1 score (threshold 0.5)\n    pred_binary = (all_preds > 0.5).astype(int)\n    \n    # Micro F1\n    tp = (pred_binary * all_labels).sum()\n    fp = (pred_binary * (1 - all_labels)).sum()\n    fn = ((1 - pred_binary) * all_labels).sum()\n    \n    precision = tp / (tp + fp + 1e-8)\n    recall = tp / (tp + fn + 1e-8)\n    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n    \n    return total_loss / len(loader), f1, precision, recall"
              },
              {
                "cell_type": "code",
                "content": "# Training loop\nEPOCHS = 15\nbest_f1 = 0\nbest_model_state = None\n\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_f1, val_prec, val_rec = validate(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_model_state = model.state_dict().copy()\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n          f\"Val F1={val_f1:.4f}, Prec={val_prec:.4f}, Rec={val_rec:.4f}\")\n\nprint(f\"\\nBest validation F1: {best_f1:.4f}\")\n\n# Load best model\nmodel.load_state_dict(best_model_state)"
              },
              {
                "cell_type": "code",
                "content": "# Parse GO ontology for hierarchy propagation\nimport re\n\ndef parse_go_obo(obo_file):\n    \"\"\"Parse GO OBO file and extract parent-child relationships\"\"\"\n    go_parents = defaultdict(set)\n    go_aspects = {}\n    \n    current_id = None\n    current_namespace = None\n    \n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            \n            if line == '[Term]':\n                current_id = None\n                current_namespace = None\n            elif line.startswith('id: GO:'):\n                current_id = line.split()[1]\n            elif line.startswith('namespace:'):\n                ns = line.split()[1]\n                if 'molecular_function' in ns:\n                    current_namespace = 'F'\n                elif 'biological_process' in ns:\n                    current_namespace = 'P'\n                elif 'cellular_component' in ns:\n                    current_namespace = 'C'\n                if current_id:\n                    go_aspects[current_id] = current_namespace\n            elif line.startswith('is_a:'):\n                parent = line.split()[1]\n                if current_id:\n                    go_parents[current_id].add(parent)\n            elif line.startswith('relationship: part_of'):\n                parent = line.split()[2]\n                if current_id:\n                    go_parents[current_id].add(parent)\n    \n    return go_parents, go_aspects\n\nprint(\"Parsing GO ontology...\")\ngo_parents, go_aspects = parse_go_obo('/home/data/Train/go-basic.obo')\nprint(f\"GO terms with parents: {len(go_parents)}\")\nprint(f\"GO terms with aspects: {len(go_aspects)}\")"
              },
              {
                "cell_type": "code",
                "content": "# Build ancestor map for efficient propagation\ndef get_all_ancestors(term, go_parents, cache=None):\n    \"\"\"Get all ancestors of a GO term\"\"\"\n    if cache is None:\n        cache = {}\n    \n    if term in cache:\n        return cache[term]\n    \n    ancestors = set()\n    for parent in go_parents.get(term, set()):\n        ancestors.add(parent)\n        ancestors.update(get_all_ancestors(parent, go_parents, cache))\n    \n    cache[term] = ancestors\n    return ancestors\n\n# Pre-compute ancestors for all selected terms\nprint(\"Computing ancestors for selected terms...\")\nancestor_cache = {}\nterm_ancestors = {}\nfor term in tqdm(all_selected_terms):\n    term_ancestors[term] = get_all_ancestors(term, go_parents, ancestor_cache)\n\nprint(f\"Computed ancestors for {len(term_ancestors)} terms\")"
              },
              {
                "cell_type": "code",
                "content": "# Extract features for test proteins\nprint(\"Extracting features for test proteins...\")\ntest_protein_ids = list(test_sequences.keys())\ntest_features = []\n\nfor pid in tqdm(test_protein_ids):\n    features = compute_sequence_features(test_sequences[pid])\n    test_features.append(features)\n\ntest_features = np.array(test_features, dtype=np.float32)\nprint(f\"Test features shape: {test_features.shape}\")"
              },
              {
                "cell_type": "code",
                "content": "# Generate predictions for test set\nprint(\"Generating predictions...\")\ntest_dataset = ProteinDataset(test_features)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\nmodel.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for features in tqdm(test_loader):\n        features = features.to(device)\n        outputs = model(features)\n        # Apply temperature scaling\n        T = 1.5\n        preds = torch.sigmoid(outputs / T)\n        all_preds.append(preds.cpu().numpy())\n\ntest_preds = np.vstack(all_preds)\nprint(f\"Test predictions shape: {test_preds.shape}\")"
              },
              {
                "cell_type": "code",
                "content": "# Load IA weights\nia_df = pd.read_csv('/home/data/IA.tsv', sep='\\t', header=None, names=['term', 'ia_weight'])\nia_weights = dict(zip(ia_df['term'], ia_df['ia_weight']))\nprint(f\"Loaded IA weights for {len(ia_weights)} terms\")"
              },
              {
                "cell_type": "code",
                "content": "# Root terms for each aspect\nROOT_TERMS = {\n    'F': 'GO:0003674',  # molecular_function\n    'P': 'GO:0008150',  # biological_process\n    'C': 'GO:0005575'   # cellular_component\n}\n\ndef propagate_predictions(protein_preds, terms, term_ancestors, go_aspects):\n    \"\"\"Propagate predictions to ancestors and apply post-processing\"\"\"\n    # Create term -> score mapping\n    term_scores = {}\n    for i, term in enumerate(terms):\n        if protein_preds[i] > 0.001:  # Only consider non-trivial predictions\n            term_scores[term] = protein_preds[i]\n            # Propagate to ancestors\n            for ancestor in term_ancestors.get(term, set()):\n                if ancestor not in term_scores:\n                    term_scores[ancestor] = protein_preds[i]\n                else:\n                    term_scores[ancestor] = max(term_scores[ancestor], protein_preds[i])\n    \n    # Force root terms to 1.0\n    for root in ROOT_TERMS.values():\n        term_scores[root] = 1.0\n    \n    # Rank normalization: scale up if max non-root score is weak\n    non_root_scores = [s for t, s in term_scores.items() if t not in ROOT_TERMS.values()]\n    if non_root_scores:\n        max_score = max(non_root_scores)\n        if max_score < 0.95 and max_score > 0:\n            scale_factor = 0.95 / max_score\n            for term in term_scores:\n                if term not in ROOT_TERMS.values():\n                    term_scores[term] = min(term_scores[term] * scale_factor, 0.99)\n    \n    return term_scores\n\n# Test propagation on one protein\ntest_prop = propagate_predictions(test_preds[0], all_selected_terms, term_ancestors, go_aspects)\nprint(f\"Example propagated predictions: {len(test_prop)} terms\")"
              },
              {
                "cell_type": "code",
                "content": "# Generate submission\nprint(\"Generating submission file...\")\nsubmission_rows = []\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score descending\n    sorted_terms = sorted(term_scores.items(), key=lambda x: -x[1])\n    \n    # Keep top predictions (max 1500 per protein)\n    for term, score in sorted_terms[:1500]:\n        if score > 0.001:  # Filter very low scores\n            submission_rows.append((pid, term, score))\n\nprint(f\"Total submission rows: {len(submission_rows)}\")"
              },
              {
                "cell_type": "code",
                "content": "# Save submission\nos.makedirs('/home/submission', exist_ok=True)\nsubmission_path = '/home/submission/submission.csv'\n\nwith open(submission_path, 'w') as f:\n    for pid, term, score in submission_rows:\n        f.write(f\"{pid}\\t{term}\\t{score:.6f}\\n\")\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"File size: {os.path.getsize(submission_path) / 1e6:.2f} MB\")"
              },
              {
                "cell_type": "code",
                "content": "# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASELINE MODEL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Features: Hand-crafted (AA composition + sequence properties)\")\nprint(f\"Feature dimension: {INPUT_DIM}\")\nprint(f\"Model: MLP with hidden dims {HIDDEN_DIMS}\")\nprint(f\"Number of GO terms predicted: {len(all_selected_terms)}\")\nprint(f\"Best validation F1: {best_f1:.4f}\")\nprint(f\"Submission rows: {len(submission_rows)}\")\nprint(\"=\"*50)"
              }
            ]
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_016Ke1tTZYcSNi8h7E3kt4tr",
          "content": "Created: /home/code/experiments/001_baseline/baseline.ipynb\n\nStrong Note: OOM is unrecoverable. Use strong models - just estimate memory first.",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Now let me execute the notebook to train the baseline model."
        },
        {
          "type": "tool_use",
          "id": "toolu_013joE26fdTHirFboUTrGRJy",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 0,
            "end_cell": 2,
            "duration": 120
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_013joE26fdTHirFboUTrGRJy",
          "content": "Executed cells 0-2\n==================================================\n\nCell 1:\n```\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport gc\nimport os\nfrom tqdm import tqdm\n\n# Check GPU\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_pr\n```\nOutput:\nGPU available: True\nGPU: NVIDIA H100 80GB HBM3\nMemory: 85.0 GB\nUsing device: cuda\n\n\nCell 2:\n```\n# Load data\ndef parse_fasta(fasta_file):\n    \"\"\"Parse FASTA file and return dict of protein_id -> sequence\"\"\"\n    sequences = {}\n    current_id = None\n    current_seq = []\n    \n    with open(fasta_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith('>'):\n                if current_id is not None:\n                    sequences[current_id] = ''.join(current_seq)\n                # Extract protein ID (first part after >)\n                header = lin\n```\nOutput:\nLoading training sequences...\n\nLoaded 82404 training sequences\nLoading test sequences...\n\nLoaded 224309 test sequences\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01CgnL5zxfHH6R8242obcSFn",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 3,
            "end_cell": 5,
            "duration": 60
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01CgnL5zxfHH6R8242obcSFn",
          "content": "Executed cells 3-5\n==================================================\n\nCell 3:\n```\n# Load GO terms\ntrain_terms = pd.read_csv('/home/data/Train/train_terms.tsv', sep='\\t')\nprint(f\"Train terms shape: {train_terms.shape}\")\nprint(train_terms.head())\n\n# Count term frequencies\nterm_counts = train_terms['term'].value_counts()\nprint(f\"\\nTotal unique GO terms: {len(term_counts)}\")\nprint(f\"\\nTop 10 most frequent terms:\")\nprint(term_counts.head(10))\n```\nOutput:\nTrain terms shape: (537027, 3)\n  EntryID        term aspect\n0  Q5W0B1  GO:0000785      C\n1  Q5W0B1  GO:0004842      F\n2  Q5W0B1  GO:0051865      P\n3  Q5W0B1  GO:0006275      P\n4  Q5W0B1  GO:0006513      P\n\nTotal unique GO terms: 26125\n\nTop 10 most frequent terms:\nterm\nGO:0005515    33713\nGO:0005634    13283\nGO:0005829    13040\nGO:0005886    10150\nGO:0005737     9442\nGO:0005739     5807\nGO:0005654     5065\nGO:0016020     3563\nGO:0042802     3547\nGO:0005576     3241\nName: count, dtype: int64\n\n\nCell 4:\n```\n# Select top N most frequent GO terms for each aspect\nN_TERMS = 500  # Top 500 terms per aspect\n\n# Group by aspect\naspect_terms = train_terms.groupby('aspect')['term'].value_counts()\n\n# Get top N terms per aspect\nselected_terms = {}\nfor aspect in ['F', 'P', 'C']:\n    aspect_counts = aspect_terms[aspect]\n    selected_terms[aspect] = aspect_counts.head(N_TERMS).index.tolist()\n    print(f\"Aspect {aspect}: {len(selected_terms[aspect])} terms selected\")\n\n# Combine all selected terms\nall_selected_term\n```\nOutput:\nAspect F: 500 terms selected\nAspect P: 500 terms selected\nAspect C: 500 terms selected\n\nTotal selected terms: 1500\n\n\nCell 5:\n```\n# Create protein -> terms mapping (only for selected terms)\nprotein_terms = defaultdict(list)\nfor _, row in train_terms.iterrows():\n    if row['term'] in all_selected_terms:\n        protein_terms[row['EntryID']].append(row['term'])\n\nprint(f\"Proteins with at least one selected term: {len(protein_terms)}\")\n```\nOutput:\nProteins with at least one selected term: 76029\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01DAGKivfkrLFphkvsd63LkT",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 6,
            "end_cell": 8,
            "duration": 120
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01DAGKivfkrLFphkvsd63LkT",
          "content": "Executed cells 6-8\n==================================================\n\nCell 6:\n```\n# Feature extraction functions\nAMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n\ndef compute_aa_composition(sequence):\n    \"\"\"Compute amino acid composition (20 features)\"\"\"\n    seq_len = len(sequence)\n    if seq_len == 0:\n        return np.zeros(20)\n    \n    composition = np.zeros(20)\n    for i, aa in enumerate(AMINO_ACIDS):\n        composition[i] = sequence.count(aa) / seq_len\n    return composition\n\ndef compute_sequence_features(sequence):\n    \"\"\"Compute hand-crafted features from protein sequence\"\"\"\n   \n```\nOutput:\nFeature dimension: 25\n\n\nCell 7:\n```\n# Extract features for all training proteins\nprint(\"Extracting features for training proteins...\")\ntrain_protein_ids = list(protein_terms.keys())\ntrain_features = []\n\nfor pid in tqdm(train_protein_ids):\n    if pid in train_sequences:\n        features = compute_sequence_features(train_sequences[pid])\n    else:\n        features = np.zeros(25)  # Default features if sequence not found\n    train_features.append(features)\n\ntrain_features = np.array(train_features, dtype=np.float32)\nprint(f\"Training f\n```\nOutput:\nExtracting features for training proteins...\n\n\r  0%|          | 0/76029 [00:00<?, ?it/s]\n\r  1%|          | 914/76029 [00:00<00:08, 9138.11it/s]\n\r  3%|\u258e         | 1916/76029 [00:00<00:07, 9653.58it/s]\n\r  4%|\u258d         | 2967/76029 [00:00<00:07, 10041.54it/s]\n\r  5%|\u258c         | 3972/76029 [00:00<00:07, 9717.66it/s] \n\r  7%|\u258b         | 4946/76029 [00:00<00:07, 9669.61it/s]\n\r  8%|\u258a         | 5938/76029 [00:00<00:07, 9751.71it/s]\n\r  9%|\u2589         | 6914/76029 [00:00<00:07, 9678.88it/s]\n\r 10%|\u2588         | 7917/76029 [00:00<00:06, 9787.71it/s]\n\r 12%|\u2588\u258f        | 8919/76029 [00:00<00:06, 9858.81it/s]\n\r 13%|\u2588\u258e        | 9907/76029 [00:01<00:06, 9863.51it/s]\n\r 14%|\u2588\u258d        | 10894/76029 [00:01<00:06, 9667.45it/s]\n\r 16%|\u2588\u258c        | 11862/76029 [00:01<00:06, 9600.51it/s]\n\r 17%|\u2588\u258b        | 12886/76029 [00:01<00:06, 9788.68it/s]\n\r 18%|\u2588\u258a        | 13866/76029 [00:01<00:06, 9707.13it/s]\n\r 20%|\u2588\u2589        | 14878/76029 [00:01<00:06, 9827.22it/s]\n\r 21%|\u2588\u2588        | 15862/76029 [00:01<00:06, 9807.90it/s]\n\r 22%|\u2588\u2588\u258f       | 16858/76029 [00:01<00:06, 9852.15it/s]\n\r 24%|\u2588\u2588\u258e       | 17885/76029 [00:01<00:05, 9975.17it/s]\n\r 25%|\u2588\u2588\u258d       | 18883/76029 [00:01<00:05, 9945.48it/s]\n\r 26%|\u2588\u2588\u258c       | 19913/76029 [00:02<00:05, 10048.25it/s]\n\r 28%|\u2588\u2588\u258a       | 20975/76029 [00:02<00:05, 10217.44it/s]\n\r 29%|\u2588\u2588\u2589       | 21997/76029 [00:02<00:05, 9947.81it/s] \n\r 30%|\u2588\u2588\u2588       | 22994/76029 [00:02<00:05, 9897.79it/s]\n\r 32%|\u2588\u2588\u2588\u258f      | 24003/76029 [00:02<00:05, 9950.94it/s]\n\r 33%|\u2588\u2588\u2588\u258e      | 25032/76029 [00:02<00:05, 10049.86it/s]\n\r 34%|\u2588\u2588\u2588\u258d      | 26048/76029 [00:02<00:04, 10070.78it/s]\n\r 36%|\u2588\u2588\u2588\u258c      | 27056/76029 [00:02<00:04, 9932.22it/s] \n\r 37%|\u2588\u2588\u2588\u258b      | 28050/76029 [00:02<00:04, 9826.68it/s]\n\r 38%|\u2588\u2588\u2588\u258a      | 29097/76029 [00:02<00:04, 10012.34it/s]\n\r 40%|\u2588\u2588\u2588\u2589      | 30103/76029 [00:03<00:04, 10024.62it/s]\n\r 41%|\u2588\u2588\u2588\u2588      | 31107/76029 [00:03<00:04, 9885.92it/s] \n\r 42%|\u2588\u2588\u2588\u2588\u258f     | 32122/76029 [00:03<00:04, 9962.55it/s]\n\r 44%|\u2588\u2588\u2588\u2588\u258e     | 33119/76029 [00:03<00:04, 9909.56it/s]\n\r 45%|\u2588\u2588\u2588\u2588\u258d     | 34170/76029 [00:03<00:04, 10080.71it/s]\n\r 46%|\u2588\u2588\u2588\u2588\u258b     | 35179/76029 [00:03<00:04, 10071.15it/s]\n\r 48%|\u2588\u2588\u2588\u2588\u258a     | 36187/76029 [00:03<00:03, 10042.26it/s]\n\r 49%|\u2588\u2588\u2588\u2588\u2589     | 37231/76029 [00:03<00:03, 10160.47it/s]\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 38248/76029 [00:03<00:03, 10013.64it/s]\n\r 52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 39250/76029 [00:03<00:03, 9977.02it/s] \n\r 53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 40289/76029 [00:04<00:03, 10098.28it/s]\n\r 54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 41319/76029 [00:04<00:03, 10156.98it/s]\n\r 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 42336/76029 [00:04<00:03, 10108.28it/s]\n\r 57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 43348/76029 [00:04<00:03, 10035.17it/s]\n\r 58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 44352/76029 [00:04<00:03, 9921.82it/s] \n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 45363/76029 [00:04<00:03, 9975.73it/s]\n\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 46421/76029 [00:04<00:02, 10154.11it/s]\n\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 47437/76029 [00:04<00:02, 9968.56it/s] \n\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 48464/76029 [00:04<00:02, 10056.29it/s]\n\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 49514/76029 [00:04<00:02, 10187.38it/s]\n\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 50588/76029 [00:05<00:02, 10347.47it/s]\n\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 51624/76029 [00:05<00:02, 10206.08it/s]\n\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 52662/76029 [00:05<00:02, 10255.82it/s]\n\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 53691/76029 [00:05<00:02, 10264.42it/s]\n\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 54718/76029 [00:05<00:02, 10138.14it/s]\n\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 55776/76029 [00:05<00:01, 10266.31it/s]\n\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 56804/76029 [00:05<00:01, 10055.27it/s]\n\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 57811/76029 [00:05<00:01, 9957.70it/s] \n\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 58824/76029 [00:05<00:01, 10006.57it/s]\n\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 59898/76029 [00:05<00:01, 10221.34it/s]\n\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 60921/76029 [00:06<00:01, 10048.52it/s]\n\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 61932/76029 [00:06<00:01, 10064.65it/s]\n\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 62955/76029 [00:06<00:01, 10112.92it/s]\n\r 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 63967/76029 [00:06<00:01, 9934.65it/s] \n\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 65030/76029 [00:06<00:01, 10137.96it/s]\n\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 66083/76029 [00:06<00:00, 10253.07it/s]\n\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 67110/76029 [00:06<00:00, 10138.61it/s]\n\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 68166/76029 [00:06<00:00, 10262.93it/s]\n\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 69209/76029 [00:06<00:00, 10308.59it/s]\n\r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 70241/76029 [00:07<00:00, 10041.88it/s]\n\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 71249/76029 [00:07<00:00, 10051.27it/s]\n\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 72323/76029 [00:07<00:00, 10252.83it/s]\n\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 73350/76029 [00:07<00:00, 9992.29it/s] \n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 74392/76029 [00:07<00:00, 10111.69it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 75405/76029 [00:07<00:00, 10115.08it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 76029/76029 [00:07<00:00, 10011.66it/s]\nTraining features shape: (76029, 25)\n\n\n\n\nCell 8:\n```\n# Create multi-label targets\nmlb = MultiLabelBinarizer(classes=all_selected_terms)\ntrain_labels = mlb.fit_transform([protein_terms[pid] for pid in train_protein_ids])\nprint(f\"Training labels shape: {train_labels.shape}\")\nprint(f\"Average labels per protein: {train_labels.sum(axis=1).mean():.2f}\")\n```\nOutput:\nTraining labels shape: (76029, 1500)\nAverage labels per protein: 4.36\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01QYHQfFufRoJKDFVTo3SVrh",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 9,
            "end_cell": 12,
            "duration": 60
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01QYHQfFufRoJKDFVTo3SVrh",
          "content": "Executed cells 9-12\n==================================================\n\nCell 9:\n```\n# Train/validation split\nX_train, X_val, y_train, y_val, train_ids, val_ids = train_test_split(\n    train_features, train_labels, train_protein_ids,\n    test_size=0.1, random_state=42\n)\nprint(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}\")\n```\nOutput:\nTrain: 68426, Val: 7603\n\n\nCell 10:\n```\n# PyTorch Dataset\nclass ProteinDataset(Dataset):\n    def __init__(self, features, labels=None):\n        self.features = torch.FloatTensor(features)\n        self.labels = torch.FloatTensor(labels) if labels is not None else None\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return self.features[idx], self.labels[idx]\n        return self.features[idx]\n\n# Create datasets\ntrain_dataset = ProteinDatas\n```\n\nCell 11:\n```\n# MLP Model\nclass ProteinMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.3):\n        super().__init__()\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        \n        lay\n```\nOutput:\nModel parameters: 531,676\n\n\nCell 12:\n```\n# Training setup\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for features, labels in loader:\n        features, labels = features.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, \n```",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01Hw69YTLhEvW4ycn6TW8CY7",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 13,
            "end_cell": 13,
            "duration": 300
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01Hw69YTLhEvW4ycn6TW8CY7",
          "content": "Executed cells 13-13\n==================================================\n\nCell 13:\n```\n# Training loop\nEPOCHS = 15\nbest_f1 = 0\nbest_model_state = None\n\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_f1, val_prec, val_rec = validate(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_model_state = model.state_dict().copy()\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss={train_loss:.4f}, Val Los\n```\nOutput:\nStarting training...\n\nEpoch 1/15: Train Loss=0.0359, Val Loss=0.0155, Val F1=0.0662, Prec=0.5672, Rec=0.0352\n\nEpoch 2/15: Train Loss=0.0159, Val Loss=0.0152, Val F1=0.0876, Prec=0.5646, Rec=0.0475\n\nEpoch 3/15: Train Loss=0.0157, Val Loss=0.0152, Val F1=0.0819, Prec=0.5666, Rec=0.0441\n\nEpoch 4/15: Train Loss=0.0157, Val Loss=0.0152, Val F1=0.0561, Prec=0.5881, Rec=0.0294\n\nEpoch 5/15: Train Loss=0.0156, Val Loss=0.0150, Val F1=0.0753, Prec=0.5777, Rec=0.0403\n\nEpoch 6/15: Train Loss=0.0155, Val Loss=0.0152, Val F1=0.0524, Prec=0.6035, Rec=0.0274\n\nEpoch 7/15: Train Loss=0.0154, Val Loss=0.0151, Val F1=0.1065, Prec=0.5560, Rec=0.0589\n\nEpoch 8/15: Train Loss=0.0153, Val Loss=0.0152, Val F1=0.0837, Prec=0.5791, Rec=0.0451\n\nEpoch 9/15: Train Loss=0.0152, Val Loss=0.0148, Val F1=0.0995, Prec=0.5711, Rec=0.0545\n\nEpoch 10/15: Train Loss=0.0151, Val Loss=0.0148, Val F1=0.0836, Prec=0.5829, Rec=0.0450\n\nEpoch 11/15: Train Loss=0.0151, Val Loss=0.0147, Val F1=0.0864, Prec=0.5863, Rec=0.0466\n\nEpoch 12/15: Train Loss=0.0150, Val Loss=0.0146, Val F1=0.0964, Prec=0.5807, Rec=0.0525\n\nEpoch 13/15: Train Loss=0.0150, Val Loss=0.0146, Val F1=0.0908, Prec=0.5861, Rec=0.0492\n\nEpoch 14/15: Train Loss=0.0149, Val Loss=0.0145, Val F1=0.0936, Prec=0.5860, Rec=0.0509\n\nEpoch 15/15: Train Loss=0.0149, Val Loss=0.0146, Val F1=0.0913, Prec=0.5863, Rec=0.0495\n\nBest validation F1: 0.1065\n\n<All keys matched successfully>",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01Kng7LYKT83uCxgG4ATNiRs",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 14,
            "end_cell": 16,
            "duration": 180
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01Kng7LYKT83uCxgG4ATNiRs",
          "content": "Executed cells 14-16\n==================================================\n\nCell 14:\n```\n# Parse GO ontology for hierarchy propagation\nimport re\n\ndef parse_go_obo(obo_file):\n    \"\"\"Parse GO OBO file and extract parent-child relationships\"\"\"\n    go_parents = defaultdict(set)\n    go_aspects = {}\n    \n    current_id = None\n    current_namespace = None\n    \n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            \n            if line == '[Term]':\n                current_id = None\n                current_namespace = None\n            elif line.\n```\nOutput:\nParsing GO ontology...\n\nGO terms with parents: 40119\nGO terms with aspects: 48101\n\n\nCell 15:\n```\n# Build ancestor map for efficient propagation\ndef get_all_ancestors(term, go_parents, cache=None):\n    \"\"\"Get all ancestors of a GO term\"\"\"\n    if cache is None:\n        cache = {}\n    \n    if term in cache:\n        return cache[term]\n    \n    ancestors = set()\n    for parent in go_parents.get(term, set()):\n        ancestors.add(parent)\n        ancestors.update(get_all_ancestors(parent, go_parents, cache))\n    \n    cache[term] = ancestors\n    return ancestors\n\n# Pre-compute ancestors for all se\n```\nOutput:\nComputing ancestors for selected terms...\n\n\r  0%|          | 0/1500 [00:00<?, ?it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [00:00<00:00, 271054.93it/s]\nComputed ancestors for 1500 terms\n\n\n\n\nCell 16:\n```\n# Extract features for test proteins\nprint(\"Extracting features for test proteins...\")\ntest_protein_ids = list(test_sequences.keys())\ntest_features = []\n\nfor pid in tqdm(test_protein_ids):\n    features = compute_sequence_features(test_sequences[pid])\n    test_features.append(features)\n\ntest_features = np.array(test_features, dtype=np.float32)\nprint(f\"Test features shape: {test_features.shape}\")\n```\nOutput:\nExtracting features for test proteins...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\r  0%|          | 903/224309 [00:00<00:24, 9027.53it/s]\n\r  1%|          | 1806/224309 [00:00<00:26, 8283.56it/s]\n\r  1%|          | 2643/224309 [00:00<00:26, 8311.65it/s]\n\r  2%|\u258f         | 3581/224309 [00:00<00:25, 8715.89it/s]\n\r  2%|\u258f         | 4456/224309 [00:00<00:25, 8637.99it/s]\n\r  2%|\u258f         | 5390/224309 [00:00<00:24, 8868.48it/s]\n\r  3%|\u258e         | 6279/224309 [00:00<00:25, 8620.64it/s]\n\r  3%|\u258e         | 7229/224309 [00:00<00:24, 8891.84it/s]\n\r  4%|\u258e         | 8121/224309 [00:00<00:25, 8620.77it/s]\n\r  4%|\u258d         | 9000/224309 [00:01<00:24, 8667.30it/s]\n\r  4%|\u258d         | 9974/224309 [00:01<00:23, 8982.06it/s]\n\r  5%|\u258d         | 10875/224309 [00:01<00:24, 8640.41it/s]\n\r  5%|\u258c         | 11743/224309 [00:01<00:24, 8645.60it/s]\n\r  6%|\u258c         | 12745/224309 [00:01<00:23, 9047.33it/s]\n\r  6%|\u258c         | 13653/224309 [00:01<00:24, 8530.31it/s]\n\r  6%|\u258b         | 14514/224309 [00:01<00:24, 8516.68it/s]\n\r  7%|\u258b         | 15371/224309 [00:01<00:25, 8243.68it/s]\n\r  7%|\u258b         | 16471/224309 [00:01<00:23, 9026.62it/s]\n\r  8%|\u258a         | 17479/224309 [00:01<00:22, 9329.41it/s]\n\r  8%|\u258a         | 18419/224309 [00:02<00:22, 9150.09it/s]\n\r  9%|\u258a         | 19339/224309 [00:02<00:23, 8839.00it/s]\n\r  9%|\u2589         | 20300/224309 [00:02<00:22, 9058.71it/s]\n\r  9%|\u2589         | 21211/224309 [00:02<00:23, 8496.79it/s]\n\r 10%|\u2589         | 22289/224309 [00:02<00:22, 9132.46it/s]\n\r 10%|\u2588         | 23214/224309 [00:02<00:22, 9110.17it/s]\n\r 11%|\u2588         | 24133/224309 [00:02<00:22, 9046.23it/s]\n\r 11%|\u2588         | 25043/224309 [00:02<00:22, 8872.57it/s]\n\r 12%|\u2588\u258f        | 25989/224309 [00:02<00:21, 9041.72it/s]\n\r 12%|\u2588\u258f        | 26897/224309 [00:03<00:22, 8827.91it/s]\n\r 12%|\u2588\u258f        | 27783/224309 [00:03<00:22, 8810.92it/s]\n\r 13%|\u2588\u258e        | 28748/224309 [00:03<00:21, 9053.24it/s]\n\r 13%|\u2588\u258e        | 29686/224309 [00:03<00:21, 9147.06it/s]\n\r 14%|\u2588\u258e        | 30603/224309 [00:03<00:22, 8738.89it/s]\n\r 14%|\u2588\u258d        | 31482/224309 [00:03<00:22, 8598.50it/s]\n\r 14%|\u2588\u258d        | 32442/224309 [00:03<00:21, 8884.86it/s]\n\r 15%|\u2588\u258d        | 33334/224309 [00:03<00:23, 8253.63it/s]\n\r 15%|\u2588\u258c        | 34352/224309 [00:03<00:21, 8789.66it/s]\n\r 16%|\u2588\u258c        | 35361/224309 [00:04<00:20, 9152.16it/s]\n\r 16%|\u2588\u258c        | 36307/224309 [00:04<00:20, 9239.29it/s]\n\r 17%|\u2588\u258b        | 37239/224309 [00:04<00:21, 8780.31it/s]\n\r 17%|\u2588\u258b        | 38127/224309 [00:04<00:21, 8753.88it/s]\n\r 17%|\u2588\u258b        | 39034/224309 [00:04<00:20, 8842.51it/s]\n\r 18%|\u2588\u258a        | 39924/224309 [00:04<00:21, 8440.59it/s]\n\r 18%|\u2588\u258a        | 40947/224309 [00:04<00:20, 8935.08it/s]\n\r 19%|\u2588\u258a        | 41936/224309 [00:04<00:19, 9209.17it/s]\n\r 19%|\u2588\u2589        | 42864/224309 [00:04<00:20, 8936.54it/s]\n\r 20%|\u2588\u2589        | 43764/224309 [00:04<00:20, 8634.64it/s]\n\r 20%|\u2588\u2589        | 44753/224309 [00:05<00:20, 8976.00it/s]\n\r 20%|\u2588\u2588        | 45656/224309 [00:05<00:19, 8938.83it/s]\n\r 21%|\u2588\u2588        | 46760/224309 [00:05<00:18, 9546.68it/s]\n\r 21%|\u2588\u2588\u258f       | 47720/224309 [00:05<00:18, 9480.70it/s]\n\r 22%|\u2588\u2588\u258f       | 48817/224309 [00:05<00:17, 9909.84it/s]\n\r 22%|\u2588\u2588\u258f       | 49812/224309 [00:05<00:17, 9771.22it/s]\n\r 23%|\u2588\u2588\u258e       | 50928/224309 [00:05<00:17, 10177.44it/s]\n\r 23%|\u2588\u2588\u258e       | 51949/224309 [00:05<00:17, 9887.24it/s] \n\r 24%|\u2588\u2588\u258e       | 52987/224309 [00:05<00:17, 10029.57it/s]\n\r 24%|\u2588\u2588\u258d       | 53993/224309 [00:05<00:17, 9914.27it/s] \n\r 25%|\u2588\u2588\u258d       | 55048/224309 [00:06<00:16, 10099.71it/s]\n\r 25%|\u2588\u2588\u258d       | 56060/224309 [00:06<00:16, 10104.52it/s]\n\r 25%|\u2588\u2588\u258c       | 57104/224309 [00:06<00:16, 10200.26it/s]\n\r 26%|\u2588\u2588\u258c       | 58126/224309 [00:06<00:16, 10166.82it/s]\n\r 26%|\u2588\u2588\u258b       | 59285/224309 [00:06<00:15, 10588.51it/s]\n\r 27%|\u2588\u2588\u258b       | 60345/224309 [00:06<00:15, 10461.29it/s]\n\r 27%|\u2588\u2588\u258b       | 61466/224309 [00:06<00:15, 10676.06it/s]\n\r 28%|\u2588\u2588\u258a       | 62621/224309 [00:06<00:14, 10934.13it/s]\n\r 28%|\u2588\u2588\u258a       | 63716/224309 [00:06<00:15, 10628.95it/s]\n\r 29%|\u2588\u2588\u2589       | 64801/224309 [00:07<00:14, 10692.14it/s]\n\r 29%|\u2588\u2588\u2589       | 65923/224309 [00:07<00:14, 10847.54it/s]\n\r 30%|\u2588\u2588\u2589       | 67010/224309 [00:07<00:14, 10593.35it/s]\n\r 30%|\u2588\u2588\u2588       | 68098/224309 [00:07<00:14, 10674.63it/s]\n\r 31%|\u2588\u2588\u2588       | 69168/224309 [00:07<00:14, 10570.37it/s]\n\r 31%|\u2588\u2588\u2588\u258f      | 70232/224309 [00:07<00:14, 10590.42it/s]\n\r 32%|\u2588\u2588\u2588\u258f      | 71429/224309 [00:07<00:13, 10998.11it/s]\n\r 32%|\u2588\u2588\u2588\u258f      | 72531/224309 [00:07<00:14, 10700.25it/s]\n\r 33%|\u2588\u2588\u2588\u258e      | 73604/224309 [00:07<00:14, 10370.09it/s]\n\r 33%|\u2588\u2588\u2588\u258e      | 74860/224309 [00:07<00:13, 10998.72it/s]\n\r 34%|\u2588\u2588\u2588\u258d      | 75990/224309 [00:08<00:13, 11081.14it/s]\n\r 34%|\u2588\u2588\u2588\u258d      | 77102/224309 [00:08<00:14, 10398.48it/s]\n\r 35%|\u2588\u2588\u2588\u258d      | 78315/224309 [00:08<00:13, 10884.84it/s]\n\r 35%|\u2588\u2588\u2588\u258c      | 79414/224309 [00:08<00:13, 10899.59it/s]\n\r 36%|\u2588\u2588\u2588\u258c      | 80512/224309 [00:08<00:14, 10146.73it/s]\n\r 36%|\u2588\u2588\u2588\u258b      | 81541/224309 [00:08<00:14, 9874.57it/s] \n\r 37%|\u2588\u2588\u2588\u258b      | 82841/224309 [00:08<00:13, 10742.97it/s]\n\r 37%|\u2588\u2588\u2588\u258b      | 83929/224309 [00:08<00:13, 10660.22it/s]\n\r 38%|\u2588\u2588\u2588\u258a      | 85005/224309 [00:08<00:13, 10415.25it/s]\n\r 39%|\u2588\u2588\u2588\u258a      | 86425/224309 [00:09<00:11, 11492.52it/s]\n\r 39%|\u2588\u2588\u2588\u2589      | 87585/224309 [00:09<00:12, 11017.59it/s]\n\r 40%|\u2588\u2588\u2588\u2589      | 88698/224309 [00:09<00:12, 10469.44it/s]\n\r 40%|\u2588\u2588\u2588\u2588      | 90043/224309 [00:09<00:11, 11293.50it/s]\n\r 41%|\u2588\u2588\u2588\u2588      | 91187/224309 [00:09<00:12, 11064.41it/s]\n\r 41%|\u2588\u2588\u2588\u2588      | 92381/224309 [00:09<00:11, 11311.37it/s]\n\r 42%|\u2588\u2588\u2588\u2588\u258f     | 93591/224309 [00:09<00:11, 11537.21it/s]\n\r 42%|\u2588\u2588\u2588\u2588\u258f     | 94805/224309 [00:09<00:11, 11711.02it/s]\n\r 43%|\u2588\u2588\u2588\u2588\u258e     | 95982/224309 [00:09<00:11, 11265.16it/s]\n\r 43%|\u2588\u2588\u2588\u2588\u258e     | 97154/224309 [00:09<00:11, 11395.21it/s]\n\r 44%|\u2588\u2588\u2588\u2588\u258d     | 98454/224309 [00:10<00:10, 11857.55it/s]\n\r 44%|\u2588\u2588\u2588\u2588\u258d     | 99645/224309 [00:10<00:11, 11247.60it/s]\n\r 45%|\u2588\u2588\u2588\u2588\u258d     | 100779/224309 [00:10<00:11, 11125.43it/s]\n\r 45%|\u2588\u2588\u2588\u2588\u258c     | 102053/224309 [00:10<00:10, 11587.38it/s]\n\r 46%|\u2588\u2588\u2588\u2588\u258c     | 103219/224309 [00:10<00:10, 11344.81it/s]\n\r 47%|\u2588\u2588\u2588\u2588\u258b     | 104359/224309 [00:10<00:10, 11289.20it/s]\n\r 47%|\u2588\u2588\u2588\u2588\u258b     | 105579/224309 [00:10<00:10, 11552.19it/s]\n\r 48%|\u2588\u2588\u2588\u2588\u258a     | 106738/224309 [00:10<00:10, 11530.61it/s]\n\r 48%|\u2588\u2588\u2588\u2588\u258a     | 107915/224309 [00:10<00:10, 11598.42it/s]\n\r 49%|\u2588\u2588\u2588\u2588\u258a     | 109163/224309 [00:11<00:09, 11858.38it/s]\n\r 49%|\u2588\u2588\u2588\u2588\u2589     | 110351/224309 [00:11<00:09, 11860.89it/s]\n\r 50%|\u2588\u2588\u2588\u2588\u2589     | 111539/224309 [00:11<00:09, 11539.32it/s]\n\r 50%|\u2588\u2588\u2588\u2588\u2588     | 112696/224309 [00:11<00:09, 11423.02it/s]\n\r 51%|\u2588\u2588\u2588\u2588\u2588     | 113841/224309 [00:11<00:10, 10926.69it/s]\n\r 51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 114959/224309 [00:11<00:09, 10998.33it/s]\n\r 52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 116551/224309 [00:11<00:08, 12423.06it/s]\n\r 53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 118047/224309 [00:11<00:08, 13165.27it/s]\n\r 53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 119371/224309 [00:11<00:08, 12432.65it/s]\n\r 54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 120628/224309 [00:11<00:08, 11964.77it/s]\n\r 54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 121951/224309 [00:12<00:08, 12318.85it/s]\n\r 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 123194/224309 [00:12<00:08, 11960.02it/s]\n\r 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 124444/224309 [00:12<00:08, 12111.26it/s]\n\r 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 126065/224309 [00:12<00:07, 13289.61it/s]\n\r 57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 127690/224309 [00:12<00:06, 14152.62it/s]\n\r 58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 129115/224309 [00:12<00:07, 12989.03it/s]\n\r 58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 130438/224309 [00:12<00:07, 12757.71it/s]\n\r 59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 131731/224309 [00:12<00:07, 12783.54it/s]\n\r 59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 133021/224309 [00:12<00:07, 12295.85it/s]\n\r 60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 134261/224309 [00:13<00:07, 12237.12it/s]\n\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 135899/224309 [00:13<00:06, 13414.67it/s]\n\r 61%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 137495/224309 [00:13<00:06, 14149.60it/s]\n\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 138921/224309 [00:13<00:06, 12982.74it/s]\n\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 140244/224309 [00:13<00:06, 12651.96it/s]\n\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 141527/224309 [00:13<00:06, 12637.77it/s]\n\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 142803/224309 [00:13<00:06, 12389.77it/s]\n\r 64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 144051/224309 [00:13<00:06, 12349.37it/s]\n\r 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 145700/224309 [00:13<00:05, 13535.89it/s]\n\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 147166/224309 [00:13<00:05, 13857.89it/s]\n\r 66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 148560/224309 [00:14<00:05, 13140.41it/s]\n\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 149887/224309 [00:14<00:05, 12944.40it/s]\n\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 151190/224309 [00:14<00:05, 12641.48it/s]\n\r 68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 152461/224309 [00:14<00:05, 12264.63it/s]\n\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 153757/224309 [00:14<00:05, 12457.72it/s]\n\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 155337/224309 [00:14<00:05, 13417.61it/s]\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 156739/224309 [00:14<00:04, 13584.33it/s]\n\r 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 158104/224309 [00:14<00:05, 12899.61it/s]\n\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 159405/224309 [00:14<00:05, 12463.23it/s]\n\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 160701/224309 [00:15<00:05, 12602.61it/s]\n\r 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 162194/224309 [00:15<00:04, 13268.38it/s]\n\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 163529/224309 [00:15<00:04, 13145.50it/s]\n\r 73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 164850/224309 [00:15<00:04, 13094.79it/s]\n\r 74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 166324/224309 [00:15<00:04, 13570.03it/s]\n\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 167719/224309 [00:15<00:04, 13681.38it/s]\n\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 169090/224309 [00:15<00:04, 13348.93it/s]\n\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 170429/224309 [00:15<00:04, 13271.85it/s]\n\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 171811/224309 [00:15<00:03, 13430.65it/s]\n\r 77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 173157/224309 [00:16<00:03, 13087.64it/s]\n\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 174487/224309 [00:16<00:03, 13148.02it/s]\n\r 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 175912/224309 [00:16<00:03, 13471.40it/s]\n\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 177473/224309 [00:16<00:03, 14100.97it/s]\n\r 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 179305/224309 [00:16<00:02, 15352.30it/s]\n\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 181101/224309 [00:16<00:02, 16124.56it/s]\n\r 81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 182717/224309 [00:16<00:02, 15450.61it/s]\n\r 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 184282/224309 [00:16<00:02, 15505.10it/s]\n\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 185951/224309 [00:16<00:02, 15850.79it/s]\n\r 84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 188017/224309 [00:16<00:02, 17264.43it/s]\n\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 189750/224309 [00:17<00:02, 16044.63it/s]\n\r 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 191375/224309 [00:17<00:02, 15902.04it/s]\n\r 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 193012/224309 [00:17<00:01, 16032.11it/s]\n\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 195219/224309 [00:17<00:01, 17771.39it/s]\n\r 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 197010/224309 [00:17<00:01, 16581.91it/s]\n\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 198693/224309 [00:17<00:01, 16215.39it/s]\n\r 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 200332/224309 [00:17<00:01, 16140.44it/s]\n\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 202234/224309 [00:17<00:01, 16957.91it/s]\n\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 204126/224309 [00:17<00:01, 17520.87it/s]\n\r 92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 205889/224309 [00:18<00:01, 16674.60it/s]\n\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 207572/224309 [00:18<00:01, 15921.30it/s]\n\r 93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 209325/224309 [00:18<00:00, 16369.28it/s]\n\r 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 211360/224309 [00:18<00:00, 17502.02it/s]\n\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 213126/224309 [00:18<00:00, 16421.03it/s]\n\r 96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 215088/224309 [00:18<00:00, 17312.19it/s]\n\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 216842/224309 [00:18<00:00, 17318.60it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 218869/224309 [00:18<00:00, 18169.55it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220889/224309 [00:18<00:00, 18761.52it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222963/224309 [00:18<00:00, 19342.79it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 224309/224309 [00:19<00:00, 11796.87it/s]\n\n\nTest features shape: (224309, 25)\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01Q5ZbuqC49UiXwwh7bNGvK3",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 17,
            "end_cell": 19,
            "duration": 120
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01Q5ZbuqC49UiXwwh7bNGvK3",
          "content": "Executed cells 17-19\n==================================================\n\nCell 17:\n```\n# Generate predictions for test set\nprint(\"Generating predictions...\")\ntest_dataset = ProteinDataset(test_features)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\nmodel.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for features in tqdm(test_loader):\n        features = features.to(device)\n        outputs = model(features)\n        # Apply temperature scaling\n        T = 1.5\n        preds = torch.sigmoid(outputs / T)\n        all_preds.append(preds.cpu().numpy())\n\ntest_pre\n```\nOutput:\nGenerating predictions...\n\n\r  0%|          | 0/877 [00:00<?, ?it/s]\n\r 10%|\u2589         | 84/877 [00:00<00:00, 827.86it/s]\n\r 19%|\u2588\u2589        | 167/877 [00:00<00:01, 671.77it/s]\n\r 27%|\u2588\u2588\u258b       | 236/877 [00:00<00:01, 630.72it/s]\n\r 34%|\u2588\u2588\u2588\u258d      | 300/877 [00:00<00:00, 618.30it/s]\n\r 41%|\u2588\u2588\u2588\u2588\u258f     | 363/877 [00:00<00:00, 610.73it/s]\n\r 48%|\u2588\u2588\u2588\u2588\u258a     | 425/877 [00:00<00:00, 605.67it/s]\n\r 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 486/877 [00:00<00:00, 601.73it/s]\n\r 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 547/877 [00:00<00:00, 599.47it/s]\n\r 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 607/877 [00:00<00:00, 598.32it/s]\n\r 76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 667/877 [00:01<00:00, 596.90it/s]\n\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 727/877 [00:01<00:00, 595.99it/s]\n\r 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 787/877 [00:01<00:00, 595.01it/s]\n\r 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 847/877 [00:01<00:00, 595.32it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 877/877 [00:01<00:00, 610.05it/s]\n\n\nTest predictions shape: (224309, 1500)\n\n\nCell 18:\n```\n# Load IA weights\nia_df = pd.read_csv('/home/data/IA.tsv', sep='\\t', header=None, names=['term', 'ia_weight'])\nia_weights = dict(zip(ia_df['term'], ia_df['ia_weight']))\nprint(f\"Loaded IA weights for {len(ia_weights)} terms\")\n```\nOutput:\nLoaded IA weights for 40122 terms\n\n\nCell 19:\n```\n# Root terms for each aspect\nROOT_TERMS = {\n    'F': 'GO:0003674',  # molecular_function\n    'P': 'GO:0008150',  # biological_process\n    'C': 'GO:0005575'   # cellular_component\n}\n\ndef propagate_predictions(protein_preds, terms, term_ancestors, go_aspects):\n    \"\"\"Propagate predictions to ancestors and apply post-processing\"\"\"\n    # Create term -> score mapping\n    term_scores = {}\n    for i, term in enumerate(terms):\n        if protein_preds[i] > 0.001:  # Only consider non-trivial predictions\n```\nOutput:\nExample propagated predictions: 1216 terms\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01T7Aa5EeVr3G42gnMpo6LdJ",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 20,
            "end_cell": 22,
            "duration": 600
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01T7Aa5EeVr3G42gnMpo6LdJ",
          "content": "Executed cells 20-22\n==================================================\n\u23f0 Cells interrupted due to timeout: [20]\n\nCell 20: \u23f0 INTERRUPTED\n```\n# Generate submission\nprint(\"Generating submission file...\")\nsubmission_rows = []\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score descending\n    sorted_terms = sorted(term_scores.items(), key=lambda x: -x[1])\n    \n    # Keep top predictions (max 1500 per protein)\n    for term, score in sorted_terms[:1500]:\n        if score > 0.001:  # \n```\nOutput:\nGenerating submission file...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\r  0%|          | 14/224309 [00:00<27:23, 136.44it/s]\n\r  0%|          | 28/224309 [00:00<28:29, 131.18it/s]\n\r  0%|          | 42/224309 [00:00<28:23, 131.68it/s]\n\r  0%|          | 56/224309 [00:00<28:42, 130.21it/s]\n\r  0%|          | 70/224309 [00:00<29:01, 128.76it/s]\n\r  0%|          | 83/224309 [00:00<29:07, 128.32it/s]\n\r  0%|          | 96/224309 [00:00<29:25, 127.01it/s]\n\r  0%|          | 109/224309 [00:00<29:20, 127.35it/s]\n\r  0%|          | 122/224309 [00:00<29:27, 126.84it/s]\n\r  0%|          | 135/224309 [00:01<29:14, 127.75it/s]\n\r  0%|          | 148/224309 [00:01<29:24, 127.06it/s]\n\r  0%|          | 161/224309 [00:01<29:36, 126.20it/s]\n\r  0%|          | 174/224309 [00:01<29:39, 125.93it/s]\n\r  0%|          | 187/224309 [00:01<29:30, 126.57it/s]\n\r  0%|          | 200/224309 [00:01<29:28, 126.69it/s]\n\r  0%|          | 213/224309 [00:01<29:29, 126.67it/s]\n\r  0%|          | 226/224309 [00:01<29:23, 127.06it/s]\n\r  0%|          | 239/224309 [00:01<29:31, 126.51it/s]\n\r  0%|          | 252/224309 [00:01<29:27, 126.74it/s]\n\r  0%|          | 265/224309 [00:02<29:28, 126.66it/s]\n\r  0%|          | 278/224309 [00:02<29:32, 126.36it/s]\n\r  0%|          | 291/224309 [00:02<29:29, 126.62it/s]\n\r  0%|          | 304/224309 [00:02<29:34, 126.21it/s]\n\r  0%|          | 317/224309 [00:02<29:42, 125.63it/s]\n\r  0%|          | 330/224309 [00:02<29:34, 126.20it/s]\n\r  0%|          | 343/224309 [00:02<29:36, 126.09it/s]\n\r  0%|          | 356/224309 [00:02<29:25, 126.83it/s]\n\r  0%|          | 369/224309 [00:02<29:26, 126.76it/s]\n\r  0%|          | 382/224309 [00:03<29:23, 127.00it/s]\n\r  0%|          | 395/224309 [00:03<29:24, 126.92it/s]\n\r  0%|          | 408/224309 [00:03<29:35, 126.14it/s]\n\r  0%|          | 422/224309 [00:03<29:01, 128.59it/s]\n\r  0%|          | 435/224309 [00:03<28:57, 128.87it/s]\n\r  0%|          | 448/224309 [00:03<29:00, 128.62it/s]\n\r  0%|          | 461/224309 [00:03<28:58, 128.79it/s]\n\r  0%|          | 474/224309 [00:03<29:04, 128.31it/s]\n\r  0%|          | 487/224309 [00:03<29:13, 127.63it/s]\n\r  0%|          | 500/224309 [00:03<29:22, 126.99it/s]\n\r  0%|          | 514/224309 [00:04<29:00, 128.55it/s]\n\r  0%|          | 527/224309 [00:04<29:14, 127.57it/s]\n\r  0%|          | 540/224309 [00:04<29:24, 126.84it/s]\n\r  0%|          | 553/224309 [00:04<29:30, 126.37it/s]\n\r  0%|          | 566/224309 [00:04<29:27, 126.61it/s]\n\r  0%|          | 579/224309 [00:04<29:36, 125.93it/s]\n\r  0%|          | 592/224309 [00:04<29:39, 125.74it/s]\n\r  0%|          | 605/224309 [00:04<29:32, 126.19it/s]\n\r  0%|          | 618/224309 [00:04<29:38, 125.76it/s]\n\r  0%|          | 631/224309 [00:04<29:50, 124.95it/s]\n\r  0%|          | 644/224309 [00:05<29:45, 125.23it/s]\n\r  0%|          | 657/224309 [00:05<29:43, 125.40it/s]\n\r  0%|          | 670/224309 [00:05<29:40, 125.62it/s]\n\r  0%|          | 683/224309 [00:05<29:39, 125.65it/s]\n\r  0%|          | 696/224309 [00:05<29:25, 126.66it/s]\n\r  0%|          | 709/224309 [00:05<29:27, 126.52it/s]\n\r  0%|          | 722/224309 [00:05<29:19, 127.08it/s]\n\r  0%|          | 735/224309 [00:05<29:16, 127.29it/s]\n\r  0%|          | 749/224309 [00:05<28:59, 128.51it/s]\n\r  0%|          | 762/224309 [00:05<29:00, 128.42it/s]\n\r  0%|          | 775/224309 [00:06<28:58, 128.56it/s]\n\r  0%|          | 789/224309 [00:06<28:34, 130.34it/s]\n\r  0%|          | 803/224309 [00:06<28:56, 128.68it/s]\n\r  0%|          | 816/224309 [00:06<29:04, 128.09it/s]\n\r  0%|          | 830/224309 [00:06<28:54, 128.85it/s]\n\r  0%|          | 843/224309 [00:06<29:04, 128.07it/s]\n\r  0%|          | 857/224309 [00:06<28:50, 129.09it/s]\n\r  0%|          | 871/224309 [00:06<28:39, 129.94it/s]\n\r  0%|          | 884/224309 [00:06<28:42, 129.73it/s]\n\r  0%|          | 897/224309 [00:07<28:52, 128.98it/s]\n\r  0%|          | 910/224309 [00:07<28:52, 128.98it/s]\n\r  0%|          | 923/224309 [00:07<29:03, 128.12it/s]\n\r  0%|          | 936/224309 [00:07<29:15, 127.22it/s]\n\r  0%|          | 949/224309 [00:07<29:15, 127.22it/s]\n\r  0%|          | 962/224309 [00:07<29:24, 126.55it/s]\n\r  0%|          | 975/224309 [00:07<29:34, 125.87it/s]\n\r  0%|          | 988/224309 [00:07<29:29, 126.17it/s]\n\r  0%|          | 1001/224309 [00:07<29:28, 126.29it/s]\n\r  0%|          | 1015/224309 [00:07<28:47, 129.27it/s]\n\r  0%|          | 1028/224309 [00:08<28:52, 128.91it/s]\n\r  0%|          | 1041/224309 [00:08<28:52, 128.86it/s]\n\r  0%|          | 1054/224309 [00:08<28:54, 128.74it/s]\n\r  0%|          | 1067/224309 [00:08<29:00, 128.25it/s]\n\r  0%|          | 1080/224309 [00:08<28:57, 128.47it/s]\n\r  0%|          | 1093/224309 [00:08<29:06, 127.80it/s]\n\r  0%|          | 1106/224309 [00:08<29:02, 128.09it/s]\n\r  0%|          | 1119/224309 [00:08<29:14, 127.17it/s]\n\r  1%|          | 1133/224309 [00:08<28:59, 128.28it/s]\n\r  1%|          | 1146/224309 [00:08<29:10, 127.51it/s]\n\r  1%|          | 1159/224309 [00:09<29:10, 127.45it/s]\n\r  1%|          | 1172/224309 [00:09<29:08, 127.58it/s]\n\r  1%|          | 1185/224309 [00:09<29:23, 126.52it/s]\n\r  1%|          | 1198/224309 [00:09<29:10, 127.46it/s]\n\r  1%|          | 1211/224309 [00:09<29:18, 126.88it/s]\n\r  1%|          | 1224/224309 [00:09<29:25, 126.37it/s]\n\r  1%|          | 1237/224309 [00:09<29:27, 126.20it/s]\n\r  1%|          | 1250/224309 [00:09<29:27, 126.22it/s]\n\r  1%|          | 1263/224309 [00:09<29:32, 125.87it/s]\n\r  1%|          | 1276/224309 [00:10<29:29, 126.03it/s]\n\r  1%|          | 1289/224309 [00:10<29:29, 126.01it/s]\n\r  1%|          | 1302/224309 [00:10<29:34, 125.69it/s]\n\r  1%|          | 1315/224309 [00:10<29:36, 125.54it/s]\n\r  1%|          | 1329/224309 [00:10<29:14, 127.08it/s]\n\r  1%|          | 1342/224309 [00:10<29:24, 126.36it/s]\n\r  1%|          | 1355/224309 [00:10<29:26, 126.22it/s]\n\r  1%|          | 1369/224309 [00:10<28:53, 128.61it/s]\n\r  1%|          | 1382/224309 [00:10<28:58, 128.20it/s]\n\r  1%|          | 1395/224309 [00:10<29:03, 127.86it/s]\n\r  1%|          | 1408/224309 [00:11<28:58, 128.20it/s]\n\r  1%|          | 1421/224309 [00:11<28:57, 128.26it/s]\n\r  1%|          | 1435/224309 [00:11<28:37, 129.80it/s]\n\r  1%|          | 1449/224309 [00:11<28:08, 131.97it/s]\n\r  1%|          | 1463/224309 [00:11<28:13, 131.61it/s]\n\r  1%|          | 1477/224309 [00:11<28:29, 130.34it/s]\n\r  1%|          | 1491/224309 [00:11<28:46, 129.07it/s]\n\r  1%|          | 1504/224309 [00:11<29:00, 128.03it/s]\n\r  1%|          | 1517/224309 [00:11<29:12, 127.15it/s]\n\r  1%|          | 1530/224309 [00:12<29:16, 126.85it/s]\n\r  1%|          | 1543/224309 [00:12<29:08, 127.40it/s]\n\r  1%|          | 1556/224309 [00:12<29:19, 126.59it/s]\n\r  1%|          | 1569/224309 [00:12<29:24, 126.24it/s]\n\r  1%|          | 1583/224309 [00:12<29:05, 127.57it/s]\n\r  1%|          | 1596/224309 [00:12<29:11, 127.18it/s]\n\r  1%|          | 1609/224309 [00:12<29:12, 127.07it/s]\n\r  1%|          | 1623/224309 [00:12<29:00, 127.95it/s]\n\r  1%|          | 1636/224309 [00:12<29:03, 127.70it/s]\n\r  1%|          | 1649/224309 [00:12<29:08, 127.34it/s]\n\r  1%|          | 1662/224309 [00:13<29:04, 127.65it/s]\n\r  1%|          | 1675/224309 [00:13<29:14, 126.89it/s]\n\r  1%|          | 1688/224309 [00:13<29:12, 127.01it/s]\n\r  1%|          | 1701/224309 [00:13<29:19, 126.48it/s]\n\r  1%|          | 1714/224309 [00:13<29:24, 126.18it/s]\n\r  1%|          | 1727/224309 [00:13<29:25, 126.05it/s]\n\r  1%|          | 1740/224309 [00:13<29:21, 126.36it/s]\n\r  1%|          | 1753/224309 [00:13<29:22, 126.29it/s]\n\r  1%|          | 1766/224309 [00:13<29:18, 126.55it/s]\n\r  1%|          | 1779/224309 [00:13<29:23, 126.20it/s]\n\r  1%|          | 1792/224309 [00:14<29:15, 126.77it/s]\n\r  1%|          | 1805/224309 [00:14<29:11, 127.04it/s]\n\r  1%|          | 1818/224309 [00:14<28:59, 127.90it/s]\n\r  1%|          | 1831/224309 [00:14<28:56, 128.12it/s]\n\r  1%|          | 1844/224309 [00:14<29:01, 127.75it/s]\n\r  1%|          | 1857/224309 [00:14<29:08, 127.24it/s]\n\r  1%|          | 1870/224309 [00:14<29:03, 127.57it/s]\n\r  1%|          | 1883/224309 [00:14<29:17, 126.59it/s]\n\r  1%|          | 1896/224309 [00:14<29:06, 127.37it/s]\n\r  1%|          | 1909/224309 [00:14<29:05, 127.41it/s]\n\r  1%|          | 1922/224309 [00:15<29:07, 127.30it/s]\n\r  1%|          | 1935/224309 [00:15<29:00, 127.78it/s]\n\r  1%|          | 1949/224309 [00:15<28:43, 129.02it/s]\n\r  1%|          | 1962/224309 [00:15<28:50, 128.49it/s]\n\r  1%|          | 1975/224309 [00:15<28:50, 128.48it/s]\n\r  1%|          | 1988/224309 [00:15<29:07, 127.20it/s]\n\r  1%|          | 2001/224309 [00:15<29:02, 127.56it/s]\n\r  1%|          | 2014/224309 [00:15<29:12, 126.82it/s]\n\r  1%|          | 2027/224309 [00:15<29:04, 127.45it/s]\n\r  1%|          | 2040/224309 [00:16<29:10, 127.00it/s]\n\r  1%|          | 2053/224309 [00:16<29:13, 126.72it/s]\n\r  1%|          | 2066/224309 [00:16<29:08, 127.09it/s]\n\r  1%|          | 2079/224309 [00:16<29:15, 126.60it/s]\n\r  1%|          | 2092/224309 [00:16<29:11, 126.88it/s]\n\r  1%|          | 2105/224309 [00:16<29:13, 126.73it/s]\n\r  1%|          | 2118/224309 [00:16<29:19, 126.26it/s]\n\r  1%|          | 2131/224309 [00:16<29:22, 126.08it/s]\n\r  1%|          | 2145/224309 [00:16<29:03, 127.43it/s]\n\r  1%|          | 2158/224309 [00:16<29:12, 126.79it/s]\n\r  1%|          | 2171/224309 [00:17<29:11, 126.84it/s]\n\r  1%|          | 2184/224309 [00:17<29:12, 126.76it/s]\n\r  1%|          | 2197/224309 [00:17<29:20, 126.18it/s]\n\r  1%|          | 2210/224309 [00:17<29:12, 126.77it/s]\n\r  1%|          | 2223/224309 [00:17<29:16, 126.47it/s]\n\r  1%|          | 2236/224309 [00:17<29:24, 125.84it/s]\n\r  1%|          | 2249/224309 [00:17<29:22, 126.01it/s]\n\r  1%|          | 2262/224309 [00:17<29:11, 126.77it/s]\n\r  1%|          | 2276/224309 [00:17<28:50, 128.31it/s]\n\r  1%|          | 2289/224309 [00:17<30:02, 123.17it/s]\n\r  1%|          | 2302/224309 [00:18<29:37, 124.88it/s]\n\r  1%|          | 2315/224309 [00:18<29:43, 124.47it/s]\n\r  1%|          | 2328/224309 [00:18<29:39, 124.76it/s]\n\r  1%|          | 2341/224309 [00:18<29:26, 125.63it/s]\n\r  1%|          | 2355/224309 [00:18<28:57, 127.75it/s]\n\r  1%|          | 2368/224309 [00:18<28:52, 128.14it/s]\n\r  1%|          | 2381/224309 [00:18<28:58, 127.64it/s]\n\r  1%|          | 2394/224309 [00:18<29:04, 127.20it/s]\n\r  1%|          | 2408/224309 [00:18<28:54, 127.96it/s]\n\r  1%|          | 2421/224309 [00:19<29:06, 127.03it/s]\n\r  1%|          | 2434/224309 [00:19<28:57, 127.69it/s]\n\r  1%|          | 2448/224309 [00:19<28:44, 128.65it/s]\n\r  1%|          | 2461/224309 [00:19<28:56, 127.77it/s]\n\r  1%|          | 2474/224309 [00:19<28:55, 127.79it/s]\n\r  1%|          | 2488/224309 [00:19<28:39, 129.03it/s]\n\r  1%|          | 2501/224309 [00:19<28:55, 127.82it/s]\n\r  1%|          | 2514/224309 [00:19<29:01, 127.39it/s]\n\r  1%|          | 2527/224309 [00:19<28:57, 127.62it/s]\n\r  1%|          | 2540/224309 [00:19<29:06, 127.00it/s]\n\r  1%|          | 2553/224309 [00:20<29:08, 126.80it/s]\n\r  1%|          | 2567/224309 [00:20<28:56, 127.72it/s]\n\r  1%|          | 2580/224309 [00:20<28:50, 128.16it/s]\n\r  1%|          | 2593/224309 [00:20<28:56, 127.70it/s]\n\r  1%|          | 2606/224309 [00:20<28:59, 127.42it/s]\n\r  1%|          | 2619/224309 [00:20<29:10, 126.61it/s]\n\r  1%|          | 2632/224309 [00:20<29:09, 126.72it/s]\n\r  1%|          | 2645/224309 [00:20<29:11, 126.53it/s]\n\r  1%|          | 2658/224309 [00:20<29:21, 125.84it/s]\n\r  1%|          | 2671/224309 [00:20<29:16, 126.21it/s]\n\r  1%|          | 2684/224309 [00:21<29:23, 125.65it/s]\n\r  1%|          | 2697/224309 [00:21<29:18, 126.02it/s]\n\r  1%|          | 2710/224309 [00:21<29:16, 126.14it/s]\n\r  1%|          | 2723/224309 [00:21<29:21, 125.81it/s]\n\r  1%|          | 2736/224309 [00:21<29:15, 126.23it/s]\n\r  1%|          | 2749/224309 [00:21<29:12, 126.42it/s]\n\r  1%|          | 2762/224309 [00:21<29:09, 126.61it/s]\n\r  1%|          | 2775/224309 [00:21<29:01, 127.19it/s]\n\r  1%|          | 2788/224309 [00:21<29:05, 126.92it/s]\n\r  1%|          | 2801/224309 [00:22<29:07, 126.78it/s]\n\r  1%|\u258f         | 2814/224309 [00:22<29:05, 126.91it/s]\n\r  1%|\u258f         | 2827/224309 [00:22<29:04, 126.93it/s]\n\r  1%|\u258f         | 2840/224309 [00:22<29:08, 126.63it/s]\n\r  1%|\u258f         | 2853/224309 [00:22<29:04, 126.96it/s]\n\r  1%|\u258f         | 2866/224309 [00:22<29:06, 126.81it/s]\n\r  1%|\u258f         | 2879/224309 [00:22<29:12, 126.32it/s]\n\r  1%|\u258f         | 2892/224309 [00:22<28:59, 127.28it/s]\n\r  1%|\u258f         | 2905/224309 [00:22<29:06, 126.80it/s]\n\r  1%|\u258f         | 2918/224309 [00:22<29:04, 126.93it/s]\n\r  1%|\u258f         | 2931/224309 [00:23<29:08, 126.62it/s]\n\r  1%|\u258f         | 2944/224309 [00:23<28:59, 127.23it/s]\n\r  1%|\u258f         | 2957/224309 [00:23<29:07, 126.64it/s]\n\r  1%|\u258f         | 2970/224309 [00:23<29:04, 126.91it/s]\n\r  1%|\u258f         | 2984/224309 [00:23<28:50, 127.89it/s]\n\r  1%|\u258f         | 2997/224309 [00:23<28:42, 128.48it/s]\n\r  1%|\u258f         | 3010/224309 [00:23<28:45, 128.23it/s]\n\r  1%|\u258f         | 3023/224309 [00:23<28:40, 128.60it/s]\n\r  1%|\u258f         | 3036/224309 [00:23<28:48, 127.98it/s]\n\r  1%|\u258f         | 3049/224309 [00:23<28:54, 127.53it/s]\n\r  1%|\u258f         | 3062/224309 [00:24<29:07, 126.59it/s]\n\r  1%|\u258f         | 3075/224309 [00:24<28:55, 127.48it/s]\n\r  1%|\u258f         | 3088/224309 [00:24<29:06, 126.63it/s]\n\r  1%|\u258f         | 3101/224309 [00:24<29:06, 126.68it/s]\n\r  1%|\u258f         | 3114/224309 [00:24<28:57, 127.32it/s]\n\r  1%|\u258f         | 3127/224309 [00:24<29:03, 126.89it/s]\n\r  1%|\u258f         | 3140/224309 [00:24<29:12, 126.21it/s]\n\r  1%|\u258f         | 3153/224309 [00:24<29:09, 126.44it/s]\n\r  1%|\u258f         | 3166/224309 [00:24<29:15, 125.95it/s]\n\r  1%|\u258f         | 3179/224309 [00:24<29:20, 125.58it/s]\n\r  1%|\u258f         | 3192/224309 [00:25<29:12, 126.17it/s]\n\r  1%|\u258f         | 3205/224309 [00:25<29:13, 126.06it/s]\n\r  1%|\u258f         | 3218/224309 [00:25<29:23, 125.38it/s]\n\r  1%|\u258f         | 3231/224309 [00:25<29:24, 125.29it/s]\n\r  1%|\u258f         | 3244/224309 [00:25<29:29, 124.92it/s]\n\r  1%|\u258f         | 3257/224309 [00:25<29:23, 125.37it/s]\n\r  1%|\u258f         | 3270/224309 [00:25<29:22, 125.40it/s]\n\r  1%|\u258f         | 3283/224309 [00:25<29:21, 125.47it/s]\n\r  1%|\u258f         | 3296/224309 [00:25<29:29, 124.87it/s]\n\r  1%|\u258f         | 3309/224309 [00:26<29:24, 125.23it/s]\n\r  1%|\u258f         | 3322/224309 [00:26<29:30, 124.81it/s]\n\r  1%|\u258f         | 3335/224309 [00:26<29:16, 125.82it/s]\n\r  1%|\u258f         | 3348/224309 [00:26<29:18, 125.69it/s]\n\r  1%|\u258f         | 3361/224309 [00:26<29:22, 125.39it/s]\n\r  2%|\u258f         |\n...(truncated)...\n/224309 [28:46<00:26, 129.64it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220913/224309 [28:46<00:26, 129.69it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220927/224309 [28:46<00:25, 130.99it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220941/224309 [28:46<00:26, 129.23it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220954/224309 [28:46<00:26, 128.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220969/224309 [28:46<00:24, 134.27it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220983/224309 [28:46<00:25, 132.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220997/224309 [28:46<00:25, 130.98it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221011/224309 [28:47<00:24, 132.74it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221025/224309 [28:47<00:24, 132.61it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221039/224309 [28:47<00:24, 131.59it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221053/224309 [28:47<00:24, 131.65it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221067/224309 [28:47<00:24, 133.99it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221081/224309 [28:47<00:23, 134.80it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221095/224309 [28:47<00:23, 135.05it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221109/224309 [28:47<00:23, 135.28it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221124/224309 [28:47<00:23, 137.97it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221138/224309 [28:47<00:23, 136.12it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221152/224309 [28:48<00:23, 134.13it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221166/224309 [28:48<00:23, 134.11it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221180/224309 [28:48<00:23, 132.03it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221197/224309 [28:48<00:21, 142.01it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221214/224309 [28:48<00:20, 147.77it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221232/224309 [28:48<00:19, 156.31it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221252/224309 [28:48<00:18, 167.37it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221273/224309 [28:48<00:16, 179.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221291/224309 [28:48<00:16, 178.67it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221309/224309 [28:49<00:18, 163.91it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221326/224309 [28:49<00:19, 152.04it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221342/224309 [28:49<00:20, 145.60it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221357/224309 [28:49<00:20, 140.74it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221372/224309 [28:49<00:21, 137.50it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221386/224309 [28:49<00:21, 135.06it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221400/224309 [28:49<00:21, 133.06it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221414/224309 [28:49<00:22, 130.97it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221428/224309 [28:49<00:22, 129.95it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221442/224309 [28:50<00:22, 128.55it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221455/224309 [28:50<00:22, 128.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221469/224309 [28:50<00:21, 130.07it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221483/224309 [28:50<00:21, 131.63it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221497/224309 [28:50<00:21, 130.40it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221511/224309 [28:50<00:21, 129.80it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221524/224309 [28:50<00:21, 129.34it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221537/224309 [28:50<00:21, 128.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221550/224309 [28:50<00:21, 128.27it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221564/224309 [28:51<00:21, 128.92it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221578/224309 [28:51<00:21, 129.69it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221592/224309 [28:51<00:20, 130.57it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221606/224309 [28:51<00:20, 129.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221619/224309 [28:51<00:20, 128.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221632/224309 [28:51<00:20, 128.77it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221645/224309 [28:51<00:20, 128.18it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221658/224309 [28:51<00:20, 128.00it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221672/224309 [28:51<00:20, 129.09it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221686/224309 [28:51<00:20, 129.88it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221700/224309 [28:52<00:19, 130.60it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221714/224309 [28:52<00:19, 130.27it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221728/224309 [28:52<00:19, 130.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221742/224309 [28:52<00:19, 129.99it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221755/224309 [28:52<00:19, 129.92it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221769/224309 [28:52<00:19, 131.14it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221783/224309 [28:52<00:19, 131.42it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221797/224309 [28:52<00:19, 131.19it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221811/224309 [28:52<00:19, 129.53it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221825/224309 [28:53<00:18, 131.15it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221839/224309 [28:53<00:18, 132.99it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221853/224309 [28:53<00:18, 132.40it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221867/224309 [28:53<00:18, 132.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221881/224309 [28:53<00:18, 130.86it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221895/224309 [28:53<00:18, 131.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221909/224309 [28:53<00:18, 131.32it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221924/224309 [28:53<00:17, 133.67it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221940/224309 [28:53<00:16, 139.86it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221955/224309 [28:53<00:17, 137.65it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221970/224309 [28:54<00:16, 138.67it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221984/224309 [28:54<00:17, 136.55it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221998/224309 [28:54<00:17, 133.47it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222013/224309 [28:54<00:16, 137.93it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222027/224309 [28:54<00:16, 137.36it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222041/224309 [28:54<00:16, 137.20it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222062/224309 [28:54<00:14, 155.78it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222081/224309 [28:54<00:13, 164.94it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222099/224309 [28:54<00:13, 166.48it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222121/224309 [28:55<00:12, 179.77it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222141/224309 [28:55<00:11, 184.47it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222160/224309 [28:55<00:12, 165.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222177/224309 [28:55<00:13, 155.13it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222193/224309 [28:55<00:14, 147.50it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222208/224309 [28:55<00:14, 141.13it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222223/224309 [28:55<00:15, 138.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222237/224309 [28:55<00:15, 134.65it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222251/224309 [28:55<00:15, 134.93it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222265/224309 [28:56<00:15, 134.06it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222279/224309 [28:56<00:15, 132.65it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222293/224309 [28:56<00:15, 130.50it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222307/224309 [28:56<00:15, 131.87it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222321/224309 [28:56<00:15, 131.04it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222335/224309 [28:56<00:15, 129.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222348/224309 [28:56<00:15, 128.64it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222362/224309 [28:56<00:15, 129.38it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222375/224309 [28:56<00:14, 129.27it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222388/224309 [28:57<00:14, 128.99it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222401/224309 [28:57<00:14, 128.20it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222415/224309 [28:57<00:14, 129.42it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222428/224309 [28:57<00:14, 128.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222441/224309 [28:57<00:14, 128.07it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222454/224309 [28:57<00:14, 127.78it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222467/224309 [28:57<00:14, 127.59it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222480/224309 [28:57<00:14, 128.18it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222494/224309 [28:57<00:13, 129.98it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222507/224309 [28:57<00:13, 129.79it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222520/224309 [28:58<00:13, 128.59it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222533/224309 [28:58<00:13, 128.96it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222547/224309 [28:58<00:13, 129.42it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222560/224309 [28:58<00:13, 127.96it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222574/224309 [28:58<00:13, 129.72it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222587/224309 [28:58<00:13, 128.91it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222600/224309 [28:58<00:13, 129.10it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222614/224309 [28:58<00:13, 130.08it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222628/224309 [28:58<00:13, 129.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222642/224309 [28:58<00:12, 130.05it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222656/224309 [28:59<00:12, 129.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222670/224309 [28:59<00:12, 130.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222684/224309 [28:59<00:12, 129.88it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222697/224309 [28:59<00:12, 129.33it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222710/224309 [28:59<00:12, 129.24it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222724/224309 [28:59<00:12, 130.03it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222738/224309 [28:59<00:12, 129.17it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222754/224309 [28:59<00:11, 135.48it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222769/224309 [28:59<00:11, 138.17it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222783/224309 [29:00<00:11, 138.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222798/224309 [29:00<00:10, 141.16it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222813/224309 [29:00<00:10, 137.56it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222827/224309 [29:00<00:10, 136.72it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222841/224309 [29:00<00:10, 135.58it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222855/224309 [29:00<00:10, 134.01it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222869/224309 [29:00<00:10, 133.84it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222884/224309 [29:00<00:10, 137.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222901/224309 [29:00<00:09, 146.79it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222920/224309 [29:00<00:08, 157.41it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222940/224309 [29:01<00:08, 168.02it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222958/224309 [29:01<00:07, 169.45it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222978/224309 [29:01<00:07, 178.33it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222999/224309 [29:01<00:07, 186.84it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223018/224309 [29:01<00:07, 173.77it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223036/224309 [29:01<00:08, 158.96it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223053/224309 [29:01<00:08, 148.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223069/224309 [29:01<00:08, 141.72it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223084/224309 [29:02<00:08, 139.07it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223099/224309 [29:02<00:08, 137.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223113/224309 [29:02<00:08, 134.43it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223127/224309 [29:02<00:08, 132.34it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223141/224309 [29:02<00:08, 133.29it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223155/224309 [29:02<00:08, 131.81it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223169/224309 [29:02<00:08, 132.91it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223183/224309 [29:02<00:08, 132.10it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223197/224309 [29:02<00:08, 132.13it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223211/224309 [29:02<00:08, 132.18it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223225/224309 [29:03<00:08, 131.16it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223239/224309 [29:03<00:08, 129.49it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223252/224309 [29:03<00:08, 129.46it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223266/224309 [29:03<00:07, 131.36it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223280/224309 [29:03<00:07, 130.58it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223294/224309 [29:03<00:07, 129.56it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223307/224309 [29:03<00:07, 129.55it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223320/224309 [29:03<00:07, 128.63it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223333/224309 [29:03<00:07, 128.42it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223346/224309 [29:04<00:07, 127.53it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223359/224309 [29:04<00:07, 128.15it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223372/224309 [29:04<00:07, 128.04it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223386/224309 [29:04<00:07, 128.58it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223399/224309 [29:04<00:07, 128.56it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223412/224309 [29:04<00:07, 127.89it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223425/224309 [29:04<00:06, 127.83it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223438/224309 [29:04<00:06, 128.12it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223452/224309 [29:04<00:06, 129.19it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223466/224309 [29:04<00:06, 130.06it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223480/224309 [29:05<00:06, 129.53it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223494/224309 [29:05<00:06, 130.08it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223509/224309 [29:05<00:05, 135.18it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223523/224309 [29:05<00:05, 132.10it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223537/224309 [29:05<00:05, 133.86it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223551/224309 [29:05<00:05, 134.14it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223565/224309 [29:05<00:05, 132.24it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223579/224309 [29:05<00:05, 131.21it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223593/224309 [29:05<00:05, 132.38it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223607/224309 [29:06<00:05, 132.06it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223621/224309 [29:06<00:05, 132.41it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223635/224309 [29:06<00:05, 133.26it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223649/224309 [29:06<00:05, 131.65it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223664/224309 [29:06<00:04, 134.76it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223678/224309 [29:06<00:04, 135.26it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223693/224309 [29:06<00:04, 138.28it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223707/224309 [29:06<00:04, 138.17it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223721/224309 [29:06<00:04, 135.11it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223735/224309 [29:06<00:04, 134.54it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223749/224309 [29:07<00:04, 133.82it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223763/224309 [29:07<00:04, 134.94it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223782/224309 [29:07<00:03, 150.70it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223799/224309 [29:07<00:03, 156.32it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223819/224309 [29:07<00:02, 167.83it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223841/224309 [29:07<00:02, 181.77it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223860/224309 [29:07<00:02, 182.54it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223879/224309 [29:07<00:02, 161.56it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223896/224309 [29:07<00:02, 150.15it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223912/224309 [29:08<00:02, 142.48it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223927/224309 [29:08<00:02, 138.58it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223942/224309 [29:08<00:02, 134.92it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223956/224309 [29:08<00:02, 133.93it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223970/224309 [29:08<00:02, 132.21it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223984/224309 [29:08<00:02, 133.59it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223998/224309 [29:08<00:02, 132.90it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224012/224309 [29:08<00:02, 130.72it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224026/224309 [29:08<00:02, 129.73it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224040/224309 [29:09<00:02, 130.80it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224054/224309 [29:09<00:01, 130.96it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224068/224309 [29:09<00:01, 129.51it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224081/224309 [29:09<00:01, 128.68it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224094/224309 [29:09<00:01, 128.06it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224108/224309 [29:09<00:01, 128.93it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224121/224309 [29:09<00:01, 128.60it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224134/224309 [29:09<00:01, 127.35it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224147/224309 [29:09<00:01, 127.70it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224160/224309 [29:10<00:01, 127.16it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224173/224309 [29:10<00:01, 127.28it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224186/224309 [29:10<00:00, 126.88it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224199/224309 [29:10<00:00, 126.14it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224212/224309 [29:10<00:00, 126.27it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224226/224309 [29:10<00:00, 127.73it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224240/224309 [29:10<00:00, 128.76it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224253/224309 [29:10<00:00, 128.59it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224267/224309 [29:10<00:00, 129.42it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224281/224309 [29:10<00:00, 131.90it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224295/224309 [29:11<00:00, 131.48it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 224309/224309 [29:11<00:00, 131.22it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 224309/224309 [29:11<00:00, 128.09it/s]\nTotal submission rows: 334900334\n\n\n\n\nCell 21:\n```\n# Save submission\nos.makedirs('/home/submission', exist_ok=True)\nsubmission_path = '/home/submission/submission.csv'\n\nwith open(submission_path, 'w') as f:\n    for pid, term, score in submission_rows:\n        f.write(f\"{pid}\\t{term}\\t{score:.6f}\\n\")\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"File size: {os.path.getsize(submission_path) / 1e6:.2f} MB\")\n```\nOutput:\nSubmission saved to /home/submission/submission.csv\nFile size: 9069.05 MB\n\n\nCell 22:\n```\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASELINE MODEL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Features: Hand-crafted (AA composition + sequence properties)\")\nprint(f\"Feature dimension: {INPUT_DIM}\")\nprint(f\"Model: MLP with hidden dims {HIDDEN_DIMS}\")\nprint(f\"Number of GO terms predicted: {len(all_selected_terms)}\")\nprint(f\"Best validation F1: {best_f1:.4f}\")\nprint(f\"Submission rows: {len(submission_rows)}\")\nprint(\"=\"*50)\n```\nOutput:\n\n==================================================\nBASELINE MODEL SUMMARY\n==================================================\nFeatures: Hand-crafted (AA composition + sequence properties)\nFeature dimension: 25\nModel: MLP with hidden dims [512, 256]\nNumber of GO terms predicted: 1500\nBest validation F1: 0.1065\nSubmission rows: 334900334\n==================================================\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: The submission file is too large (9GB with 334M rows). This is because the propagation is adding too many terms. I need to filter more aggressively. Let me fix this by reducing the number of predictions per protein and filtering low-confidence predictions more aggressively."
        },
        {
          "type": "tool_use",
          "id": "toolu_01RSyK2ZJW8sssiuioY7bs3m",
          "name": "AddNotebookCell",
          "input": {
            "cell_type": "code",
            "content": "# The submission is too large - need to filter more aggressively\n# Let's regenerate with stricter thresholds\n\nprint(\"Regenerating submission with stricter filtering...\")\nsubmission_rows = []\n\nMIN_SCORE = 0.01  # Higher threshold\nMAX_PREDS_PER_PROTEIN = 500  # Fewer predictions per protein\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score descending\n    sorted_terms = sorted(term_scores.items(), key=lambda x: -x[1])\n    \n    # Keep top predictions\n    count = 0\n    for term, score in sorted_terms:\n        if score > MIN_SCORE and count < MAX_PREDS_PER_PROTEIN:\n            submission_rows.append((pid, term, score))\n            count += 1\n\nprint(f\"Total submission rows: {len(submission_rows)}\")",
            "position": 23
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01RSyK2ZJW8sssiuioY7bs3m",
          "content": "Added code cell at 23",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01RQeL9DZ3xNUEeethMowEDr",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 23,
            "end_cell": 23,
            "duration": 600
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01RQeL9DZ3xNUEeethMowEDr",
          "content": "Executed cells 23-23\n==================================================\n\u23f0 Cells interrupted due to timeout: [23]\n\nCell 23: \u23f0 INTERRUPTED\n```\n# The submission is too large - need to filter more aggressively\n# Let's regenerate with stricter thresholds\n\nprint(\"Regenerating submission with stricter filtering...\")\nsubmission_rows = []\n\nMIN_SCORE = 0.01  # Higher threshold\nMAX_PREDS_PER_PROTEIN = 500  # Fewer predictions per protein\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score\n```\nOutput:\nRegenerating submission with stricter filtering...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\r  0%|          | 14/224309 [00:00<27:32, 135.72it/s]\n\r  0%|          | 28/224309 [00:00<28:31, 131.07it/s]\n\r  0%|          | 42/224309 [00:00<28:32, 131.00it/s]\n\r  0%|          | 56/224309 [00:00<28:50, 129.55it/s]\n\r  0%|          | 69/224309 [00:00<29:01, 128.74it/s]\n\r  0%|          | 82/224309 [00:00<29:01, 128.72it/s]\n\r  0%|          | 95/224309 [00:00<29:19, 127.42it/s]\n\r  0%|          | 108/224309 [00:00<29:14, 127.76it/s]\n\r  0%|          | 121/224309 [00:00<29:20, 127.37it/s]\n\r  0%|          | 135/224309 [00:01<29:05, 128.41it/s]\n\r  0%|          | 148/224309 [00:01<29:12, 127.88it/s]\n\r  0%|          | 161/224309 [00:01<29:19, 127.41it/s]\n\r  0%|          | 174/224309 [00:01<29:23, 127.11it/s]\n\r  0%|          | 187/224309 [00:01<29:14, 127.74it/s]\n\r  0%|          | 200/224309 [00:01<29:08, 128.14it/s]\n\r  0%|          | 213/224309 [00:01<29:11, 127.98it/s]\n\r  0%|          | 226/224309 [00:01<29:12, 127.85it/s]\n\r  0%|          | 239/224309 [00:01<29:23, 127.08it/s]\n\r  0%|          | 252/224309 [00:01<29:19, 127.36it/s]\n\r  0%|          | 265/224309 [00:02<29:19, 127.35it/s]\n\r  0%|          | 278/224309 [00:02<29:24, 126.93it/s]\n\r  0%|          | 291/224309 [00:02<29:21, 127.20it/s]\n\r  0%|          | 304/224309 [00:02<29:25, 126.85it/s]\n\r  0%|          | 317/224309 [00:02<29:37, 126.02it/s]\n\r  0%|          | 330/224309 [00:02<29:33, 126.28it/s]\n\r  0%|          | 343/224309 [00:02<29:30, 126.50it/s]\n\r  0%|          | 357/224309 [00:02<29:15, 127.59it/s]\n\r  0%|          | 370/224309 [00:02<29:24, 126.89it/s]\n\r  0%|          | 383/224309 [00:02<29:19, 127.24it/s]\n\r  0%|          | 396/224309 [00:03<29:21, 127.09it/s]\n\r  0%|          | 409/224309 [00:03<29:33, 126.26it/s]\n\r  0%|          | 423/224309 [00:03<28:59, 128.74it/s]\n\r  0%|          | 437/224309 [00:03<28:52, 129.20it/s]\n\r  0%|          | 451/224309 [00:03<28:45, 129.75it/s]\n\r  0%|          | 464/224309 [00:03<28:49, 129.39it/s]\n\r  0%|          | 477/224309 [00:03<28:48, 129.51it/s]\n\r  0%|          | 490/224309 [00:03<29:03, 128.35it/s]\n\r  0%|          | 503/224309 [00:03<29:11, 127.78it/s]\n\r  0%|          | 516/224309 [00:04<29:07, 128.05it/s]\n\r  0%|          | 529/224309 [00:04<29:14, 127.57it/s]\n\r  0%|          | 542/224309 [00:04<29:36, 125.96it/s]\n\r  0%|          | 555/224309 [00:04<29:35, 126.02it/s]\n\r  0%|          | 568/224309 [00:04<29:30, 126.41it/s]\n\r  0%|          | 581/224309 [00:04<29:47, 125.15it/s]\n\r  0%|          | 594/224309 [00:04<29:42, 125.53it/s]\n\r  0%|          | 607/224309 [00:04<29:29, 126.39it/s]\n\r  0%|          | 620/224309 [00:04<29:43, 125.41it/s]\n\r  0%|          | 633/224309 [00:04<29:41, 125.54it/s]\n\r  0%|          | 646/224309 [00:05<29:51, 124.85it/s]\n\r  0%|          | 659/224309 [00:05<29:46, 125.19it/s]\n\r  0%|          | 672/224309 [00:05<29:42, 125.44it/s]\n\r  0%|          | 685/224309 [00:05<29:51, 124.80it/s]\n\r  0%|          | 698/224309 [00:05<29:40, 125.56it/s]\n\r  0%|          | 711/224309 [00:05<29:41, 125.51it/s]\n\r  0%|          | 724/224309 [00:05<29:25, 126.64it/s]\n\r  0%|          | 737/224309 [00:05<29:49, 124.90it/s]\n\r  0%|          | 751/224309 [00:05<29:25, 126.61it/s]\n\r  0%|          | 764/224309 [00:06<29:38, 125.67it/s]\n\r  0%|          | 777/224309 [00:06<29:30, 126.25it/s]\n\r  0%|          | 791/224309 [00:06<28:57, 128.64it/s]\n\r  0%|          | 804/224309 [00:06<29:15, 127.29it/s]\n\r  0%|          | 817/224309 [00:06<29:24, 126.65it/s]\n\r  0%|          | 831/224309 [00:06<29:06, 127.95it/s]\n\r  0%|          | 844/224309 [00:06<29:13, 127.43it/s]\n\r  0%|          | 858/224309 [00:06<28:55, 128.77it/s]\n\r  0%|          | 872/224309 [00:06<28:41, 129.81it/s]\n\r  0%|          | 885/224309 [00:06<28:45, 129.45it/s]\n\r  0%|          | 898/224309 [00:07<29:14, 127.31it/s]\n\r  0%|          | 911/224309 [00:07<29:08, 127.80it/s]\n\r  0%|          | 924/224309 [00:07<29:14, 127.31it/s]\n\r  0%|          | 937/224309 [00:07<29:24, 126.63it/s]\n\r  0%|          | 950/224309 [00:07<29:24, 126.58it/s]\n\r  0%|          | 963/224309 [00:07<29:31, 126.11it/s]\n\r  0%|          | 976/224309 [00:07<29:33, 125.90it/s]\n\r  0%|          | 989/224309 [00:07<29:27, 126.32it/s]\n\r  0%|          | 1002/224309 [00:07<29:29, 126.17it/s]\n\r  0%|          | 1016/224309 [00:07<28:49, 129.09it/s]\n\r  0%|          | 1029/224309 [00:08<28:50, 129.03it/s]\n\r  0%|          | 1042/224309 [00:08<28:50, 129.04it/s]\n\r  0%|          | 1055/224309 [00:08<28:54, 128.75it/s]\n\r  0%|          | 1068/224309 [00:08<29:02, 128.14it/s]\n\r  0%|          | 1081/224309 [00:08<29:00, 128.25it/s]\n\r  0%|          | 1094/224309 [00:08<29:09, 127.59it/s]\n\r  0%|          | 1107/224309 [00:08<29:01, 128.17it/s]\n\r  0%|          | 1120/224309 [00:08<29:14, 127.18it/s]\n\r  1%|          | 1134/224309 [00:08<29:02, 128.05it/s]\n\r  1%|          | 1147/224309 [00:09<29:11, 127.38it/s]\n\r  1%|          | 1160/224309 [00:09<29:15, 127.10it/s]\n\r  1%|          | 1173/224309 [00:09<29:15, 127.08it/s]\n\r  1%|          | 1186/224309 [00:09<29:14, 127.20it/s]\n\r  1%|          | 1199/224309 [00:09<29:12, 127.28it/s]\n\r  1%|          | 1212/224309 [00:09<29:19, 126.77it/s]\n\r  1%|          | 1225/224309 [00:09<29:25, 126.33it/s]\n\r  1%|          | 1238/224309 [00:09<29:29, 126.04it/s]\n\r  1%|          | 1251/224309 [00:09<29:24, 126.41it/s]\n\r  1%|          | 1264/224309 [00:09<29:26, 126.29it/s]\n\r  1%|          | 1277/224309 [00:10<29:24, 126.38it/s]\n\r  1%|          | 1290/224309 [00:10<29:32, 125.86it/s]\n\r  1%|          | 1303/224309 [00:10<29:34, 125.69it/s]\n\r  1%|          | 1316/224309 [00:10<29:34, 125.67it/s]\n\r  1%|          | 1330/224309 [00:10<29:15, 127.00it/s]\n\r  1%|          | 1343/224309 [00:10<29:27, 126.14it/s]\n\r  1%|          | 1356/224309 [00:10<29:30, 125.92it/s]\n\r  1%|          | 1370/224309 [00:10<29:22, 126.46it/s]\n\r  1%|          | 1383/224309 [00:10<29:28, 126.08it/s]\n\r  1%|          | 1396/224309 [00:10<29:24, 126.31it/s]\n\r  1%|          | 1409/224309 [00:11<29:11, 127.25it/s]\n\r  1%|          | 1422/224309 [00:11<29:06, 127.62it/s]\n\r  1%|          | 1436/224309 [00:11<28:27, 130.50it/s]\n\r  1%|          | 1450/224309 [00:11<28:13, 131.57it/s]\n\r  1%|          | 1464/224309 [00:11<28:19, 131.12it/s]\n\r  1%|          | 1478/224309 [00:11<28:40, 129.54it/s]\n\r  1%|          | 1491/224309 [00:11<28:54, 128.48it/s]\n\r  1%|          | 1504/224309 [00:11<29:09, 127.36it/s]\n\r  1%|          | 1517/224309 [00:11<29:20, 126.52it/s]\n\r  1%|          | 1530/224309 [00:12<29:24, 126.25it/s]\n\r  1%|          | 1543/224309 [00:12<29:11, 127.22it/s]\n\r  1%|          | 1556/224309 [00:12<29:20, 126.55it/s]\n\r  1%|          | 1569/224309 [00:12<29:23, 126.27it/s]\n\r  1%|          | 1582/224309 [00:12<29:09, 127.31it/s]\n\r  1%|          | 1595/224309 [00:12<29:15, 126.84it/s]\n\r  1%|          | 1608/224309 [00:12<29:03, 127.72it/s]\n\r  1%|          | 1621/224309 [00:12<28:55, 128.33it/s]\n\r  1%|          | 1634/224309 [00:12<29:00, 127.92it/s]\n\r  1%|          | 1647/224309 [00:12<29:12, 127.05it/s]\n\r  1%|          | 1660/224309 [00:13<29:17, 126.68it/s]\n\r  1%|          | 1673/224309 [00:13<29:19, 126.53it/s]\n\r  1%|          | 1686/224309 [00:13<29:20, 126.44it/s]\n\r  1%|          | 1699/224309 [00:13<29:28, 125.88it/s]\n\r  1%|          | 1712/224309 [00:13<29:31, 125.68it/s]\n\r  1%|          | 1725/224309 [00:13<29:27, 125.92it/s]\n\r  1%|          | 1738/224309 [00:13<29:35, 125.38it/s]\n\r  1%|          | 1751/224309 [00:13<29:35, 125.34it/s]\n\r  1%|          | 1764/224309 [00:13<29:29, 125.76it/s]\n\r  1%|          | 1777/224309 [00:13<29:33, 125.45it/s]\n\r  1%|          | 1790/224309 [00:14<29:34, 125.40it/s]\n\r  1%|          | 1803/224309 [00:14<29:17, 126.61it/s]\n\r  1%|          | 1816/224309 [00:14<29:11, 127.05it/s]\n\r  1%|          | 1829/224309 [00:14<29:05, 127.46it/s]\n\r  1%|          | 1842/224309 [00:14<29:09, 127.15it/s]\n\r  1%|          | 1855/224309 [00:14<29:17, 126.55it/s]\n\r  1%|          | 1868/224309 [00:14<29:13, 126.83it/s]\n\r  1%|          | 1881/224309 [00:14<29:25, 125.97it/s]\n\r  1%|          | 1894/224309 [00:14<29:22, 126.19it/s]\n\r  1%|          | 1907/224309 [00:14<29:11, 126.97it/s]\n\r  1%|          | 1920/224309 [00:15<29:16, 126.62it/s]\n\r  1%|          | 1933/224309 [00:15<29:05, 127.43it/s]\n\r  1%|          | 1946/224309 [00:15<29:08, 127.21it/s]\n\r  1%|          | 1960/224309 [00:15<28:55, 128.12it/s]\n\r  1%|          | 1974/224309 [00:15<28:45, 128.85it/s]\n\r  1%|          | 1987/224309 [00:15<29:00, 127.75it/s]\n\r  1%|          | 2000/224309 [00:15<28:59, 127.83it/s]\n\r  1%|          | 2013/224309 [00:15<29:11, 126.88it/s]\n\r  1%|          | 2026/224309 [00:15<29:06, 127.25it/s]\n\r  1%|          | 2039/224309 [00:16<29:13, 126.75it/s]\n\r  1%|          | 2052/224309 [00:16<29:19, 126.34it/s]\n\r  1%|          | 2065/224309 [00:16<29:11, 126.88it/s]\n\r  1%|          | 2078/224309 [00:16<29:22, 126.07it/s]\n\r  1%|          | 2091/224309 [00:16<29:20, 126.24it/s]\n\r  1%|          | 2104/224309 [00:16<29:22, 126.08it/s]\n\r  1%|          | 2117/224309 [00:16<29:23, 126.00it/s]\n\r  1%|          | 2130/224309 [00:16<29:30, 125.48it/s]\n\r  1%|          | 2143/224309 [00:16<29:17, 126.43it/s]\n\r  1%|          | 2156/224309 [00:16<29:13, 126.72it/s]\n\r  1%|          | 2169/224309 [00:17<29:15, 126.57it/s]\n\r  1%|          | 2182/224309 [00:17<29:17, 126.42it/s]\n\r  1%|          | 2195/224309 [00:17<29:25, 125.83it/s]\n\r  1%|          | 2208/224309 [00:17<29:20, 126.16it/s]\n\r  1%|          | 2221/224309 [00:17<29:21, 126.09it/s]\n\r  1%|          | 2234/224309 [00:17<29:28, 125.58it/s]\n\r  1%|          | 2247/224309 [00:17<29:26, 125.74it/s]\n\r  1%|          | 2261/224309 [00:17<29:06, 127.14it/s]\n\r  1%|          | 2275/224309 [00:17<28:44, 128.76it/s]\n\r  1%|          | 2288/224309 [00:17<28:42, 128.87it/s]\n\r  1%|          | 2302/224309 [00:18<28:35, 129.42it/s]\n\r  1%|          | 2315/224309 [00:18<28:52, 128.16it/s]\n\r  1%|          | 2328/224309 [00:18<29:02, 127.38it/s]\n\r  1%|          | 2341/224309 [00:18<28:56, 127.82it/s]\n\r  1%|          | 2355/224309 [00:18<28:34, 129.44it/s]\n\r  1%|          | 2369/224309 [00:18<28:28, 129.90it/s]\n\r  1%|          | 2382/224309 [00:18<28:48, 128.42it/s]\n\r  1%|          | 2395/224309 [00:18<28:55, 127.84it/s]\n\r  1%|          | 2408/224309 [00:18<28:51, 128.16it/s]\n\r  1%|          | 2421/224309 [00:19<29:06, 127.06it/s]\n\r  1%|          | 2434/224309 [00:19<28:58, 127.64it/s]\n\r  1%|          | 2448/224309 [00:19<28:41, 128.91it/s]\n\r  1%|          | 2461/224309 [00:19<28:56, 127.73it/s]\n\r  1%|          | 2474/224309 [00:19<28:59, 127.54it/s]\n\r  1%|          | 2488/224309 [00:19<28:46, 128.44it/s]\n\r  1%|          | 2501/224309 [00:19<29:00, 127.44it/s]\n\r  1%|          | 2514/224309 [00:19<29:06, 127.02it/s]\n\r  1%|          | 2527/224309 [00:19<29:00, 127.42it/s]\n\r  1%|          | 2540/224309 [00:19<29:06, 126.97it/s]\n\r  1%|          | 2553/224309 [00:20<29:08, 126.85it/s]\n\r  1%|          | 2567/224309 [00:20<28:55, 127.75it/s]\n\r  1%|          | 2581/224309 [00:20<28:44, 128.59it/s]\n\r  1%|          | 2594/224309 [00:20<28:52, 127.99it/s]\n\r  1%|          | 2607/224309 [00:20<28:59, 127.49it/s]\n\r  1%|          | 2620/224309 [00:20<29:09, 126.69it/s]\n\r  1%|          | 2633/224309 [00:20<29:09, 126.72it/s]\n\r  1%|          | 2646/224309 [00:20<29:13, 126.42it/s]\n\r  1%|          | 2659/224309 [00:20<29:20, 125.87it/s]\n\r  1%|          | 2672/224309 [00:21<29:18, 126.02it/s]\n\r  1%|          | 2685/224309 [00:21<29:20, 125.89it/s]\n\r  1%|          | 2698/224309 [00:21<29:17, 126.09it/s]\n\r  1%|          | 2711/224309 [00:21<29:08, 126.72it/s]\n\r  1%|          | 2724/224309 [00:21<29:15, 126.23it/s]\n\r  1%|          | 2737/224309 [00:21<29:09, 126.68it/s]\n\r  1%|          | 2750/224309 [00:21<29:05, 126.91it/s]\n\r  1%|          | 2763/224309 [00:21<28:59, 127.39it/s]\n\r  1%|          | 2776/224309 [00:21<28:51, 127.93it/s]\n\r  1%|          | 2789/224309 [00:21<28:52, 127.85it/s]\n\r  1%|          | 2802/224309 [00:22<29:06, 126.82it/s]\n\r  1%|\u258f         | 2815/224309 [00:22<28:56, 127.54it/s]\n\r  1%|\u258f         | 2828/224309 [00:22<28:59, 127.30it/s]\n\r  1%|\u258f         | 2841/224309 [00:22<28:49, 128.05it/s]\n\r  1%|\u258f         | 2854/224309 [00:22<28:45, 128.35it/s]\n\r  1%|\u258f         | 2867/224309 [00:22<28:49, 128.01it/s]\n\r  1%|\u258f         | 2880/224309 [00:22<28:57, 127.45it/s]\n\r  1%|\u258f         | 2894/224309 [00:22<28:43, 128.44it/s]\n\r  1%|\u258f         | 2907/224309 [00:22<28:53, 127.73it/s]\n\r  1%|\u258f         | 2920/224309 [00:22<28:52, 127.79it/s]\n\r  1%|\u258f         | 2933/224309 [00:23<28:59, 127.28it/s]\n\r  1%|\u258f         | 2947/224309 [00:23<28:50, 127.94it/s]\n\r  1%|\u258f         | 2960/224309 [00:23<28:58, 127.32it/s]\n\r  1%|\u258f         | 2973/224309 [00:23<28:56, 127.46it/s]\n\r  1%|\u258f         | 2987/224309 [00:23<28:37, 128.87it/s]\n\r  1%|\u258f         | 3000/224309 [00:23<28:38, 128.75it/s]\n\r  1%|\u258f         | 3013/224309 [00:23<28:41, 128.52it/s]\n\r  1%|\u258f         | 3027/224309 [00:23<28:28, 129.55it/s]\n\r  1%|\u258f         | 3040/224309 [00:23<28:42, 128.46it/s]\n\r  1%|\u258f         | 3053/224309 [00:23<28:46, 128.15it/s]\n\r  1%|\u258f         | 3066/224309 [00:24<28:59, 127.19it/s]\n\r  1%|\u258f         | 3080/224309 [00:24<28:43, 128.40it/s]\n\r  1%|\u258f         | 3093/224309 [00:24<28:54, 127.54it/s]\n\r  1%|\u258f         | 3107/224309 [00:24<28:41, 128.52it/s]\n\r  1%|\u258f         | 3120/224309 [00:24<28:46, 128.11it/s]\n\r  1%|\u258f         | 3133/224309 [00:24<28:54, 127.50it/s]\n\r  1%|\u258f         | 3146/224309 [00:24<28:56, 127.35it/s]\n\r  1%|\u258f         | 3159/224309 [00:24<29:05, 126.67it/s]\n\r  1%|\u258f         | 3172/224309 [00:24<29:05, 126.70it/s]\n\r  1%|\u258f         | 3185/224309 [00:25<29:00, 127.06it/s]\n\r  1%|\u258f         | 3198/224309 [00:25<29:09, 126.40it/s]\n\r  1%|\u258f         | 3211/224309 [00:25<29:06, 126.59it/s]\n\r  1%|\u258f         | 3224/224309 [00:25<29:12, 126.13it/s]\n\r  1%|\u258f         | 3237/224309 [00:25<29:18, 125.70it/s]\n\r  1%|\u258f         | 3250/224309 [00:25<29:09, 126.33it/s]\n\r  1%|\u258f         | 3263/224309 [00:25<29:11, 126.23it/s]\n\r  1%|\u258f         | 3276/224309 [00:25<29:11, 126.23it/s]\n\r  1%|\u258f         | 3289/224309 [00:25<29:14, 125.99it/s]\n\r  1%|\u258f         | 3302/224309 [00:25<29:09, 126.33it/s]\n\r  1%|\u258f         | 3315/224309 [00:26<29:10, 126.28it/s]\n\r  1%|\u258f         | 3328/224309 [00:26<29:10, 126.23it/s]\n\r  1%|\u258f         | 3341/224309 [00:26<29:00, 126.97it/s]\n\r  1%|\u258f         | 3354/224309 [00:26<29:00, 126.95it/s]\n\r  2%|\u258f         | 3367/224309 [00:26<29:08, 126.33it\n...(truncated)...\n\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220590/224309 [28:20<00:28, 130.77it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220604/224309 [28:20<00:28, 130.00it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220618/224309 [28:20<00:28, 129.59it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220632/224309 [28:20<00:28, 129.90it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220645/224309 [28:20<00:28, 129.56it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220659/224309 [28:20<00:28, 129.63it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220672/224309 [28:20<00:28, 129.51it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220686/224309 [28:20<00:27, 130.93it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220700/224309 [28:20<00:27, 130.85it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220714/224309 [28:20<00:27, 129.78it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220727/224309 [28:21<00:27, 128.87it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220740/224309 [28:21<00:27, 127.48it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220753/224309 [28:21<00:27, 128.19it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220766/224309 [28:21<00:27, 128.43it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220779/224309 [28:21<00:27, 128.39it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220792/224309 [28:21<00:27, 127.81it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220806/224309 [28:21<00:27, 128.74it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220820/224309 [28:21<00:26, 131.04it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220834/224309 [28:21<00:26, 130.70it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220848/224309 [28:22<00:26, 130.98it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220862/224309 [28:22<00:26, 130.99it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220876/224309 [28:22<00:26, 131.43it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220890/224309 [28:22<00:26, 130.50it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220904/224309 [28:22<00:26, 129.04it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220917/224309 [28:22<00:26, 129.15it/s]\n\r 98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220931/224309 [28:22<00:25, 130.40it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220945/224309 [28:22<00:26, 128.80it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220959/224309 [28:22<00:25, 130.70it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220974/224309 [28:22<00:25, 133.38it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 220988/224309 [28:23<00:25, 131.55it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221002/224309 [28:23<00:25, 131.33it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221016/224309 [28:23<00:24, 132.32it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221030/224309 [28:23<00:24, 132.36it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221044/224309 [28:23<00:24, 130.87it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221058/224309 [28:23<00:24, 131.70it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221072/224309 [28:23<00:24, 132.53it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221087/224309 [28:23<00:23, 135.55it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221102/224309 [28:23<00:23, 137.09it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221116/224309 [28:24<00:23, 137.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221130/224309 [28:24<00:23, 137.84it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221144/224309 [28:24<00:23, 134.16it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221158/224309 [28:24<00:23, 132.61it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221172/224309 [28:24<00:23, 132.33it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221186/224309 [28:24<00:23, 132.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221205/224309 [28:24<00:20, 148.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221220/224309 [28:24<00:20, 148.08it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221240/224309 [28:24<00:18, 162.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221261/224309 [28:24<00:17, 174.75it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221283/224309 [28:25<00:16, 185.12it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221302/224309 [28:25<00:17, 168.07it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221320/224309 [28:25<00:19, 156.43it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221336/224309 [28:25<00:20, 148.45it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221352/224309 [28:25<00:20, 142.01it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221367/224309 [28:25<00:21, 138.72it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221381/224309 [28:25<00:21, 134.97it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221395/224309 [28:25<00:21, 132.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221409/224309 [28:26<00:22, 131.51it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221423/224309 [28:26<00:22, 129.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221436/224309 [28:26<00:22, 128.76it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221449/224309 [28:26<00:22, 128.24it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221463/224309 [28:26<00:22, 129.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221478/224309 [28:26<00:21, 132.17it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221492/224309 [28:26<00:21, 129.85it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221505/224309 [28:26<00:21, 129.66it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221518/224309 [28:26<00:21, 128.50it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221531/224309 [28:26<00:21, 127.83it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221544/224309 [28:27<00:21, 127.66it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221558/224309 [28:27<00:21, 128.45it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221571/224309 [28:27<00:21, 128.00it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221585/224309 [28:27<00:20, 130.84it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221599/224309 [28:27<00:20, 129.10it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221612/224309 [28:27<00:21, 128.29it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221625/224309 [28:27<00:20, 128.32it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221638/224309 [28:27<00:20, 128.12it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221651/224309 [28:27<00:20, 127.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221665/224309 [28:28<00:20, 128.84it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221678/224309 [28:28<00:20, 128.93it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221692/224309 [28:28<00:20, 130.51it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221706/224309 [28:28<00:20, 129.82it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221719/224309 [28:28<00:20, 129.43it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221733/224309 [28:28<00:19, 130.29it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221747/224309 [28:28<00:19, 128.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221761/224309 [28:28<00:19, 130.19it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221775/224309 [28:28<00:19, 130.59it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221789/224309 [28:28<00:19, 129.88it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221802/224309 [28:29<00:19, 129.79it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221815/224309 [28:29<00:19, 128.23it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221830/224309 [28:29<00:18, 131.58it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221844/224309 [28:29<00:18, 132.54it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221858/224309 [28:29<00:18, 131.35it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221872/224309 [28:29<00:18, 130.95it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221886/224309 [28:29<00:18, 129.32it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221900/224309 [28:29<00:18, 130.35it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221914/224309 [28:29<00:18, 131.28it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221928/224309 [28:30<00:17, 133.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221944/224309 [28:30<00:17, 138.99it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221958/224309 [28:30<00:17, 135.63it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221973/224309 [28:30<00:17, 137.23it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 221987/224309 [28:30<00:17, 135.22it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222001/224309 [28:30<00:17, 131.95it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222017/224309 [28:30<00:16, 138.16it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222031/224309 [28:30<00:16, 135.52it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222047/224309 [28:30<00:16, 141.08it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222067/224309 [28:31<00:14, 156.11it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222086/224309 [28:31<00:13, 165.85it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222103/224309 [28:31<00:13, 166.45it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222126/224309 [28:31<00:11, 183.40it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222145/224309 [28:31<00:11, 181.86it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222164/224309 [28:31<00:13, 164.76it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222181/224309 [28:31<00:13, 153.48it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222197/224309 [28:31<00:14, 145.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222212/224309 [28:31<00:15, 139.35it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222227/224309 [28:32<00:15, 137.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222241/224309 [28:32<00:15, 134.28it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222255/224309 [28:32<00:15, 134.83it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222269/224309 [28:32<00:15, 132.94it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222283/224309 [28:32<00:15, 132.06it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222297/224309 [28:32<00:15, 130.62it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222311/224309 [28:32<00:15, 130.45it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222325/224309 [28:32<00:15, 129.67it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222338/224309 [28:32<00:15, 128.27it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222351/224309 [28:33<00:15, 127.67it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222365/224309 [28:33<00:15, 128.68it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222378/224309 [28:33<00:14, 128.74it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222391/224309 [28:33<00:14, 128.48it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222404/224309 [28:33<00:14, 128.11it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222418/224309 [28:33<00:14, 129.33it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222431/224309 [28:33<00:14, 128.47it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222444/224309 [28:33<00:14, 127.58it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222457/224309 [28:33<00:14, 127.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222470/224309 [28:33<00:14, 127.04it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222484/224309 [28:34<00:14, 128.01it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222498/224309 [28:34<00:13, 129.87it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222511/224309 [28:34<00:13, 129.83it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222524/224309 [28:34<00:13, 128.30it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222538/224309 [28:34<00:13, 128.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222552/224309 [28:34<00:13, 129.25it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222565/224309 [28:34<00:13, 128.69it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222579/224309 [28:34<00:13, 129.04it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222592/224309 [28:34<00:13, 128.92it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222605/224309 [28:34<00:13, 127.94it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222619/224309 [28:35<00:13, 129.69it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222633/224309 [28:35<00:12, 130.64it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222647/224309 [28:35<00:12, 128.01it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222661/224309 [28:35<00:12, 129.74it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222674/224309 [28:35<00:12, 129.29it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222687/224309 [28:35<00:12, 128.57it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222701/224309 [28:35<00:12, 128.98it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222714/224309 [28:35<00:12, 128.57it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222728/224309 [28:35<00:12, 129.71it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222741/224309 [28:36<00:12, 128.54it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222757/224309 [28:36<00:11, 136.91it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222772/224309 [28:36<00:11, 137.77it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222787/224309 [28:36<00:10, 138.70it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222802/224309 [28:36<00:10, 140.65it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222817/224309 [28:36<00:10, 138.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222831/224309 [28:36<00:10, 136.58it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222845/224309 [28:36<00:10, 133.50it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222859/224309 [28:36<00:10, 132.82it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222873/224309 [28:36<00:10, 133.61it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222888/224309 [28:37<00:10, 136.64it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222909/224309 [28:37<00:09, 155.46it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222927/224309 [28:37<00:08, 161.56it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222946/224309 [28:37<00:08, 168.86it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222964/224309 [28:37<00:07, 169.81it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 222987/224309 [28:37<00:07, 185.41it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223006/224309 [28:37<00:07, 184.38it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223025/224309 [28:37<00:07, 167.03it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223043/224309 [28:37<00:08, 154.14it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223059/224309 [28:38<00:08, 145.10it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223074/224309 [28:38<00:08, 139.42it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223089/224309 [28:38<00:08, 137.24it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223103/224309 [28:38<00:08, 136.80it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223117/224309 [28:38<00:08, 132.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223131/224309 [28:38<00:08, 132.44it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223145/224309 [28:38<00:08, 131.68it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223159/224309 [28:38<00:08, 131.21it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223173/224309 [28:39<00:08, 131.70it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223187/224309 [28:39<00:08, 131.23it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223201/224309 [28:39<00:08, 130.83it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223215/224309 [28:39<00:08, 131.92it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223229/224309 [28:39<00:08, 130.39it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223243/224309 [28:39<00:08, 128.39it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223257/224309 [28:39<00:08, 129.76it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223271/224309 [28:39<00:07, 130.48it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223285/224309 [28:39<00:07, 129.74it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223298/224309 [28:39<00:07, 129.14it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223312/224309 [28:40<00:07, 129.48it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223325/224309 [28:40<00:07, 128.49it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223338/224309 [28:40<00:07, 127.84it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223351/224309 [28:40<00:07, 127.16it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223364/224309 [28:40<00:07, 127.78it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223377/224309 [28:40<00:07, 128.09it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223390/224309 [28:40<00:07, 128.09it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223403/224309 [28:40<00:07, 127.84it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223416/224309 [28:40<00:07, 127.38it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223429/224309 [28:40<00:06, 126.67it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223442/224309 [28:41<00:06, 127.11it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223456/224309 [28:41<00:06, 129.46it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223469/224309 [28:41<00:06, 128.31it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223482/224309 [28:41<00:06, 128.23it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223496/224309 [28:41<00:06, 129.70it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223511/224309 [28:41<00:05, 134.23it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223525/224309 [28:41<00:05, 130.88it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223540/224309 [28:41<00:05, 135.26it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223554/224309 [28:41<00:05, 132.75it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223568/224309 [28:42<00:05, 130.05it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223582/224309 [28:42<00:05, 129.94it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223596/224309 [28:42<00:05, 131.05it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223610/224309 [28:42<00:05, 129.99it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223624/224309 [28:42<00:05, 130.85it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223638/224309 [28:42<00:05, 131.39it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223652/224309 [28:42<00:05, 130.08it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223667/224309 [28:42<00:04, 133.25it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223682/224309 [28:42<00:04, 135.98it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223696/224309 [28:43<00:04, 136.82it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223710/224309 [28:43<00:04, 137.02it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223724/224309 [28:43<00:04, 134.03it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223738/224309 [28:43<00:04, 133.36it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223752/224309 [28:43<00:04, 133.69it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223766/224309 [28:43<00:04, 134.51it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223787/224309 [28:43<00:03, 155.06it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223805/224309 [28:43<00:03, 161.92it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223824/224309 [28:43<00:02, 169.64it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223847/224309 [28:43<00:02, 185.11it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223866/224309 [28:44<00:02, 177.35it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223884/224309 [28:44<00:02, 158.10it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223901/224309 [28:44<00:02, 146.72it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223917/224309 [28:44<00:02, 140.32it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223932/224309 [28:44<00:02, 136.55it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223946/224309 [28:44<00:02, 133.11it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223960/224309 [28:44<00:02, 132.53it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223974/224309 [28:44<00:02, 132.50it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 223988/224309 [28:45<00:02, 131.40it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224002/224309 [28:45<00:02, 131.25it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224016/224309 [28:45<00:02, 129.59it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224029/224309 [28:45<00:02, 128.20it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224043/224309 [28:45<00:02, 130.94it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224057/224309 [28:45<00:01, 129.56it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224070/224309 [28:45<00:01, 128.21it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224083/224309 [28:45<00:01, 127.66it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224096/224309 [28:45<00:01, 127.16it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224110/224309 [28:45<00:01, 128.25it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224123/224309 [28:46<00:01, 127.78it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224136/224309 [28:46<00:01, 126.65it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224149/224309 [28:46<00:01, 127.03it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224162/224309 [28:46<00:01, 125.96it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224175/224309 [28:46<00:01, 126.08it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224188/224309 [28:46<00:00, 125.97it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224201/224309 [28:46<00:00, 125.65it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224214/224309 [28:46<00:00, 125.21it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224228/224309 [28:46<00:00, 127.91it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224241/224309 [28:47<00:00, 127.14it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224254/224309 [28:47<00:00, 127.00it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224268/224309 [28:47<00:00, 128.01it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224282/224309 [28:47<00:00, 131.28it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 224296/224309 [28:47<00:00, 130.95it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 224309/224309 [28:47<00:00, 129.84it/s]\nTotal submission rows: 111768027\n\n\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Still too many rows (111M). The propagation is adding too many ancestor terms. Let me simplify the approach - skip propagation for now and just use the direct predictions with a higher threshold. This will be a simpler baseline."
        },
        {
          "type": "tool_use",
          "id": "toolu_01HEutSnttmPS6ebVYxehySQ",
          "name": "AddNotebookCell",
          "input": {
            "cell_type": "code",
            "content": "# Simpler approach - skip propagation, just use direct predictions\\n# This is a baseline - we can add propagation later\\n\\nprint(\\\"Generating simple submission without propagation...\\\")\\nsubmission_rows = []\\n\\nMIN_SCORE = 0.1  # Higher threshold\\nMAX_PREDS_PER_PROTEIN = 100  # Fewer predictions\\n\\n# Get term to aspect mapping\\nterm_to_aspect = {}\\nfor _, row in train_terms.iterrows():\\n    term_to_aspect[row['term']] = row['aspect']\\n\\nfor i, pid in enumerate(tqdm(test_protein_ids)):\\n    protein_preds = test_preds[i]\\n    \\n    # Get predictions above threshold\\n    pred_indices = np.where(protein_preds > MIN_SCORE)[0]\\n    \\n    # Sort by score\\n    sorted_indices = pred_indices[np.argsort(-protein_preds[pred_indices])]\\n    \\n    # Add root terms with confidence 1.0\\n    submission_rows.append((pid, 'GO:0003674', 1.0))  # MF root\\n    submission_rows.append((pid, 'GO:0008150', 1.0))  # BP root\\n    submission_rows.append((pid, 'GO:0005575', 1.0))  # CC root\\n    \\n    # Add top predictions\\n    count = 0\\n    for idx in sorted_indices[:MAX_PREDS_PER_PROTEIN]:\\n        term = all_selected_terms[idx]\\n        score = protein_preds[idx]\\n        submission_rows.append((pid, term, float(score)))\\n        count += 1\\n\\nprint(f\\\"Total submission rows: {len(submission_rows)}\\\")",
            "position": 24
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01HEutSnttmPS6ebVYxehySQ",
          "content": "Added code cell at 24",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_012YYBn9wGDtRF1eschKwvMf",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 24,
            "end_cell": 24,
            "duration": 120
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_012YYBn9wGDtRF1eschKwvMf",
          "content": "Executed cells 24-24\n==================================================\n\nCell 24:\n```\n# Simpler approach - skip propagation, just use direct predictions\\n# This is a baseline - we can add propagation later\\n\\nprint(\\\"Generating simple submission without propagation...\\\")\\nsubmission_rows = []\\n\\nMIN_SCORE = 0.1  # Higher threshold\\nMAX_PREDS_PER_PROTEIN = 100  # Fewer predictions\\n\\n# Get term to aspect mapping\\nterm_to_aspect = {}\\nfor _, row in train_terms.iterrows():\\n    term_to_aspect[row['term']] = row['aspect']\\n\\nfor i, pid in enumerate(tqdm(test_protein_ids)):\\n    prote\n```",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_01UL679dCBC7ZF3Avt9MS2xx",
          "name": "ReadNotebook",
          "input": {}
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01UL679dCBC7ZF3Avt9MS2xx",
          "content": "Notebook: /home/code/experiments/001_baseline/baseline.ipynb\nCells: 25\n==================================================\n\nCell 0 [markdown] In[ ]:\n# Baseline Model for Protein Function Prediction\n\nThis baseline uses:\n1. Hand-crafted features (amino acid composition, sequence length, etc.)\n2. MLP classifier for multi-label classification\n3. Top 500 most frequent GO terms\n4. GO hierarchy propagation for post-processing\n\nCell 1 [code] In[1]:\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport gc\nimport os\nfrom tqdm import tqdm\n\n# Check GPU\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nOutput:\nGPU available: True\nGPU: NVIDIA H100 80GB HBM3\nMemory: 85.0 GB\nUsing device: cuda\n\n\nCell 2 [code] In[2]:\n# Load data\ndef parse_fasta(fasta_file):\n    \"\"\"Parse FASTA file and return dict of protein_id -> sequence\"\"\"\n    sequences = {}\n    current_id = None\n    current_seq = []\n    \n    with open(fasta_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith('>'):\n                if current_id is not None:\n                    sequences[current_id] = ''.join(current_seq)\n                # Extract protein ID (first part after >)\n                header = line[1:]\n                # Handle different FASTA header formats\n                if '|' in header:\n                    parts = header.split('|')\n                    current_id = parts[1] if len(parts) > 1 else parts[0]\n                else:\n                    current_id = header.split()[0]\n                current_seq = []\n            else:\n                current_seq.append(line)\n        \n        # Don't forget the last sequence\n        if current_id is not None:\n            sequences[current_id] \nOutput:\nLoading training sequences...\n\nLoaded 82404 training sequences\nLoading test sequences...\n\n\nCell 3 [code] In[3]:\n# Load GO terms\ntrain_terms = pd.read_csv('/home/data/Train/train_terms.tsv', sep='\\t')\nprint(f\"Train terms shape: {train_terms.shape}\")\nprint(train_terms.head())\n\n# Count term frequencies\nterm_counts = train_terms['term'].value_counts()\nprint(f\"\\nTotal unique GO terms: {len(term_counts)}\")\nprint(f\"\\nTop 10 most frequent terms:\")\nprint(term_counts.head(10))\nOutput:\nTrain terms shape: (537027, 3)\n  EntryID        term aspect\n0  Q5W0B1  GO:0000785      C\n1  Q5W0B1  GO:0004842      F\n2  Q5W0B1  GO:0051865      P\n3  Q5W0B1  GO:0006275      P\n4  Q5W0B1  GO:0006513      P\n\nTotal unique GO terms: 26125\n\nTop 10 most frequent terms:\nterm\nGO:0005515    33713\nGO:0005634    13283\nGO:0005829    13040\nGO:0005886    10150\nGO:0005737     9442\nGO:0005739     5807\nGO:0005654     5065\nGO:0016020     3563\nGO:0042802     3547\nGO:0005576     3241\nName: count, dtype: int64\n\n\nCell 4 [code] In[4]:\n# Select top N most frequent GO terms for each aspect\nN_TERMS = 500  # Top 500 terms per aspect\n\n# Group by aspect\naspect_terms = train_terms.groupby('aspect')['term'].value_counts()\n\n# Get top N terms per aspect\nselected_terms = {}\nfor aspect in ['F', 'P', 'C']:\n    aspect_counts = aspect_terms[aspect]\n    selected_terms[aspect] = aspect_counts.head(N_TERMS).index.tolist()\n    print(f\"Aspect {aspect}: {len(selected_terms[aspect])} terms selected\")\n\n# Combine all selected terms\nall_selected_terms = []\nfor aspect in ['F', 'P', 'C']:\n    all_selected_terms.extend(selected_terms[aspect])\nall_selected_terms = list(set(all_selected_terms))\nprint(f\"\\nTotal selected terms: {len(all_selected_terms)}\")\nOutput:\nAspect F: 500 terms selected\nAspect P: 500 terms selected\nAspect C: 500 terms selected\n\nTotal selected terms: 1500\n\n\nCell 5 [code] In[5]:\n# Create protein -> terms mapping (only for selected terms)\nprotein_terms = defaultdict(list)\nfor _, row in train_terms.iterrows():\n    if row['term'] in all_selected_terms:\n        protein_terms[row['EntryID']].append(row['term'])\n\nprint(f\"Proteins with at least one selected term: {len(protein_terms)}\")\nOutput:\nProteins with at least one selected term: 76029\n\n\nCell 6 [code] In[6]:\n# Feature extraction functions\nAMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n\ndef compute_aa_composition(sequence):\n    \"\"\"Compute amino acid composition (20 features)\"\"\"\n    seq_len = len(sequence)\n    if seq_len == 0:\n        return np.zeros(20)\n    \n    composition = np.zeros(20)\n    for i, aa in enumerate(AMINO_ACIDS):\n        composition[i] = sequence.count(aa) / seq_len\n    return composition\n\ndef compute_sequence_features(sequence):\n    \"\"\"Compute hand-crafted features from protein sequence\"\"\"\n    seq_len = len(sequence)\n    \n    # Amino acid composition (20 features)\n    aa_comp = compute_aa_composition(sequence)\n    \n    # Sequence length (log-transformed)\n    log_len = np.log1p(seq_len)\n    \n    # Molecular weight approximation (average AA weight ~110 Da)\n    aa_weights = {\n        'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165, 'G': 75, 'H': 155,\n        'I': 131, 'K': 146, 'L': 131, 'M': 149, 'N': 132, 'P': 115, 'Q': 146,\n        'R': 174, 'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y\nOutput:\nFeature dimension: 25\n\n\nCell 7 [code] In[7]:\n# Extract features for all training proteins\nprint(\"Extracting features for training proteins...\")\ntrain_protein_ids = list(protein_terms.keys())\ntrain_features = []\n\nfor pid in tqdm(train_protein_ids):\n    if pid in train_sequences:\n        features = compute_sequence_features(train_sequences[pid])\n    else:\n        features = np.zeros(25)  # Default features if sequence not found\n    train_features.append(features)\n\ntrain_features = np.array(train_features, dtype=np.float32)\nprint(f\"Training features shape: {train_features.shape}\")\nOutput:\nExtracting features for training proteins...\n\n\r  0%|          | 0/76029 [00:00<?, ?it/s]\n\nCell 8 [code] In[8]:\n# Create multi-label targets\nmlb = MultiLabelBinarizer(classes=all_selected_terms)\ntrain_labels = mlb.fit_transform([protein_terms[pid] for pid in train_protein_ids])\nprint(f\"Training labels shape: {train_labels.shape}\")\nprint(f\"Average labels per protein: {train_labels.sum(axis=1).mean():.2f}\")\nOutput:\nTraining labels shape: (76029, 1500)\nAverage labels per protein: 4.36\n\n\nCell 9 [code] In[9]:\n# Train/validation split\nX_train, X_val, y_train, y_val, train_ids, val_ids = train_test_split(\n    train_features, train_labels, train_protein_ids,\n    test_size=0.1, random_state=42\n)\nprint(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}\")\nOutput:\nTrain: 68426, Val: 7603\n\n\nCell 10 [code] In[10]:\n# PyTorch Dataset\nclass ProteinDataset(Dataset):\n    def __init__(self, features, labels=None):\n        self.features = torch.FloatTensor(features)\n        self.labels = torch.FloatTensor(labels) if labels is not None else None\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return self.features[idx], self.labels[idx]\n        return self.features[idx]\n\n# Create datasets\ntrain_dataset = ProteinDataset(X_train, y_train)\nval_dataset = ProteinDataset(X_val, y_val)\n\n# DataLoaders\nBATCH_SIZE = 128\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nCell 11 [code] In[11]:\n# MLP Model\nclass ProteinMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.3):\n        super().__init__()\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.BatchNorm1d(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.append(nn.Linear(prev_dim, output_dim))\n        \n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Create model\nINPUT_DIM = train_features.shape[1]\nOUTPUT_DIM = len(all_selected_terms)\nHIDDEN_DIMS = [512, 256]\n\nmodel = ProteinMLP(INPUT_DIM, HIDDEN_DIMS, OUTPUT_DIM, dropout=0.3)\nmodel = model.to(device)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nOutput:\nModel parameters: 531,676\n\n\nCell 12 [code] In[12]:\n# Training setup\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for features, labels in loader:\n        features, labels = features.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for features, labels in loader:\n            features, labels = features.to(device), labels.to(device)\n            outputs =\n\nCell 13 [code] In[13]:\n# Training loop\nEPOCHS = 15\nbest_f1 = 0\nbest_model_state = None\n\nprint(\"Starting training...\")\nfor epoch in range(EPOCHS):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_f1, val_prec, val_rec = validate(model, val_loader, criterion, device)\n    scheduler.step()\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_model_state = model.state_dict().copy()\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n          f\"Val F1={val_f1:.4f}, Prec={val_prec:.4f}, Rec={val_rec:.4f}\")\n\nprint(f\"\\nBest validation F1: {best_f1:.4f}\")\n\n# Load best model\nmodel.load_state_dict(best_model_state)\nOutput:\nStarting training...\n\nEpoch 1/15: Train Loss=0.0359, Val Loss=0.0155, Val F1=0.0662, Prec=0.5672, Rec=0.0352\n\n\nCell 14 [code] In[14]:\n# Parse GO ontology for hierarchy propagation\nimport re\n\ndef parse_go_obo(obo_file):\n    \"\"\"Parse GO OBO file and extract parent-child relationships\"\"\"\n    go_parents = defaultdict(set)\n    go_aspects = {}\n    \n    current_id = None\n    current_namespace = None\n    \n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            \n            if line == '[Term]':\n                current_id = None\n                current_namespace = None\n            elif line.startswith('id: GO:'):\n                current_id = line.split()[1]\n            elif line.startswith('namespace:'):\n                ns = line.split()[1]\n                if 'molecular_function' in ns:\n                    current_namespace = 'F'\n                elif 'biological_process' in ns:\n                    current_namespace = 'P'\n                elif 'cellular_component' in ns:\n                    current_namespace = 'C'\n                if current_id:\n                    go_aspects[current_\nOutput:\nParsing GO ontology...\n\nGO terms with parents: 40119\nGO terms with aspects: 48101\n\n\nCell 15 [code] In[15]:\n# Build ancestor map for efficient propagation\ndef get_all_ancestors(term, go_parents, cache=None):\n    \"\"\"Get all ancestors of a GO term\"\"\"\n    if cache is None:\n        cache = {}\n    \n    if term in cache:\n        return cache[term]\n    \n    ancestors = set()\n    for parent in go_parents.get(term, set()):\n        ancestors.add(parent)\n        ancestors.update(get_all_ancestors(parent, go_parents, cache))\n    \n    cache[term] = ancestors\n    return ancestors\n\n# Pre-compute ancestors for all selected terms\nprint(\"Computing ancestors for selected terms...\")\nancestor_cache = {}\nterm_ancestors = {}\nfor term in tqdm(all_selected_terms):\n    term_ancestors[term] = get_all_ancestors(term, go_parents, ancestor_cache)\n\nprint(f\"Computed ancestors for {len(term_ancestors)} terms\")\nOutput:\nComputing ancestors for selected terms...\n\n\r  0%|          | 0/1500 [00:00<?, ?it/s]\n\nCell 16 [code] In[16]:\n# Extract features for test proteins\nprint(\"Extracting features for test proteins...\")\ntest_protein_ids = list(test_sequences.keys())\ntest_features = []\n\nfor pid in tqdm(test_protein_ids):\n    features = compute_sequence_features(test_sequences[pid])\n    test_features.append(features)\n\ntest_features = np.array(test_features, dtype=np.float32)\nprint(f\"Test features shape: {test_features.shape}\")\nOutput:\nExtracting features for test proteins...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\nCell 17 [code] In[17]:\n# Generate predictions for test set\nprint(\"Generating predictions...\")\ntest_dataset = ProteinDataset(test_features)\ntest_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\nmodel.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for features in tqdm(test_loader):\n        features = features.to(device)\n        outputs = model(features)\n        # Apply temperature scaling\n        T = 1.5\n        preds = torch.sigmoid(outputs / T)\n        all_preds.append(preds.cpu().numpy())\n\ntest_preds = np.vstack(all_preds)\nprint(f\"Test predictions shape: {test_preds.shape}\")\nOutput:\nGenerating predictions...\n\n\r  0%|          | 0/877 [00:00<?, ?it/s]\n\nCell 18 [code] In[18]:\n# Load IA weights\nia_df = pd.read_csv('/home/data/IA.tsv', sep='\\t', header=None, names=['term', 'ia_weight'])\nia_weights = dict(zip(ia_df['term'], ia_df['ia_weight']))\nprint(f\"Loaded IA weights for {len(ia_weights)} terms\")\nOutput:\nLoaded IA weights for 40122 terms\n\n\nCell 19 [code] In[19]:\n# Root terms for each aspect\nROOT_TERMS = {\n    'F': 'GO:0003674',  # molecular_function\n    'P': 'GO:0008150',  # biological_process\n    'C': 'GO:0005575'   # cellular_component\n}\n\ndef propagate_predictions(protein_preds, terms, term_ancestors, go_aspects):\n    \"\"\"Propagate predictions to ancestors and apply post-processing\"\"\"\n    # Create term -> score mapping\n    term_scores = {}\n    for i, term in enumerate(terms):\n        if protein_preds[i] > 0.001:  # Only consider non-trivial predictions\n            term_scores[term] = protein_preds[i]\n            # Propagate to ancestors\n            for ancestor in term_ancestors.get(term, set()):\n                if ancestor not in term_scores:\n                    term_scores[ancestor] = protein_preds[i]\n                else:\n                    term_scores[ancestor] = max(term_scores[ancestor], protein_preds[i])\n    \n    # Force root terms to 1.0\n    for root in ROOT_TERMS.values():\n        term_scores[root] = 1.0\n    \n    # Rank normalizatio\nOutput:\nExample propagated predictions: 1216 terms\n\n\nCell 20 [code] In[20]:\n# Generate submission\nprint(\"Generating submission file...\")\nsubmission_rows = []\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score descending\n    sorted_terms = sorted(term_scores.items(), key=lambda x: -x[1])\n    \n    # Keep top predictions (max 1500 per protein)\n    for term, score in sorted_terms[:1500]:\n        if score > 0.001:  # Filter very low scores\n            submission_rows.append((pid, term, score))\n\nprint(f\"Total submission rows: {len(submission_rows)}\")\nOutput:\nGenerating submission file...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\nCell 21 [code] In[21]:\n# Save submission\nos.makedirs('/home/submission', exist_ok=True)\nsubmission_path = '/home/submission/submission.csv'\n\nwith open(submission_path, 'w') as f:\n    for pid, term, score in submission_rows:\n        f.write(f\"{pid}\\t{term}\\t{score:.6f}\\n\")\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"File size: {os.path.getsize(submission_path) / 1e6:.2f} MB\")\nOutput:\nSubmission saved to /home/submission/submission.csv\nFile size: 9069.05 MB\n\n\nCell 22 [code] In[22]:\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASELINE MODEL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Features: Hand-crafted (AA composition + sequence properties)\")\nprint(f\"Feature dimension: {INPUT_DIM}\")\nprint(f\"Model: MLP with hidden dims {HIDDEN_DIMS}\")\nprint(f\"Number of GO terms predicted: {len(all_selected_terms)}\")\nprint(f\"Best validation F1: {best_f1:.4f}\")\nprint(f\"Submission rows: {len(submission_rows)}\")\nprint(\"=\"*50)\nOutput:\n\n==================================================\nBASELINE MODEL SUMMARY\n==================================================\nFeatures: Hand-crafted (AA composition + sequence properties)\nFeature dimension: 25\nModel: MLP with hidden dims [512, 256]\nNumber of GO terms predicted: 1500\nBest validation F1: 0.1065\nSubmission rows: 334900334\n==================================================\n\n\nCell 23 [code] In[23]:\n# The submission is too large - need to filter more aggressively\n# Let's regenerate with stricter thresholds\n\nprint(\"Regenerating submission with stricter filtering...\")\nsubmission_rows = []\n\nMIN_SCORE = 0.01  # Higher threshold\nMAX_PREDS_PER_PROTEIN = 500  # Fewer predictions per protein\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    # Get propagated predictions\n    term_scores = propagate_predictions(test_preds[i], all_selected_terms, term_ancestors, go_aspects)\n    \n    # Sort by score descending\n    sorted_terms = sorted(term_scores.items(), key=lambda x: -x[1])\n    \n    # Keep top predictions\n    count = 0\n    for term, score in sorted_terms:\n        if score > MIN_SCORE and count < MAX_PREDS_PER_PROTEIN:\n            submission_rows.append((pid, term, score))\n            count += 1\n\nprint(f\"Total submission rows: {len(submission_rows)}\")\nOutput:\nRegenerating submission with stricter filtering...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\nCell 24 [code] In[24]:\n# Simpler approach - skip propagation, just use direct predictions\\n# This is a baseline - we can add propagation later\\n\\nprint(\\\"Generating simple submission without propagation...\\\")\\nsubmission_rows = []\\n\\nMIN_SCORE = 0.1  # Higher threshold\\nMAX_PREDS_PER_PROTEIN = 100  # Fewer predictions\\n\\n# Get term to aspect mapping\\nterm_to_aspect = {}\\nfor _, row in train_terms.iterrows():\\n    term_to_aspect[row['term']] = row['aspect']\\n\\nfor i, pid in enumerate(tqdm(test_protein_ids)):\\n    protein_preds = test_preds[i]\\n    \\n    # Get predictions above threshold\\n    pred_indices = np.where(protein_preds > MIN_SCORE)[0]\\n    \\n    # Sort by score\\n    sorted_indices = pred_indices[np.argsort(-protein_preds[pred_indices])]\\n    \\n    # Add root terms with confidence 1.0\\n    submission_rows.append((pid, 'GO:0003674', 1.0))  # MF root\\n    submission_rows.append((pid, 'GO:0008150', 1.0))  # BP root\\n    submission_rows.append((pid, 'GO:0005575', 1.0))  # CC root\\n    \\n    # Add top pre",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: The cell 24 didn't execute properly. Let me edit it and run it again."
        },
        {
          "type": "tool_use",
          "id": "toolu_01Rr9rkzCUdY2sM9A1vxh5J7",
          "name": "EditNotebookCell",
          "input": {
            "cell_index": 24,
            "new_content": "# Simpler approach - skip propagation, just use direct predictions\n# This is a baseline - we can add propagation later\n\nprint(\"Generating simple submission without propagation...\")\nsubmission_rows = []\n\nMIN_SCORE = 0.1  # Higher threshold\nMAX_PREDS_PER_PROTEIN = 100  # Fewer predictions\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    protein_preds = test_preds[i]\n    \n    # Get predictions above threshold\n    pred_indices = np.where(protein_preds > MIN_SCORE)[0]\n    \n    # Sort by score\n    sorted_indices = pred_indices[np.argsort(-protein_preds[pred_indices])]\n    \n    # Add root terms with confidence 1.0\n    submission_rows.append((pid, 'GO:0003674', 1.0))  # MF root\n    submission_rows.append((pid, 'GO:0008150', 1.0))  # BP root\n    submission_rows.append((pid, 'GO:0005575', 1.0))  # CC root\n    \n    # Add top predictions\n    count = 0\n    for idx in sorted_indices[:MAX_PREDS_PER_PROTEIN]:\n        term = all_selected_terms[idx]\n        score = protein_preds[idx]\n        submission_rows.append((pid, term, float(score)))\n        count += 1\n\nprint(f\"Total submission rows: {len(submission_rows)}\")"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01Rr9rkzCUdY2sM9A1vxh5J7",
          "content": "Edited cell 24. Re-run to update kernel state.",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_016zVnwbKjBhXjBV4jx3feUZ",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 24,
            "end_cell": 24,
            "duration": 120
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_016zVnwbKjBhXjBV4jx3feUZ",
          "content": "Executed cells 24-24\n==================================================\n\nCell 24:\n```\n# Simpler approach - skip propagation, just use direct predictions\n# This is a baseline - we can add propagation later\n\nprint(\"Generating simple submission without propagation...\")\nsubmission_rows = []\n\nMIN_SCORE = 0.1  # Higher threshold\nMAX_PREDS_PER_PROTEIN = 100  # Fewer predictions\n\nfor i, pid in enumerate(tqdm(test_protein_ids)):\n    protein_preds = test_preds[i]\n    \n    # Get predictions above threshold\n    pred_indices = np.where(protein_preds > MIN_SCORE)[0]\n    \n    # Sort by score\n  \n```\nOutput:\nGenerating simple submission without propagation...\n\n\r  0%|          | 0/224309 [00:00<?, ?it/s]\n\r  4%|\u258e         | 8111/224309 [00:00<00:02, 81100.80it/s]\n\r  7%|\u258b         | 16465/224309 [00:00<00:02, 82530.86it/s]\n\r 11%|\u2588         | 25179/224309 [00:00<00:02, 84634.02it/s]\n\r 15%|\u2588\u258c        | 33846/224309 [00:00<00:02, 85434.75it/s]\n\r 19%|\u2588\u2589        | 42624/224309 [00:00<00:02, 86277.18it/s]\n\r 23%|\u2588\u2588\u258e       | 51378/224309 [00:00<00:01, 86705.49it/s]\n\r 27%|\u2588\u2588\u258b       | 60201/224309 [00:00<00:01, 87201.58it/s]\n\r 31%|\u2588\u2588\u2588       | 69085/224309 [00:00<00:01, 87720.58it/s]\n\r 35%|\u2588\u2588\u2588\u258d      | 77951/224309 [00:00<00:01, 88011.84it/s]\n\r 39%|\u2588\u2588\u2588\u258a      | 86892/224309 [00:01<00:01, 88442.90it/s]\n\r 43%|\u2588\u2588\u2588\u2588\u258e     | 95737/224309 [00:01<00:01, 88349.30it/s]\n\r 47%|\u2588\u2588\u2588\u2588\u258b     | 104589/224309 [00:01<00:01, 88399.17it/s]\n\r 51%|\u2588\u2588\u2588\u2588\u2588     | 113537/224309 [00:01<00:01, 88725.28it/s]\n\r 55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 122565/224309 [00:01<00:01, 89193.88it/s]\n\r 59%|\u2588\u2588\u2588\u2588\u2588\u258a    | 131618/224309 [00:01<00:01, 89595.83it/s]\n\r 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 140668/224309 [00:01<00:00, 89867.05it/s]\n\r 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 149729/224309 [00:01<00:00, 90088.32it/s]\n\r 71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 158748/224309 [00:01<00:00, 90118.25it/s]\n\r 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 167760/224309 [00:01<00:00, 89862.78it/s]\n\r 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 176747/224309 [00:02<00:00, 89325.52it/s]\n\r 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 185681/224309 [00:02<00:00, 89017.31it/s]\n\r 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 194584/224309 [00:02<00:00, 89008.78it/s]\n\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 203486/224309 [00:02<00:00, 85091.13it/s]\n\r 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 212283/224309 [00:02<00:00, 85923.90it/s]\n\r 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 221206/224309 [00:02<00:00, 86890.23it/s]\n\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 224309/224309 [00:02<00:00, 87793.51it/s]\nTotal submission rows: 3506936\n\n\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: Now we have a reasonable submission size (~3.5M rows). Let me save this submission."
        },
        {
          "type": "tool_use",
          "id": "toolu_015Kp9bW6NgRyNBEWFZECwdR",
          "name": "AddNotebookCell",
          "input": {
            "cell_type": "code",
            "content": "# Save the simplified submission\nsubmission_path = '/home/submission/submission.csv'\n\nwith open(submission_path, 'w') as f:\n    for pid, term, score in submission_rows:\n        f.write(f\"{pid}\\t{term}\\t{score:.6f}\\n\")\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"File size: {os.path.getsize(submission_path) / 1e6:.2f} MB\")\nprint(f\"Total rows: {len(submission_rows)}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASELINE MODEL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Features: Hand-crafted (AA composition + sequence properties)\")\nprint(f\"Feature dimension: {INPUT_DIM}\")\nprint(f\"Model: MLP with hidden dims {HIDDEN_DIMS}\")\nprint(f\"Number of GO terms predicted: {len(all_selected_terms)}\")\nprint(f\"Best validation F1: {best_f1:.4f}\")\nprint(f\"Submission rows: {len(submission_rows)}\")\nprint(\"=\"*50)",
            "position": 25
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_015Kp9bW6NgRyNBEWFZECwdR",
          "content": "Added code cell at 25",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_017HJxWMRWGEyBuAX586eDuc",
          "name": "ExecuteCellRange",
          "input": {
            "start_cell": 25,
            "end_cell": 25,
            "duration": 60
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_017HJxWMRWGEyBuAX586eDuc",
          "content": "Executed cells 25-25\n==================================================\n\nCell 25:\n```\n# Save the simplified submission\nsubmission_path = '/home/submission/submission.csv'\n\nwith open(submission_path, 'w') as f:\n    for pid, term, score in submission_rows:\n        f.write(f\"{pid}\\t{term}\\t{score:.6f}\\n\")\n\nprint(f\"Submission saved to {submission_path}\")\nprint(f\"File size: {os.path.getsize(submission_path) / 1e6:.2f} MB\")\nprint(f\"Total rows: {len(submission_rows)}\")\n\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"BASELINE MODEL SUMMARY\")\nprint(\"=\"*50)\nprint(f\"Features: Hand-crafted (AA compos\n```\nOutput:\nSubmission saved to /home/submission/submission.csv\nFile size: 94.97 MB\nTotal rows: 3506936\n\n==================================================\nBASELINE MODEL SUMMARY\n==================================================\nFeatures: Hand-crafted (AA composition + sequence properties)\nFeature dimension: 25\nModel: MLP with hidden dims [512, 256]\nNumber of GO terms predicted: 1500\nBest validation F1: 0.1065\nSubmission rows: 3506936\n==================================================\n",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "text",
          "text": "REASONING: The baseline is complete. Let me close the notebook and log the experiment. The validation F1 is 0.1065 which is a starting point. This baseline uses only hand-crafted features (25 dimensions) without protein embeddings. The next step would be to add ESM2 or ProtBERT embeddings for better performance."
        },
        {
          "type": "tool_use",
          "id": "toolu_011nfaKV9m6MS1MdHWS2qU3e",
          "name": "CloseNotebook",
          "input": {}
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_011nfaKV9m6MS1MdHWS2qU3e",
          "content": "Closed: /home/code/experiments/001_baseline/baseline.ipynb",
          "is_error": false
        }
      ]
    },
    {
      "role": "assistant",
      "content": [
        {
          "type": "tool_use",
          "id": "toolu_014QoTaTFC7roMr5B6rjXZa1",
          "name": "LogExperiment",
          "input": {
            "name": "001_baseline_handcrafted_features",
            "model_type": "MLP",
            "score": 0.1065,
            "notes": "Baseline model using hand-crafted features only (25 dimensions: AA composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity). MLP with hidden dims [512, 256], dropout 0.3. Trained for 15 epochs with BCEWithLogitsLoss and CosineAnnealingLR. Top 500 GO terms per aspect (1500 total). Simple submission without GO hierarchy propagation (too slow). Temperature scaling T=1.5 applied. Validation F1=0.1065 (micro F1 at threshold 0.5). This is a weak baseline - need protein embeddings (ESM2/ProtBERT) for competitive performance.",
            "experiment_folder": "/home/code/experiments/001_baseline",
            "submission_path": "/home/submission/submission.csv"
          }
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_014QoTaTFC7roMr5B6rjXZa1",
          "content": "Transitioning to next phase",
          "is_error": false
        }
      ]
    }
  ],
  "experiments": [
    {
      "id": "exp_000",
      "name": "001_baseline_handcrafted_features",
      "model_type": "MLP",
      "score": 0.1065,
      "notes": "Baseline model using hand-crafted features only (25 dimensions: AA composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity). MLP with hidden dims [512, 256], dropout 0.3. Trained for 15 epochs with BCEWithLogitsLoss and CosineAnnealingLR. Top 500 GO terms per aspect (1500 total). Simple submission without GO hierarchy propagation (too slow). Temperature scaling T=1.5 applied. Validation F1=0.1065 (micro F1 at threshold 0.5). This is a weak baseline - need protein embeddings (ESM2/ProtBERT) for competitive performance.",
      "experiment_folder": "/home/code/experiments/001_baseline",
      "timestamp": "2026-01-13T11:52:18.733747"
    }
  ],
  "submissions": [],
  "candidates": [
    {
      "file_path": "/home/code/submission_candidates/candidate_000.csv",
      "score": 0.1065,
      "model_name": "001_baseline_handcrafted_features",
      "experiment_id": "exp_000",
      "timestamp": "2026-01-13T11:52:18.781656"
    }
  ],
  "findings": [],
  "metric_lower_is_better": false,
  "strategy_history": [
    {
      "strategy": "# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence length statistics\n- Training: 82,404 proteins, 26,125 unique GO terms, 537K protein-term pairs\n- Test: 224,309 proteins to predict\n- Three aspects: Biological Process (P), Cellular Component (C), Molecular Function (F)\n- GO term frequency is highly skewed: only 33 terms in >1000 proteins, median frequency is 4\n\n## Evaluation Metric\n- Weighted F1-measure using Information Accretion (IA) weights\n- Evaluated separately for MF, BP, CC subontologies, then averaged\n- IA weights provided in IA.tsv - deeper/rarer GO terms have higher weights (range 0-16)\n- Must predict GO terms with confidence scores (0-1)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM2 (Recommended)**: 1280-dimensional embeddings\n   - State-of-the-art protein language model from Meta\n   - Best performance for protein function prediction\n   - Use mean pooling over sequence positions\n   - Models: esm2_t33_650M_UR50D or esm2_t36_3B_UR50D\n\n2. **ProtBERT**: 1024-dimensional embeddings\n   - Good alternative, well-tested on previous competitions\n   - Slightly faster than ESM2\n\n3. **T5 embeddings**: 1024-dimensional\n   - Another viable option\n\n## Model Architecture\n**Multi-Layer Perceptron on top of embeddings:**\n```\nInput (embeddings + features) -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> BatchNorm -> ReLU -> Dropout\n                              -> Linear -> Output (num_classes)\n```\n\n**Key hyperparameters from top solutions:**\n- Hidden dimensions: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.3\n- Batch size: 64-128\n- Learning rate: 1e-3 to 1e-2\n- Epochs: 10-15\n- Use BCEWithLogitsLoss for multi-label classification\n- CosineAnnealingLR scheduler works well\n\n## Feature Engineering\n**Hand-crafted features to combine with embeddings:**\n- Amino acid composition (20 features)\n- Sequence length\n- Molecular weight\n- Isoelectric point\n- Aromaticity\n- Instability index\n- Gravy (hydropathicity)\n\n## Training Strategy\n1. **Train separate models per aspect (MF, BP, CC)**\n   - Each aspect has different GO term distributions\n   - Use aspect-specific thresholds for predictions\n\n2. **Multi-label classification setup:**\n   - Use top N most frequent GO terms (500-1500)\n   - MultiLabelBinarizer for encoding (use sparse_output=True)\n   - BCEWithLogitsLoss or BCELoss\n\n3. **Validation:**\n   - 90/10 train/val split\n   - Monitor MultilabelF1Score\n\n## Ensembling (IMPORTANT)\n**Ensemble multiple approaches for best results:**\n\n1. **Model ensemble:**\n   - Train 3+ MLPs with different hyperparameters\n   - Average predictions from all models\n   - Different batch sizes, hidden dims, dropout rates\n\n2. **Embedding ensemble:**\n   - Combine predictions from ESM2-based and ProtBERT-based models\n\n3. **BLAST/Sequence similarity predictions:**\n   - Use sequence similarity to transfer annotations from known proteins\n   - Weight: DL_WEIGHT=0.6, BLAST_WEIGHT=0.4 for combining\n   - Diamond BLAST is faster than standard BLAST\n\n## Post-Processing (CRITICAL FOR SCORE)\n**GO Hierarchy Propagation:**\n- GO is a hierarchical ontology - parent terms must have scores >= child terms\n- Propagate predictions upward: if predicting GO:X, also predict all ancestors\n- Force root terms (GO:0003674, GO:0008150, GO:0005575) to confidence 1.0\n- Parse go-basic.obo for is_a and part_of relationships\n\n**Rank Normalization:**\n- If max prediction score is weak (< 0.95), scale up all scores\n- Target: make best non-root prediction at least 0.95\n- scale_factor = 0.95 / max_val\n\n**GOA Corrections (if available):**\n- Apply known positive annotations with confidence 1.0\n- Remove predictions conflicting with 'NOT' annotations\n- Propagate negative annotations to descendants\n\n## Temperature Scaling\n- Apply temperature scaling to logits before sigmoid: `sigmoid(logits / T)`\n- T=1.5 often improves calibration\n- Helps with confidence score distribution\n\n## Prediction Thresholds\n- Use aspect-specific thresholds:\n  - MF: 0.1-0.2\n  - BP: 0.1-0.2  \n  - CC: 0.1-0.2\n- Ensure minimum predictions per protein (e.g., MIN_PREDS=5)\n- Maximum predictions per protein: 1000-1500\n\n## Advanced Techniques (from literature)\n1. **GO-aware architectures:**\n   - Graph attention networks (GATs) to model GO hierarchy\n   - Multimodal transformers for sequence + GO term embeddings\n   - Contrastive learning for GO term relationships\n\n2. **Semantic entailment:**\n   - DeepGO-SE approach: predict truth values for GO statements\n   - Exploit GO axioms and hierarchical constraints\n\n3. **Zero-shot prediction:**\n   - Learn GO term embeddings from textual definitions\n   - Enables prediction of unseen GO terms\n\n## Key Implementation Notes\n1. Parse GO ontology from go-basic.obo using obonet library\n2. Build ancestor map for efficient propagation\n3. Handle variable-length sequences with mean pooling\n4. Use sparse matrices for large label spaces\n5. Memory management: process in batches, use gc.collect()\n6. Gradient clipping (max_norm=1.0) for stable training\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence\n- No header\n- Confidence scores between 0 and 1\n- Sort by protein_id, then by confidence (descending)\n- Drop extremely low scores (< 0.001) to reduce file size\n\n## Reference Kernels\n- `../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/` - Starter with EDA and model\n- `../research/kernels/nihilisticneuralnet_protbert-ensemble/` - ProtBERT ensemble approach\n- `../research/kernels/khoatran512_ktdk-int3405e2-final/` - ESM2 + features + GOA corrections\n- `../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/` - GO propagation post-processing\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T10:20:26.464389"
    }
  ],
  "feedback_history": [
    {
      "feedback": "## What I Understood\n\nThe junior researcher implemented a baseline model for protein function prediction using hand-crafted features (amino acid composition, sequence length, molecular weight, hydropathicity, net charge, aromaticity - 25 dimensions total) with an MLP classifier. The goal was to establish a working pipeline and baseline score before moving to more sophisticated approaches like protein language model embeddings (ESM2/ProtBERT). The validation F1 of 0.1065 was achieved, and a submission candidate was generated.\n\n## Technical Execution Assessment\n\n**Validation**: The validation methodology is sound - a 90/10 train/val split with fixed random_state=42 for reproducibility. However, this is a single split, not cross-validation, so we have no variance estimate. The validation F1 of 0.1065 is reported, but this is micro-F1 at threshold 0.5, which is NOT the competition metric (weighted F1 with Information Accretion weights). This makes the validation score difficult to interpret relative to the target.\n\n**Leakage Risk**: None detected. The MultiLabelBinarizer is fit only on training data. Features are computed per-sequence without any global statistics that could leak. The train/val split is done correctly before any model training.\n\n**Score Integrity**: The validation F1 of 0.1065 is verified in the training logs (Epoch 7 achieved best). The submission file was generated correctly with 3.5M rows covering 224K test proteins. Root GO terms are correctly set to confidence 1.0.\n\n**Code Quality**: The code executed successfully. Seeds are set (random_state=42). The notebook is well-structured. One concern: GO hierarchy propagation was attempted but then abandoned due to being \"too slow\" - the final submission uses direct predictions only without propagation, which is known to be critical for this competition.\n\nVerdict: **TRUSTWORTHY** (but with important caveats about metric mismatch)\n\n## Strategic Assessment\n\n**Approach Fit**: This is a reasonable baseline approach, but the researcher correctly identified in their notes that \"need protein embeddings (ESM2/ProtBERT) for competitive performance.\" The 25-dimensional hand-crafted features are extremely limited compared to the 1280-dimensional ESM2 embeddings that top solutions use. The gap between 0.1065 validation F1 and the 0.441 target is enormous (4x improvement needed).\n\n**Effort Allocation**: The baseline was a necessary first step to establish a working pipeline. However, the effort allocation going forward is critical:\n1. **Protein embeddings are the bottleneck** - The data findings clearly show ESM2/ProtBERT embeddings are essential. This should be the immediate next priority.\n2. **GO hierarchy propagation was skipped** - This is documented as \"critical for score\" in the strategy notes but was abandoned. This is a significant gap.\n3. **Separate models per aspect** - Top solutions train separate models for MF, BP, CC. The baseline trains a single model.\n\n**Assumptions**: \n- The baseline assumes hand-crafted features can capture protein function - this is known to be insufficient.\n- Using micro-F1 at threshold 0.5 as validation metric doesn't match the competition's weighted F1 with IA weights.\n- The threshold of 0.1 for predictions may not be optimal.\n\n**Blind Spots**:\n1. **No protein embeddings** - This is the single biggest gap. The reference kernels all use ESM2 (1280-dim) or ProtBERT (1024-dim).\n2. **No GO hierarchy propagation** - Parent terms must have scores >= child terms. This was attempted but abandoned.\n3. **No GOA corrections** - Known positive annotations should be applied with confidence 1.0.\n4. **No ensemble** - Top solutions use 3+ models with different hyperparameters.\n5. **Single model for all aspects** - Should train separate models per aspect (MF, BP, CC).\n\n**Trajectory**: This is experiment 0 - a baseline. The trajectory is appropriate for establishing a working pipeline. However, the gap to target (0.1065 \u2192 0.441) is substantial. The researcher correctly identified the path forward (protein embeddings), but execution is critical.\n\n## What's Working\n\n1. **Clean, reproducible code** - The notebook is well-structured with clear sections.\n2. **Correct data handling** - FASTA parsing, GO term filtering, train/val split all done correctly.\n3. **Working submission pipeline** - Generated a valid submission file with correct format.\n4. **Good documentation** - The researcher clearly documented limitations and next steps.\n5. **Appropriate model architecture** - MLP with BatchNorm, ReLU, Dropout is the right choice for this task.\n\n## Key Concerns\n\n1. **Observation**: No protein language model embeddings used.\n   **Why it matters**: ESM2/ProtBERT embeddings are the foundation of all competitive solutions. Hand-crafted features (25-dim) vs embeddings (1280-dim) is a 50x difference in representation power. This alone likely accounts for most of the gap to target.\n   **Suggestion**: Prioritize generating ESM2 embeddings for train and test proteins. Pre-computed embeddings may be available in Kaggle datasets.\n\n2. **Observation**: GO hierarchy propagation was abandoned.\n   **Why it matters**: The competition metric uses IA weights that favor deeper GO terms. Propagation ensures parent terms have scores >= child terms, which is required by GO semantics. The strategy notes explicitly call this \"CRITICAL FOR SCORE.\"\n   **Suggestion**: Implement efficient propagation using pre-computed ancestor maps. Process in batches if memory is an issue.\n\n3. **Observation**: Validation metric doesn't match competition metric.\n   **Why it matters**: Micro-F1 at threshold 0.5 is not the same as weighted F1 with IA weights. This makes it hard to estimate how the model will perform on the leaderboard.\n   **Suggestion**: Implement the competition metric for local validation, or at least use IA-weighted F1.\n\n4. **Observation**: Single model for all aspects instead of per-aspect models.\n   **Why it matters**: MF, BP, and CC have different GO term distributions and characteristics. Top solutions train separate models and use aspect-specific thresholds.\n   **Suggestion**: Train separate models for each aspect in future experiments.\n\n## Top Priority for Next Experiment\n\n**Implement protein language model embeddings (ESM2 or ProtBERT).** This is the single highest-leverage change that will close the gap to the target. The reference kernels show that ESM2 embeddings (1280-dim) combined with hand-crafted features, trained with an MLP, can achieve competitive scores. Check if pre-computed embeddings are available in Kaggle datasets to save computation time. If not, use the esm2_t33_650M_UR50D model with mean pooling over sequence positions.\n\nSecondary priorities (in order):\n1. Implement GO hierarchy propagation\n2. Train separate models per aspect\n3. Add ensemble of 3 MLPs with different hyperparameters\n4. Apply GOA corrections for known annotations\n\nThe baseline has established a working pipeline - now it's time to add the components that actually drive performance.\n",
      "experiment_id": "exp_000",
      "agent": "evaluator",
      "timestamp": "2026-01-13T11:54:33.452138"
    }
  ],
  "data_findings": [
    {
      "finding": "ProtBERT embeddings (1024-dim) are effective for protein function prediction. MLP with hidden layers (864, 712) on top of embeddings works well.",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble approach: combine deep learning predictions with BLAST-based predictions using weighted averaging (DL_WEIGHT=0.6, BLAST_WEIGHT=0.4)",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Temperature scaling (T=1.5) applied to logits before sigmoid improves calibration. Use top 500 most frequent GO terms as prediction targets.",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO hierarchy propagation is critical: ensure parent terms have scores >= child terms. Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0. Apply rank normalization to boost weak predictions to 0.95.",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "ESM2 embeddings (1280-dim) combined with hand-crafted features (amino acid composition, molecular weight, etc.) work well. MLP with BatchNorm, ReLU, Dropout. Train separate models per aspect (MF, BP, CC).",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble 3 MLPs with different hyperparameters (batch size, hidden dims, dropout). Use CosineAnnealingLR scheduler. Apply aspect-specific thresholds for predictions.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA (Gene Ontology Annotation) corrections: Apply known positive annotations with confidence 1.0. Remove predictions that conflict with 'NOT' annotations (negative annotations) and their descendants.",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Training data: 82,404 proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Test set: 224,309 proteins. Aspect distribution: P (Biological Process) 250K, C (Cellular Component) 158K, F (Molecular Function) 128K.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO term frequency is highly skewed: only 33 terms appear in >1000 proteins, 663 terms in >100 proteins. Most common term GO:0005515 appears in 33,713 proteins. Median term frequency is only 4.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Protein sequence lengths: Train mean 526, median 409. Test mean 429. Range from 2-35,213 amino acids. Need to handle variable length sequences.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced techniques: DeepGO-SE uses semantic entailment with GO axioms. GOProFormer uses multimodal transformer for sequence + GO term embeddings. Graph attention networks (GATs) can model GO hierarchy. STAR-GO integrates GO textual definitions with ontology graph structure.",
      "source": "WebSearch: protein function prediction techniques",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best techniques for protein function prediction using deep learning and Gene Ontology?",
      "finding": "Protein function prediction relies on mapping an amino\u2011acid sequence to the structured terms of the Gene Ontology (GO), which encodes molecular functions, biological processes and cellular components.  Deep\u2011learning models are now the dominant approach because they can learn rich sequence representations while simultaneously exploiting the hierarchical constraints of GO, leading to more consistent and accurate annotations ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).\n\nThe most effective recent techniques combine large\u2011scale protein language models with GO\u2011aware architectures.  **DeepGO\u2011SE** uses a pretrained language model to generate multiple approximate GO models and a neural network that predicts truth values for GO statements, effectively performing semantic entailment and achieving state\u2011of\u2011the\u2011art performance on benchmark sets ([nature.com](https://www.nature.com/articles/s42256-024-00795-w)).  **POSA\u2011GO** fuses hierarchical GO information with protein langu",
      "agent": "explorer"
    }
  ],
  "remaining_submissions": 4,
  "max_submissions": 5,
  "last_reset_date_utc": "2026-01-13",
  "lb_scores": [],
  "bronze_threshold": 0.429,
  "bronze_submitted": false,
  "best_lb_score": null,
  "max_experiments_without_submission": 5,
  "last_submitted_experiment_idx": 0,
  "start_time": "2026-01-13T10:08:38.546583",
  "time_limit_minutes": 2100,
  "_saved_at": "2026-01-13T11:54:33.469024"
}