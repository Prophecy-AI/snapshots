# Protein Function Prediction (GO Term Prediction) - Techniques Guide

## Data Understanding
**Reference notebooks for data characteristics:**
- `exploration/eda.ipynb` - Contains EDA: 82,404 training proteins, 26,125 unique GO terms, multi-label classification with 3 aspects (MF, BP, CC)

## Problem Overview
This is a multi-label classification problem predicting Gene Ontology (GO) terms for proteins based on amino acid sequences. Key challenges:
- Hierarchical label structure (GO ontology with is_a and part_of relationships)
- Three separate subontologies: Molecular Function (MF), Biological Process (BP), Cellular Component (CC)
- Evaluation uses weighted F1-measure with Information Accretion (IA) weights
- Highly imbalanced labels (some GO terms appear in 33K proteins, others in few)

## Protein Embeddings (CRITICAL)
Pre-trained protein language models are essential for this task:

1. **ESM-2** (Recommended): Facebook's protein language model
   - esm2_t6_8M_UR50D: 320-dim embeddings (fast, smaller)
   - esm2_t33_650M_UR50D: 1280-dim embeddings (best performance)
   - Use mean pooling over sequence positions

2. **ProtBERT**: 1024-dim embeddings, good alternative

3. **T5 (ProtT5)**: 1024-dim embeddings, competitive performance

4. **Hybrid Features**: Combine embeddings with handcrafted features:
   - Amino Acid Composition (AAC)
   - Dipeptide Composition (DPC)
   - Conjoint Triad Descriptor (CTD)
   - Quasi-Sequence Order (QSO)
   - Pseudo Amino Acid Composition (PAAC)

## Model Architecture
Multi-layer perceptron (MLP) works well for this task:

```
Input (embedding_dim + feature_dim) 
-> Linear(hidden1) -> BatchNorm -> ReLU -> Dropout
-> Linear(hidden2) -> BatchNorm -> ReLU -> Dropout
-> Linear(num_classes)
```

**Recommended configurations:**
- Hidden dims: [1024, 512] or [864, 712]
- Dropout: 0.2-0.4
- Batch size: 64-256
- Learning rate: 1e-3 to 1e-2 with cosine annealing

**Alternative architectures:**
- 1D CNN (ProtCNN): Dilated convolutions for sequence modeling
- BiLSTM-Attention: Bidirectional LSTM with multi-head self-attention
- Graph Neural Networks: Model GO term dependencies

## Training Strategy

1. **Train Separate Models Per Aspect**: Train 3 separate models for MF, BP, CC
   - Each aspect has different label distributions
   - Use aspect-specific thresholds (F=0.15, P=0.10, C=0.12)

2. **Ensemble Multiple Models**: Train 3+ models with different hyperparameters
   - Average predictions from ensemble
   - Suggested weights: ESM-2=0.35, ProtCNN=0.25, ProtBERT=0.25, BiLSTM=0.15
   - Improves robustness

3. **Loss Function**: BCEWithLogitsLoss for multi-label classification

4. **Validation**: Use 90/10 train/val split, monitor F1 score

5. **Label Selection**: Focus on top 500-1500 most frequent GO terms per aspect
   - Rare terms are hard to predict and have low impact on score

## GO Hierarchy Propagation (CRITICAL)
The GO ontology is hierarchical - predictions must be consistent:

1. **Parse GO Ontology**: Use obonet or custom parser to get parent-child relationships
   - Include both `is_a` and `part_of` relationships
   - Root terms: GO:0003674 (MF), GO:0008150 (BP), GO:0005575 (CC)

2. **Positive Propagation**: After prediction, propagate scores upward
   - Parent score = max(parent_score, child_score)
   - Ensures hierarchical consistency

3. **Force Root Terms**: Set root term scores to 1.0 for all proteins

4. **Rank Normalization**: Scale max non-root prediction to 0.95
   - Helps predictions survive thresholding

5. **Negative Propagation**: If a term is negative, all descendants should be negative

## External Data Integration (GOA)
Gene Ontology Annotation (GOA) database provides known annotations:

1. **Positive Annotations**: Apply known GO terms with confidence 1.0
2. **Negative Annotations**: Remove predictions that contradict known negatives
   - Also remove descendants of negative terms (propagate negatives down)

## Sequence Similarity Methods (BLAST/Diamond)
Homology-based predictions are strong baselines:

1. **BLAST/Diamond**: Find similar proteins in training set
2. **Transfer Annotations**: Copy GO terms from similar proteins
3. **Foldseek**: Structure-based similarity (if structures available)
4. **Ensemble with ML**: Combine BLAST predictions with neural network predictions
   - Weighted averaging: DL_weight * DL_pred + BLAST_weight * BLAST_pred

## Advanced Techniques (State-of-the-Art)

1. **GO Term Embeddings**: Learn embeddings for GO terms from ontology structure
   - Use graph neural networks on GO DAG
   - Capture semantic relationships between terms

2. **Multi-Modal Transformers**: Joint attention over protein and GO embeddings
   - GOProFormer, POSA-GO approaches
   - Model protein-function associations dynamically

3. **Hierarchical Loss Functions**: Weight loss by term depth/specificity
   - Deeper terms are harder but more informative
   - Use IA weights in loss function

4. **Zero-Shot Prediction**: Predict functions for unseen GO terms
   - Use GO term text definitions as features
   - STAR-GO approach for ontology-informed semantic embeddings

## Post-Processing

1. **Confidence Thresholding**: Use per-aspect thresholds
   - MF: 0.15, BP: 0.10, CC: 0.12 (tune on validation)
2. **Minimum Predictions**: Ensure at least 5-10 predictions per protein
3. **Temperature Scaling**: Apply temperature to logits before sigmoid
4. **Max Predictions Per Protein**: Limit to top 1000-1500 predictions

## Submission Format
- TSV format: protein_id, GO_term, confidence
- Confidence in range (0, 1]
- Sort by protein_id, then by confidence descending

## Key Techniques Summary (Priority Order)
1. **Use ESM-2 embeddings** (1280-dim from esm2_t33_650M_UR50D)
2. **Train separate MLP models per aspect** (MF, BP, CC)
3. **Apply GO hierarchy propagation** (parent >= child, force roots to 1.0)
4. **Ensemble 3+ models** with different hyperparameters
5. **Integrate GOA annotations** (positive at 1.0, remove negatives)
6. **Ensemble with BLAST/Diamond predictions**
7. **Use aspect-specific thresholds** for final predictions
8. **Add handcrafted features** (AAC, DPC, CTD) to embeddings
