{
  "workspace_dir": "/home/code",
  "competition_id": "cafa-6-protein-function-prediction",
  "metric_direction": false,
  "start_time": "2026-01-13T20:50:32.771242",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Protein Function Prediction (GO Term Prediction) - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains EDA: 82,404 training proteins, 26,125 unique GO terms, multi-label classification with 3 aspects (MF, BP, CC)\n\n## Problem Overview\nThis is a multi-label classification problem predicting Gene Ontology (GO) terms for proteins based on amino acid sequences. Key challenges:\n- Hierarchical label structure (GO ontology with is_a and part_of relationships)\n- Three separate subontologies: Molecular Function (MF), Biological Process (BP), Cellular Component (CC)\n- Evaluation uses weighted F1-measure with Information Accretion (IA) weights\n- Highly imbalanced labels (some GO terms appear in 33K proteins, others in few)\n\n## Protein Embeddings (CRITICAL)\nPre-trained protein language models are essential for this task:\n\n1. **ESM-2** (Recommended): Facebook's protein language model\n   - esm2_t6_8M_UR50D: 320-dim embeddings (fast, smaller)\n   - esm2_t33_650M_UR50D: 1280-dim embeddings (best performance)\n   - Use mean pooling over sequence positions\n\n2. **ProtBERT**: 1024-dim embeddings, good alternative\n\n3. **T5 (ProtT5)**: 1024-dim embeddings, competitive performance\n\n4. **Hybrid Features**: Combine embeddings with handcrafted features:\n   - Amino Acid Composition (AAC)\n   - Dipeptide Composition (DPC)\n   - Conjoint Triad Descriptor (CTD)\n   - Quasi-Sequence Order (QSO)\n   - Pseudo Amino Acid Composition (PAAC)\n\n## Model Architecture\nMulti-layer perceptron (MLP) works well for this task:\n\n```\nInput (embedding_dim + feature_dim) \n-> Linear(hidden1) -> BatchNorm -> ReLU -> Dropout\n-> Linear(hidden2) -> BatchNorm -> ReLU -> Dropout\n-> Linear(num_classes)\n```\n\n**Recommended configurations:**\n- Hidden dims: [1024, 512] or [864, 712]\n- Dropout: 0.2-0.4\n- Batch size: 64-256\n- Learning rate: 1e-3 to 1e-2 with cosine annealing\n\n**Alternative architectures:**\n- 1D CNN (ProtCNN): Dilated convolutions for sequence modeling\n- BiLSTM-Attention: Bidirectional LSTM with multi-head self-attention\n- Graph Neural Networks: Model GO term dependencies\n\n## Training Strategy\n\n1. **Train Separate Models Per Aspect**: Train 3 separate models for MF, BP, CC\n   - Each aspect has different label distributions\n   - Use aspect-specific thresholds (F=0.15, P=0.10, C=0.12)\n\n2. **Ensemble Multiple Models**: Train 3+ models with different hyperparameters\n   - Average predictions from ensemble\n   - Suggested weights: ESM-2=0.35, ProtCNN=0.25, ProtBERT=0.25, BiLSTM=0.15\n   - Improves robustness\n\n3. **Loss Function**: BCEWithLogitsLoss for multi-label classification\n\n4. **Validation**: Use 90/10 train/val split, monitor F1 score\n\n5. **Label Selection**: Focus on top 500-1500 most frequent GO terms per aspect\n   - Rare terms are hard to predict and have low impact on score\n\n## GO Hierarchy Propagation (CRITICAL)\nThe GO ontology is hierarchical - predictions must be consistent:\n\n1. **Parse GO Ontology**: Use obonet or custom parser to get parent-child relationships\n   - Include both `is_a` and `part_of` relationships\n   - Root terms: GO:0003674 (MF), GO:0008150 (BP), GO:0005575 (CC)\n\n2. **Positive Propagation**: After prediction, propagate scores upward\n   - Parent score = max(parent_score, child_score)\n   - Ensures hierarchical consistency\n\n3. **Force Root Terms**: Set root term scores to 1.0 for all proteins\n\n4. **Rank Normalization**: Scale max non-root prediction to 0.95\n   - Helps predictions survive thresholding\n\n5. **Negative Propagation**: If a term is negative, all descendants should be negative\n\n## External Data Integration (GOA)\nGene Ontology Annotation (GOA) database provides known annotations:\n\n1. **Positive Annotations**: Apply known GO terms with confidence 1.0\n2. **Negative Annotations**: Remove predictions that contradict known negatives\n   - Also remove descendants of negative terms (propagate negatives down)\n\n## Sequence Similarity Methods (BLAST/Diamond)\nHomology-based predictions are strong baselines:\n\n1. **BLAST/Diamond**: Find similar proteins in training set\n2. **Transfer Annotations**: Copy GO terms from similar proteins\n3. **Foldseek**: Structure-based similarity (if structures available)\n4. **Ensemble with ML**: Combine BLAST predictions with neural network predictions\n   - Weighted averaging: DL_weight * DL_pred + BLAST_weight * BLAST_pred\n\n## Advanced Techniques (State-of-the-Art)\n\n1. **GO Term Embeddings**: Learn embeddings for GO terms from ontology structure\n   - Use graph neural networks on GO DAG\n   - Capture semantic relationships between terms\n\n2. **Multi-Modal Transformers**: Joint attention over protein and GO embeddings\n   - GOProFormer, POSA-GO approaches\n   - Model protein-function associations dynamically\n\n3. **Hierarchical Loss Functions**: Weight loss by term depth/specificity\n   - Deeper terms are harder but more informative\n   - Use IA weights in loss function\n\n4. **Zero-Shot Prediction**: Predict functions for unseen GO terms\n   - Use GO term text definitions as features\n   - STAR-GO approach for ontology-informed semantic embeddings\n\n## Post-Processing\n\n1. **Confidence Thresholding**: Use per-aspect thresholds\n   - MF: 0.15, BP: 0.10, CC: 0.12 (tune on validation)\n2. **Minimum Predictions**: Ensure at least 5-10 predictions per protein\n3. **Temperature Scaling**: Apply temperature to logits before sigmoid\n4. **Max Predictions Per Protein**: Limit to top 1000-1500 predictions\n\n## Submission Format\n- TSV format: protein_id, GO_term, confidence\n- Confidence in range (0, 1]\n- Sort by protein_id, then by confidence descending\n\n## Key Techniques Summary (Priority Order)\n1. **Use ESM-2 embeddings** (1280-dim from esm2_t33_650M_UR50D)\n2. **Train separate MLP models per aspect** (MF, BP, CC)\n3. **Apply GO hierarchy propagation** (parent >= child, force roots to 1.0)\n4. **Ensemble 3+ models** with different hyperparameters\n5. **Integrate GOA annotations** (positive at 1.0, remove negatives)\n6. **Ensemble with BLAST/Diamond predictions**\n7. **Use aspect-specific thresholds** for final predictions\n8. **Add handcrafted features** (AAC, DPC, CTD) to embeddings\n",
      "agent": "explorer",
      "timestamp": "2026-01-13T20:56:00.890537"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Starter approach uses: 1) GO ontology hierarchy with obonet, 2) IA (Information Accretion) weights for evaluation, 3) ESM-2 protein embeddings (esm2_t6_8M_UR50D), 4) 3-layer neural network with BatchNorm and dropout, 5) Temperature scaling for predictions, 6) Ensemble with BLAST predictions using weighted averaging",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "ProtBERT ensemble approach: 1) Uses multiple protein embeddings (ProtBERT 1024-dim, T5 1024-dim, ESM2 1280-dim), 2) MLP architecture (864->712->num_classes) or 1D CNN, 3) Predicts top 500 GO terms, 4) Ensembles with BLAST/Foldseek predictions by merging on (protein_id, GO_term) pairs",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GO Hierarchy Propagation technique: 1) Parse GO ontology to get parent-child relationships (is_a and part_of), 2) Build ancestor map for each term, 3) Positive propagation: ensure parent score >= child score, 4) Force root terms (GO:0003674, GO:0008150, GO:0005575) to 1.0, 5) Rank normalization: scale max non-root prediction to 0.95 to survive thresholding",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Data characteristics: 82,404 training proteins, 26,125 unique GO terms, 537,027 protein-term pairs. Aspect distribution: P (Biological Process)=250,805, C (Cellular Component)=157,770, F (Molecular Function)=128,452. Average 6.5 terms per protein (max 233). Most common GO term: GO:0005515 (protein binding) with 33,713 occurrences.",
      "source": "exploration/eda.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced KTDK approach: 1) ESM-2 embeddings (1280-dim) + handcrafted features (AAC, DPC, CTD, QSO, PAAC), 2) Ensemble of 3 MLP models with different hyperparameters, 3) Train separate models per aspect (MF, BP, CC), 4) Per-aspect thresholds (F=0.15, P=0.10, C=0.12), 5) GOA corrections: apply known positive annotations at 1.0 confidence, remove negative annotations and their descendants, 6) Positive propagation through GO hierarchy",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Additional model architectures: 1) ProtCNN - 1D CNN with dilated convolutions for sequence modeling, 2) BiLSTM-Attention - bidirectional LSTM with multi-head self-attention, 3) Ensemble weights suggestion: ESM-2=0.35, ProtCNN=0.25, ProtBERT=0.25, BiLSTM=0.15",
      "source": "../research/kernels/analyticaobscura_cafa-6-protein-function-prediction/cafa-6-protein-function-prediction.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "State-of-the-art techniques from literature: 1) POSA-GO - fuses ESM-2 embeddings with GO DAG graph representation using multi-head self-attention, 2) GOProFormer - multi-modal transformer with sequence and GO term embeddings, 3) STAR-GO - hierarchical integration of ontology-informed semantic embeddings, 4) DeepGO - uses protein-protein interaction networks with sequence features, 5) Downward random walks on GO DAG for function propagation",
      "source": "WebSearch: protein function prediction techniques",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best techniques for protein function prediction using Gene Ontology terms?",
      "finding": "Protein function prediction with Gene Ontology (GO) terms has evolved from simple homology\u2011based annotation to sophisticated deep\u2011learning frameworks that explicitly model the GO hierarchy.  Early methods relied on sequence similarity (BLAST), domain/family signatures, or protein\u2011protein interaction (PPI) networks, often combined with machine\u2011learning classifiers such as SVMs or random forests\u202f([DeepGO](https://pmc.ncbi.nlm.nih.gov/articles/PMC5860606))\u200b.  More recent approaches treat GO prediction as a multi\u2011label, hierarchical classification problem: they embed proteins (e.g., using convolutional or recurrent networks on raw sequences) and jointly learn the dependencies among GO terms, which improves both precision and recall for rare functions\u202f([Kulmanov\u202fet\u202fal., 2017](https://arxiv.org/abs/1705.05919))\u200b.\n\nState\u2011of\u2011the\u2011art techniques integrate protein language models with hierarchical GO information.  POSA\u2011GO fuses embeddings from large\u2011scale protein language models (e.g., ESM, ProtB",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 4
}