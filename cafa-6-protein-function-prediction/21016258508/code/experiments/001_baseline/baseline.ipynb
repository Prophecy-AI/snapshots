{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bad80ff",
   "metadata": {},
   "source": [
    "# Baseline: ESM2 Embeddings + MLP Classifier\n",
    "\n",
    "This notebook implements:\n",
    "1. ESM2 embedding generation for train/test proteins\n",
    "2. MLP classifier per aspect (MF, BP, CC)\n",
    "3. GO term propagation\n",
    "4. Submission generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f970c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('/home/data')\n",
    "TRAIN_DIR = DATA_DIR / 'Train'\n",
    "TEST_DIR = DATA_DIR / 'Test'\n",
    "OUTPUT_DIR = Path('/home/code/experiments/001_baseline')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "def parse_fasta(fasta_path):\n",
    "    \"\"\"Parse FASTA file and return dict of protein_id -> sequence\"\"\"\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with open(fasta_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id is not None:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                # Extract protein ID (first part after >)\n",
    "                current_id = line[1:].split()[0]\n",
    "                # Handle sp|XXX|YYY format\n",
    "                if '|' in current_id:\n",
    "                    parts = current_id.split('|')\n",
    "                    current_id = parts[1] if len(parts) > 1 else parts[0]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        if current_id is not None:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "print('Loading train sequences...')\n",
    "train_sequences = parse_fasta(TRAIN_DIR / 'train_sequences.fasta')\n",
    "print(f'Loaded {len(train_sequences)} train sequences')\n",
    "\n",
    "print('Loading test sequences...')\n",
    "test_sequences = parse_fasta(TEST_DIR / 'testsuperset.fasta')\n",
    "print(f'Loaded {len(test_sequences)} test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3210ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "train_terms = pd.read_csv(TRAIN_DIR / 'train_terms.tsv', sep='\\t')\n",
    "print(f'Train terms shape: {train_terms.shape}')\n",
    "print(f'Unique proteins: {train_terms[\"EntryID\"].nunique()}')\n",
    "print(f'Unique GO terms: {train_terms[\"term\"].nunique()}')\n",
    "print(f'\\nAspect distribution:')\n",
    "print(train_terms['aspect'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IA weights\n",
    "ia_df = pd.read_csv(DATA_DIR / 'IA.tsv', sep='\\t', header=None, names=['term', 'ia_weight'])\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['ia_weight']))\n",
    "print(f'Loaded {len(ia_weights)} IA weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef051f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse GO ontology for propagation\n",
    "def parse_go_obo(obo_path):\n",
    "    \"\"\"Parse GO OBO file to get parent-child relationships\"\"\"\n",
    "    go_parents = defaultdict(set)  # child -> set of parents\n",
    "    go_aspect = {}  # term -> aspect (P, F, C)\n",
    "    \n",
    "    current_term = None\n",
    "    current_namespace = None\n",
    "    is_obsolete = False\n",
    "    \n",
    "    with open(obo_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                current_term = None\n",
    "                current_namespace = None\n",
    "                is_obsolete = False\n",
    "            elif line.startswith('id: GO:'):\n",
    "                current_term = line.split('id: ')[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                ns = line.split('namespace: ')[1]\n",
    "                if ns == 'biological_process':\n",
    "                    current_namespace = 'P'\n",
    "                elif ns == 'molecular_function':\n",
    "                    current_namespace = 'F'\n",
    "                elif ns == 'cellular_component':\n",
    "                    current_namespace = 'C'\n",
    "            elif line.startswith('is_obsolete: true'):\n",
    "                is_obsolete = True\n",
    "            elif line.startswith('is_a: GO:') and current_term and not is_obsolete:\n",
    "                parent = line.split('is_a: ')[1].split(' !')[0]\n",
    "                go_parents[current_term].add(parent)\n",
    "            elif line.startswith('relationship: part_of GO:') and current_term and not is_obsolete:\n",
    "                parent = line.split('part_of ')[1].split(' !')[0]\n",
    "                go_parents[current_term].add(parent)\n",
    "            \n",
    "            if current_term and current_namespace and not is_obsolete:\n",
    "                go_aspect[current_term] = current_namespace\n",
    "    \n",
    "    return go_parents, go_aspect\n",
    "\n",
    "print('Parsing GO ontology...')\n",
    "go_parents, go_aspect = parse_go_obo(TRAIN_DIR / 'go-basic.obo')\n",
    "print(f'Loaded {len(go_parents)} terms with parents')\n",
    "print(f'Loaded {len(go_aspect)} terms with aspects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all ancestors for a term\n",
    "def get_all_ancestors(term, go_parents):\n",
    "    \"\"\"Get all ancestors of a GO term\"\"\"\n",
    "    ancestors = set()\n",
    "    to_visit = list(go_parents.get(term, set()))\n",
    "    \n",
    "    while to_visit:\n",
    "        parent = to_visit.pop()\n",
    "        if parent not in ancestors:\n",
    "            ancestors.add(parent)\n",
    "            to_visit.extend(go_parents.get(parent, set()))\n",
    "    \n",
    "    return ancestors\n",
    "\n",
    "# Root terms\n",
    "ROOT_TERMS = {\n",
    "    'GO:0003674': 'F',  # molecular_function\n",
    "    'GO:0008150': 'P',  # biological_process  \n",
    "    'GO:0005575': 'C'   # cellular_component\n",
    "}\n",
    "\n",
    "print('Root terms:', ROOT_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11df96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESM2 model for embeddings\n",
    "print('Loading ESM2 model...')\n",
    "import esm\n",
    "\n",
    "# Use esm2_t33_650M_UR50D for good balance of speed and quality\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "model = model.eval().cuda()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "print('ESM2 model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings in batches\n",
    "def generate_embeddings(sequences_dict, model, batch_converter, batch_size=8, max_len=1022):\n",
    "    \"\"\"Generate ESM2 embeddings for sequences\"\"\"\n",
    "    embeddings = {}\n",
    "    protein_ids = list(sequences_dict.keys())\n",
    "    \n",
    "    # Sort by length for efficient batching\n",
    "    protein_ids_sorted = sorted(protein_ids, key=lambda x: len(sequences_dict[x]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(protein_ids_sorted), batch_size)):\n",
    "            batch_ids = protein_ids_sorted[i:i+batch_size]\n",
    "            batch_data = [(pid, sequences_dict[pid][:max_len]) for pid in batch_ids]\n",
    "            \n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
    "            batch_tokens = batch_tokens.cuda()\n",
    "            \n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_representations = results['representations'][33]\n",
    "            \n",
    "            # Mean pooling over sequence length (excluding BOS and EOS tokens)\n",
    "            for j, pid in enumerate(batch_ids):\n",
    "                seq_len = len(sequences_dict[pid][:max_len])\n",
    "                emb = token_representations[j, 1:seq_len+1].mean(dim=0).cpu().numpy()\n",
    "                embeddings[pid] = emb\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate train embeddings\n",
    "print('Generating train embeddings...')\n",
    "train_embeddings = generate_embeddings(train_sequences, model, batch_converter, batch_size=8)\n",
    "print(f'Generated {len(train_embeddings)} train embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c261989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train embeddings\n",
    "train_emb_path = OUTPUT_DIR / 'train_embeddings.npz'\n",
    "np.savez_compressed(train_emb_path, \n",
    "                    ids=np.array(list(train_embeddings.keys())),\n",
    "                    embeddings=np.array(list(train_embeddings.values())))\n",
    "print(f'Saved train embeddings to {train_emb_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test embeddings\n",
    "print('Generating test embeddings...')\n",
    "test_embeddings = generate_embeddings(test_sequences, model, batch_converter, batch_size=8)\n",
    "print(f'Generated {len(test_embeddings)} test embeddings')\n",
    "\n",
    "# Save test embeddings\n",
    "test_emb_path = OUTPUT_DIR / 'test_embeddings.npz'\n",
    "np.savez_compressed(test_emb_path,\n",
    "                    ids=np.array(list(test_embeddings.keys())),\n",
    "                    embeddings=np.array(list(test_embeddings.values())))\n",
    "print(f'Saved test embeddings to {test_emb_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print('Freed GPU memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "# Get GO terms per aspect\n",
    "aspect_terms = {}\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    terms = train_terms[train_terms['aspect'] == aspect]['term'].unique()\n",
    "    aspect_terms[aspect] = sorted(terms)\n",
    "    print(f'Aspect {aspect}: {len(terms)} unique terms')\n",
    "\n",
    "# Create term to index mapping per aspect\n",
    "term_to_idx = {}\n",
    "idx_to_term = {}\n",
    "for aspect, terms in aspect_terms.items():\n",
    "    term_to_idx[aspect] = {term: i for i, term in enumerate(terms)}\n",
    "    idx_to_term[aspect] = {i: term for i, term in enumerate(terms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908854b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training labels matrix per aspect\n",
    "def create_label_matrix(train_terms, train_embeddings, term_to_idx, aspect):\n",
    "    \"\"\"Create label matrix for a specific aspect\"\"\"\n",
    "    protein_ids = list(train_embeddings.keys())\n",
    "    n_proteins = len(protein_ids)\n",
    "    n_terms = len(term_to_idx[aspect])\n",
    "    \n",
    "    # Create protein to index mapping\n",
    "    protein_to_idx = {pid: i for i, pid in enumerate(protein_ids)}\n",
    "    \n",
    "    # Create label matrix\n",
    "    labels = np.zeros((n_proteins, n_terms), dtype=np.float32)\n",
    "    \n",
    "    # Filter terms for this aspect\n",
    "    aspect_data = train_terms[train_terms['aspect'] == aspect]\n",
    "    \n",
    "    for _, row in aspect_data.iterrows():\n",
    "        pid = row['EntryID']\n",
    "        term = row['term']\n",
    "        if pid in protein_to_idx and term in term_to_idx[aspect]:\n",
    "            labels[protein_to_idx[pid], term_to_idx[aspect][term]] = 1.0\n",
    "    \n",
    "    return labels, protein_ids\n",
    "\n",
    "# Create label matrices\n",
    "label_matrices = {}\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    labels, protein_ids = create_label_matrix(train_terms, train_embeddings, term_to_idx, aspect)\n",
    "    label_matrices[aspect] = labels\n",
    "    print(f'Aspect {aspect}: labels shape {labels.shape}, positive rate: {labels.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18aae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "protein_ids = list(train_embeddings.keys())\n",
    "X_train = np.array([train_embeddings[pid] for pid in protein_ids])\n",
    "print(f'X_train shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print('MLP model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with 5-fold CV\n",
    "def train_model_cv(X, y, aspect, n_folds=5, epochs=10, batch_size=128, lr=0.001):\n",
    "    \"\"\"Train MLP with cross-validation\"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "    hidden_dims = [864, 712]\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    all_val_preds = np.zeros_like(y)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f'\\nFold {fold+1}/{n_folds}')\n",
    "        \n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_tr),\n",
    "            torch.FloatTensor(y_tr)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = MLPClassifier(input_dim, hidden_dims, output_dim, dropout=0.2).cuda()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f'  Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = torch.sigmoid(model(torch.FloatTensor(X_val).cuda())).cpu().numpy()\n",
    "        \n",
    "        all_val_preds[val_idx] = val_preds\n",
    "        \n",
    "        # Calculate F1 score (simplified)\n",
    "        threshold = 0.1\n",
    "        y_pred_binary = (val_preds > threshold).astype(int)\n",
    "        y_true_binary = y_val.astype(int)\n",
    "        \n",
    "        # Micro F1\n",
    "        tp = np.sum(y_pred_binary * y_true_binary)\n",
    "        fp = np.sum(y_pred_binary * (1 - y_true_binary))\n",
    "        fn = np.sum((1 - y_pred_binary) * y_true_binary)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-10)\n",
    "        recall = tp / (tp + fn + 1e-10)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        \n",
    "        fold_scores.append(f1)\n",
    "        print(f'  Fold {fold+1} F1: {f1:.4f}')\n",
    "    \n",
    "    mean_f1 = np.mean(fold_scores)\n",
    "    std_f1 = np.std(fold_scores)\n",
    "    print(f'\\nAspect {aspect} - Mean F1: {mean_f1:.4f} ± {std_f1:.4f}')\n",
    "    \n",
    "    return mean_f1, std_f1, all_val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each aspect\n",
    "results = {}\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Training model for aspect {aspect}')\n",
    "    print(f'{\"=\"*50}')\n",
    "    \n",
    "    y = label_matrices[aspect]\n",
    "    mean_f1, std_f1, val_preds = train_model_cv(X_train, y, aspect, n_folds=5, epochs=10)\n",
    "    results[aspect] = {'mean_f1': mean_f1, 'std_f1': std_f1}\n",
    "\n",
    "# Overall CV score\n",
    "overall_f1 = np.mean([results[a]['mean_f1'] for a in ['P', 'F', 'C']])\n",
    "print(f'\\n{\"=\"*50}')\n",
    "print(f'Overall CV F1: {overall_f1:.4f}')\n",
    "print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6214fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models on full data for submission\n",
    "def train_final_model(X, y, epochs=10, batch_size=128, lr=0.001):\n",
    "    \"\"\"Train final model on full data\"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "    hidden_dims = [864, 712]\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X),\n",
    "        torch.FloatTensor(y)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = MLPClassifier(input_dim, hidden_dims, output_dim, dropout=0.2).cuda()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "final_models = {}\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    print(f'\\nTraining final model for aspect {aspect}')\n",
    "    y = label_matrices[aspect]\n",
    "    final_models[aspect] = train_final_model(X_train, y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfaac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print('Generating test predictions...')\n",
    "\n",
    "# Create test embedding matrix\n",
    "test_protein_ids = list(test_embeddings.keys())\n",
    "X_test = np.array([test_embeddings[pid] for pid in test_protein_ids])\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "# Generate predictions per aspect\n",
    "test_predictions = {}\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    print(f'Predicting for aspect {aspect}...')\n",
    "    model = final_models[aspect]\n",
    "    model.eval()\n",
    "    \n",
    "    # Batch prediction\n",
    "    batch_size = 256\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_X = torch.FloatTensor(X_test[i:i+batch_size]).cuda()\n",
    "            preds = torch.sigmoid(model(batch_X)).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    test_predictions[aspect] = np.vstack(all_preds)\n",
    "    print(f'  Predictions shape: {test_predictions[aspect].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GO term propagation\n",
    "def propagate_predictions(protein_id, predictions, aspect, idx_to_term, go_parents, threshold=0.01):\n",
    "    \"\"\"Propagate predictions to parent terms\"\"\"\n",
    "    term_scores = {}\n",
    "    \n",
    "    # Get predictions above threshold\n",
    "    for idx, score in enumerate(predictions):\n",
    "        if score > threshold:\n",
    "            term = idx_to_term[aspect][idx]\n",
    "            term_scores[term] = max(term_scores.get(term, 0), score)\n",
    "            \n",
    "            # Propagate to ancestors\n",
    "            ancestors = get_all_ancestors(term, go_parents)\n",
    "            for ancestor in ancestors:\n",
    "                term_scores[ancestor] = max(term_scores.get(ancestor, 0), score)\n",
    "    \n",
    "    # Add root term with score 1.0\n",
    "    root_term = {'P': 'GO:0008150', 'F': 'GO:0003674', 'C': 'GO:0005575'}[aspect]\n",
    "    term_scores[root_term] = 1.0\n",
    "    \n",
    "    return term_scores\n",
    "\n",
    "print('Propagating predictions...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ff6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "print('Generating submission...')\n",
    "\n",
    "submission_rows = []\n",
    "thresholds = {'P': 0.05, 'F': 0.1, 'C': 0.1}\n",
    "\n",
    "for i, protein_id in enumerate(tqdm(test_protein_ids)):\n",
    "    for aspect in ['P', 'F', 'C']:\n",
    "        predictions = test_predictions[aspect][i]\n",
    "        threshold = thresholds[aspect]\n",
    "        \n",
    "        # Propagate predictions\n",
    "        term_scores = propagate_predictions(\n",
    "            protein_id, predictions, aspect, \n",
    "            idx_to_term, go_parents, threshold=threshold\n",
    "        )\n",
    "        \n",
    "        # Add to submission\n",
    "        for term, score in term_scores.items():\n",
    "            submission_rows.append((protein_id, term, score))\n",
    "\n",
    "print(f'Total submission rows: {len(submission_rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe and save\n",
    "submission_df = pd.DataFrame(submission_rows, columns=['protein_id', 'GO_term', 'confidence'])\n",
    "\n",
    "# Sort by protein_id and confidence\n",
    "submission_df = submission_df.sort_values(['protein_id', 'confidence'], ascending=[True, False])\n",
    "\n",
    "# Save submission (TSV format, no header)\n",
    "submission_path = '/home/submission/submission.csv'\n",
    "submission_df.to_csv(submission_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(f'Submission saved to {submission_path}')\n",
    "print(f'Submission shape: {submission_df.shape}')\n",
    "print(f'\\nSample submission:')\n",
    "print(submission_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print('\\n' + '='*50)\n",
    "print('FINAL RESULTS')\n",
    "print('='*50)\n",
    "for aspect in ['P', 'F', 'C']:\n",
    "    print(f\"Aspect {aspect}: F1 = {results[aspect]['mean_f1']:.4f} ± {results[aspect]['std_f1']:.4f}\")\n",
    "print(f'\\nOverall CV F1: {overall_f1:.4f}')\n",
    "print('='*50)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
