{
  "workspace_dir": "/home/code",
  "competition_id": "cafa-6-protein-function-prediction",
  "metric_direction": false,
  "start_time": "2026-01-15T01:30:19.249816",
  "time_limit_minutes": 2100,
  "experiments": [],
  "candidates": [],
  "submissions": [],
  "strategy_history": [
    {
      "strategy": "# Protein Function Prediction - Techniques Guide\n\n## Data Understanding\n**Reference notebooks for data characteristics:**\n- `exploration/eda.ipynb` - Contains full EDA: protein counts, GO term distributions, sequence lengths, aspect breakdown\n\n**Key data characteristics:**\n- Multi-label classification: Predict Gene Ontology (GO) terms for proteins based on amino acid sequences\n- Three aspects: Biological Process (P), Molecular Function (F), Cellular Component (C)\n- ~82K training proteins, ~26K unique GO terms\n- Average ~6.5 GO terms per protein (highly variable, 1-233)\n- Evaluation: Information-accretion weighted F1-measure averaged across three aspects\n\n## Feature Engineering\n\n### Protein Language Model Embeddings (PRIMARY APPROACH - HIGHEST PRIORITY)\nUse pre-trained protein language models to generate embeddings:\n- **ESM2** (1280-dim): Best performing, captures evolutionary and structural information. Use esm2_t33_650M_UR50D or larger\n- **ProtBERT** (1024-dim): Good alternative, BERT-based\n- **T5** (1024-dim): Another option\n\nPre-compute embeddings for all proteins and use as input features to downstream classifiers.\n\n### Sequence-Based Features (SUPPLEMENTARY)\nCombine embeddings with traditional sequence features for hybrid approach:\n- Amino acid composition (20 features)\n- Dipeptide composition (400 features)\n- Physicochemical properties (molecular weight, isoelectric point, hydrophobicity)\n- Sequence length features\n\n### BLAST/Homology Features\n- Run BLAST against training proteins to find similar sequences\n- Transfer GO terms from similar proteins with confidence based on sequence identity\n- Ensemble BLAST predictions with deep learning predictions\n- FoldSeek can also be used for structure-based similarity\n\n## Model Architecture\n\n### MLP Classifier (RECOMMENDED - PROVEN EFFECTIVE)\nTrain neural network on protein embeddings:\n```\nArchitecture: Input -> Linear -> BatchNorm -> ReLU -> Dropout -> Linear -> BatchNorm -> ReLU -> Dropout -> Output\nHidden dims: [864, 712] or [512, 256]\nDropout: 0.2-0.3\nOutput: Sigmoid for multi-label classification\nLoss: BCELoss or BCEWithLogitsLoss\n```\n\n### Advanced Architectures (FOR FURTHER IMPROVEMENT)\n- **DeepGO-style**: Ontology-aware classifier that models GO dependencies\n- **GOProFormer**: Multi-modal transformer with graph transformer for GO hierarchy\n- **POSA-GO**: Fuses hierarchical GO information with protein language models\n\n### Training Strategy\n- Train separate models per aspect (MF, BP, CC) for better specialization\n- Use AdamW optimizer with weight decay (1e-4)\n- Learning rate: 0.001-0.01\n- Scheduler: CosineAnnealingLR or ReduceLROnPlateau\n- Epochs: 10-20\n- Batch size: 64-256\n- Gradient clipping (max_norm=1.0)\n\n### Model Ensemble (CRITICAL FOR COMPETITIVE SCORE)\nTrain 3+ models with different hyperparameters and average predictions:\n- Vary batch size (64, 128, 256)\n- Vary hidden dimensions\n- Vary dropout rates\n- Average sigmoid outputs for final prediction\n- Can also ensemble different embedding types (ESM2 + ProtBERT)\n\n## Post-Processing (CRITICAL FOR SCORE - DO NOT SKIP)\n\n### GO Term Propagation (MANDATORY)\n**ESSENTIAL**: GO is hierarchical - predictions must be propagated to parent terms:\n1. Parse GO ontology (go-basic.obo) to get parent-child relationships (is_a and part_of)\n2. For each predicted term, propagate score to all ancestors\n3. Parent score = max(parent_score, child_score)\n4. Root terms (GO:0003674, GO:0008150, GO:0005575) should be set to 1.0\n\n```python\ndef propagate_predictions(df, go_parents):\n    # For each protein, propagate predictions to ancestors\n    for term in predicted_terms:\n        for ancestor in get_all_ancestors(term, go_parents):\n            final_scores[ancestor] = max(final_scores.get(ancestor, 0), score)\n```\n\n### GOA Database Corrections (SIGNIFICANT BOOST)\nUse Gene Ontology Annotation (GOA) database for known annotations:\n1. Add known positive annotations with confidence 1.0\n2. Remove negative annotations (NOT qualifiers) and their descendants\n3. This provides \"free\" correct predictions for proteins with existing annotations\n\n### Confidence Thresholding\n- Use aspect-specific thresholds: BP=0.05, MF=0.1, CC=0.1\n- Ensure minimum predictions per protein (MIN_PREDS=5-10)\n- Temperature scaling can help calibrate confidences\n\n### Score Normalization\n- If max prediction for a protein is weak (<0.95), scale up predictions\n- This ensures predictions survive thresholding\n- Target: Make max non-root score at least 0.95\n\n## Validation Strategy\n\n### Cross-Validation\n- Use stratified K-fold (k=5) based on protein taxonomy or random split\n- 90/10 train/validation split is common\n- Monitor validation F1-score (MultilabelF1Score with threshold=0.1)\n\n### Evaluation Metric\n- Competition uses information-accretion weighted F1-measure\n- IA weights provided in IA.tsv - terms deep in hierarchy have higher weights\n- Final score = arithmetic mean of F1 scores across MF, BP, CC aspects\n\n## Submission Format\n- TSV file: protein_id, GO_term, confidence (no header)\n- Confidence between 0 and 1\n- Multiple rows per protein (one per predicted GO term)\n- Include propagated parent terms\n\n## Key Insights from Top Solutions\n\n1. **Embeddings are crucial**: ESM2 embeddings significantly outperform hand-crafted features\n2. **Propagation is mandatory**: Without GO hierarchy propagation, scores will be poor\n3. **GOA corrections boost score**: Using existing annotations from GOA database\n4. **Ensemble helps**: Averaging multiple models improves robustness\n5. **Aspect-specific models**: Training separate models per aspect (MF/BP/CC) works better\n6. **BLAST ensemble**: Combining deep learning with homology-based predictions\n\n## Implementation Priority (RECOMMENDED ORDER)\n\n1. **Generate ESM2 embeddings** for all proteins (train and test)\n2. **Train MLP classifier** per aspect (MF, BP, CC)\n3. **Implement GO term propagation** - parse go-basic.obo, propagate to ancestors\n4. **Add GOA corrections** - add known positives, remove negatives\n5. **Ensemble multiple models** - vary hyperparameters, average predictions\n6. **Add BLAST-based predictions** - transfer annotations from similar proteins\n7. **Tune thresholds per aspect** - optimize for F1 score\n\n## Common Pitfalls to Avoid\n\n1. **Forgetting propagation**: Always propagate predictions to parent GO terms\n2. **Wrong submission format**: Must be TSV with no header, 3 columns\n3. **Not using IA weights**: The evaluation is weighted by information accretion\n4. **Single model**: Ensemble of diverse models almost always helps\n5. **Ignoring GOA**: Free annotations from existing database\n",
      "agent": "explorer",
      "timestamp": "2026-01-15T01:35:53.346993"
    }
  ],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Key approach: Use protein language model embeddings (ProtBERT 1024-dim, T5 1024-dim, ESM2 1280-dim) as features, then train MLP classifier on top for GO term prediction",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Neural network architecture: 3-layer MLP with BatchNorm, ReLU, Dropout. Hidden dims like [864, 712]. Use BCELoss for multi-label classification with sigmoid output",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble strategy: Combine deep learning predictions with BLAST-based predictions using weighted averaging. DL_WEIGHT and BLAST_WEIGHT parameters for tuning",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Critical post-processing: GO term propagation - predictions must be propagated to parent terms in GO hierarchy (parent score >= child score). Root terms (GO:0003674, GO:0008150, GO:0005575) should be set to 1.0",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid features: Combine ESM2 embeddings (1280-dim) with sequence features (amino acid composition, dipeptide composition, physicochemical properties). Train separate models per aspect (MF, BP, CC)",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model ensemble: Train 3 MLP models with different hyperparameters (batch size, hidden dims, dropout) and average their predictions. Use CosineAnnealingLR scheduler",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA corrections: Use Gene Ontology Annotation (GOA) database to add known positive annotations (set to 1.0) and remove negative annotations (NOT qualifiers) including their descendants",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Aspect-specific thresholds: Different confidence thresholds per ontology aspect - e.g., P (BP): 0.05, F (MF): 0.1, C (CC): 0.1. MIN_PREDS ensures minimum predictions per protein",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced approaches: DeepGO uses ontology-aware deep classifier combining sequences and PPI networks. GOProFormer uses multi-modal transformer with graph transformer for GO hierarchy. POSA-GO fuses hierarchical GO info with protein language models for state-of-the-art results",
      "source": "WebSearch: protein function prediction GO terms",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best approaches for protein function prediction using Gene Ontology terms?",
      "finding": "Protein function prediction with Gene Ontology (GO) terms builds on the hierarchical DAG that organizes\u202f\u2248\u202f40\u202f000 GO classes into molecular\u2011function, biological\u2011process and cellular\u2011component domains.  Early pipelines relied on **homology\u2011based transfer** (e.g., BLAST) and classic machine\u2011learning classifiers that combine sequence similarity, domain architecture or protein\u2011protein interaction (PPI) networks\u202f\u2013\u202fapproaches that remain useful for well\u2011studied families but struggle with remote homologs and multi\u2011label prediction\u202f([ncbi.nlm.nih.gov](https://ncbi.nlm.nih.gov/pmc/articles/PMC8395570)).  These \u201ctraditional\u201d methods set the baseline for community challenges such as CAFA\u202f3, but their performance is limited by the need for explicit feature engineering and by ignoring the GO hierarchy.\n\nSince 2017, **deep learning** has become the dominant strategy.  **DeepGO** introduced an ontology\u2011aware deep classifier that jointly consumes raw amino\u2011acid sequences and interaction graphs, explici",
      "agent": "explorer"
    }
  ],
  "max_submissions": 5,
  "remaining_submissions": 5
}