{
  "phase": "explorer",
  "loop_count": 0,
  "last_submission": null,
  "competition_id": "cafa-6-protein-function-prediction",
  "seed_prompt": "",
  "evaluator_feedback": "",
  "latest_experiment": {},
  "competitive_intel": "## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`",
  "target_score": 0.441,
  "manual_instructions": "",
  "messages": [],
  "experiments": [],
  "submissions": [],
  "candidates": [],
  "findings": [],
  "metric_lower_is_better": false,
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Key approach: Use protein language model embeddings (ProtBERT 1024-dim, T5 1024-dim, ESM2 1280-dim) as features, then train MLP classifier on top for GO term prediction",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Neural network architecture: 3-layer MLP with BatchNorm, ReLU, Dropout. Hidden dims like [864, 712]. Use BCELoss for multi-label classification with sigmoid output",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble strategy: Combine deep learning predictions with BLAST-based predictions using weighted averaging. DL_WEIGHT and BLAST_WEIGHT parameters for tuning",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Critical post-processing: GO term propagation - predictions must be propagated to parent terms in GO hierarchy (parent score >= child score). Root terms (GO:0003674, GO:0008150, GO:0005575) should be set to 1.0",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid features: Combine ESM2 embeddings (1280-dim) with sequence features (amino acid composition, dipeptide composition, physicochemical properties). Train separate models per aspect (MF, BP, CC)",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model ensemble: Train 3 MLP models with different hyperparameters (batch size, hidden dims, dropout) and average their predictions. Use CosineAnnealingLR scheduler",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA corrections: Use Gene Ontology Annotation (GOA) database to add known positive annotations (set to 1.0) and remove negative annotations (NOT qualifiers) including their descendants",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Aspect-specific thresholds: Different confidence thresholds per ontology aspect - e.g., P (BP): 0.05, F (MF): 0.1, C (CC): 0.1. MIN_PREDS ensures minimum predictions per protein",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Advanced approaches: DeepGO uses ontology-aware deep classifier combining sequences and PPI networks. GOProFormer uses multi-modal transformer with graph transformer for GO hierarchy. POSA-GO fuses hierarchical GO info with protein language models for state-of-the-art results",
      "source": "WebSearch: protein function prediction GO terms",
      "agent": "explorer"
    }
  ],
  "web_research": [
    {
      "query": "What are the best approaches for protein function prediction using Gene Ontology terms?",
      "finding": "Protein function prediction with Gene Ontology (GO) terms builds on the hierarchical DAG that organizes\u202f\u2248\u202f40\u202f000 GO classes into molecular\u2011function, biological\u2011process and cellular\u2011component domains.  Early pipelines relied on **homology\u2011based transfer** (e.g., BLAST) and classic machine\u2011learning classifiers that combine sequence similarity, domain architecture or protein\u2011protein interaction (PPI) networks\u202f\u2013\u202fapproaches that remain useful for well\u2011studied families but struggle with remote homologs and multi\u2011label prediction\u202f([ncbi.nlm.nih.gov](https://ncbi.nlm.nih.gov/pmc/articles/PMC8395570)).  These \u201ctraditional\u201d methods set the baseline for community challenges such as CAFA\u202f3, but their performance is limited by the need for explicit feature engineering and by ignoring the GO hierarchy.\n\nSince 2017, **deep learning** has become the dominant strategy.  **DeepGO** introduced an ontology\u2011aware deep classifier that jointly consumes raw amino\u2011acid sequences and interaction graphs, explici",
      "agent": "explorer"
    }
  ],
  "remaining_submissions": 5,
  "max_submissions": 5,
  "last_reset_date_utc": "2026-01-15",
  "start_time": "2026-01-15T01:30:19.249816",
  "time_limit_minutes": 2100,
  "_saved_at": "2026-01-15T01:34:52.292774"
}