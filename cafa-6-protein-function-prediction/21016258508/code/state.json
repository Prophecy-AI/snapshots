{
  "phase": "explorer",
  "loop_count": 0,
  "last_submission": null,
  "competition_id": "cafa-6-protein-function-prediction",
  "seed_prompt": "",
  "evaluator_feedback": "",
  "latest_experiment": {},
  "competitive_intel": "## Available Competitive Intelligence\n\n- Competition description: `../research/description.md`\n- 5 top kernel notebooks in `../research/kernels/`",
  "target_score": 0.441,
  "manual_instructions": "",
  "messages": [],
  "experiments": [],
  "submissions": [],
  "candidates": [],
  "findings": [],
  "metric_lower_is_better": false,
  "strategy_history": [],
  "feedback_history": [],
  "data_findings": [
    {
      "finding": "Key approach: Use protein language model embeddings (ProtBERT 1024-dim, T5 1024-dim, ESM2 1280-dim) as features, then train MLP classifier on top for GO term prediction",
      "source": "../research/kernels/nihilisticneuralnet_protbert-ensemble/protbert-ensemble.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Neural network architecture: 3-layer MLP with BatchNorm, ReLU, Dropout. Hidden dims like [864, 712]. Use BCELoss for multi-label classification with sigmoid output",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Ensemble strategy: Combine deep learning predictions with BLAST-based predictions using weighted averaging. DL_WEIGHT and BLAST_WEIGHT parameters for tuning",
      "source": "../research/kernels/taylorsamarel_cafa-6-protein-function-starter-eda-model/cafa-6-protein-function-starter-eda-model.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Critical post-processing: GO term propagation - predictions must be propagated to parent terms in GO hierarchy (parent score >= child score). Root terms (GO:0003674, GO:0008150, GO:0005575) should be set to 1.0",
      "source": "../research/kernels/yongsukprasertsuk_cafa-6-goa-propagation/cafa-6-goa-propagation.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Hybrid features: Combine ESM2 embeddings (1280-dim) with sequence features (amino acid composition, dipeptide composition, physicochemical properties). Train separate models per aspect (MF, BP, CC)",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Model ensemble: Train 3 MLP models with different hyperparameters (batch size, hidden dims, dropout) and average their predictions. Use CosineAnnealingLR scheduler",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "GOA corrections: Use Gene Ontology Annotation (GOA) database to add known positive annotations (set to 1.0) and remove negative annotations (NOT qualifiers) including their descendants",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    },
    {
      "finding": "Aspect-specific thresholds: Different confidence thresholds per ontology aspect - e.g., P (BP): 0.05, F (MF): 0.1, C (CC): 0.1. MIN_PREDS ensures minimum predictions per protein",
      "source": "../research/kernels/khoatran512_ktdk-int3405e2-final/ktdk-int3405e2-final.ipynb",
      "agent": "explorer"
    }
  ],
  "web_research": [],
  "remaining_submissions": 5,
  "max_submissions": 5,
  "last_reset_date_utc": "2026-01-15",
  "start_time": "2026-01-15T01:30:19.249816",
  "time_limit_minutes": 2100,
  "_saved_at": "2026-01-15T01:32:17.023270"
}