{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3034b697",
   "metadata": {},
   "source": [
    "# BERT Baseline for Google QUEST Challenge\n",
    "\n",
    "This notebook implements a BERT-based model following the winning solution architecture.\n",
    "- Process question_title, question_body, and answer as separate inputs\n",
    "- Use max_sequence_length=500 (26/260/210 split)\n",
    "- GroupKFold with question_title groups to prevent leakage\n",
    "- Multi-Sample Dropout for better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b71b7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:23:04.321010Z",
     "iopub.status.busy": "2026-01-15T17:23:04.320325Z",
     "iopub.status.idle": "2026-01-15T17:23:08.579228Z",
     "shell.execute_reply": "2026-01-15T17:23:08.578615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n",
      "Number of target columns: 30\n",
      "Target columns: ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking']...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "print(f\"Number of target columns: {len(target_cols)}\")\n",
    "print(f\"Target columns: {target_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f4ab5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:23:08.581561Z",
     "iopub.status.busy": "2026-01-15T17:23:08.581317Z",
     "iopub.status.idle": "2026-01-15T17:23:08.586346Z",
     "shell.execute_reply": "2026-01-15T17:23:08.585758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    MODEL_NAME = 'bert-base-uncased'\n",
    "    MAX_LEN = 500\n",
    "    TITLE_MAX_LEN = 26\n",
    "    QUESTION_MAX_LEN = 260\n",
    "    ANSWER_MAX_LEN = 210\n",
    "    BATCH_SIZE = 4  # Small batch size due to long sequences\n",
    "    EPOCHS = 3\n",
    "    LEARNING_RATE = 2e-5\n",
    "    HEAD_LEARNING_RATE = 1e-3\n",
    "    N_FOLDS = 5\n",
    "    SEED = 42\n",
    "    DROPOUT = 0.2\n",
    "    HIDDEN_DIM = 768\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6597f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:23:08.588510Z",
     "iopub.status.busy": "2026-01-15T17:23:08.588307Z",
     "iopub.status.idle": "2026-01-15T17:23:09.716171Z",
     "shell.execute_reply": "2026-01-15T17:23:09.715517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833807bd9df74149a3f734063c33245a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69590730f6d4a7796b0cd62de308eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2cc5fe69ba44639861a69d13f28ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae099afe9e441a1a07e59837fc0080b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: bert-base-uncased\n",
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {config.MODEL_NAME}\")\n",
    "\n",
    "# Custom Dataset\n",
    "class QuestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, targets=None, max_len=500):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.targets = targets\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Combine title and body for question\n",
    "        question_text = (row['question_title'] + ' ' + row['question_body']).strip()\n",
    "        answer_text = row['answer'].strip()\n",
    "        \n",
    "        # Tokenize question and answer separately\n",
    "        question_tokens = self.tokenizer.encode_plus(\n",
    "            question_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.TITLE_MAX_LEN + config.QUESTION_MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        answer_tokens = self.tokenizer.encode_plus(\n",
    "            answer_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=config.ANSWER_MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'question_input_ids': question_tokens['input_ids'].squeeze(),\n",
    "            'question_attention_mask': question_tokens['attention_mask'].squeeze(),\n",
    "            'answer_input_ids': answer_tokens['input_ids'].squeeze(),\n",
    "            'answer_attention_mask': answer_tokens['attention_mask'].squeeze(),\n",
    "        }\n",
    "        \n",
    "        if self.targets is not None:\n",
    "            item['targets'] = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            \n",
    "        return item\n",
    "\n",
    "# Model definition with Multi-Sample Dropout\n",
    "class QuestModel(nn.Module):\n",
    "    def __init__(self, model_name, num_targets, hidden_dim=768, dropout=0.2):\n",
    "        super(QuestModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze BERT initially (will unfreeze gradually)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Multi-sample dropout layers\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for _ in range(5)])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),  # *2 for question and answer concatenation\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_targets)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.classifier.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, question_input_ids, question_attention_mask, answer_input_ids, answer_attention_mask):\n",
    "        # Get BERT embeddings for question and answer\n",
    "        question_output = self.bert(\n",
    "            input_ids=question_input_ids,\n",
    "            attention_mask=question_attention_mask\n",
    "        )\n",
    "        \n",
    "        answer_output = self.bert(\n",
    "            input_ids=answer_input_ids,\n",
    "            attention_mask=answer_attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use CLS token embeddings\n",
    "        question_cls = question_output.last_hidden_state[:, 0, :]  # [batch, hidden_dim]\n",
    "        answer_cls = answer_output.last_hidden_state[:, 0, :]      # [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate question and answer representations\n",
    "        combined = torch.cat([question_cls, answer_cls], dim=1)  # [batch, hidden_dim * 2]\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        dropout_outputs = []\n",
    "        for dropout in self.dropouts:\n",
    "            dropped = dropout(combined)\n",
    "            output = self.classifier(dropped)\n",
    "            dropout_outputs.append(output)\n",
    "        \n",
    "        # Average predictions from all dropout samples\n",
    "        output = torch.stack(dropout_outputs).mean(dim=0)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1906297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:24:00.020272Z",
     "iopub.status.busy": "2026-01-15T17:24:00.019968Z",
     "iopub.status.idle": "2026-01-15T17:24:00.031664Z",
     "shell.execute_reply": "2026-01-15T17:24:00.031020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 6079\n",
      "Test dataset size: 476\n",
      "Using GroupKFold with 5 folds\n",
      "Number of unique question titles: 3583\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "train_targets = train[target_cols].values\n",
    "test_targets = None  # For test set\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QuestDataset(train, tokenizer, train_targets, config.MAX_LEN)\n",
    "test_dataset = QuestDataset(test, tokenizer, test_targets, config.MAX_LEN)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# GroupKFold to prevent leakage from duplicate questions\n",
    "gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
    "groups = train['question_title'].values\n",
    "\n",
    "print(f\"Using GroupKFold with {config.N_FOLDS} folds\")\n",
    "print(f\"Number of unique question titles: {len(set(groups))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3b1610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T17:24:00.033697Z",
     "iopub.status.busy": "2026-01-15T17:24:00.033447Z",
     "iopub.status.idle": "2026-01-15T17:24:00.044602Z",
     "shell.execute_reply": "2026-01-15T17:24:00.044084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        question_input_ids = batch['question_input_ids'].to(device)\n",
    "        question_attention_mask = batch['question_attention_mask'].to(device)\n",
    "        answer_input_ids = batch['answer_input_ids'].to(device)\n",
    "        answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        outputs = model(question_input_ids, question_attention_mask, answer_input_ids, answer_attention_mask)\n",
    "        \n",
    "        # BCE loss (as used by winners)\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            question_input_ids = batch['question_input_ids'].to(device)\n",
    "            question_attention_mask = batch['question_attention_mask'].to(device)\n",
    "            answer_input_ids = batch['answer_input_ids'].to(device)\n",
    "            answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            outputs = model(question_input_ids, question_attention_mask, answer_input_ids, answer_attention_mask)\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "def calculate_spearman(targets, predictions):\n",
    "    \"\"\"Calculate mean column-wise Spearman correlation\"\"\"\n",
    "    scores = []\n",
    "    for i in range(targets.shape[1]):\n",
    "        corr, _ = spearmanr(targets[:, i], predictions[:, i])\n",
    "        scores.append(corr)\n",
    "    return np.mean(scores), scores\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f939c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation training\n",
    "fold_scores = []\n",
    "oof_predictions = np.zeros((len(train), len(target_cols)))\n",
    "test_predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train, groups=groups)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold + 1}/{config.N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create fold datasets\n",
    "    fold_train_dataset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "    fold_val_dataset = torch.utils.data.Subset(train_dataset, val_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(fold_train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(fold_val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = QuestModel(config.MODEL_NAME, len(target_cols), config.HIDDEN_DIM, config.DROPOUT)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer with different learning rates for encoder and head\n",
    "    head_params = list(model.classifier.parameters()) + list(model.dropouts.parameters())\n",
    "    \n",
    "    optimizer = AdamW([\n",
    "        {'params': head_params, 'lr': config.HEAD_LEARNING_RATE},\n",
    "    ])\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_score = 0\n",
    "    patience_counter = 0\n",
    "    patience = 2\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.EPOCHS}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_predictions, val_targets = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        # Calculate Spearman correlation\n",
    "        mean_score, target_scores = calculate_spearman(val_targets, val_predictions)\n",
    "        print(f\"Validation Spearman: {mean_score:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            patience_counter = 0\n",
    "            # Save OOF predictions\n",
    "            oof_predictions[val_idx] = val_predictions\n",
    "            print(f\"New best score: {best_score:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    fold_scores.append(best_score)\n",
    "    print(f\"Fold {fold + 1} Best Score: {best_score:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    fold_test_predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            question_input_ids = batch['question_input_ids'].to(device)\n",
    "            question_attention_mask = batch['question_attention_mask'].to(device)\n",
    "            answer_input_ids = batch['answer_input_ids'].to(device)\n",
    "            answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(question_input_ids, question_attention_mask, answer_input_ids, answer_attention_mask)\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            fold_test_predictions.append(predictions.cpu().numpy())\n",
    "    \n",
    "    fold_test_predictions = np.concatenate(fold_test_predictions, axis=0)\n",
    "    test_predictions += fold_test_predictions / config.N_FOLDS\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Cross-validation completed\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Mean CV Score: {np.mean(fold_scores):.4f} Â± {np.std(fold_scores):.4f}\")\n",
    "print(f\"Individual fold scores: {[f'{score:.4f}' for score in fold_scores]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall Spearman correlation\n",
    "overall_score, target_scores = calculate_spearman(train_targets, oof_predictions)\n",
    "print(f\"\\nOverall Spearman Correlation: {overall_score:.4f}\")\n",
    "print(f\"\\nPer-target Spearman correlations:\")\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(f\"{target}: {target_scores[i]:.4f}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'qa_id': test['qa_id']\n",
    "})\n",
    "\n",
    "for i, target in enumerate(target_cols):\n",
    "    submission[target] = test_predictions[:, i]\n",
    "\n",
    "# Clip predictions to [0, 1] range\n",
    "submission[target_cols] = submission[target_cols].clip(0, 1)\n",
    "\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Submission columns: {submission.columns.tolist()}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"\\nSubmission saved to /home/submission/submission.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
