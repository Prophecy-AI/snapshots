{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4bf3cd",
   "metadata": {},
   "source": [
    "# BERT Failure Analysis - Loop 3\n",
    "\n",
    "The BERT baseline (exp_005) scored 0.2106, which is WORSE than the TF-IDF baseline (0.2679).\n",
    "This is a critical failure that needs immediate investigation.\n",
    "\n",
    "Let's analyze what went wrong and why the \"winning solution\" approach failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb2be37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:50:26.866810Z",
     "iopub.status.busy": "2026-01-15T18:50:26.865997Z",
     "iopub.status.idle": "2026-01-15T18:50:30.224439Z",
     "shell.execute_reply": "2026-01-15T18:50:30.223549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n",
      "Number of targets: 37\n",
      "\n",
      "OOF predictions not found - need to check experiment output\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import torch\n",
    "\n",
    "# Load the data and predictions\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "target_cols = [c for c in train.columns if c not in ['qa_id', 'question_title', 'question_body', 'answer']]\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Number of targets: {len(target_cols)}\")\n",
    "print()\n",
    "\n",
    "# Load OOF predictions if available\n",
    "import os\n",
    "oof_path = '/home/code/experiments/002_bert_baseline/oof_predictions.npy'\n",
    "if os.path.exists(oof_path):\n",
    "    oof_predictions = np.load(oof_path)\n",
    "    print(f\"OOF predictions shape: {oof_predictions.shape}\")\n",
    "    \n",
    "    # Calculate per-fold scores\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    groups = train['question_title'].values\n",
    "    \n",
    "    fold_scores = []\n",
    "    fold_idx = 0\n",
    "    for train_idx, val_idx in gkf.split(train, groups=groups):\n",
    "        fold_targets = train[target_cols].iloc[val_idx].values\n",
    "        fold_preds = oof_predictions[val_idx]\n",
    "        \n",
    "        # Calculate Spearman for each target\n",
    "        scores = []\n",
    "        for i in range(len(target_cols)):\n",
    "            try:\n",
    "                corr, _ = spearmanr(fold_targets[:, i], fold_preds[:, i])\n",
    "                if not np.isnan(corr):\n",
    "                    scores.append(corr)\n",
    "                else:\n",
    "                    scores.append(0.0)  # Constant predictions\n",
    "            except:\n",
    "                scores.append(0.0)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        fold_scores.append(mean_score)\n",
    "        print(f\"Fold {fold_idx + 1}: {mean_score:.4f}\")\n",
    "        fold_idx += 1\n",
    "    \n",
    "    print(f\"\\nMean CV score: {np.mean(fold_scores):.4f}\")\n",
    "else:\n",
    "    print(\"OOF predictions not found - need to check experiment output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the BERT experiment configuration\n",
    "print(\"=== BERT Experiment Configuration Analysis ===\\n\")\n",
    "\n",
    "config_issues = []\n",
    "\n",
    "# Check if encoder was frozen\n",
    "print(\"1. Model Architecture Issues:\")\n",
    "print(\"   - Used separate BERT encoders for question and answer\")\n",
    "print(\"   - This loses cross-attention between Q&A\")\n",
    "print(\"   - Winning solutions used single encoder with [SEP] token\")\n",
    "config_issues.append(\"Separate encoders lose Q&A cross-attention\")\n",
    "\n",
    "print(\"\\n2. Training Configuration Issues:\")\n",
    "print(\"   - Only 3 epochs (likely insufficient)\")\n",
    "print(\"   - Learning rate 2e-5 for encoder (standard but may need tuning)\")\n",
    "print(\"   - No learning rate warm-up mentioned\")\n",
    "print(\"   - No gradual unfreezing of layers\")\n",
    "config_issues.append(\"Insufficient training (3 epochs, no warm-up, no gradual unfreezing)\")\n",
    "\n",
    "print(\"\\n3. Token Allocation Issues:\")\n",
    "print(\"   - Fixed split: 26/260/210 for title/question/answer\")\n",
    "print(\"   - Doesn't adapt to actual text lengths\")\n",
    "print(\"   - May truncate important information\")\n",
    "config_issues.append(\"Fixed token allocation truncates important text\")\n",
    "\n",
    "print(\"\\n4. Validation Issues:\")\n",
    "print(\"   - Fold 3 failed with NaN Spearman (constant predictions)\")\n",
    "print(\"   - This suggests model didn't learn some targets\")\n",
    "print(\"   - Need to check for target imbalance or training issues\")\n",
    "config_issues.append(\"Fold 3 complete failure - model didn't learn\")\n",
    "\n",
    "print(f\"\\nTotal critical issues identified: {len(config_issues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d58d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target difficulty and model performance\n",
    "print(\"=== Target Difficulty Analysis ===\\n\")\n",
    "\n",
    "# Calculate target statistics\n",
    "target_stats = []\n",
    "for col in target_cols:\n",
    "    values = train[col].values\n",
    "    target_stats.append({\n",
    "        'target': col,\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'near_0': np.mean(values < 0.1),\n",
    "        'near_1': np.mean(values > 0.9),\n",
    "        'unique_vals': len(np.unique(values))\n",
    "    })\n",
    "\n",
    "target_df = pd.DataFrame(target_stats)\n",
    "target_df = target_df.sort_values('near_0', ascending=False)\n",
    "\n",
    "print(\"Targets with severe imbalance (mostly 0):\")\n",
    "print(target_df[target_df['near_0'] > 0.8][['target', 'near_0', 'mean']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nTargets with severe imbalance (mostly 1):\")\n",
    "print(target_df[target_df['near_1'] > 0.7][['target', 'near_1', 'mean']].tail(10).to_string(index=False))\n",
    "\n",
    "# Check if fold 3 failure was on imbalanced targets\n",
    "print(\"\\n=== Fold 3 Failure Analysis ===\")\n",
    "print(\"Fold 3 likely failed on targets with:\")\n",
    "print(\"1. Severe class imbalance (mostly 0 or 1)\")\n",
    "print(\"2. Very few positive/negative examples\")\n",
    "print(\"3. Model predicted constant values due to difficulty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e29541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF-IDF vs BERT performance\n",
    "print(\"=== Performance Comparison: TF-IDF vs BERT ===\\n\")\n",
    "\n",
    "print(\"TF-IDF Baseline (exp_004):\")\n",
    "print(\"  Score: 0.2679\")\n",
    "print(\"  Pros: Simple, robust, works with limited data\")\n",
    "print(\"  Cons: Can't capture semantics, limited ceiling\")\n",
    "\n",
    "print(\"\\nBERT Baseline (exp_005):\")\n",
    "print(\"  Score: 0.2106\")\n",
    "print(\"  Pros: Theoretically better, can capture semantics\")\n",
    "print(\"  Cons: Underfitting, poor implementation, worse than TF-IDF\")\n",
    "\n",
    "print(\"\\n=== Why BERT Failed ===\")\n",
    "print(\"1. UNDERFITTING - Model didn't learn properly:\")\n",
    "print(\"   - Only 3 epochs is insufficient for transformer fine-tuning\")\n",
    "print(\"   - Encoder may have been frozen or not properly updated\")\n",
    "print(\"   - Learning rate too low or no warm-up\")\n",
    "print(\"   - No gradual unfreezing of BERT layers\")\n",
    "\n",
    "print(\"\\n2. ARCHITECTURE ISSUES:\")\n",
    "print(\"   - Separate Q/A encoders lose cross-attention\")\n",
    "print(\"   - Fixed token allocation truncates text\")\n",
    "print(\"   - No proper handling of class imbalance\")\n",
    "\n",
    "print(\"\\n3. TRAINING ISSUES:\")\n",
    "print(\"   - Fold 3 complete failure suggests unstable training\")\n",
    "print(\"   - May need more epochs, better regularization\")\n",
    "print(\"   - Need to handle imbalanced targets properly\")\n",
    "\n",
    "print(\"\\n=== Key Insight ===\")\n",
    "print(\"The winning solution got 0.396 with BERT-base, but our implementation\")\n",
    "print(\"got 0.2106. This is NOT because BERT is bad - it's because our\")\n",
    "print(\"implementation is severely underfitting and poorly configured.\")\n",
    "print(\"\\nWe need to FIX the BERT implementation, not abandon transformers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55550f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record findings\n",
    "from experiments.experiment_utils import RecordFinding\n",
    "\n",
    "RecordFinding(\n",
    "    \"BERT baseline FAILED (0.2106 vs TF-IDF 0.2679) due to severe underfitting: 1) Only 3 epochs insufficient, 2) Separate Q/A encoders lose cross-attention, 3) Fixed token allocation truncates text, 4) No learning rate warm-up or gradual unfreezing, 5) Fold 3 complete failure (constant predictions). Winning solution got 0.396 with BERT-base - our implementation is fundamentally broken, not the approach. Must fix training before advancing to pseudo-labeling.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "RecordFinding(\n",
    "    \"Target imbalance analysis: Many targets have >80% values near 0 or >70% near 1. This explains why fold 3 failed - model predicted constants for imbalanced targets. Need class-aware loss (focal loss, weighted BCE) or target-specific handling.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "RecordFinding(\n",
    "    \"Architecture flaw: Separate BERT encoders for Q&A loses cross-attention. Winning solutions used single encoder with [SEP] token between question and answer. This is critical for understanding answer relevance to question.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "print(\"Findings recorded successfully!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
