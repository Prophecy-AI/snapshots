{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4bf3cd",
   "metadata": {},
   "source": [
    "# Evolver Loop 3 Analysis: Understanding BERT Success and Next Steps\n",
    "\n",
    "This notebook analyzes the BERT results from exp_006 (score: 0.3571) and identifies improvements needed to reach 0.431 target.\n",
    "\n",
    "Key questions:\n",
    "1. Why is cross-fold variance so low (0.0010 vs winners' 0.02-0.03)?\n",
    "2. What text processing improvements are needed?\n",
    "3. Which targets are underperforming and why?\n",
    "4. What are the next highest-impact improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb2be37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T18:50:26.866810Z",
     "iopub.status.busy": "2026-01-15T18:50:26.865997Z",
     "iopub.status.idle": "2026-01-15T18:50:30.224439Z",
     "shell.execute_reply": "2026-01-15T18:50:30.223549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n",
      "Number of targets: 37\n",
      "\n",
      "OOF predictions not found - need to check experiment output\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import json\n",
    "\n",
    "# Load session state to understand experiment history\n",
    "with open('/home/code/session_state.json', 'r') as f:\n",
    "    session_state = json.load(f)\n",
    "\n",
    "print(\"Experiment History:\")\n",
    "for exp in session_state['experiments']:\n",
    "    print(f\"  {exp['id']}: {exp['name']} | {exp['model_type']} | {exp['score']:.4f}\")\n",
    "\n",
    "print(f\"\\nCurrent best: {session_state['experiments'][-1]['score']:.4f}\")\n",
    "print(f\"Target: 0.431\")\n",
    "print(f\"Gap: {0.431 - session_state['experiments'][-1]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data to analyze target distributions and patterns\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Test samples: {len(test)}\")\n",
    "print(f\"Target columns: {len(target_cols)}\")\n",
    "\n",
    "# Analyze target distributions\n",
    "target_stats = train[target_cols].describe().T\n",
    "print(\"\\nTarget distribution summary:\")\n",
    "print(target_stats[['mean', 'std', 'min', 'max']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d58d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance severity\n",
    "import numpy as np\n",
    "\n",
    "target_means = train[target_cols].mean()\n",
    "target_stds = train[target_cols].std()\n",
    "\n",
    "# Identify severely imbalanced targets (mean < 0.05 or mean > 0.95)\n",
    "imbalanced_mask = (target_means < 0.05) | (target_means > 0.95)\n",
    "print(f\"Severely imbalanced targets: {imbalanced_mask.sum()}/{len(target_cols)}\")\n",
    "\n",
    "imbalanced_targets = target_means[imbalanced_mask].sort_values()\n",
    "print(\"\\nMost imbalanced targets:\")\n",
    "for target, mean in imbalanced_targets.items():\n",
    "    print(f\"  {target}: mean={mean:.4f}\")\n",
    "\n",
    "# Visualize imbalance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(target_means, bins=30, edgecolor='black')\n",
    "plt.axvline(x=0.05, color='red', linestyle='--', label='Severely imbalanced (<0.05)')\n",
    "plt.axvline(x=0.95, color='red', linestyle='--', label='Severely imbalanced (>0.95)')\n",
    "plt.xlabel('Target Mean')\n",
    "plt.ylabel('Number of Targets')\n",
    "plt.title('Distribution of Target Means (Class Imbalance)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e29541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze exp_006 results to understand performance patterns\n",
    "exp_006_notes = session_state['experiments'][-1]['notes']\n",
    "print(\"exp_006 key findings:\")\n",
    "print(exp_006_notes)\n",
    "\n",
    "# Based on the notes, let's analyze which targets are hard vs easy\n",
    "# From the notes: \"Top targets: question_type_instructions (0.76), answer_type_instructions (0.73)\"\n",
    "# \"Bottom: question_not_really_a_question (0.05), question_type_spelling (0.07)\"\n",
    "\n",
    "# Let's verify these patterns with the actual data\n",
    "print(\"\\nAnalyzing why some targets are easier than others:\")\n",
    "\n",
    "# Check correlation between target mean and difficulty (assuming harder targets have extreme means)\n",
    "easy_targets = ['question_type_instructions', 'answer_type_instructions', 'question_type_choice', 'question_type_reason_explanation']\n",
    "hard_targets = ['question_not_really_a_question', 'question_type_spelling', 'answer_plausible', 'answer_relevance']\n",
    "\n",
    "print(\"\\nEasy targets (high scores in exp_006):\")\n",
    "for target in easy_targets:\n",
    "    if target in target_cols:\n",
    "        mean = train[target].mean()\n",
    "        print(f\"  {target}: mean={mean:.4f}\")\n",
    "\n",
    "print(\"\\nHard targets (low scores in exp_006):\")\n",
    "for target in hard_targets:\n",
    "    if target in target_cols:\n",
    "        mean = train[target].mean()\n",
    "        print(f\"  {target}: mean={mean:.4f}\")\n",
    "\n",
    "# Hypothesis: Targets with extreme imbalance are harder to predict\n",
    "imbalanced_and_hard = [t for t in hard_targets if t in target_cols and (train[t].mean() < 0.05 or train[t].mean() > 0.95)]\n",
    "print(f\"\\n{len(imbalanced_and_hard)} hard targets are also severely imbalanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55550f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record findings\n",
    "from experiments.experiment_utils import RecordFinding\n",
    "\n",
    "RecordFinding(\n",
    "    \"BERT baseline FAILED (0.2106 vs TF-IDF 0.2679) due to severe underfitting: 1) Only 3 epochs insufficient, 2) Separate Q/A encoders lose cross-attention, 3) Fixed token allocation truncates text, 4) No learning rate warm-up or gradual unfreezing, 5) Fold 3 complete failure (constant predictions). Winning solution got 0.396 with BERT-base - our implementation is fundamentally broken, not the approach. Must fix training before advancing to pseudo-labeling.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "RecordFinding(\n",
    "    \"Target imbalance analysis: Many targets have >80% values near 0 or >70% near 1. This explains why fold 3 failed - model predicted constants for imbalanced targets. Need class-aware loss (focal loss, weighted BCE) or target-specific handling.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "RecordFinding(\n",
    "    \"Architecture flaw: Separate BERT encoders for Q&A loses cross-attention. Winning solutions used single encoder with [SEP] token between question and answer. This is critical for understanding answer relevance to question.\",\n",
    "    \"exploration/evolver_loop3_analysis.ipynb\"\n",
    ")\n",
    "\n",
    "print(\"Findings recorded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc696bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the low variance issue (0.0010 std dev vs winners' 0.02-0.03)\n",
    "print(\"Analyzing potential causes of low cross-fold variance:\")\n",
    "\n",
    "print(\"\\n1. Possible cause: Validation splits are too similar\")\n",
    "# Check distribution of question titles across folds\n",
    "question_titles = train['question_title'].value_counts()\n",
    "print(f\"Unique question titles: {len(question_titles)}\")\n",
    "print(f\"Mean questions per title: {len(train) / len(question_titles):.2f}\")\n",
    "print(f\"Max duplicate questions: {question_titles.max()}\")\n",
    "\n",
    "# Check if some questions appear many times (could cause similar folds)\n",
    "high_dup = question_titles[question_titles > 5]\n",
    "print(f\"Questions appearing >5 times: {len(high_dup)}\")\n",
    "if len(high_dup) > 0:\n",
    "    print(\"Top duplicated questions:\")\n",
    "    print(high_dup.head())\n",
    "\n",
    "print(\"\\n2. Possible cause: Model is underfitting\")\n",
    "print(\"- Model capacity: BERT-base (110M parameters)\")\n",
    "print(\"- Training epochs: 10\")\n",
    "print(\"- Learning rate: 2e-5 (encoder), 1e-3 (head)\")\n",
    "print(\"- Regularization: Dropout 0.2, gradient clipping\")\n",
    "\n",
    "print(\"\\n3. Possible cause: Limited data diversity\")\n",
    "print(f\"Training samples: {len(train)} (relatively small)\")\n",
    "print(f\"This is typical for this competition - winners dealt with same limitation\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
