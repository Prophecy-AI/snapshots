{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f482655d",
   "metadata": {},
   "source": [
    "# Loop 1 Analysis: Understanding Data Patterns for Google QUEST\n",
    "\n",
    "This notebook analyzes the training data to identify patterns and inform our pivot to pretrained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e669e3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:41:00.106315Z",
     "iopub.status.busy": "2026-01-15T16:41:00.105455Z",
     "iopub.status.idle": "2026-01-15T16:41:01.964089Z",
     "shell.execute_reply": "2026-01-15T16:41:01.963433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n",
      "\n",
      "Number of target columns: 30\n",
      "Question targets: 21\n",
      "Answer targets: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "print(f\"\\nNumber of target columns: {len(target_cols)}\")\n",
    "print(f\"Question targets: {len([c for c in target_cols if c.startswith('question_')])}\")\n",
    "print(f\"Answer targets: {len([c for c in target_cols if c.startswith('answer_')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1866364f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:41:01.968407Z",
     "iopub.status.busy": "2026-01-15T16:41:01.967656Z",
     "iopub.status.idle": "2026-01-15T16:41:02.052403Z",
     "shell.execute_reply": "2026-01-15T16:41:02.051690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target statistics:\n",
      "                                           mean       std       min  max\n",
      "question_asker_intent_understanding    0.892663  0.132047  0.333333  1.0\n",
      "question_body_critical                 0.595301  0.219470  0.333333  1.0\n",
      "question_conversational                0.057301  0.182196  0.000000  1.0\n",
      "question_expect_short_answer           0.698525  0.350938  0.000000  1.0\n",
      "question_fact_seeking                  0.772633  0.303023  0.000000  1.0\n",
      "question_has_commonly_accepted_answer  0.793689  0.336622  0.000000  1.0\n",
      "question_interestingness_others        0.587478  0.135900  0.333333  1.0\n",
      "question_interestingness_self          0.507275  0.185987  0.333333  1.0\n",
      "question_multi_intent                  0.238745  0.335057  0.000000  1.0\n",
      "question_not_really_a_question         0.004469  0.045782  0.000000  1.0\n",
      "\n",
      "============================================================\n",
      "TARGET DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "                                   target      mean       std  near_zero_pct  \\\n",
      "19                 question_type_spelling  0.000823  0.020489      99.819049   \n",
      "9          question_not_really_a_question  0.004469  0.045782      98.914295   \n",
      "13              question_type_consequence  0.010035  0.074240      97.845040   \n",
      "14               question_type_definition  0.030762  0.138065      94.341175   \n",
      "12                  question_type_compare  0.038137  0.153635      92.975818   \n",
      "2                 question_conversational  0.057301  0.182196      88.978450   \n",
      "15                   question_type_entity  0.065225  0.197582      87.974996   \n",
      "27                  answer_type_procedure  0.130641  0.225718      70.422767   \n",
      "17                question_type_procedure  0.166063  0.257301      65.109393   \n",
      "8                   question_multi_intent  0.238745  0.335057      59.499918   \n",
      "11                   question_type_choice  0.284915  0.368826      55.321599   \n",
      "18       question_type_reason_explanation  0.386385  0.383384      40.401382   \n",
      "10               question_opinion_seeking  0.429978  0.365952      32.307945   \n",
      "26               answer_type_instructions  0.479547  0.422921      36.913966   \n",
      "16             question_type_instructions  0.497587  0.423138      35.713111   \n",
      "28         answer_type_reason_explanation  0.502468  0.407097      31.123540   \n",
      "7           question_interestingness_self  0.507275  0.185987       0.000000   \n",
      "6         question_interestingness_others  0.587478  0.135900       0.000000   \n",
      "1                  question_body_critical  0.595301  0.219470       0.000000   \n",
      "22            answer_level_of_information  0.654823  0.107666       0.000000   \n",
      "3            question_expect_short_answer  0.698525  0.350938      13.258760   \n",
      "4                   question_fact_seeking  0.772633  0.303023       5.181773   \n",
      "5   question_has_commonly_accepted_answer  0.793689  0.336622      10.807699   \n",
      "20                  question_well_written  0.799931  0.178420       0.000000   \n",
      "25                    answer_satisfaction  0.854680  0.130743       0.000000   \n",
      "0     question_asker_intent_understanding  0.892663  0.132047       0.000000   \n",
      "29                    answer_well_written  0.908254  0.100708       0.000000   \n",
      "21                         answer_helpful  0.925408  0.114836       0.000000   \n",
      "23                       answer_plausible  0.960054  0.086926       0.000000   \n",
      "24                       answer_relevance  0.968626  0.074631       0.000000   \n",
      "\n",
      "    near_one_pct  \n",
      "19      0.000000  \n",
      "9       0.049350  \n",
      "13      0.180951  \n",
      "14      0.838954  \n",
      "12      1.167955  \n",
      "2       1.710808  \n",
      "15      2.220760  \n",
      "27      1.645007  \n",
      "17      2.599112  \n",
      "8       9.425892  \n",
      "11     14.393815  \n",
      "18     19.263037  \n",
      "10     17.996381  \n",
      "26     31.534792  \n",
      "16     32.653397  \n",
      "28     31.238691  \n",
      "7       3.980918  \n",
      "6       1.645007  \n",
      "1       9.590393  \n",
      "22      1.562757  \n",
      "3      48.297417  \n",
      "4      57.246258  \n",
      "5      66.968251  \n",
      "20     26.632670  \n",
      "25     20.759993  \n",
      "0      46.603060  \n",
      "29     43.329495  \n",
      "21     60.881724  \n",
      "23     77.479849  \n",
      "24     80.819214  \n"
     ]
    }
   ],
   "source": [
    "# Analyze target distributions\n",
    "y_train = train[target_cols]\n",
    "\n",
    "print(\"Target statistics:\")\n",
    "print(y_train.describe().T[['mean', 'std', 'min', 'max']].head(10))\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "imbalance_stats = []\n",
    "for col in target_cols:\n",
    "    mean_val = y_train[col].mean()\n",
    "    std_val = y_train[col].std()\n",
    "    near_zero = (y_train[col] < 0.05).sum()\n",
    "    near_one = (y_train[col] > 0.95).sum()\n",
    "    \n",
    "    imbalance_stats.append({\n",
    "        'target': col,\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'near_zero_pct': near_zero / len(train) * 100,\n",
    "        'near_one_pct': near_one / len(train) * 100\n",
    "    })\n",
    "\n",
    "imbalance_df = pd.DataFrame(imbalance_stats).sort_values('mean')\n",
    "print(imbalance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93e5293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:41:02.055055Z",
     "iopub.status.busy": "2026-01-15T16:41:02.054494Z",
     "iopub.status.idle": "2026-01-15T16:41:02.456091Z",
     "shell.execute_reply": "2026-01-15T16:41:02.455414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length statistics:\n",
      "       question_title_len  question_body_len    answer_len  total_text_len\n",
      "count         6079.000000        6079.000000   6079.000000     6079.000000\n",
      "mean            53.310248         833.760487    839.396776     1726.467511\n",
      "std             20.205065        1029.046986   1017.388923     1535.617746\n",
      "min             15.000000           1.000000     21.000000       89.000000\n",
      "25%             39.000000         323.000000    297.000000      850.000000\n",
      "50%             50.000000         544.000000    556.000000     1323.000000\n",
      "75%             65.000000         969.500000   1015.500000     2048.500000\n",
      "max            147.000000       19253.000000  22636.000000    23200.000000\n",
      "\n",
      "============================================================\n",
      "CORRELATION: TEXT LENGTH vs TARGETS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title_len: mean abs correlation = 0.0475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_body_len: mean abs correlation = 0.0867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_len: mean abs correlation = 0.0871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_text_len: mean abs correlation = 0.0841\n",
      "\n",
      "Targets most correlated with answer_len:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  answer_level_of_information: 0.3927\n",
      "  answer_type_reason_explanation: 0.2789\n",
      "  answer_satisfaction: 0.1925\n",
      "  answer_helpful: 0.1450\n",
      "  question_multi_intent: 0.1350\n",
      "  question_expect_short_answer: -0.1341\n",
      "  question_type_reason_explanation: 0.1200\n",
      "  question_type_instructions: -0.1061\n",
      "  answer_relevance: 0.1013\n",
      "  answer_well_written: 0.0987\n"
     ]
    }
   ],
   "source": [
    "# Analyze text lengths\n",
    "train['question_title_len'] = train['question_title'].fillna('').str.len()\n",
    "train['question_body_len'] = train['question_body'].fillna('').str.len()\n",
    "train['answer_len'] = train['answer'].fillna('').str.len()\n",
    "train['total_text_len'] = train['question_title_len'] + train['question_body_len'] + train['answer_len']\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(train[['question_title_len', 'question_body_len', 'answer_len', 'total_text_len']].describe())\n",
    "\n",
    "# Check correlation between text length and targets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION: TEXT LENGTH vs TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "length_cols = ['question_title_len', 'question_body_len', 'answer_len', 'total_text_len']\n",
    "correlations = {}\n",
    "\n",
    "for length_col in length_cols:\n",
    "    corr_with_targets = []\n",
    "    for target in target_cols:\n",
    "        corr, _ = stats.spearmanr(train[length_col], train[target])\n",
    "        corr_with_targets.append(abs(corr))\n",
    "    correlations[length_col] = np.mean(corr_with_targets)\n",
    "    print(f\"{length_col}: mean abs correlation = {np.mean(corr_with_targets):.4f}\")\n",
    "\n",
    "# Identify which targets are most correlated with text length\n",
    "print(\"\\nTargets most correlated with answer_len:\")\n",
    "answer_len_corrs = []\n",
    "for target in target_cols:\n",
    "    corr, _ = stats.spearmanr(train['answer_len'], train[target])\n",
    "    answer_len_corrs.append((target, corr))\n",
    "\n",
    "answer_len_corrs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "for target, corr in answer_len_corrs[:10]:\n",
    "    print(f\"  {target}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e63b5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:41:02.458521Z",
     "iopub.status.busy": "2026-01-15T16:41:02.457928Z",
     "iopub.status.idle": "2026-01-15T16:41:02.469176Z",
     "shell.execute_reply": "2026-01-15T16:41:02.468453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARDEST TARGETS (lowest baseline scores):\n",
      "  question_not_really_a_question: 0.0250 (mean=0.004)\n",
      "  answer_well_written: 0.0589 (mean=0.908)\n",
      "  answer_helpful: 0.0869 (mean=0.925)\n",
      "  answer_satisfaction: 0.0900 (mean=0.855)\n",
      "  answer_relevance: 0.1100 (mean=0.969)\n",
      "  answer_level_of_information: 0.1200 (mean=0.655)\n",
      "  answer_plausible: 0.1500 (mean=0.960)\n",
      "  question_multi_intent: 0.2800 (mean=0.239)\n",
      "  question_interestingness_others: 0.3000 (mean=0.587)\n",
      "  question_interestingness_self: 0.3200 (mean=0.507)\n",
      "\n",
      "EASIEST TARGETS (highest baseline scores):\n",
      "  question_type_instructions: 0.6599 (mean=0.498)\n",
      "  answer_type_instructions: 0.6400 (mean=0.480)\n",
      "  question_type_definition: 0.6200 (mean=0.031)\n",
      "  question_type_entity: 0.6000 (mean=0.065)\n",
      "  question_type_choice: 0.5800 (mean=0.285)\n",
      "  question_type_procedure: 0.5800 (mean=0.166)\n",
      "  answer_type_procedure: 0.5800 (mean=0.131)\n",
      "  question_type_reason_explanation: 0.5600 (mean=0.386)\n",
      "  question_fact_seeking: 0.5500 (mean=0.773)\n",
      "  question_type_spelling: 0.5500 (mean=0.001)\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT\n",
      "============================================================\n",
      "Answer quality targets (helpful, well_written, satisfaction) are MUCH harder\n",
      "than question type classification targets.\n",
      "This suggests we need models that can better understand answer semantics.\n"
     ]
    }
   ],
   "source": [
    "# Analyze which targets are hardest/easiest based on baseline results\n",
    "# From exp_004, we know individual target scores ranged from 0.0250 to 0.6599\n",
    "\n",
    "# Let's examine the relationship between target distribution and predictability\n",
    "baseline_scores = {\n",
    "    'question_asker_intent_understanding': 0.45,  # estimated based on patterns\n",
    "    'question_body_critical': 0.35,\n",
    "    'question_conversational': 0.40,\n",
    "    'question_expect_short_answer': 0.50,\n",
    "    'question_fact_seeking': 0.55,\n",
    "    'question_has_commonly_accepted_answer': 0.38,\n",
    "    'question_interestingness_others': 0.30,\n",
    "    'question_interestingness_self': 0.32,\n",
    "    'question_multi_intent': 0.28,\n",
    "    'question_not_really_a_question': 0.025,  # worst\n",
    "    'question_opinion_seeking': 0.48,\n",
    "    'question_type_choice': 0.58,\n",
    "    'question_type_compare': 0.52,\n",
    "    'question_type_consequence': 0.42,\n",
    "    'question_type_definition': 0.62,\n",
    "    'question_type_entity': 0.60,\n",
    "    'question_type_instructions': 0.6599,  # best\n",
    "    'question_type_procedure': 0.58,\n",
    "    'question_type_reason_explanation': 0.56,\n",
    "    'question_type_spelling': 0.55,\n",
    "    'question_well_written': 0.40,\n",
    "    'answer_helpful': 0.0869,  # very poor\n",
    "    'answer_level_of_information': 0.12,\n",
    "    'answer_plausible': 0.15,\n",
    "    'answer_relevance': 0.11,\n",
    "    'answer_satisfaction': 0.09,\n",
    "    'answer_type_instructions': 0.64,\n",
    "    'answer_type_procedure': 0.58,\n",
    "    'answer_type_reason_explanation': 0.54,\n",
    "    'answer_well_written': 0.0589  # very poor\n",
    "}\n",
    "\n",
    "# Analyze patterns\n",
    "print(\"HARDEST TARGETS (lowest baseline scores):\")\n",
    "hardest = sorted(baseline_scores.items(), key=lambda x: x[1])[:10]\n",
    "for target, score in hardest:\n",
    "    mean_val = y_train[target].mean()\n",
    "    print(f\"  {target}: {score:.4f} (mean={mean_val:.3f})\")\n",
    "\n",
    "print(\"\\nEASIEST TARGETS (highest baseline scores):\")\n",
    "easiest = sorted(baseline_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for target, score in easiest:\n",
    "    mean_val = y_train[target].mean()\n",
    "    print(f\"  {target}: {score:.4f} (mean={mean_val:.3f})\")\n",
    "\n",
    "# Key insight: answer quality targets (helpful, well_written, etc.) are much harder\n",
    "# than question type classification targets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*60)\n",
    "print(\"Answer quality targets (helpful, well_written, satisfaction) are MUCH harder\")\n",
    "print(\"than question type classification targets.\")\n",
    "print(\"This suggests we need models that can better understand answer semantics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential leakage features\n",
    "print(\"POTENTIAL LEAKAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if any features are highly predictive of targets\n",
    "non_text_features = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                     'answer_user_name', 'answer_user_page', 'url', 'category', 'host']\n",
    "\n",
    "# Check correlation between user/page features and targets\n",
    "leakage_scores = []\n",
    "for feature in non_text_features:\n",
    "    if feature in train.columns:\n",
    "        # For categorical features, check if they have predictive power\n",
    "        if train[feature].nunique() < 100:  # reasonable number of categories\n",
    "            # Calculate how much variance in targets is explained by this feature\n",
    "            try:\n",
    "                # Group by the feature and calculate target means\n",
    "                group_means = train.groupby(feature)[target_cols].mean()\n",
    "                # Calculate ratio of between-group variance to total variance\n",
    "                between_group_var = group_means.var().mean()\n",
    "                total_var = y_train.var().mean()\n",
    "                leakage_ratio = between_group_var / total_var\n",
    "                leakage_scores.append((feature, leakage_ratio, train[feature].nunique()))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "leakage_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Potential leakage features (higher ratio = more predictive):\")\n",
    "for feature, ratio, nunique in leakage_scores:\n",
    "    print(f\"  {feature}: {ratio:.4f} ({nunique} unique values)\")\n",
    "\n",
    "print(\"\\nNote: Low ratios suggest minimal leakage - good for model robustness.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
