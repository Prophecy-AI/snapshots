{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f482655d",
   "metadata": {},
   "source": [
    "# Loop 1 Analysis: Understanding Data Patterns for Google QUEST\n",
    "\n",
    "This notebook analyzes the training data to identify patterns and inform our pivot to pretrained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e669e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "print(f\"\\nNumber of target columns: {len(target_cols)}\")\n",
    "print(f\"Question targets: {len([c for c in target_cols if c.startswith('question_')])}\")\n",
    "print(f\"Answer targets: {len([c for c in target_cols if c.startswith('answer_')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distributions\n",
    "y_train = train[target_cols]\n",
    "\n",
    "print(\"Target statistics:\")\n",
    "print(y_train.describe().T[['mean', 'std', 'min', 'max']].head(10))\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "imbalance_stats = []\n",
    "for col in target_cols:\n",
    "    mean_val = y_train[col].mean()\n",
    "    std_val = y_train[col].std()\n",
    "    near_zero = (y_train[col] < 0.05).sum()\n",
    "    near_one = (y_train[col] > 0.95).sum()\n",
    "    \n",
    "    imbalance_stats.append({\n",
    "        'target': col,\n",
    "        'mean': mean_val,\n",
    "        'std': std_val,\n",
    "        'near_zero_pct': near_zero / len(train) * 100,\n",
    "        'near_one_pct': near_one / len(train) * 100\n",
    "    })\n",
    "\n",
    "imbalance_df = pd.DataFrame(imbalance_stats).sort_values('mean')\n",
    "print(imbalance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "train['question_title_len'] = train['question_title'].fillna('').str.len()\n",
    "train['question_body_len'] = train['question_body'].fillna('').str.len()\n",
    "train['answer_len'] = train['answer'].fillna('').str.len()\n",
    "train['total_text_len'] = train['question_title_len'] + train['question_body_len'] + train['answer_len']\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(train[['question_title_len', 'question_body_len', 'answer_len', 'total_text_len']].describe())\n",
    "\n",
    "# Check correlation between text length and targets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION: TEXT LENGTH vs TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "length_cols = ['question_title_len', 'question_body_len', 'answer_len', 'total_text_len']\n",
    "correlations = {}\n",
    "\n",
    "for length_col in length_cols:\n",
    "    corr_with_targets = []\n",
    "    for target in target_cols:\n",
    "        corr, _ = stats.spearmanr(train[length_col], train[target])\n",
    "        corr_with_targets.append(abs(corr))\n",
    "    correlations[length_col] = np.mean(corr_with_targets)\n",
    "    print(f\"{length_col}: mean abs correlation = {np.mean(corr_with_targets):.4f}\")\n",
    "\n",
    "# Identify which targets are most correlated with text length\n",
    "print(\"\\nTargets most correlated with answer_len:\")\n",
    "answer_len_corrs = []\n",
    "for target in target_cols:\n",
    "    corr, _ = stats.spearmanr(train['answer_len'], train[target])\n",
    "    answer_len_corrs.append((target, corr))\n",
    "\n",
    "answer_len_corrs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "for target, corr in answer_len_corrs[:10]:\n",
    "    print(f\"  {target}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which targets are hardest/easiest based on baseline results\n",
    "# From exp_004, we know individual target scores ranged from 0.0250 to 0.6599\n",
    "\n",
    "# Let's examine the relationship between target distribution and predictability\n",
    "baseline_scores = {\n",
    "    'question_asker_intent_understanding': 0.45,  # estimated based on patterns\n",
    "    'question_body_critical': 0.35,\n",
    "    'question_conversational': 0.40,\n",
    "    'question_expect_short_answer': 0.50,\n",
    "    'question_fact_seeking': 0.55,\n",
    "    'question_has_commonly_accepted_answer': 0.38,\n",
    "    'question_interestingness_others': 0.30,\n",
    "    'question_interestingness_self': 0.32,\n",
    "    'question_multi_intent': 0.28,\n",
    "    'question_not_really_a_question': 0.025,  # worst\n",
    "    'question_opinion_seeking': 0.48,\n",
    "    'question_type_choice': 0.58,\n",
    "    'question_type_compare': 0.52,\n",
    "    'question_type_consequence': 0.42,\n",
    "    'question_type_definition': 0.62,\n",
    "    'question_type_entity': 0.60,\n",
    "    'question_type_instructions': 0.6599,  # best\n",
    "    'question_type_procedure': 0.58,\n",
    "    'question_type_reason_explanation': 0.56,\n",
    "    'question_type_spelling': 0.55,\n",
    "    'question_well_written': 0.40,\n",
    "    'answer_helpful': 0.0869,  # very poor\n",
    "    'answer_level_of_information': 0.12,\n",
    "    'answer_plausible': 0.15,\n",
    "    'answer_relevance': 0.11,\n",
    "    'answer_satisfaction': 0.09,\n",
    "    'answer_type_instructions': 0.64,\n",
    "    'answer_type_procedure': 0.58,\n",
    "    'answer_type_reason_explanation': 0.54,\n",
    "    'answer_well_written': 0.0589  # very poor\n",
    "}\n",
    "\n",
    "# Analyze patterns\n",
    "print(\"HARDEST TARGETS (lowest baseline scores):\")\n",
    "hardest = sorted(baseline_scores.items(), key=lambda x: x[1])[:10]\n",
    "for target, score in hardest:\n",
    "    mean_val = y_train[target].mean()\n",
    "    print(f\"  {target}: {score:.4f} (mean={mean_val:.3f})\")\n",
    "\n",
    "print(\"\\nEASIEST TARGETS (highest baseline scores):\")\n",
    "easiest = sorted(baseline_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for target, score in easiest:\n",
    "    mean_val = y_train[target].mean()\n",
    "    print(f\"  {target}: {score:.4f} (mean={mean_val:.3f})\")\n",
    "\n",
    "# Key insight: answer quality targets (helpful, well_written, etc.) are much harder\n",
    "# than question type classification targets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*60)\n",
    "print(\"Answer quality targets (helpful, well_written, satisfaction) are MUCH harder\")\n",
    "print(\"than question type classification targets.\")\n",
    "print(\"This suggests we need models that can better understand answer semantics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential leakage features\n",
    "print(\"POTENTIAL LEAKAGE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if any features are highly predictive of targets\n",
    "non_text_features = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                     'answer_user_name', 'answer_user_page', 'url', 'category', 'host']\n",
    "\n",
    "# Check correlation between user/page features and targets\n",
    "leakage_scores = []\n",
    "for feature in non_text_features:\n",
    "    if feature in train.columns:\n",
    "        # For categorical features, check if they have predictive power\n",
    "        if train[feature].nunique() < 100:  # reasonable number of categories\n",
    "            # Calculate how much variance in targets is explained by this feature\n",
    "            try:\n",
    "                # Group by the feature and calculate target means\n",
    "                group_means = train.groupby(feature)[target_cols].mean()\n",
    "                # Calculate ratio of between-group variance to total variance\n",
    "                between_group_var = group_means.var().mean()\n",
    "                total_var = y_train.var().mean()\n",
    "                leakage_ratio = between_group_var / total_var\n",
    "                leakage_scores.append((feature, leakage_ratio, train[feature].nunique()))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "leakage_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Potential leakage features (higher ratio = more predictive):\")\n",
    "for feature, ratio, nunique in leakage_scores:\n",
    "    print(f\"  {feature}: {ratio:.4f} ({nunique} unique values)\")\n",
    "\n",
    "print(\"\\nNote: Low ratios suggest minimal leakage - good for model robustness.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
