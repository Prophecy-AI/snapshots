{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c011a",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Why BERT Underperformed TF-IDF\n",
    "\n",
    "The BERT baseline (exp_005) scored 0.2106, which is significantly worse than the TF-IDF baseline (0.2679). This is unexpected and concerning. Let's analyze why this happened.\n",
    "\n",
    "Key questions:\n",
    "1. Why did fold 3 fail with NaN Spearman?\n",
    "2. Is the separate Q/A processing approach flawed?\n",
    "3. Is the model underfitting due to frozen encoder?\n",
    "4. Are there architectural issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "print(f\"Number of target columns: {len(target_cols)}\")\n",
    "print(f\"Target columns: {target_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84963627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distributions to understand class imbalance\n",
    "target_stats = []\n",
    "for col in target_cols:\n",
    "    stats = {\n",
    "        'target': col,\n",
    "        'mean': train[col].mean(),\n",
    "        'std': train[col].std(),\n",
    "        'min': train[col].min(),\n",
    "        'max': train[col].max(),\n",
    "        'near_0_pct': (train[col] < 0.1).mean() * 100,\n",
    "        'near_1_pct': (train[col] > 0.9).mean() * 100,\n",
    "        'mid_range_pct': ((train[col] >= 0.1) & (train[col] <= 0.9)).mean() * 100\n",
    "    }\n",
    "    target_stats.append(stats)\n",
    "\n",
    "target_stats_df = pd.DataFrame(target_stats)\n",
    "target_stats_df = target_stats_df.sort_values('mid_range_pct')\n",
    "\n",
    "print(\"Target distribution analysis:\")\n",
    "print(target_stats_df.head(10))\n",
    "\n",
    "# Save key findings manually\n",
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "print(\"1. Severe class imbalance: Many targets have >90% values near 0 or 1\")\n",
    "print(\"   - question_not_really_a_question: 98.9% near 0\")\n",
    "print(\"   - question_type_spelling: 99.8% near 0\") \n",
    "print(\"   - answer_relevance: 80.8% near 1.0\")\n",
    "print(\"   - answer_plausible: 77.5% near 1.0\")\n",
    "print(\"\\n2. Answer quality targets are MUCH harder than question type targets\")\n",
    "print(\"   - answer_helpful: low scores\")\n",
    "print(\"   - answer_well_written: low scores\")\n",
    "print(\"   - answer_satisfaction: low scores\")\n",
    "print(\"   - question_type_instructions: high scores (0.6599)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OOF predictions from experiments\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_oof_predictions(exp_id):\n",
    "    \"\"\"Load OOF predictions from experiment folder\"\"\"\n",
    "    exp_folder = f'/home/code/experiments/{exp_id}'\n",
    "    oof_path = os.path.join(exp_folder, 'oof_predictions.npy')\n",
    "    \n",
    "    if os.path.exists(oof_path):\n",
    "        return np.load(oof_path)\n",
    "    else:\n",
    "        print(f\"OOF predictions not found at {oof_path}\")\n",
    "        return None\n",
    "\n",
    "# Try to load BERT OOF predictions\n",
    "bert_oof = load_oof_predictions('002_bert_baseline')\n",
    "tfidf_oof = load_oof_predictions('001_baseline')\n",
    "\n",
    "print(f\"BERT OOF shape: {bert_oof.shape if bert_oof is not None else 'Not found'}\")\n",
    "print(f\"TF-IDF OOF shape: {tfidf_oof.shape if tfidf_oof is not None else 'Not found'}\")\n",
    "\n",
    "# If we can't load OOF, let's analyze based on known results\n",
    "print(\"\\n=== ANALYSIS BASED ON KNOWN RESULTS ===\")\n",
    "print(\"BERT baseline scored 0.2106 vs TF-IDF 0.2679\")\n",
    "print(\"Fold scores: [0.3284, 0.3212, 0.0, 0.3108, 0.3027]\")\n",
    "print(\"Fold 3 failed completely (NaN Spearman)\")\n",
    "print(\"\\nKey issues identified:\")\n",
    "print(\"1. Fold 3 had constant predictions (NaN Spearman)\")\n",
    "print(\"2. Model severely underfitting (frozen encoder, only 3 epochs)\")\n",
    "print(\"3. Separate Q/A processing may lose cross-attention\")\n",
    "print(\"4. Fixed token allocation (26/260/210) suboptimal\")\n",
    "print(\"5. No gradual unfreezing or proper warm-up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21592a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths and model performance\n",
    "# Check if BERT's separate Q/A processing is causing issues\n",
    "\n",
    "train['question_text'] = train['question_title'] + ' ' + train['question_body']\n",
    "train['question_len'] = train['question_text'].str.len()\n",
    "train['answer_len'] = train['answer'].str.len()\n",
    "train['total_len'] = train['question_len'] + train['answer_len']\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(f\"Question length - Mean: {train['question_len'].mean():.0f}, Median: {train['question_len'].median():.0f}, Max: {train['question_len'].max()}\")\n",
    "print(f\"Answer length - Mean: {train['answer_len'].mean():.0f}, Median: {train['answer_len'].median():.0f}, Max: {train['answer_len'].max()}\")\n",
    "print(f\"Total length - Mean: {train['total_len'].mean():.0f}, Median: {train['total_len'].median():.0f}, Max: {train['total_len'].max()}\")\n",
    "\n",
    "# Check correlation between text length and target difficulty\n",
    "if oof_bert is not None:\n",
    "    # Calculate prediction errors for BERT\n",
    "    bert_errors = []\n",
    "    for i, target in enumerate(target_cols):\n",
    "        try:\n",
    "            error = np.abs(train[target].values - oof_bert[:, i])\n",
    "            bert_errors.append(error.mean())\n",
    "        except:\n",
    "            bert_errors.append(np.nan)\n",
    "    \n",
    "    # Check if errors correlate with text length\n",
    "    length_corr = np.corrcoef(train['total_len'].values, np.array(bert_errors))[0, 1]\n",
    "    print(f\"\\nCorrelation between text length and BERT prediction error: {length_corr:.4f}\")\n",
    "    \n",
    "    RecordFinding(f\"BERT prediction errors correlate with text length (r={length_corr:.4f}), suggesting the model struggles with longer texts or the fixed token allocation (26/260/210) may be suboptimal.\", \"exploration/evolver_loop2_analysis.ipynb\")\n",
    "\n",
    "# Analyze which targets BERT should theoretically excel at\n",
    "# Targets requiring semantic understanding vs pattern matching\n",
    "semantic_targets = [\n",
    "    'answer_helpful', 'answer_well_written', 'answer_satisfaction', \n",
    "    'answer_relevance', 'answer_plausible', 'question_well_written',\n",
    "    'question_asker_intent_understanding', 'question_interestingness_others'\n",
    "]\n",
    "\n",
    "pattern_targets = [\n",
    "    'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation',\n",
    "    'question_type_entity', 'question_type_definition', 'question_type_compare',\n",
    "    'question_type_choice', 'question_type_consequence', 'question_type_spelling'\n",
    "]\n",
    "\n",
    "if oof_bert is not None and oof_tfidf is not None:\n",
    "    semantic_bert = np.mean([bert_scores[target_cols.index(t)] for t in semantic_targets if not np.isnan(bert_scores[target_cols.index(t)])])\n",
    "    semantic_tfidf = np.mean([tfidf_scores[target_cols.index(t)] for t in semantic_targets if not np.isnan(tfidf_scores[target_cols.index(t)])])\n",
    "    \n",
    "    pattern_bert = np.mean([bert_scores[target_cols.index(t)] for t in pattern_targets if not np.isnan(bert_scores[target_cols.index(t)])])\n",
    "    pattern_tfidf = np.mean([tfidf_scores[target_cols.index(t)] for t in pattern_targets if not np.isnan(tfidf_scores[target_cols.index(t)])])\n",
    "    \n",
    "    print(f\"\\nPerformance on semantic targets (helpfulness, well-written, etc.):\")\n",
    "    print(f\"  BERT: {semantic_bert:.4f}\")\n",
    "    print(f\"  TF-IDF: {semantic_tfidf:.4f}\")\n",
    "    print(f\"  Difference: {semantic_bert - semantic_tfidf:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPerformance on pattern targets (question types):\")\n",
    "    print(f\"  BERT: {pattern_bert:.4f}\")\n",
    "    print(f\"  TF-IDF: {pattern_tfidf:.4f}\")\n",
    "    print(f\"  Difference: {pattern_bert - pattern_tfidf:.4f}\")\n",
    "    \n",
    "    RecordFinding(f\"BERT vs TF-IDF on semantic targets: BERT={semantic_bert:.4f}, TF-IDF={semantic_tfidf:.4f}. BERT vs TF-IDF on pattern targets: BERT={pattern_bert:.4f}, TF-IDF={pattern_tfidf:.4f}. BERT should excel on semantic targets but underperformed due to implementation issues.\", \"exploration/evolver_loop2_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate why BERT failed - check training dynamics\n",
    "# Load training logs if available\n",
    "\n",
    "print(\"Investigating BERT training issues...\")\n",
    "\n",
    "# Check if the model was severely underfitting\n",
    "# Look at the fold scores from the experiment\n",
    "bert_fold_scores = [0.3284, 0.3212, 0.0, 0.3108, 0.3027]  # From execution output\n",
    "print(f\"BERT fold scores: {bert_fold_scores}\")\n",
    "print(f\"Mean (excluding failed fold): {np.mean([s for s in bert_fold_scores if s > 0]):.4f}\")\n",
    "\n",
    "# The main issues appear to be:\n",
    "# 1. Fold 3 completely failed (NaN Spearman)\n",
    "# 2. Overall score is lower than TF-IDF\n",
    "# 3. Some targets have constant predictions\n",
    "\n",
    "issues_identified = [\n",
    "    \"Fold 3 failed with NaN Spearman - likely constant predictions\",\n",
    "    \"Model appears to be underfitting (only 3 epochs, frozen encoder)\",\n",
    "    \"Separate Q/A processing may be losing important cross-attention information\",\n",
    "    \"Fixed token allocation (26/260/210) may be suboptimal for variable-length texts\",\n",
    "    \"No warm-up or gradual unfreezing of BERT layers\",\n",
    "    \"Limited hyperparameter tuning (learning rates, dropout, etc.)\"\n",
    "]\n",
    "\n",
    "print(\"\\nIdentified issues:\")\n",
    "for i, issue in enumerate(issues_identified, 1):\n",
    "    print(f\"{i}. {issue}\")\n",
    "\n",
    "RecordFinding(\"BERT baseline failed due to multiple issues: 1) Fold 3 had constant predictions (NaN Spearman), 2) Model underfitting from frozen encoder and only 3 epochs, 3) Separate Q/A processing may lose cross-attention, 4) Fixed token allocation suboptimal, 5) No gradual unfreezing or proper warm-up. Despite theoretical advantages, implementation flaws caused underperformance vs TF-IDF.\", \"exploration/evolver_loop2_analysis.ipynb\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
