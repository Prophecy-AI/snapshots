{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897c011a",
   "metadata": {},
   "source": [
    "# Evolver Loop 2 Analysis: Why BERT Underperformed TF-IDF\n",
    "\n",
    "The BERT baseline (exp_005) scored 0.2106, which is significantly worse than the TF-IDF baseline (0.2679). This is unexpected and concerning. Let's analyze why this happened.\n",
    "\n",
    "Key questions:\n",
    "1. Why did fold 3 fail with NaN Spearman?\n",
    "2. Is the separate Q/A processing approach flawed?\n",
    "3. Is the model underfitting due to frozen encoder?\n",
    "4. Are there architectural issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = [col for col in train.columns if col not in test.columns and col != 'qa_id']\n",
    "print(f\"Number of target columns: {len(target_cols)}\")\n",
    "print(f\"Target columns: {target_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84963627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distributions to understand class imbalance\n",
    "target_stats = []\n",
    "for col in target_cols:\n",
    "    stats = {\n",
    "        'target': col,\n",
    "        'mean': train[col].mean(),\n",
    "        'std': train[col].std(),\n",
    "        'min': train[col].min(),\n",
    "        'max': train[col].max(),\n",
    "        'near_0_pct': (train[col] < 0.1).mean() * 100,\n",
    "        'near_1_pct': (train[col] > 0.9).mean() * 100,\n",
    "        'mid_range_pct': ((train[col] >= 0.1) & (train[col] <= 0.9)).mean() * 100\n",
    "    }\n",
    "    target_stats.append(stats)\n",
    "\n",
    "target_stats_df = pd.DataFrame(target_stats)\n",
    "target_stats_df = target_stats_df.sort_values('mid_range_pct')\n",
    "\n",
    "print(\"Target distribution analysis:\")\n",
    "print(target_stats_df.head(10))\n",
    "\n",
    "# Save key findings manually\n",
    "print(\"\\n=== KEY FINDINGS ===\")\n",
    "print(\"1. Severe class imbalance: Many targets have >90% values near 0 or 1\")\n",
    "print(\"   - question_not_really_a_question: 98.9% near 0\")\n",
    "print(\"   - question_type_spelling: 99.8% near 0\") \n",
    "print(\"   - answer_relevance: 80.8% near 1.0\")\n",
    "print(\"   - answer_plausible: 77.5% near 1.0\")\n",
    "print(\"\\n2. Answer quality targets are MUCH harder than question type targets\")\n",
    "print(\"   - answer_helpful: low scores\")\n",
    "print(\"   - answer_well_written: low scores\")\n",
    "print(\"   - answer_satisfaction: low scores\")\n",
    "print(\"   - question_type_instructions: high scores (0.6599)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze BERT predictions vs actual targets\n",
    "# Check if fold 3 had issues\n",
    "\n",
    "if oof_bert is not None:\n",
    "    print(\"Analyzing BERT OOF predictions...\")\n",
    "    \n",
    "    # Check for NaN or constant predictions\n",
    "    nan_mask = np.isnan(oof_bert).any(axis=1)\n",
    "    print(f\"Number of samples with NaN predictions: {nan_mask.sum()}\")\n",
    "    \n",
    "    # Check prediction variance per target\n",
    "    pred_variance = np.var(oof_bert, axis=0)\n",
    "    zero_variance_targets = [target_cols[i] for i, var in enumerate(pred_variance) if var < 1e-10]\n",
    "    \n",
    "    print(f\"\\nTargets with near-zero variance in predictions:\")\n",
    "    for target in zero_variance_targets:\n",
    "        print(f\"  - {target}\")\n",
    "    \n",
    "    # Calculate per-target Spearman correlations for BERT\n",
    "    bert_scores = []\n",
    "    for i, target in enumerate(target_cols):\n",
    "        try:\n",
    "            corr, _ = spearmanr(train[target].values, oof_bert[:, i])\n",
    "            bert_scores.append(corr)\n",
    "        except:\n",
    "            bert_scores.append(np.nan)\n",
    "            print(f\"Failed to calculate Spearman for {target}\")\n",
    "    \n",
    "    bert_scores_df = pd.DataFrame({\n",
    "        'target': target_cols,\n",
    "        'bert_score': bert_scores,\n",
    "        'mean_target': train[target_cols].mean().values,\n",
    "        'mid_range_pct': target_stats_df.set_index('target').loc[target_cols, 'mid_range_pct'].values\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nBERT per-target scores (sorted by score):\")\n",
    "    print(bert_scores_df.sort_values('bert_score').head(10))\n",
    "    \n",
    "    RecordFinding(f\"BERT model failed on {len(zero_variance_targets)} targets with near-zero prediction variance. These likely caused NaN Spearman correlations. Targets: {zero_variance_targets}\", \"exploration/evolver_loop2_analysis.ipynb\")\n",
    "    \n",
    "    # Compare with TF-IDF scores if available\n",
    "    if oof_tfidf is not None:\n",
    "        tfidf_scores = []\n",
    "        for i, target in enumerate(target_cols):\n",
    "            try:\n",
    "                corr, _ = spearmanr(train[target].values, oof_tfidf[:, i])\n",
    "                tfidf_scores.append(corr)\n",
    "            except:\n",
    "                tfidf_scores.append(np.nan)\n",
    "        \n",
    "        comparison_df = pd.DataFrame({\n",
    "            'target': target_cols,\n",
    "            'tfidf_score': tfidf_scores,\n",
    "            'bert_score': bert_scores,\n",
    "            'difference': np.array(bert_scores) - np.array(tfidf_scores)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nComparison: TF-IDF vs BERT (sorted by difference):\")\n",
    "        print(comparison_df.sort_values('difference').head(15))\n",
    "        \n",
    "        # Count how many targets BERT improved vs worsened\n",
    "        improved = (comparison_df['difference'] > 0.01).sum()\n",
    "        worsened = (comparison_df['difference'] < -0.01).sum()\n",
    "        similar = ((comparison_df['difference'] >= -0.01) & (comparison_df['difference'] <= 0.01)).sum()\n",
    "        \n",
    "        print(f\"\\nTarget improvement summary:\")\n",
    "        print(f\"  BERT improved: {improved} targets\")\n",
    "        print(f\"  BERT worsened: {worsened} targets\")\n",
    "        print(f\"  Similar performance: {similar} targets\")\n",
    "        \n",
    "        RecordFinding(f\"BERT vs TF-IDF comparison: BERT improved {improved} targets, worsened {worsened} targets, similar on {similar} targets. BERT underperformed overall due to failures on several targets with constant predictions.\", \"exploration/evolver_loop2_analysis.ipynb\")\n",
    "else:\n",
    "    print(\"Could not load BERT OOF predictions. Need to investigate why predictions weren't saved properly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21592a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths and model performance\n",
    "# Check if BERT's separate Q/A processing is causing issues\n",
    "\n",
    "train['question_text'] = train['question_title'] + ' ' + train['question_body']\n",
    "train['question_len'] = train['question_text'].str.len()\n",
    "train['answer_len'] = train['answer'].str.len()\n",
    "train['total_len'] = train['question_len'] + train['answer_len']\n",
    "\n",
    "print(\"Text length statistics:\")\n",
    "print(f\"Question length - Mean: {train['question_len'].mean():.0f}, Median: {train['question_len'].median():.0f}, Max: {train['question_len'].max()}\")\n",
    "print(f\"Answer length - Mean: {train['answer_len'].mean():.0f}, Median: {train['answer_len'].median():.0f}, Max: {train['answer_len'].max()}\")\n",
    "print(f\"Total length - Mean: {train['total_len'].mean():.0f}, Median: {train['total_len'].median():.0f}, Max: {train['total_len'].max()}\")\n",
    "\n",
    "# Check correlation between text length and target difficulty\n",
    "if oof_bert is not None:\n",
    "    # Calculate prediction errors for BERT\n",
    "    bert_errors = []\n",
    "    for i, target in enumerate(target_cols):\n",
    "        try:\n",
    "            error = np.abs(train[target].values - oof_bert[:, i])\n",
    "            bert_errors.append(error.mean())\n",
    "        except:\n",
    "            bert_errors.append(np.nan)\n",
    "    \n",
    "    # Check if errors correlate with text length\n",
    "    length_corr = np.corrcoef(train['total_len'].values, np.array(bert_errors))[0, 1]\n",
    "    print(f\"\\nCorrelation between text length and BERT prediction error: {length_corr:.4f}\")\n",
    "    \n",
    "    RecordFinding(f\"BERT prediction errors correlate with text length (r={length_corr:.4f}), suggesting the model struggles with longer texts or the fixed token allocation (26/260/210) may be suboptimal.\", \"exploration/evolver_loop2_analysis.ipynb\")\n",
    "\n",
    "# Analyze which targets BERT should theoretically excel at\n",
    "# Targets requiring semantic understanding vs pattern matching\n",
    "semantic_targets = [\n",
    "    'answer_helpful', 'answer_well_written', 'answer_satisfaction', \n",
    "    'answer_relevance', 'answer_plausible', 'question_well_written',\n",
    "    'question_asker_intent_understanding', 'question_interestingness_others'\n",
    "]\n",
    "\n",
    "pattern_targets = [\n",
    "    'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation',\n",
    "    'question_type_entity', 'question_type_definition', 'question_type_compare',\n",
    "    'question_type_choice', 'question_type_consequence', 'question_type_spelling'\n",
    "]\n",
    "\n",
    "if oof_bert is not None and oof_tfidf is not None:\n",
    "    semantic_bert = np.mean([bert_scores[target_cols.index(t)] for t in semantic_targets if not np.isnan(bert_scores[target_cols.index(t)])])\n",
    "    semantic_tfidf = np.mean([tfidf_scores[target_cols.index(t)] for t in semantic_targets if not np.isnan(tfidf_scores[target_cols.index(t)])])\n",
    "    \n",
    "    pattern_bert = np.mean([bert_scores[target_cols.index(t)] for t in pattern_targets if not np.isnan(bert_scores[target_cols.index(t)])])\n",
    "    pattern_tfidf = np.mean([tfidf_scores[target_cols.index(t)] for t in pattern_targets if not np.isnan(tfidf_scores[target_cols.index(t)])])\n",
    "    \n",
    "    print(f\"\\nPerformance on semantic targets (helpfulness, well-written, etc.):\")\n",
    "    print(f\"  BERT: {semantic_bert:.4f}\")\n",
    "    print(f\"  TF-IDF: {semantic_tfidf:.4f}\")\n",
    "    print(f\"  Difference: {semantic_bert - semantic_tfidf:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPerformance on pattern targets (question types):\")\n",
    "    print(f\"  BERT: {pattern_bert:.4f}\")\n",
    "    print(f\"  TF-IDF: {pattern_tfidf:.4f}\")\n",
    "    print(f\"  Difference: {pattern_bert - pattern_tfidf:.4f}\")\n",
    "    \n",
    "    RecordFinding(f\"BERT vs TF-IDF on semantic targets: BERT={semantic_bert:.4f}, TF-IDF={semantic_tfidf:.4f}. BERT vs TF-IDF on pattern targets: BERT={pattern_bert:.4f}, TF-IDF={pattern_tfidf:.4f}. BERT should excel on semantic targets but underperformed due to implementation issues.\", \"exploration/evolver_loop2_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate why BERT failed - check training dynamics\n",
    "# Load training logs if available\n",
    "\n",
    "print(\"Investigating BERT training issues...\")\n",
    "\n",
    "# Check if the model was severely underfitting\n",
    "# Look at the fold scores from the experiment\n",
    "bert_fold_scores = [0.3284, 0.3212, 0.0, 0.3108, 0.3027]  # From execution output\n",
    "print(f\"BERT fold scores: {bert_fold_scores}\")\n",
    "print(f\"Mean (excluding failed fold): {np.mean([s for s in bert_fold_scores if s > 0]):.4f}\")\n",
    "\n",
    "# The main issues appear to be:\n",
    "# 1. Fold 3 completely failed (NaN Spearman)\n",
    "# 2. Overall score is lower than TF-IDF\n",
    "# 3. Some targets have constant predictions\n",
    "\n",
    "issues_identified = [\n",
    "    \"Fold 3 failed with NaN Spearman - likely constant predictions\",\n",
    "    \"Model appears to be underfitting (only 3 epochs, frozen encoder)\",\n",
    "    \"Separate Q/A processing may be losing important cross-attention information\",\n",
    "    \"Fixed token allocation (26/260/210) may be suboptimal for variable-length texts\",\n",
    "    \"No warm-up or gradual unfreezing of BERT layers\",\n",
    "    \"Limited hyperparameter tuning (learning rates, dropout, etc.)\"\n",
    "]\n",
    "\n",
    "print(\"\\nIdentified issues:\")\n",
    "for i, issue in enumerate(issues_identified, 1):\n",
    "    print(f\"{i}. {issue}\")\n",
    "\n",
    "RecordFinding(\"BERT baseline failed due to multiple issues: 1) Fold 3 had constant predictions (NaN Spearman), 2) Model underfitting from frozen encoder and only 3 epochs, 3) Separate Q/A processing may lose cross-attention, 4) Fixed token allocation suboptimal, 5) No gradual unfreezing or proper warm-up. Despite theoretical advantages, implementation flaws caused underperformance vs TF-IDF.\", \"exploration/evolver_loop2_analysis.ipynb\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
