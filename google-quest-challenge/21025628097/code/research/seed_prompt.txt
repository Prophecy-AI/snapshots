## Current Status
- Best CV score: 0.2679 from exp_004 (TF-IDF + Ridge baseline)
- Best LB score: None submitted yet
- CV-LB gap: Unknown (need first submission for calibration)
- Gap to target: 0.1631 points (0.2679 → 0.4310)

## Response to Evaluator

**Technical verdict was CONCERNS on TF-IDF baseline.** The evaluator correctly identified that TF-IDF is the wrong tool for semantic understanding. I completely agree and pivoted to BERT as recommended.

**However, BERT baseline (exp_005) FAILED catastrophically** - scoring 0.2106, which is WORSE than TF-IDF (0.2679). This is NOT because transformers are wrong for this problem (winning solution got 0.468 with BERT), but because our implementation is severely broken.

**Evaluator's top priority: Implement pretrained language models immediately.** I agree, but the implementation must be FIXED first. The current BERT baseline has fundamental flaws:

**Critical Issues Identified:**
1. **Severe underfitting**: Only 3 epochs, no learning rate warm-up, no gradual unfreezing
2. **Architecture flaw**: Separate Q/A encoders lose cross-attention (winners used single encoder with [SEP])
3. **Token allocation**: Fixed 26/260/210 split truncates important text
4. **Training failure**: Fold 3 completely failed (constant predictions, NaN Spearman)
5. **Class imbalance**: No handling of severely imbalanced targets (many >80% near 0 or >70% near 1)

**My synthesis:** The evaluator is right that transformers are the path forward, but we cannot advance to pseudo-labeling or ensembling until we fix the basic BERT implementation. The gap between our score (0.2106) and the winning solution's BERT score (0.396) is 0.1854 points - this is entirely due to implementation flaws, not approach limitations.

## Data Understanding

**Reference notebooks:** See `exploration/evolver_loop3_analysis.ipynb` for detailed failure analysis

**Key patterns discovered:**
1. **Severe target imbalance**: Many targets have >80% values near 0 or >70% near 1
   - question_not_really_a_question: 98.9% near 0
   - question_type_spelling: 99.8% near 0
   - answer_relevance: 80.8% near 1.0
   - answer_plausible: 77.5% near 1.0

2. **BERT underperforms on imbalanced targets**: Fold 3 failed completely on targets with severe imbalance, producing constant predictions (NaN Spearman)

3. **Architecture matters**: Separate Q/A encoders scored 0.2106 while winning solution's single encoder with [SEP] scored 0.396 - cross-attention is critical

4. **Training duration insufficient**: 3 epochs is far below the 5-10 epochs typically needed for BERT fine-tuning on small datasets

**Key insight:** The BERT baseline didn't fail because transformers are wrong for this problem - it failed because of severe underfitting, poor architecture choices, and no handling of class imbalance. The winning solution proves BERT can achieve 0.396+ with proper implementation.

## Recommended Approaches (Priority Order)

### 1. FIX BERT Architecture (Immediate Priority - CRITICAL)
**Why:** Current architecture (separate Q/A encoders) loses cross-attention and scored 0.2106. Winners used single encoder with [SEP] token and scored 0.396.

**Specific implementation:**
- Use SINGLE BERT encoder (not separate for Q and A)
- Format: [CLS] question_title [SEP] question_body [SEP] answer [SEP]
- Use token_type_ids to distinguish question (0) from answer (1)
- Use dynamic token allocation instead of fixed 26/260/210 split
- Take [CLS] token from final layer (or weighted sum of all layers)
- This captures Q&A interactions through cross-attention

**Expected score:** 0.35-0.40 (matching winning solution baseline)

### 2. FIX Training Configuration (High Priority)
**Why:** 3 epochs caused severe underfitting. Need proper training schedule.

**Specific implementation:**
- Increase to 8-10 epochs (standard for BERT fine-tuning)
- Add learning rate warm-up (10% of total steps)
- Implement gradual unfreezing: start with top 2 layers, then unfreeze 2 more every 2 epochs
- Use different learning rates: encoder 2e-5, head 1e-3 (as in winning solution)
- Add gradient clipping (max_norm=1.0)
- Use early stopping based on validation Spearman

**Expected improvement:** +0.05-0.10 points (from 0.21 to 0.30-0.35)

### 3. Handle Class Imbalance (High Priority)
**Why:** Fold 3 failed on imbalanced targets. Need loss weighting.

**Specific implementation:**
- Calculate per-target weights: weight_i = 1 / (mean(target_i) + epsilon)
- Use weighted BCE loss: loss = weight_i * BCE(pred_i, target_i)
- Alternative: Try focal loss with gamma=2 for hard examples
- For targets with >90% near 0 or 1, apply label smoothing (epsilon=0.1)
- Monitor per-target Spearman during training to catch failures early

**Expected improvement:** +0.02-0.05 points (prevents fold failures)

### 4. Add Multi-Sample Dropout Correctly (Medium Priority)
**Why:** Winning solution used this for +0.01-0.02 improvement.

**Specific implementation:**
- Apply multiple dropout samples (K=5) during training
- Average predictions across samples
- Use dropout rate 0.2-0.3
- Only apply to classification head, not BERT encoder

**Expected improvement:** +0.01-0.02 points

### 5. Implement Proper Validation (Medium Priority)
**Why:** Need reliable CV that matches LB.

**Specific implementation:**
- Use GroupKFold with question_title groups (already done)
- Calculate Spearman correlation per target, then average
- Save OOF predictions for error analysis
- Track per-fold scores to identify unstable training
- If CV-LB gap > 0.02, investigate with adversarial validation

### 6. Add Post-Processing (Lower Priority - Do Last)
**Why:** Winners got +0.027-0.030 boost, but only after good base model.

**Specific implementation:**
- Match prediction distribution to training distribution
- Apply only to specific targets (see winning solution)
- Use quantile-based discretization
- Only apply after achieving CV > 0.38

**Expected improvement:** +0.02-0.03 points (but only on top of good model)

## What NOT to Try

1. **More TF-IDF variations** - Already proven insufficient (0.2679 ceiling)
2. **Separate Q/A encoders** - Proven failure (0.2106), loses cross-attention
3. **Only 3 epochs** - Proven underfitting, need 8-10 epochs minimum
4. **Pseudo-labeling now** - Cannot help if base model is broken (0.2106)
5. **Complex ensembles** - Useless until single model reaches 0.35+
6. **StackExchange pretraining yet** - Too slow, fix basic BERT first

## Validation Notes

**CV Scheme:** GroupKFold with question_title groups (prevents leakage from duplicate questions)

**Metric:** Spearman correlation (must use consistently for training and evaluation)

**Success Criteria:** 
- Single BERT model must achieve CV > 0.35 before advancing
- No fold failures (all folds should have reasonable Spearman > 0.15)
- Per-target scores should show improvement over TF-IDF baseline

**Early Stopping:** Monitor validation Spearman, stop if no improvement for 3 epochs

## Expected Timeline

1. **Loop 3 (THIS LOOP):** Fix architecture (single encoder) + training (8-10 epochs) → Target: 0.32-0.38
2. **Loop 4:** Add class imbalance handling + multi-sample dropout → Target: 0.38-0.42
3. **Loop 5:** Add post-processing + submit for LB calibration → Target: 0.42-0.44
4. **Loop 6:** Ensemble diverse models (BERT, RoBERTa, BART) → Target: 0.431+ (SUBMIT)

**CRITICAL:** Do NOT advance to pseudo-labeling or ensembling until single BERT reaches 0.35+. The current 0.2106 score indicates fundamental implementation flaws that must be fixed first.