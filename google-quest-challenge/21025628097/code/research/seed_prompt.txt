## Current Status
- Best CV score: 0.2679 from exp_004 (TF-IDF + Ridge baseline)
- Best LB score: None submitted yet
- CV-LB gap: Unknown (need first submission for calibration)
- Gap to target: 0.1631 points (0.2679 → 0.4310)

## Response to Evaluator

**Technical verdict was CONCERNS.** The evaluator correctly identified that optimizing for RMSE during CV while evaluating with Spearman creates a disconnect. However, this baseline was intentionally simple to verify the pipeline - the real issue isn't the validation metric mismatch, it's that TF-IDF cannot capture semantic understanding needed for subjective quality assessment.

**Evaluator's top priority: Implement pretrained language models immediately.** I completely agree. The winning solution writeup (1st place, 0.468 LB) confirms that BERT/RoBERTa/BART with pseudo-labeling and post-processing is the proven path. The gap from 0.2679 to 0.431 is too large for incremental TF-IDF improvements.

**Key concerns raised:**
1. **Wrong tool for the job** - TF-IDF can't capture semantic understanding ✓ Agreed, pivoting to transformers
2. **Validation metric mismatch** - RMSE vs Spearman disconnect ✓ Will use Spearman consistently
3. **Feature engineering limitations** - Combined text loses structure ✓ Will process Q&A separately
4. **Ignoring competition meta** - Winners used BERT/RoBERTa ✓ Research confirms this
5. **Computational inefficiency** - Time wasted on limited approach ✓ Pivoting immediately

**My synthesis:** The evaluator is absolutely right. The baseline served its purpose (pipeline verification), but continuing with TF-IDF would be wasted effort. The winning solution clearly shows the path: pretrained transformers + pseudo-labeling + post-processing. I need to implement this immediately.

## Data Understanding

**Reference notebooks:** See `exploration/evolver_loop1_analysis.ipynb` for detailed analysis

**Key patterns discovered:**
1. **Severe class imbalance**: Many targets have >90% values near 0 or 1. For example:
   - question_not_really_a_question: 98.9% near 0
   - question_type_spelling: 99.8% near 0  
   - answer_relevance: 80.8% near 1.0
   - answer_plausible: 77.5% near 1.0
   
2. **Answer quality targets are MUCH harder**: Baseline scores show answer_helpful (0.0869), answer_well_written (0.0589), answer_satisfaction (0.0900) are 5-10x lower than question_type_instructions (0.6599). This confirms semantic understanding is critical.

3. **Text length correlations**: Answer length correlates with answer_level_of_information (0.39) and answer_satisfaction (0.19), suggesting it could be a useful auxiliary feature.

4. **Minimal leakage**: Non-text features (user names, pages, URLs) show low predictive power, suggesting the competition is about text understanding, not metadata.

**Key insight:** The hardest targets (answer quality metrics) require deep semantic understanding that only pretrained language models can provide. Bag-of-words approaches fail because they can't capture nuanced discourse structure.

## Recommended Approaches (Priority Order)

### 1. Implement BERT/RoBERTa Baseline (Immediate Priority)
**Why:** Winning solution achieved 0.468 with BERT/RoBERTa/BART ensemble. Even simple BERT should reach ~0.38-0.40.

**Specific implementation:**
- Use HuggingFace transformers
- Process question_title, question_body, and answer as separate inputs (not combined)
- Use max_sequence_length=500 (as per winning solution)
- Split as: max_title_length=26, max_question_length=260, max_answer_length=210
- Use CLS token from final layer or weighted sum of all layers
- Train with BCE loss (winners used this despite it being regression)
- Use GroupKFold with question_title groups to prevent leakage
- Implement Multi-Sample Dropout for better generalization
- Use different learning rates for encoder (1e-5) and head (1e-3)

**Expected score:** 0.38-0.40 CV

### 2. Add StackExchange Pretraining (High Priority)
**Why:** 1st place team used SE-pretrained models with 110k vocab including code tokens, plus auxiliary targets (question_score, view_count, favorite_count, answer_score, answers_count, is_answer_accepted). This gave +0.037 boost (0.414 → 0.451).

**Specific implementation:**
- Pretrain BERT/RoBERTa on 7M StackExchange samples
- Add auxiliary regression heads for 6 metadata targets
- Use whole-word masking during pretraining
- This is time-consuming but high-impact

**Alternative:** If full pretraining is too slow, start with standard BERT/RoBERTa and add this later.

### 3. Implement Pseudo-Labeling (High Priority)
**Why:** Winners got +0.008 boost (0.414 → 0.422) with proper pseudo-labeling. Used 100k additional samples.

**Critical detail:** Must generate 5 different pseudo-label sets (one per fold) to avoid leakage. Using all folds to generate pseudo-labels creates optimistic bias because training labels leak through similar samples.

**Implementation:**
- Train 5-fold models on training data
- For each fold, use the other 4 folds to pseudo-label external StackExchange data
- Add pseudo-labeled data to training set
- Retrain models

### 4. Add Post-Processing (Medium Priority)
**Why:** Simple distribution matching gave +0.027-0.030 boost for winners.

**Implementation:**
- For each target, match prediction distribution to training distribution
- Apply only to specific targets where it helps (question_conversational, question_type_compare, question_type_definition, question_type_entity, question_has_commonly_accepted_answer, question_type_consequence, question_type_spelling)
- Discretize predictions based on training set distribution percentiles

### 5. Ensemble Diverse Models (Medium Priority)
**Why:** Winners blended BERT-base, BERT-large, RoBERTa, BART with weights [0.1, 0.2, 0.1, 0.3].

**Implementation:**
- Train multiple architectures (BERT, RoBERTa, BART)
- Use different seeds, hyperparameters
- Blend predictions with learned weights
- Focus on diversity: different models should make different errors

### 6. Handle Class Imbalance (Lower Priority)
**Why:** Severe imbalance in targets (many near 0 or 1).

**Implementation:**
- Try focal loss instead of BCE
- Use class weights based on inverse frequency
- Apply label smoothing for targets near 0/1
- Note: Winners used simple BCE and handled imbalance through post-processing

## What NOT to Try

1. **More TF-IDF variations** - Already established this approach has low ceiling. Even with optimal hyperparameters, TF-IDF cannot bridge the 0.163 gap.

2. **Combining all text fields** - Loses question-answer relationship structure. Process separately.

3. **Standard KFold** - Use GroupKFold with question_title groups to prevent leakage from duplicate questions.

4. **Using all folds for pseudo-labeling** - This creates optimistic bias. Must generate separate pseudo-labels per fold.

5. **Complex stacking architectures** - Winners found simple linear blending worked better than complex stacking.

## Validation Notes

**CV Scheme:** Use GroupKFold with question_title groups (5 folds). This prevents leakage because training data contains duplicate questions with different answers.

**Metric:** Use Spearman correlation consistently for both training and evaluation. The baseline used RMSE during CV and Spearman afterward - this disconnect must be fixed.

**Calibration:** After first BERT submission, compare CV vs LB score to check for distribution shift. If gap > 0.02, investigate adversarial validation.

**Early Stopping:** Monitor validation Spearman correlation per target, not overall loss.

## Expected Timeline

1. **Loop 2:** Implement BERT baseline with proper architecture (separate Q/A inputs) → Expected CV: 0.38-0.40
2. **Loop 3:** Add pseudo-labeling → Expected CV: 0.40-0.42  
3. **Loop 4:** Add post-processing → Expected CV: 0.42-0.44
4. **Loop 5:** Ensemble multiple models → Target: 0.431+ (submit if CV > 0.42)

The path is clear: pretrained transformers are the proven solution. Time to implement.