## Current Status
- **Best CV score: 0.3612** from exp_007 (BERT with token_type_ids)
- **Best LB score: None** (no submissions yet)
- **CV-LB gap: Unknown** (need first submission for calibration)
- **Gap to target: 0.0698** (0.3612 → 0.431)

## Response to Evaluator

**Technical verdict was CONCERNS.** The evaluator correctly identified that token_type_ids implementation was needed. The +0.0041 improvement (0.3571 → 0.3612) is positive but smaller than the expected +0.02-0.03 from the evolver analysis. This suggests either: (1) the implementation needs refinement, (2) other bottlenecks exist, or (3) the model needs more training/hyperparameter tuning.

**Evaluator's top priority: Implement post-processing.** I completely agree. The 1st place writeup confirms post-processing gave +0.027-0.030 boost (0.448 for BERT-base). A 77th place writeup details a DBSCAN clustering approach that gave +0.03 boost (0.39X → 0.42X). This is high-impact (+0.03), low-effort, and directly addresses the gap to target.

**Key concerns raised:**
1. **Small improvement from token_type_ids** - Valid concern. The +0.0041 gain is modest. However, the implementation is correct per winning solution specs. The smaller gain may indicate diminishing returns or need for complementary improvements.
2. **No CV-LB calibration** - Critical gap. We have zero submissions, so we don't know if our CV is optimistic/pessimistic.
3. **Still far from target** - 0.0698 gap remains. Need multiple improvements.

**My synthesis:** The evaluator is right that post-processing is the highest-leverage next step. However, I disagree that we should submit immediately. Implementing post-processing first gives us a stronger model to submit, providing better calibration feedback. The technique is simple (DBSCAN clustering) and high-impact (+0.03). This positions us better for subsequent improvements.

## Data Understanding

**Reference notebooks:** See `exploration/evolver_loop3_analysis.ipynb` for improvement roadmap

**Key patterns:**
- **Post-processing impact**: Winners got +0.027-0.030 from distribution matching. DBSCAN clustering approach gave +0.03 boost by smoothing predictions.
- **Target distributions**: Severe imbalance in many targets (question_not_really_a_question: 98.9% near 0, question_type_spelling: 99.8% near 0). Post-processing helps by matching prediction distributions to training distributions.
- **Text structure**: Current token_type_ids implementation uses [CLS] title [SEP] body [SEP] answer [SEP] with token_type=0 for question, =1 for answer. This is correct per winning solution.

## Recommended Approaches

### 1. Implement DBSCAN Post-Processing (Immediate Priority)
**Why:** High-impact (+0.03), low-effort, proven by 77th place solution. Simple technique: cluster nearby predictions and replace with cluster median.

**Specific implementation:**
- Load OOF predictions from exp_007 (`/home/code/experiments/004_bert_token_type_ids/oof_predictions.npy`)
- For each target column:
  - Sort predictions, compute differences
  - Set eps = 95th percentile of differences (q=0.95)
  - Apply DBSCAN clustering (eps=eps, min_samples=2)
  - Replace each cluster's predictions with cluster median
- Recompute CV score
- Apply to test predictions before submission

**Expected improvement:** +0.025-0.030 (0.3612 → 0.386-0.391)

### 2. Submit Post-Processed Model (High Priority)
**Why:** Get CV-LB calibration feedback. We have zero submissions, so we don't know if our CV is reliable.

**Decision criteria:** Submit if post-processing improves CV by >0.02 (expected). This validates the technique and gives us calibration data.

### 3. Generate Pseudo-Labels (Medium Priority)
**Why:** Winners got +0.008 boost. Requires external StackExchange data.

**Critical detail:** Must generate 5 different pseudo-label sets (one per fold) to avoid leakage. Using all folds creates optimistic bias.

**Implementation:**
- Train 5-fold models on training data
- For each fold, use other 4 folds to pseudo-label external data
- Add pseudo-labeled data to training set
- Retrain models

### 4. Train Diverse Architectures (Medium Priority)
**Why:** Ensemble diversity adds +0.02-0.03. Winners blended BERT, RoBERTa, BART.

**Implementation:**
- Train RoBERTa-base/large with same setup
- Train BART (seq2seq approach)
- Use different seeds, hyperparameters
- Blend predictions with learned weights

### 5. StackExchange Pretraining (Lower Priority)
**Why:** Winners got +0.037 boost, but high effort (pretrain on 7M samples).

**Alternative:** Defer until other improvements exhausted. High effort, high reward, but lower priority than post-processing and ensembling.

## What NOT to Try

1. **More token_type_ids variations** - Implementation is correct per winning solution. Diminishing returns expected.
2. **Complex stacking** - Winners found simple linear blending worked better.
3. **Hyperparameter tuning** - Current config (10 epochs, 2e-5/1e-3 LR, batch_size=8) is reasonable. Focus on high-impact features first.
4. **All-fold pseudo-labeling** - Creates optimistic bias. Must generate per-fold pseudo-labels.

## Validation Notes

**CV Scheme:** GroupKFold with question_title groups (5 folds). Prevents leakage from duplicate questions.

**Metric:** Spearman correlation consistently for training and evaluation.

**Calibration:** After first submission, compare CV vs LB to identify distribution shift. If gap >0.01, investigate adversarial validation.

**Next experiment:** Implement DBSCAN post-processing on exp_007 OOF predictions. Expected CV: 0.386-0.391. If improvement >0.02, submit to get LB feedback.