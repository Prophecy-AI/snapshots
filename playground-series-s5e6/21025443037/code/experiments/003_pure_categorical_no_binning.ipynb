{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a248c7",
   "metadata": {},
   "source": [
    "# Experiment 003: Pure Categorical Treatment (No Binning)\n",
    "\n",
    "**Hypothesis**: Converting numerical features directly to categorical dtype (as strings) without binning will preserve information and yield significant improvement over baseline.\n",
    "\n",
    "**Expected CV**: 0.430+ (improvement of +0.100 from baseline 0.3311)\n",
    "\n",
    "**Approach**:\n",
    "- Convert all numerical features (Temp, Humidity, Moisture, N, P, K) to strings then category dtype\n",
    "- Keep Soil Type and Crop Type as categorical\n",
    "- Use XGBoost, LightGBM, CatBoost with native categorical support\n",
    "- Hyperparameters: max_depth=6-7, learning_rate=0.07\n",
    "- Stratified 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e606b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513beeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MAP@3 metric for optimization\n",
    "def map_at_3(predictions, true_labels):\n",
    "    \"\"\"Calculate MAP@3 score\"\"\"\n",
    "    map_scores = []\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        # Get top 3 predictions\n",
    "        pred_idx = np.argsort(predictions[i])[-3:][::-1]\n",
    "        \n",
    "        # Calculate average precision for this observation\n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_idx, 1):\n",
    "            if pred == true_labels[i]:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "        \n",
    "        map_scores.append(score / min(3, 1))  # Divide by min(3, num_relevant_items)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "print(\"MAP@3 metric defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(train['Fertilizer Name'])\n",
    "print(f\"Classes: {le_target.classes_}\")\n",
    "print(f\"Number of classes: {len(le_target.classes_)}\")\n",
    "\n",
    "# Save target encoder\n",
    "os.makedirs('experiments/003_pure_categorical', exist_ok=True)\n",
    "pickle.dump(le_target, open('experiments/003_pure_categorical/target_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Categorical Treatment - Convert numericals to categorical WITHOUT binning\n",
    "# All numerical features have low cardinality (14-43 unique values)\n",
    "\n",
    "numerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "categorical_features = ['Soil Type', 'Crop Type']\n",
    "\n",
    "print(\"Converting numerical features to categorical dtype (as strings)...\")\n",
    "print(\"\\nOriginal cardinalities:\")\n",
    "for col in numerical_features:\n",
    "    unique_count = train[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "\n",
    "# Convert numericals to strings then category dtype\n",
    "for col in numerical_features:\n",
    "    train[col] = train[col].astype(str).astype('category')\n",
    "    test[col] = test[col].astype(str).astype('category')\n",
    "\n",
    "# Convert original categorical features to category dtype\n",
    "for col in categorical_features:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "print(\"\\n✓ All features converted to categorical dtype\")\n",
    "print(f\"\\nCategorical features ({len(numerical_features + categorical_features)}):\")\n",
    "for col in numerical_features + categorical_features:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrices\n",
    "feature_columns = numerical_features + categorical_features\n",
    "\n",
    "X = train[feature_columns].copy()\n",
    "X_test = test[feature_columns].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Test feature matrix X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nFinal feature columns ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize predictions\n",
    "oof_predictions = np.zeros((len(X), len(le_target.classes_)))\n",
    "test_predictions_xgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_lgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_cat = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "\n",
    "# Model parameters (optimized for categorical features)\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'max_depth': 6,  # Shallower for categorical\n",
    "    'learning_rate': 0.07,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'enable_categorical': True,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.07,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Check GPU availability for CatBoost\n",
    "device = 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Model parameters defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation loop\n",
    "fold_scores = []\n",
    "fold_scores_xgb = []\n",
    "fold_scores_lgb = []\n",
    "fold_scores_cat = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"  Training XGBoost...\")\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predict XGBoost\n",
    "    val_pred_xgb = model_xgb.predict_proba(X_val)\n",
    "    oof_predictions[val_idx] = val_pred_xgb\n",
    "    score_xgb = map_at_3(val_pred_xgb, y_val)\n",
    "    fold_scores_xgb.append(score_xgb)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print(\"  Training LightGBM...\")\n",
    "    train_data_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)\n",
    "    \n",
    "    model_lgb = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data_lgb,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[val_data_lgb],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    \n",
    "    # Predict LightGBM\n",
    "    val_pred_lgb = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n",
    "    score_lgb = map_at_3(val_pred_lgb, y_val)\n",
    "    fold_scores_lgb.append(score_lgb)\n",
    "    \n",
    "    # Train CatBoost\n",
    "    print(\"  Training CatBoost...\")\n",
    "    model_cat = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        depth=6,\n",
    "        learning_rate=0.07,\n",
    "        loss_function='MultiClass',\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        task_type=device\n",
    "    )\n",
    "    \n",
    "    model_cat.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "    \n",
    "    # Predict CatBoost\n",
    "    val_pred_cat = model_cat.predict_proba(X_val)\n",
    "    score_cat = map_at_3(val_pred_cat, y_val)\n",
    "    fold_scores_cat.append(score_cat)\n",
    "    \n",
    "    # Ensemble predictions for this fold\n",
    "    val_pred_ensemble = (val_pred_xgb + val_pred_lgb + val_pred_cat) / 3\n",
    "    score_ensemble = map_at_3(val_pred_ensemble, y_val)\n",
    "    fold_scores.append(score_ensemble)\n",
    "    \n",
    "    print(f\"  XGBoost MAP@3: {score_xgb:.4f}\")\n",
    "    print(f\"  LightGBM MAP@3: {score_lgb:.4f}\")\n",
    "    print(f\"  CatBoost MAP@3: {score_cat:.4f}\")\n",
    "    print(f\"  Ensemble MAP@3: {score_ensemble:.4f}\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred_xgb = model_xgb.predict_proba(X_test)\n",
    "    test_pred_lgb = model_lgb.predict_proba(X_test)\n",
    "    test_pred_cat = model_cat.predict_proba(X_test)\n",
    "    \n",
    "    test_predictions_xgb += test_pred_xgb / n_splits\n",
    "    test_predictions_lgb += test_pred_lgb / n_splits\n",
    "    test_predictions_cat += test_pred_cat / n_splits\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CV Results:\")\n",
    "print(f\"  XGBoost: {np.mean(fold_scores_xgb):.4f} ± {np.std(fold_scores_xgb):.4f}\")\n",
    "print(f\"  LightGBM: {np.mean(fold_scores_lgb):.4f} ± {np.std(fold_scores_lgb):.4f}\")\n",
    "print(f\"  CatBoost: {np.mean(fold_scores_cat):.4f} ± {np.std(fold_scores_cat):.4f}\")\n",
    "print(f\"  Ensemble: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af13b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final CV score\n",
    "cv_score = map_at_3(oof_predictions, y_encoded)\n",
    "baseline_score = 0.3311\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Final CV MAP@3 Score: {cv_score:.4f}\")\n",
    "print(f\"Baseline Score: {baseline_score:.4f}\")\n",
    "print(f\"Improvement: {cv_score - baseline_score:.4f}\")\n",
    "print(f\"Relative Improvement: {((cv_score - baseline_score) / baseline_score * 100):.2f}%\")\n",
    "\n",
    "if cv_score > baseline_score:\n",
    "    print(f\"\\n✓ SUCCESS: Beat baseline by {cv_score - baseline_score:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n✗ REGRESSION: Worse than baseline by {baseline_score - cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67254343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions from all three models\n",
    "final_test_predictions = (test_predictions_xgb + test_predictions_lgb + test_predictions_cat) / 3\n",
    "\n",
    "# Get top 3 predictions for each test sample\n",
    "top3_predictions = np.argsort(final_test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert back to fertilizer names\n",
    "predicted_names = []\n",
    "for pred in top3_predictions:\n",
    "    names = le_target.inverse_transform(pred)\n",
    "    predicted_names.append(' '.join(names))\n",
    "\n",
    "# Create submission\n",
    "test_ids = test['id'].values\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Fertilizer Name': predicted_names\n",
    "})\n",
    "\n",
    "print(\"\\nSubmission format:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "os.makedirs('/home/submission', exist_ok=True)\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"\\n✓ Submission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment artifacts\n",
    "np.save('experiments/003_pure_categorical/oof_predictions.npy', oof_predictions)\n",
    "np.save('experiments/003_pure_categorical/test_predictions_xgb.npy', test_predictions_xgb)\n",
    "np.save('experiments/003_pure_categorical/test_predictions_lgb.npy', test_predictions_lgb)\n",
    "np.save('experiments/003_pure_categorical/test_predictions_cat.npy', test_predictions_cat)\n",
    "pickle.dump(feature_columns, open('experiments/003_pure_categorical/feature_columns.pkl', 'wb'))\n",
    "\n",
    "print(\"\\n✓ Experiment artifacts saved to experiments/003_pure_categorical/\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
