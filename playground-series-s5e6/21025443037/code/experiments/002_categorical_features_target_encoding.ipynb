{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6097ab",
   "metadata": {},
   "source": [
    "# Enhanced Feature Engineering: Categorical Treatment + Target Encoding\n",
    "\n",
    "This experiment addresses evaluator concerns:\n",
    "1. Treat all numerical features as categorical (low cardinality: 14-43 unique values)\n",
    "2. Add interaction features\n",
    "3. Implement proper target encoding with leakage prevention\n",
    "4. Add CatBoost for model diversity\n",
    "5. Optimize directly for MAP@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a09389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec68172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MAP@3 metric for optimization\n",
    "def map_at_3(predictions, true_labels):\n",
    "    \"\"\"Calculate MAP@3 score\"\"\"\n",
    "    map_scores = []\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        # Get top 3 predictions\n",
    "        pred_idx = np.argsort(predictions[i])[-3:][::-1]\n",
    "        \n",
    "        # Calculate average precision for this observation\n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_idx, 1):\n",
    "            if pred == true_labels[i]:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "                break  # Only one correct label per observation\n",
    "        \n",
    "        map_scores.append(score)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "print(\"MAP@3 metric defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Phase 1: Categorical Treatment\n",
    "# All numerical features have low cardinality (14-43 unique values) - treat as categorical\n",
    "\n",
    "# Original numerical features\n",
    "numerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "categorical_features = ['Soil Type', 'Crop Type']\n",
    "\n",
    "print(\"Converting numerical features to categorical via binning...\")\n",
    "\n",
    "# Create binned versions of numerical features (10 bins each)\n",
    "for col in numerical_features:\n",
    "    # Use qcut for equal frequency binning to ensure good distribution\n",
    "    train[f'{col}_binned'] = pd.qcut(train[col], q=10, labels=False, duplicates='drop')\n",
    "    test[f'{col}_binned'] = pd.qcut(test[col], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Convert to category dtype\n",
    "    train[f'{col}_binned'] = train[f'{col}_binned'].astype('category')\n",
    "    test[f'{col}_binned'] = test[f'{col}_binned'].astype('category')\n",
    "\n",
    "print(\"Binned features created\")\n",
    "\n",
    "# Keep original numerical features as backup\n",
    "train[numerical_features] = train[numerical_features].astype(float)\n",
    "test[numerical_features] = test[numerical_features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Phase 2: Interaction Features\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Environmental interactions\n",
    "for df in [train, test]:\n",
    "    # Temp × Humidity\n",
    "    df['Temp_Humidity'] = df['Temparature'] * df['Humidity']\n",
    "    # Temp × Moisture  \n",
    "    df['Temp_Moisture'] = df['Temparature'] * df['Moisture']\n",
    "    # Humidity × Moisture\n",
    "    df['Humidity_Moisture'] = df['Humidity'] * df['Moisture']\n",
    "    \n",
    "    # Nutrient-Environment interactions\n",
    "    df['N_Temp'] = df['Nitrogen'] * df['Temparature']\n",
    "    df['P_Humidity'] = df['Phosphorous'] * df['Humidity']\n",
    "    df['K_Moisture'] = df['Potassium'] * df['Moisture']\n",
    "    \n",
    "    # NPK balance features\n",
    "    df['NPK_sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "    df['NPK_balance'] = df['Nitrogen'] / (df['NPK_sum'] + 1)\n",
    "    df['P_balance'] = df['Phosphorous'] / (df['NPK_sum'] + 1)\n",
    "    df['K_balance'] = df['Potassium'] / (df['NPK_sum'] + 1)\n",
    "    \n",
    "    # Soil-Crop interaction (as string category)\n",
    "    df['Soil_Crop'] = df['Soil Type'] + '_' + df['Crop Type']\n",
    "\n",
    "print(\"Interaction features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature lists\n",
    "binned_features = [f'{col}_binned' for col in numerical_features]\n",
    "interaction_features = ['Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', \n",
    "                       'N_Temp', 'P_Humidity', 'K_Moisture',\n",
    "                       'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance']\n",
    "\n",
    "# All categorical features (binned + original categorical + interactions)\n",
    "all_categorical = binned_features + categorical_features + ['Soil_Crop']\n",
    "all_numerical = numerical_features + interaction_features\n",
    "\n",
    "print(f\"Categorical features: {len(all_categorical)}\")\n",
    "print(f\"Numerical features: {len(all_numerical)}\")\n",
    "print(f\"Total features: {len(all_categorical) + len(all_numerical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(train['Fertilizer Name'])\n",
    "print(f\"Classes: {le_target.classes_}\")\n",
    "print(f\"Number of classes: {len(le_target.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e60db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices\n",
    "X = train[all_categorical + all_numerical].copy()\n",
    "X_test = test[all_categorical + all_numerical].copy()\n",
    "\n",
    "# Convert categorical features to category dtype\n",
    "for col in all_categorical:\n",
    "    X[col] = X[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding with Leakage Prevention\n",
    "# Use sklearn's TargetEncoder with proper CV\n",
    "\n",
    "print(\"Setting up target encoding...\")\n",
    "\n",
    "# Features to target encode\n",
    "target_encode_features = ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "\n",
    "# Initialize target encoder (multiclass mode)\n",
    "target_encoder = TargetEncoder(target_type='multiclass', smooth='auto', cv=5)\n",
    "\n",
    "# We'll do target encoding inside the CV loop to prevent leakage\n",
    "# For now, just prepare the data structure\n",
    "target_encode_cols = target_encode_features\n",
    "\n",
    "print(f\"Target encoding will be applied to: {target_encode_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize predictions\n",
    "oof_predictions = np.zeros((len(X), len(le_target.classes_)))\n",
    "test_predictions_xgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_lgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_cat = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "\n",
    "# Model parameters\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,  # Shallower depth for categorical features\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'enable_categorical': True,  # Enable native categorical support\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'task_type': 'GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU'\n",
    "}\n",
    "\n",
    "print(\"Starting cross-validation with target encoding...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation loop with proper target encoding\n",
    "fold_scores = []\n",
    "fold_scores_xgb = []\n",
    "fold_scores_lgb = []\n",
    "fold_scores_cat = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # Apply target encoding (fit on train, transform on val)\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Fit target encoder on training data\n",
    "    encoder = TargetEncoder(target_type='multiclass', smooth='auto')\n",
    "    encoder.fit(X_train[target_encode_cols], y_train)\n",
    "    \n",
    "    # Transform training, validation, and test data\n",
    "    # For multiclass, TargetEncoder returns 2D array of shape (n_samples, n_classes)\n",
    "    # for each feature, so we need to handle this properly\n",
    "    \n",
    "    for i, col in enumerate(target_encode_cols):\n",
    "        # Transform each column separately to get proper shape\n",
    "        train_col_encoded = encoder.transform(X_train[[col]])\n",
    "        val_col_encoded = encoder.transform(X_val[[col]])\n",
    "        test_col_encoded = encoder.transform(X_test[[col]])\n",
    "        \n",
    "        # The output is 2D with shape (n_samples, n_classes)\n",
    "        # Take mean across classes to get a single value per sample\n",
    "        if train_col_encoded.ndim == 2:\n",
    "            X_train_enc[f'{col}_te'] = train_col_encoded.mean(axis=1)\n",
    "            X_val_enc[f'{col}_te'] = val_col_encoded.mean(axis=1)\n",
    "            X_test_enc[f'{col}_te'] = test_col_encoded.mean(axis=1)\n",
    "        else:\n",
    "            # Fallback for 1D output\n",
    "            X_train_enc[f'{col}_te'] = train_col_encoded.ravel()\n",
    "            X_val_enc[f'{col}_te'] = val_col_encoded.ravel()\n",
    "            X_test_enc[f'{col}_te'] = test_col_encoded.ravel()\n",
    "    \n",
    "    # Ensure all datasets have the same columns in the same order\n",
    "    feature_cols = X_train_enc.columns.tolist()\n",
    "    X_val_enc = X_val_enc[feature_cols]\n",
    "    X_test_enc = X_test_enc[feature_cols]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print(\"Training LightGBM...\")\n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Train CatBoost\n",
    "    print(\"Training CatBoost...\")\n",
    "    # CatBoost requires Pool objects for categorical features\n",
    "    # Only include categorical features that exist in the dataframe\n",
    "    cat_features_indices = []\n",
    "    for col in all_categorical:\n",
    "        if col in X_train_enc.columns:\n",
    "            cat_features_indices.append(X_train_enc.columns.get_loc(col))\n",
    "    \n",
    "    train_pool = Pool(X_train_enc, y_train, cat_features=cat_features_indices)\n",
    "    val_pool = Pool(X_val_enc, y_val, cat_features=cat_features_indices)\n",
    "    \n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    model_cat.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    pred_xgb = model_xgb.predict_proba(X_val_enc)\n",
    "    pred_lgb = model_lgb.predict_proba(X_val_enc)\n",
    "    pred_cat = model_cat.predict_proba(val_pool)\n",
    "    \n",
    "    # Average predictions (equal weighting for now)\n",
    "    pred_avg = (pred_xgb + pred_lgb + pred_cat) / 3\n",
    "    oof_predictions[val_idx] = pred_avg\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred_xgb = model_xgb.predict_proba(X_test_enc)\n",
    "    test_pred_lgb = model_lgb.predict_proba(X_test_enc)\n",
    "    \n",
    "    test_pool = Pool(X_test_enc, cat_features=cat_features_indices)\n",
    "    test_pred_cat = model_cat.predict_proba(test_pool)\n",
    "    \n",
    "    test_predictions_xgb += test_pred_xgb / n_splits\n",
    "    test_predictions_lgb += test_pred_lgb / n_splits\n",
    "    test_predictions_cat += test_pred_cat / n_splits\n",
    "    \n",
    "    # Calculate fold scores using proper MAP@3\n",
    "    score_xgb = map_at_3(pred_xgb, y_val)\n",
    "    score_lgb = map_at_3(pred_lgb, y_val)\n",
    "    score_cat = map_at_3(pred_cat, y_val)\n",
    "    score_avg = map_at_3(pred_avg, y_val)\n",
    "    \n",
    "    fold_scores_xgb.append(score_xgb)\n",
    "    fold_scores_lgb.append(score_lgb)\n",
    "    fold_scores_cat.append(score_cat)\n",
    "    fold_scores.append(score_avg)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} MAP@3 - XGB: {score_xgb:.4f}, LGB: {score_lgb:.4f}, CAT: {score_cat:.4f}, AVG: {score_avg:.4f}\")\n",
    "\n",
    "print(f\"\\nMean MAP@3 - XGB: {np.mean(fold_scores_xgb):.4f} ± {np.std(fold_scores_xgb):.4f}\")\n",
    "print(f\"Mean MAP@3 - LGB: {np.mean(fold_scores_lgb):.4f} ± {np.std(fold_scores_lgb):.4f}\")\n",
    "print(f\"Mean MAP@3 - CAT: {np.mean(fold_scores_cat):.4f} ± {np.std(fold_scores_cat):.4f}\")\n",
    "print(f\"Mean MAP@3 - AVG: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918967a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final CV score\n",
    "cv_score = map_at_3(oof_predictions, y_encoded)\n",
    "print(f\"\\nFinal CV MAP@3 Score: {cv_score:.4f}\")\n",
    "print(f\"Improvement over baseline: {cv_score - 0.3311:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions from all three models\n",
    "final_test_predictions = (test_predictions_xgb + test_predictions_lgb + test_predictions_cat) / 3\n",
    "\n",
    "# Get top 3 predictions for each test sample\n",
    "top3_predictions = np.argsort(final_test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert back to fertilizer names\n",
    "predicted_names = []\n",
    "for pred in top3_predictions:\n",
    "    names = le_target.inverse_transform(pred)\n",
    "    predicted_names.append(' '.join(names))\n",
    "\n",
    "# Create submission\n",
    "test_ids = test['id'].values\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Fertilizer Name': predicted_names\n",
    "})\n",
    "\n",
    "print(\"Submission format:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"Submission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee52cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment folder\n",
    "os.makedirs('experiments/002_categorical_target_encoding', exist_ok=True)\n",
    "\n",
    "# Save OOF predictions and test predictions\n",
    "np.save('experiments/002_categorical_target_encoding/oof_predictions.npy', oof_predictions)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_xgb.npy', test_predictions_xgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_lgb.npy', test_predictions_lgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_cat.npy', test_predictions_cat)\n",
    "\n",
    "# Save feature information\n",
    "with open('experiments/002_categorical_target_encoding/feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'categorical_features': all_categorical,\n",
    "        'numerical_features': all_numerical,\n",
    "        'target_encode_cols': target_encode_cols,\n",
    "        'le_target': le_target\n",
    "    }, f)\n",
    "\n",
    "print(\"Experiment artifacts saved to experiments/002_categorical_target_encoding/\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
