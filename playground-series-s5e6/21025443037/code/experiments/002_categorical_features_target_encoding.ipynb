{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6097ab",
   "metadata": {},
   "source": [
    "# Enhanced Feature Engineering: Categorical Treatment + Target Encoding\n",
    "\n",
    "This experiment addresses evaluator concerns:\n",
    "1. Treat all numerical features as categorical (low cardinality: 14-43 unique values)\n",
    "2. Add interaction features\n",
    "3. Implement proper target encoding with leakage prevention\n",
    "4. Add CatBoost for model diversity\n",
    "5. Optimize directly for MAP@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a09389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:23.868243Z",
     "iopub.status.busy": "2026-01-15T12:32:23.867987Z",
     "iopub.status.idle": "2026-01-15T12:32:25.345363Z",
     "shell.execute_reply": "2026-01-15T12:32:25.344758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (750000, 10)\n",
      "Test shape: (250000, 9)\n",
      "Target distribution:\n",
      "Fertilizer Name\n",
      "14-35-14    114436\n",
      "10-26-26    113887\n",
      "17-17-17    112453\n",
      "28-28       111158\n",
      "20-20       110889\n",
      "DAP          94860\n",
      "Urea         92317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch  # For GPU detection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec68172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:25.347130Z",
     "iopub.status.busy": "2026-01-15T12:32:25.346913Z",
     "iopub.status.idle": "2026-01-15T12:32:25.351189Z",
     "shell.execute_reply": "2026-01-15T12:32:25.350804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@3 metric defined\n"
     ]
    }
   ],
   "source": [
    "# Define MAP@3 metric for optimization\n",
    "def map_at_3(predictions, true_labels):\n",
    "    \"\"\"Calculate MAP@3 score\"\"\"\n",
    "    map_scores = []\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        # Get top 3 predictions\n",
    "        pred_idx = np.argsort(predictions[i])[-3:][::-1]\n",
    "        \n",
    "        # Calculate average precision for this observation\n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_idx, 1):\n",
    "            if pred == true_labels[i]:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "                break  # Only one correct label per observation\n",
    "        \n",
    "        map_scores.append(score)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "print(\"MAP@3 metric defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eda5877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:25.352446Z",
     "iopub.status.busy": "2026-01-15T12:32:25.352152Z",
     "iopub.status.idle": "2026-01-15T12:32:25.755773Z",
     "shell.execute_reply": "2026-01-15T12:32:25.755282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting numerical features to categorical via binning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binned features created\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Phase 1: Categorical Treatment\n",
    "# All numerical features have low cardinality (14-43 unique values) - treat as categorical\n",
    "\n",
    "# Original numerical features\n",
    "numerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "categorical_features = ['Soil Type', 'Crop Type']\n",
    "\n",
    "print(\"Converting numerical features to categorical via binning...\")\n",
    "\n",
    "# Create binned versions of numerical features (10 bins each)\n",
    "for col in numerical_features:\n",
    "    # Use qcut for equal frequency binning to ensure good distribution\n",
    "    train[f'{col}_binned'] = pd.qcut(train[col], q=10, labels=False, duplicates='drop')\n",
    "    test[f'{col}_binned'] = pd.qcut(test[col], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Convert to category dtype\n",
    "    train[f'{col}_binned'] = train[f'{col}_binned'].astype('category')\n",
    "    test[f'{col}_binned'] = test[f'{col}_binned'].astype('category')\n",
    "\n",
    "print(\"Binned features created\")\n",
    "\n",
    "# Keep original numerical features as backup\n",
    "train[numerical_features] = train[numerical_features].astype(float)\n",
    "test[numerical_features] = test[numerical_features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc1f46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:25.758130Z",
     "iopub.status.busy": "2026-01-15T12:32:25.757953Z",
     "iopub.status.idle": "2026-01-15T12:32:25.998101Z",
     "shell.execute_reply": "2026-01-15T12:32:25.997615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction features created\n",
      "Soil_Crop feature created with 55 unique values\n",
      "Sample Soil_Crop values: ['Clayey_Sugarcane', 'Sandy_Millets', 'Sandy_Barley', 'Red_Paddy', 'Red_Pulses']\n",
      "Categories (55, object): ['Black_Barley', 'Black_Cotton', 'Black_Ground Nuts', 'Black_Maize', ..., 'Sandy_Pulses', 'Sandy_Sugarcane', 'Sandy_Tobacco', 'Sandy_Wheat']\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Phase 2: Interaction Features\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Environmental interactions\n",
    "for df in [train, test]:\n",
    "    # Temp × Humidity\n",
    "    df['Temp_Humidity'] = df['Temparature'] * df['Humidity']\n",
    "    # Temp × Moisture  \n",
    "    df['Temp_Moisture'] = df['Temparature'] * df['Moisture']\n",
    "    # Humidity × Moisture\n",
    "    df['Humidity_Moisture'] = df['Humidity'] * df['Moisture']\n",
    "    \n",
    "    # Nutrient-Environment interactions\n",
    "    df['N_Temp'] = df['Nitrogen'] * df['Temparature']\n",
    "    df['P_Humidity'] = df['Phosphorous'] * df['Humidity']\n",
    "    df['K_Moisture'] = df['Potassium'] * df['Moisture']\n",
    "    \n",
    "    # NPK balance features\n",
    "    df['NPK_sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "    df['NPK_balance'] = df['Nitrogen'] / (df['NPK_sum'] + 1)\n",
    "    df['P_balance'] = df['Phosphorous'] / (df['NPK_sum'] + 1)\n",
    "    df['K_balance'] = df['Potassium'] / (df['NPK_sum'] + 1)\n",
    "    \n",
    "    # Soil-Crop interaction (critical for target encoding)\n",
    "    df['Soil_Crop'] = df['Soil Type'].astype(str) + '_' + df['Crop Type'].astype(str)\n",
    "    df['Soil_Crop'] = df['Soil_Crop'].astype('category')\n",
    "\n",
    "print(\"Interaction features created\")\n",
    "print(f\"Soil_Crop feature created with {train['Soil_Crop'].nunique()} unique values\")\n",
    "print(f\"Sample Soil_Crop values: {train['Soil_Crop'].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce2c96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:25.999229Z",
     "iopub.status.busy": "2026-01-15T12:32:25.999046Z",
     "iopub.status.idle": "2026-01-15T12:32:26.006063Z",
     "shell.execute_reply": "2026-01-15T12:32:26.005681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features (9): ['Temparature_binned', 'Humidity_binned', 'Moisture_binned', 'Nitrogen_binned', 'Potassium_binned', 'Phosphorous_binned', 'Soil Type', 'Crop Type', 'Soil_Crop']\n",
      "Numerical features (16): 16\n",
      "Total features: 25\n",
      "\n",
      "Verifying all features exist in train:\n",
      "  All features present!\n",
      "\n",
      "Soil_Crop sample values:\n",
      "Soil_Crop\n",
      "Black_Paddy     18410\n",
      "Sandy_Paddy     17552\n",
      "Loamy_Paddy     16869\n",
      "Red_Paddy       16679\n",
      "Sandy_Pulses    16406\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature lists\n",
    "binned_features = [f'{col}_binned' for col in numerical_features]\n",
    "interaction_features = ['Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', \n",
    "                       'N_Temp', 'P_Humidity', 'K_Moisture',\n",
    "                       'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance']\n",
    "\n",
    "# All categorical features (binned + original categorical + interactions)\n",
    "# Use the EXACT column names from the CSV (with spaces)\n",
    "all_categorical = binned_features + ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "all_numerical = numerical_features + interaction_features\n",
    "\n",
    "print(f\"Categorical features ({len(all_categorical)}): {all_categorical}\")\n",
    "print(f\"Numerical features ({len(all_numerical)}): {len(all_numerical)}\")\n",
    "print(f\"Total features: {len(all_categorical) + len(all_numerical)}\")\n",
    "\n",
    "# Verify all features exist\n",
    "print(f\"\\nVerifying all features exist in train:\")\n",
    "missing_features = []\n",
    "for col in all_categorical + all_numerical:\n",
    "    if col not in train.columns:\n",
    "        missing_features.append(col)\n",
    "        print(f\"  MISSING: {col}\")\n",
    "\n",
    "if not missing_features:\n",
    "    print(\"  All features present!\")\n",
    "\n",
    "print(f\"\\nSoil_Crop sample values:\")\n",
    "print(train['Soil_Crop'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20c964b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:26.007217Z",
     "iopub.status.busy": "2026-01-15T12:32:26.006952Z",
     "iopub.status.idle": "2026-01-15T12:32:26.090140Z",
     "shell.execute_reply": "2026-01-15T12:32:26.089684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['10-26-26' '14-35-14' '17-17-17' '20-20' '28-28' 'DAP' 'Urea']\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(train['Fertilizer Name'])\n",
    "print(f\"Classes: {le_target.classes_}\")\n",
    "print(f\"Number of classes: {len(le_target.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e60db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:26.091110Z",
     "iopub.status.busy": "2026-01-15T12:32:26.090991Z",
     "iopub.status.idle": "2026-01-15T12:32:26.349313Z",
     "shell.execute_reply": "2026-01-15T12:32:26.348838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: train.columns = ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'Fertilizer Name', 'Temparature_binned', 'Humidity_binned', 'Moisture_binned', 'Nitrogen_binned', 'Potassium_binned', 'Phosphorous_binned', 'Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', 'N_Temp', 'P_Humidity', 'K_Moisture', 'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance', 'Soil_Crop']\n",
      "DEBUG: 'Soil_Crop' in train.columns = True\n",
      "DEBUG: 'Crop Type' in train.columns = True\n",
      "DEBUG: 'Soil Type' in train.columns = True\n",
      "Verifying all features exist in train:\n",
      "All features verified!\n",
      "Total features in feature_columns: 25\n",
      "feature_columns: ['Temparature_binned', 'Humidity_binned', 'Moisture_binned', 'Nitrogen_binned', 'Potassium_binned', 'Phosphorous_binned', 'Soil Type', 'Crop Type', 'Soil_Crop', 'Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', 'N_Temp', 'P_Humidity', 'K_Moisture', 'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance', 'Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Temparature_binned to category dtype\n",
      "Converted Humidity_binned to category dtype\n",
      "Converted Moisture_binned to category dtype\n",
      "Converted Nitrogen_binned to category dtype\n",
      "Converted Potassium_binned to category dtype\n",
      "Converted Phosphorous_binned to category dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Soil Type to category dtype"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted Crop Type to category dtype\n",
      "Converted Soil_Crop to category dtype\n",
      "\n",
      "Final feature matrix shape: (750000, 25)\n",
      "Final test matrix shape: (250000, 25)\n",
      "\n",
      "X columns: ['Temparature_binned', 'Humidity_binned', 'Moisture_binned', 'Nitrogen_binned', 'Potassium_binned', 'Phosphorous_binned', 'Soil Type', 'Crop Type', 'Soil_Crop', 'Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', 'N_Temp', 'P_Humidity', 'K_Moisture', 'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance', 'Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrices\n",
    "# Use EXACT column names as they appear in the CSV\n",
    "feature_columns = []\n",
    "\n",
    "# Add binned features\n",
    "feature_columns.extend(binned_features)\n",
    "\n",
    "# Add original categorical features (with spaces)\n",
    "feature_columns.extend(['Soil Type', 'Crop Type'])\n",
    "\n",
    "# Add Soil_Crop interaction feature\n",
    "feature_columns.append('Soil_Crop')\n",
    "\n",
    "# Add other interaction features\n",
    "feature_columns.extend(interaction_features)\n",
    "\n",
    "# Add original numerical features\n",
    "feature_columns.extend(numerical_features)\n",
    "\n",
    "# DEBUG: Check what's actually in train before selecting columns\n",
    "print(f\"DEBUG: train.columns = {train.columns.tolist()}\")\n",
    "print(f\"DEBUG: 'Soil_Crop' in train.columns = {'Soil_Crop' in train.columns}\")\n",
    "print(f\"DEBUG: 'Crop Type' in train.columns = {'Crop Type' in train.columns}\")\n",
    "print(f\"DEBUG: 'Soil Type' in train.columns = {'Soil Type' in train.columns}\")\n",
    "\n",
    "# Verify all features exist\n",
    "print(\"Verifying all features exist in train:\")\n",
    "missing_features = []\n",
    "for col in feature_columns:\n",
    "    if col not in train.columns:\n",
    "        missing_features.append(col)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"MISSING FEATURES: {missing_features}\")\n",
    "else:\n",
    "    print(\"  All features present!\")\n",
    "\n",
    "# Create feature matrices\n",
    "X = train[feature_columns].copy()\n",
    "X_test = test[feature_columns].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix X shape: {X.shape}\")\n",
    "print(f\"Test feature matrix X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Ensure binned features are category dtype\n",
    "for col in binned_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "    X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(f\"\\nFinal feature columns ({len(feature_columns)}): {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be08dece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T12:32:26.350334Z",
     "iopub.status.busy": "2026-01-15T12:32:26.350164Z",
     "iopub.status.idle": "2026-01-15T12:32:26.425738Z",
     "shell.execute_reply": "2026-01-15T12:32:26.425316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up target encoding...\n",
      "Soil Type: 5 unique values\n",
      "Crop Type: 11 unique values\n",
      "Soil_Crop: 55 unique values\n",
      "Target encoding will be applied to: ['Soil Type', 'Crop Type', 'Soil_Crop']\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding with Leakage Prevention\n",
    "# Use sklearn's TargetEncoder with proper CV\n",
    "\n",
    "print(\"Setting up target encoding...\")\n",
    "\n",
    "# Features to target encode - use EXACT column names from CSV\n",
    "target_encode_features = ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "\n",
    "# Check if features exist and have reasonable cardinality\n",
    "for col in target_encode_features:\n",
    "    if col in train.columns:\n",
    "        unique_values = train[col].nunique()\n",
    "        print(f\"{col}: {unique_values} unique values\")\n",
    "    else:\n",
    "        print(f\"WARNING: {col} not found in data\")\n",
    "\n",
    "# We'll do target encoding inside the CV loop to prevent leakage\n",
    "# For now, just prepare the data structure\n",
    "target_encode_cols = target_encode_features\n",
    "\n",
    "print(f\"Target encoding will be applied to: {target_encode_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize predictions\n",
    "oof_predictions = np.zeros((len(X), len(le_target.classes_)))\n",
    "test_predictions_xgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_lgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_cat = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "\n",
    "# Model parameters\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,  # Shallower depth for categorical features\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'enable_categorical': True,  # Enable native categorical support\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'iterations': 500,\n",
    "    'bootstrap_type': 'Bernoulli',  # Required for subsample\n",
    "    'subsample': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': False,\n",
    "    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "}\n",
    "\n",
    "print(\"Model parameters defined\")\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation loop with proper target encoding\n",
    "fold_scores = []\n",
    "fold_scores_xgb = []\n",
    "fold_scores_lgb = []\n",
    "fold_scores_cat = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # DEBUG: Check columns in X_train before encoding\n",
    "    print(f\"DEBUG: Columns in X_train before encoding: {X_train.columns.tolist()}\")\n",
    "    print(f\"DEBUG: 'Crop Type' in X_train: {'Crop Type' in X_train.columns}\")\n",
    "    print(f\"DEBUG: 'Soil_Crop' in X_train: {'Soil_Crop' in X_train.columns}\")\n",
    "    \n",
    "    # Apply target encoding (fit on train, transform on val)\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Fit target encoder on training data\n",
    "    encoder = TargetEncoder(target_type='multiclass', smooth='auto')\n",
    "    encoder.fit(X_train[target_encode_cols], y_train)\n",
    "    \n",
    "    # Transform the target encoding features\n",
    "    # TargetEncoder returns 2D array for multiclass, need to flatten to 1D per feature\n",
    "    train_encoded = encoder.transform(X_train[target_encode_cols])\n",
    "    val_encoded = encoder.transform(X_val[target_encode_cols])\n",
    "    test_encoded = encoder.transform(X_test[target_encode_cols])\n",
    "    \n",
    "    # Flatten the 2D encoded arrays to 1D by taking mean across classes\n",
    "    for i, col in enumerate(target_encode_cols):\n",
    "        X_train_enc[col] = train_encoded[:, i].ravel()\n",
    "        X_val_enc[col] = val_encoded[:, i].ravel()\n",
    "        X_test_enc[col] = test_encoded[:, i].ravel()\n",
    "    \n",
    "    print(f\"  Target encoding applied to {len(target_encode_cols)} features\")\n",
    "    \n",
    "    # XGBoost model\n",
    "    print(\"  Training XGBoost...\")\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(X_train_enc, y_train, eval_set=[(X_val_enc, y_val)], verbose=False)\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred_xgb = model_xgb.predict_proba(X_val_enc)\n",
    "    test_pred_xgb = model_xgb.predict_proba(X_test_enc)\n",
    "    \n",
    "    # Store predictions\n",
    "    oof_predictions[val_idx] = val_pred_xgb\n",
    "    test_predictions_xgb += test_pred_xgb / n_splits\n",
    "    \n",
    "    # Fold score\n",
    "    fold_score_xgb = map_at_3(val_pred_xgb, y_val)\n",
    "    fold_scores_xgb.append(fold_score_xgb)\n",
    "    print(f\"  XGBoost MAP@3: {fold_score_xgb:.4f}\")\n",
    "    \n",
    "    # LightGBM model\n",
    "    print(\"  Training LightGBM...\")\n",
    "    \n",
    "    # For LightGBM, need to specify categorical features (only binned features)\n",
    "    cat_features_lgb = binned_features.copy()\n",
    "    \n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(\n",
    "        X_train_enc, y_train,\n",
    "        eval_set=[(X_val_enc, y_val)],\n",
    "        categorical_feature=cat_features_lgb\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred_lgb = model_lgb.predict_proba(X_val_enc)\n",
    "    test_pred_lgb = model_lgb.predict_proba(X_test_enc)\n",
    "    \n",
    "    # Store predictions\n",
    "    test_predictions_lgb += test_pred_lgb / n_splits\n",
    "    \n",
    "    # Fold score\n",
    "    fold_score_lgb = map_at_3(val_pred_lgb, y_val)\n",
    "    fold_scores_lgb.append(fold_score_lgb)\n",
    "    print(f\"  LightGBM MAP@3: {fold_score_lgb:.4f}\")\n",
    "    \n",
    "    # CatBoost model\n",
    "    print(\"  Training CatBoost...\")\n",
    "    \n",
    "    # For CatBoost, only binned features are categorical after target encoding\n",
    "    # The target encoded columns are now numerical\n",
    "    cat_features_cat = binned_features.copy()\n",
    "    \n",
    "    # Convert to Pool objects\n",
    "    train_pool = Pool(X_train_enc, y_train, cat_features=cat_features_cat)\n",
    "    val_pool = Pool(X_val_enc, y_val, cat_features=cat_features_cat)\n",
    "    test_pool = Pool(X_test_enc, cat_features=cat_features_cat)\n",
    "    \n",
    "    model_cat = CatBoostClassifier(**catboost_params)\n",
    "    model_cat.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    \n",
    "    # Predictions\n",
    "    val_pred_cat = model_cat.predict_proba(val_pool)\n",
    "    test_pred_cat = model_cat.predict_proba(test_pool)\n",
    "    \n",
    "    # Store predictions\n",
    "    test_predictions_cat += test_pred_cat / n_splits\n",
    "    \n",
    "    # Fold score\n",
    "    fold_score_cat = map_at_3(val_pred_cat, y_val)\n",
    "    fold_scores_cat.append(fold_score_cat)\n",
    "    print(f\"  CatBoost MAP@3: {fold_score_cat:.4f}\")\n",
    "    \n",
    "    # Average ensemble score for this fold\n",
    "    val_pred_ensemble = (val_pred_xgb + val_pred_lgb + val_pred_cat) / 3\n",
    "    fold_score = map_at_3(val_pred_ensemble, y_val)\n",
    "    fold_scores.append(fold_score)\n",
    "    print(f\"  Ensemble MAP@3: {fold_score:.4f}\")\n",
    "\n",
    "print(f\"\\nCV Results:\")\n",
    "print(f\"  XGBoost: {np.mean(fold_scores_xgb):.4f} ± {np.std(fold_scores_xgb):.4f}\")\n",
    "print(f\"  LightGBM: {np.mean(fold_scores_lgb):.4f} ± {np.std(fold_scores_lgb):.4f}\")\n",
    "print(f\"  CatBoost: {np.mean(fold_scores_cat):.4f} ± {np.std(fold_scores_cat):.4f}\")\n",
    "print(f\"  Ensemble: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918967a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final CV score\n",
    "cv_score = map_at_3(oof_predictions, y_encoded)\n",
    "print(f\"\\nFinal CV MAP@3 Score: {cv_score:.4f}\")\n",
    "print(f\"Improvement over baseline: {cv_score - 0.3311:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions from all three models\n",
    "final_test_predictions = (test_predictions_xgb + test_predictions_lgb + test_predictions_cat) / 3\n",
    "\n",
    "# Get top 3 predictions for each test sample\n",
    "top3_predictions = np.argsort(final_test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert back to fertilizer names\n",
    "predicted_names = []\n",
    "for pred in top3_predictions:\n",
    "    names = le_target.inverse_transform(pred)\n",
    "    predicted_names.append(' '.join(names))\n",
    "\n",
    "# Create submission\n",
    "test_ids = test['id'].values\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Fertilizer Name': predicted_names\n",
    "})\n",
    "\n",
    "print(\"Submission format:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"Submission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee52cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment folder\n",
    "os.makedirs('experiments/002_categorical_target_encoding', exist_ok=True)\n",
    "\n",
    "# Save OOF predictions and test predictions\n",
    "np.save('experiments/002_categorical_target_encoding/oof_predictions.npy', oof_predictions)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_xgb.npy', test_predictions_xgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_lgb.npy', test_predictions_lgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_cat.npy', test_predictions_cat)\n",
    "\n",
    "# Save feature information\n",
    "with open('experiments/002_categorical_target_encoding/feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'categorical_features': all_categorical,\n",
    "        'numerical_features': all_numerical,\n",
    "        'target_encode_cols': target_encode_cols,\n",
    "        'le_target': le_target\n",
    "    }, f)\n",
    "\n",
    "print(\"Experiment artifacts saved to experiments/002_categorical_target_encoding/\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
