{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6097ab",
   "metadata": {},
   "source": [
    "# Enhanced Feature Engineering: Categorical Treatment + Target Encoding\n",
    "\n",
    "This experiment addresses evaluator concerns:\n",
    "1. Treat all numerical features as categorical (low cardinality: 14-43 unique values)\n",
    "2. Add interaction features\n",
    "3. Implement proper target encoding with leakage prevention\n",
    "4. Add CatBoost for model diversity\n",
    "5. Optimize directly for MAP@3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a09389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec68172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MAP@3 metric for optimization\n",
    "def map_at_3(predictions, true_labels):\n",
    "    \"\"\"Calculate MAP@3 score\"\"\"\n",
    "    map_scores = []\n",
    "    \n",
    "    for i in range(len(true_labels)):\n",
    "        # Get top 3 predictions\n",
    "        pred_idx = np.argsort(predictions[i])[-3:][::-1]\n",
    "        \n",
    "        # Calculate average precision for this observation\n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_idx, 1):\n",
    "            if pred == true_labels[i]:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "                break  # Only one correct label per observation\n",
    "        \n",
    "        map_scores.append(score)\n",
    "    \n",
    "    return np.mean(map_scores)\n",
    "\n",
    "print(\"MAP@3 metric defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Phase 1: Categorical Treatment\n",
    "# All numerical features have low cardinality (14-43 unique values) - treat as categorical\n",
    "\n",
    "# Original numerical features\n",
    "numerical_features = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "categorical_features = ['Soil Type', 'Crop Type']\n",
    "\n",
    "print(\"Converting numerical features to categorical via binning...\")\n",
    "\n",
    "# Create binned versions of numerical features (10 bins each)\n",
    "for col in numerical_features:\n",
    "    # Use qcut for equal frequency binning to ensure good distribution\n",
    "    train[f'{col}_binned'] = pd.qcut(train[col], q=10, labels=False, duplicates='drop')\n",
    "    test[f'{col}_binned'] = pd.qcut(test[col], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Convert to category dtype\n",
    "    train[f'{col}_binned'] = train[f'{col}_binned'].astype('category')\n",
    "    test[f'{col}_binned'] = test[f'{col}_binned'].astype('category')\n",
    "\n",
    "print(\"Binned features created\")\n",
    "\n",
    "# Keep original numerical features as backup\n",
    "train[numerical_features] = train[numerical_features].astype(float)\n",
    "test[numerical_features] = test[numerical_features].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Phase 2: Interaction Features\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Debug: Check current column names BEFORE creating Soil_Crop\n",
    "print(f\"Current train columns BEFORE Soil_Crop: {train.columns.tolist()}\")\n",
    "\n",
    "# Environmental interactions\n",
    "for df in [train, test]:\n",
    "    # Temp × Humidity\n",
    "    df['Temp_Humidity'] = df['Temparature'] * df['Humidity']\n",
    "    # Temp × Moisture  \n",
    "    df['Temp_Moisture'] = df['Temparature'] * df['Moisture']\n",
    "    # Humidity × Moisture\n",
    "    df['Humidity_Moisture'] = df['Humidity'] * df['Moisture']\n",
    "    \n",
    "    # Nutrient-Environment interactions\n",
    "    df['N_Temp'] = df['Nitrogen'] * df['Temparature']\n",
    "    df['P_Humidity'] = df['Phosphorous'] * df['Humidity']\n",
    "    df['K_Moisture'] = df['Potassium'] * df['Moisture']\n",
    "    \n",
    "    # NPK balance features\n",
    "    df['NPK_sum'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "    df['NPK_balance'] = df['Nitrogen'] / (df['NPK_sum'] + 1)\n",
    "    df['P_balance'] = df['Phosphorous'] / (df['NPK_sum'] + 1)\n",
    "    df['K_balance'] = df['Potassium'] / (df['NPK_sum'] + 1)\n",
    "    \n",
    "    # Soil-Crop interaction (critical for target encoding)\n",
    "    df['Soil_Crop'] = df['Soil Type'].astype(str) + '_' + df['Crop Type'].astype(str)\n",
    "    df['Soil_Crop'] = df['Soil_Crop'].astype('category')\n",
    "\n",
    "# Debug: Check column names AFTER creating Soil_Crop\n",
    "print(f\"Current train columns AFTER Soil_Crop: {train.columns.tolist()}\")\n",
    "print(f\"Soil_Crop feature created with {train['Soil_Crop'].nunique()} unique values\")\n",
    "\n",
    "print(\"Interaction features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature lists\n",
    "binned_features = [f'{col}_binned' for col in numerical_features]\n",
    "interaction_features = ['Temp_Humidity', 'Temp_Moisture', 'Humidity_Moisture', \n",
    "                       'N_Temp', 'P_Humidity', 'K_Moisture',\n",
    "                       'NPK_sum', 'NPK_balance', 'P_balance', 'K_balance']\n",
    "\n",
    "# All categorical features (binned + original categorical + interactions)\n",
    "# Use the EXACT column names from the CSV (with spaces)\n",
    "all_categorical = binned_features + ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "all_numerical = numerical_features + interaction_features\n",
    "\n",
    "print(f\"Categorical features ({len(all_categorical)}): {all_categorical}\")\n",
    "print(f\"Numerical features ({len(all_numerical)}): {len(all_numerical)}\")\n",
    "print(f\"Total features: {len(all_categorical) + len(all_numerical)}\")\n",
    "\n",
    "# Verify all features exist\n",
    "print(f\"\\nVerifying all features exist in train:\")\n",
    "missing_features = []\n",
    "for col in all_categorical + all_numerical:\n",
    "    if col not in train.columns:\n",
    "        missing_features.append(col)\n",
    "        print(f\"  MISSING: {col}\")\n",
    "\n",
    "if not missing_features:\n",
    "    print(\"  All features present!\")\n",
    "\n",
    "print(f\"\\nSoil_Crop sample values:\")\n",
    "print(train['Soil_Crop'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(train['Fertilizer Name'])\n",
    "print(f\"Classes: {le_target.classes_}\")\n",
    "print(f\"Number of classes: {len(le_target.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e60db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices\n",
    "# Use EXACT column names as they appear in the CSV\n",
    "feature_columns = []\n",
    "\n",
    "# Add binned features\n",
    "feature_columns.extend(binned_features)\n",
    "\n",
    "# Add original categorical features (with spaces)\n",
    "feature_columns.extend(['Soil Type', 'Crop Type'])\n",
    "\n",
    "# Add interaction features\n",
    "feature_columns.extend(['Soil_Crop'])\n",
    "feature_columns.extend(interaction_features)\n",
    "\n",
    "# Add original numerical features\n",
    "feature_columns.extend(numerical_features)\n",
    "\n",
    "# DEBUG: Check what's actually in train before selecting columns\n",
    "print(f\"DEBUG: train.columns = {train.columns.tolist()}\")\n",
    "print(f\"DEBUG: 'Soil_Crop' in train.columns = {'Soil_Crop' in train.columns}\")\n",
    "print(f\"DEBUG: 'Crop Type' in train.columns = {'Crop Type' in train.columns}\")\n",
    "\n",
    "# Verify all features exist\n",
    "print(\"Verifying all features exist in train:\")\n",
    "missing_features = []\n",
    "for col in feature_columns:\n",
    "    if col not in train.columns:\n",
    "        missing_features.append(col)\n",
    "        print(f\"  MISSING: {col}\")\n",
    "\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "print(\"All features verified!\")\n",
    "\n",
    "# Create feature matrices\n",
    "X = train[feature_columns].copy()\n",
    "X_test = test[feature_columns].copy()\n",
    "\n",
    "# Convert categorical features to category dtype\n",
    "categorical_for_conversion = binned_features + ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "for col in categorical_for_conversion:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "        print(f\"Converted {col} to category dtype\")\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"Final test matrix shape: {X_test.shape}\")\n",
    "print(f\"\\nFeature columns ({len(feature_columns)}): {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding with Leakage Prevention\n",
    "# Use sklearn's TargetEncoder with proper CV\n",
    "\n",
    "print(\"Setting up target encoding...\")\n",
    "\n",
    "# Features to target encode - use EXACT column names from CSV\n",
    "target_encode_features = ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "\n",
    "# Check if features exist and have reasonable cardinality\n",
    "for col in target_encode_features:\n",
    "    if col in train.columns:\n",
    "        unique_values = train[col].nunique()\n",
    "        print(f\"{col}: {unique_values} unique values\")\n",
    "    else:\n",
    "        print(f\"WARNING: {col} not found in data\")\n",
    "\n",
    "# We'll do target encoding inside the CV loop to prevent leakage\n",
    "# For now, just prepare the data structure\n",
    "target_encode_cols = target_encode_features\n",
    "\n",
    "print(f\"Target encoding will be applied to: {target_encode_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize predictions\n",
    "oof_predictions = np.zeros((len(X), len(le_target.classes_)))\n",
    "test_predictions_xgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_lgb = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "test_predictions_cat = np.zeros((len(X_test), len(le_target.classes_)))\n",
    "\n",
    "# Model parameters\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,  # Shallower depth for categorical features\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'n_estimators': 500,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'enable_categorical': True,  # Enable native categorical support\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_class': len(le_target.classes_),\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 7,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'eval_metric': 'MultiClass',\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'task_type': 'GPU' if os.environ.get('CUDA_VISIBLE_DEVICES') else 'CPU'\n",
    "}\n",
    "\n",
    "print(\"Starting cross-validation with target encoding...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation loop with proper target encoding\n",
    "fold_scores = []\n",
    "fold_scores_xgb = []\n",
    "fold_scores_lgb = []\n",
    "fold_scores_cat = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    # DEBUG: Check columns in X_train before encoding\n",
    "    print(f\"DEBUG: Columns in X_train before encoding: {X_train.columns.tolist()}\")\n",
    "    print(f\"DEBUG: 'Crop Type' in X_train: {'Crop Type' in X_train.columns}\")\n",
    "    print(f\"DEBUG: 'Soil_Crop' in X_train: {'Soil_Crop' in X_train.columns}\")\n",
    "    \n",
    "    # Apply target encoding (fit on train, transform on val)\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_val_enc = X_val.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Fit target encoder on training data\n",
    "    encoder = TargetEncoder(target_type='multiclass', smooth='auto')\n",
    "    encoder.fit(X_train[target_encode_cols], y_train)\n",
    "    \n",
    "    # Transform training, validation, and test data\n",
    "    for i, col in enumerate(target_encode_cols):\n",
    "        # Transform each column separately to get proper shape\n",
    "        train_col_encoded = encoder.transform(X_train[[col]])\n",
    "        val_col_encoded = encoder.transform(X_val[[col]])\n",
    "        test_col_encoded = encoder.transform(X_test[[col]])\n",
    "        \n",
    "        # The output is 2D with shape (n_samples, n_classes)\n",
    "        # Take mean across classes to get a single value per sample\n",
    "        if train_col_encoded.ndim == 2:\n",
    "            X_train_enc[f'{col}_te'] = train_col_encoded.mean(axis=1)\n",
    "            X_val_enc[f'{col}_te'] = val_col_encoded.mean(axis=1)\n",
    "            X_test_enc[f'{col}_te'] = test_col_encoded.mean(axis=1)\n",
    "        else:\n",
    "            # Fallback for 1D output\n",
    "            X_train_enc[f'{col}_te'] = train_col_encoded.ravel()\n",
    "            X_val_enc[f'{col}_te'] = val_col_encoded.ravel()\n",
    "            X_test_enc[f'{col}_te'] = test_col_encoded.ravel()\n",
    "    \n",
    "    # Ensure all datasets have the same columns in the same order\n",
    "    feature_cols = X_train_enc.columns.tolist()\n",
    "    X_val_enc = X_val_enc[feature_cols]\n",
    "    X_test_enc = X_test_enc[feature_cols]\n",
    "    \n",
    "    # DEBUG: Check if Crop Type and Soil_Crop are still present\n",
    "    print(f\"DEBUG: Columns in X_train_enc: {X_train_enc.columns.tolist()}\")\n",
    "    print(f\"DEBUG: 'Crop Type' in X_train_enc: {'Crop Type' in X_train_enc.columns}\")\n",
    "    print(f\"DEBUG: 'Soil_Crop' in X_train_enc: {'Soil_Crop' in X_train_enc.columns}\")\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    model_xgb = xgb.XGBClassifier(**xgb_params)\n",
    "    model_xgb.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print(\"Training LightGBM...\")\n",
    "    model_lgb = lgb.LGBMClassifier(**lgb_params)\n",
    "    model_lgb.fit(X_train_enc, y_train)\n",
    "    \n",
    "    # Train CatBoost\n",
    "    print(\"Training CatBoost...\")\n",
    "    # CatBoost requires Pool objects for categorical features\n",
    "    # Only include categorical features that exist in the dataframe\n",
    "    cat_features_indices = []\n",
    "    categorical_cols_for_catboost = binned_features + ['Soil Type', 'Crop Type', 'Soil_Crop']\n",
    "    for col in categorical_cols_for_catboost:\n",
    "        if col in X_train_enc.columns:\n",
    "            cat_features_indices.append(X_train_enc.columns.get_loc(col))\n",
    "    \n",
    "    print(f\"CatBoost categorical features: {categorical_cols_for_catboost}\")\n",
    "    print(f\"CatBoost categorical indices: {cat_features_indices}\")\n",
    "    print(f\"CatBoost will use columns: {[X_train_enc.columns[i] for i in cat_features_indices]}\")\n",
    "    \n",
    "    train_pool = Pool(X_train_enc, y_train, cat_features=cat_features_indices)\n",
    "    val_pool = Pool(X_val_enc, y_val, cat_features=cat_features_indices)\n",
    "    \n",
    "    model_cat = CatBoostClassifier(**cat_params)\n",
    "    model_cat.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    pred_xgb = model_xgb.predict_proba(X_val_enc)\n",
    "    pred_lgb = model_lgb.predict_proba(X_val_enc)\n",
    "    pred_cat = model_cat.predict_proba(val_pool)\n",
    "    \n",
    "    # Average predictions (equal weighting for now)\n",
    "    pred_avg = (pred_xgb + pred_lgb + pred_cat) / 3\n",
    "    oof_predictions[val_idx] = pred_avg\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_pred_xgb = model_xgb.predict_proba(X_test_enc)\n",
    "    test_pred_lgb = model_lgb.predict_proba(X_test_enc)\n",
    "    \n",
    "    test_pool = Pool(X_test_enc, cat_features=cat_features_indices)\n",
    "    test_pred_cat = model_cat.predict_proba(test_pool)\n",
    "    \n",
    "    test_predictions_xgb += test_pred_xgb / n_splits\n",
    "    test_predictions_lgb += test_pred_lgb / n_splits\n",
    "    test_predictions_cat += test_pred_cat / n_splits\n",
    "    \n",
    "    # Calculate fold scores using proper MAP@3\n",
    "    score_xgb = map_at_3(pred_xgb, y_val)\n",
    "    score_lgb = map_at_3(pred_lgb, y_val)\n",
    "    score_cat = map_at_3(pred_cat, y_val)\n",
    "    score_avg = map_at_3(pred_avg, y_val)\n",
    "    \n",
    "    fold_scores_xgb.append(score_xgb)\n",
    "    fold_scores_lgb.append(score_lgb)\n",
    "    fold_scores_cat.append(score_cat)\n",
    "    fold_scores.append(score_avg)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} MAP@3 - XGB: {score_xgb:.4f}, LGB: {score_lgb:.4f}, CAT: {score_cat:.4f}, AVG: {score_avg:.4f}\")\n",
    "\n",
    "print(f\"\\nMean MAP@3 - XGB: {np.mean(fold_scores_xgb):.4f} ± {np.std(fold_scores_xgb):.4f}\")\n",
    "print(f\"Mean MAP@3 - LGB: {np.mean(fold_scores_lgb):.4f} ± {np.std(fold_scores_lgb):.4f}\")\n",
    "print(f\"Mean MAP@3 - CAT: {np.mean(fold_scores_cat):.4f} ± {np.std(fold_scores_cat):.4f}\")\n",
    "print(f\"Mean MAP@3 - AVG: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918967a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final CV score\n",
    "cv_score = map_at_3(oof_predictions, y_encoded)\n",
    "print(f\"\\nFinal CV MAP@3 Score: {cv_score:.4f}\")\n",
    "print(f\"Improvement over baseline: {cv_score - 0.3311:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predictions from all three models\n",
    "final_test_predictions = (test_predictions_xgb + test_predictions_lgb + test_predictions_cat) / 3\n",
    "\n",
    "# Get top 3 predictions for each test sample\n",
    "top3_predictions = np.argsort(final_test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert back to fertilizer names\n",
    "predicted_names = []\n",
    "for pred in top3_predictions:\n",
    "    names = le_target.inverse_transform(pred)\n",
    "    predicted_names.append(' '.join(names))\n",
    "\n",
    "# Create submission\n",
    "test_ids = test['id'].values\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'Fertilizer Name': predicted_names\n",
    "})\n",
    "\n",
    "print(\"Submission format:\")\n",
    "print(submission.head())\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('/home/submission/submission.csv', index=False)\n",
    "print(\"Submission saved to /home/submission/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee52cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment folder\n",
    "os.makedirs('experiments/002_categorical_target_encoding', exist_ok=True)\n",
    "\n",
    "# Save OOF predictions and test predictions\n",
    "np.save('experiments/002_categorical_target_encoding/oof_predictions.npy', oof_predictions)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_xgb.npy', test_predictions_xgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_lgb.npy', test_predictions_lgb)\n",
    "np.save('experiments/002_categorical_target_encoding/test_predictions_cat.npy', test_predictions_cat)\n",
    "\n",
    "# Save feature information\n",
    "with open('experiments/002_categorical_target_encoding/feature_info.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'categorical_features': all_categorical,\n",
    "        'numerical_features': all_numerical,\n",
    "        'target_encode_cols': target_encode_cols,\n",
    "        'le_target': le_target\n",
    "    }, f)\n",
    "\n",
    "print(\"Experiment artifacts saved to experiments/002_categorical_target_encoding/\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
