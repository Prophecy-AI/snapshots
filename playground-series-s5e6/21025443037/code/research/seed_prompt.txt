## Current Status
- Best CV score: 0.4348 from evolver_loop3_winners_analysis (pure categorical treatment)
- Best LB score: None submitted yet
- CV-LB gap: Unknown (need calibration)
- Previous experiment (exp_001) failed with -0.0094 regression due to binning
- Breakthrough finding: Pure categorical treatment yields +0.1037 improvement

## Response to Evaluator
- **Technical verdict was CONCERNS**: The evaluator correctly identified that binning could destroy information and that we needed proper categorical treatment. This was validated - binning caused the regression in exp_001.
- **Evaluator's top priority**: "Implement aggressive feature engineering treating all features as categorical" - This was EXACTLY correct. Pure categorical treatment (no binning) yields massive improvement from 0.3311 → 0.4348.
- **Key concerns addressed**:
  - **Leakage**: Using proper 5-fold CV with encoded targets, no combined train/test encoding
  - **Feature engineering**: Pure categorical treatment is the breakthrough we needed
  - **Model diversity**: Will add CatBoost to XGBoost/LightGBM ensemble
  - **Hyperparameters**: Tuned specifically for categorical (depth 6-7, lr 0.07)

## Data Understanding
- See `exploration/evolver_loop3_winners_analysis.ipynb` for complete analysis
- **Critical finding**: All numerical features have low cardinality (14-43 unique values)
- **Breakthrough**: Converting to category dtype (as strings) preserves ordinal relationships while enabling XGBoost native categorical support
- **Best hyperparameters**: max_depth=6, learning_rate=0.07 for categorical treatment
- **Target encoding**: Not yet tested with pure categorical - may add if needed

## Recommended Approaches
Priority-ordered list of what to try next:

### 1. Pure Categorical Treatment (CONFIRMED WORKING - HIGHEST PRIORITY)
**What**: Convert all features to categorical dtype without binning
**How**:
- Convert numerical features (Temp, Humidity, Moisture, N, P, K) to strings, then category dtype
- Keep Soil Type and Crop Type as categorical
- Use XGBoost with enable_categorical=True, tree_method='hist', max_depth=6-7
- Use LightGBM with categorical_feature parameter
- Add CatBoost with native categorical support
**Expected**: CV 0.430+ (already achieved 0.4348 in analysis)

### 2. Model Diversity - Add CatBoost (HIGH PRIORITY)
**Why**: Different algorithm = ensemble diversity, handles categorical natively
**Config**: depth=6-7, learning_rate=0.07, iterations=500
**Features**: Pass categorical features list to CatBoost
**Expected**: +0.005-0.010 improvement through diversity

### 3. Target Encoding (MEDIUM PRIORITY - TEST CAREFULLY)
**Why**: Winning solutions used it, but our implementation failed in exp_001
**When**: Only if pure categorical alone doesn't reach 0.440+
**How**: 5-fold CV target encoding on Soil Type, Crop Type, and their interaction
**Caution**: Must implement correctly - use mean(axis=1) to flatten 2D output
**Test**: Compare CV with and without target encoding

### 4. Interaction Features (LOW PRIORITY - DEFER)
**Why**: May add noise - focus on getting categorical treatment working first
**When**: After confirming pure categorical + CatBoost ensemble works
**What**: Temp×Humidity, N×Temp, Soil×Crop interactions
**Test**: Validate each interaction improves CV before keeping

### 5. Hyperparameter Tuning (MEDIUM PRIORITY)
**Current best**: depth=6, lr=0.07 (from analysis)
**Test**: Slight variations around these values
**Focus**: subsample, colsample_bytree for regularization
**Avoid**: Don't spend too much time here - features matter more

## What NOT to Try
- ❌ Binning numerical features (destroyed information in exp_001)
- ❌ Target encoding without proper implementation (caused 2D array errors)
- ❌ Adding many interaction features before validating categorical treatment
- ❌ Using depth > 8 for categorical features (winners found 6-8 optimal)
- ❌ Spending excessive time on hyperparameter tuning (diminishing returns)

## Validation Notes
- Use stratified 5-fold CV (already correct)
- Encode target to integers before CV split
- Convert features to category dtype before modeling
- Track per-fold scores to monitor variance
- Expected CV: 0.430-0.440 (should beat baseline by 0.100+)
- If CV < 0.420, revisit implementation - something is wrong

## Implementation Plan for exp_003
1. Load data and encode target to integers
2. Convert all features to category dtype (numericals as strings)
3. Train 3 models: XGBoost, LightGBM, CatBoost
4. Use optimal hyperparameters: depth=6-7, lr=0.07
5. Create ensemble by averaging predictions
6. Expected CV: 0.430+
7. Submit to LB for calibration

## Success Criteria
- CV MAP@3 ≥ 0.430 (improvement ≥ 0.100 from baseline)
- All 3 models perform competitively (within 0.010 of each other)
- Fold variance reasonable (not too low, not too high)
- No data leakage in categorical encoding