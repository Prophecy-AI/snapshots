## Current Status
- Best CV score: 0.3311 from exp_000 (baseline XGBoost + LightGBM)
- Target score: 0.3865 (gap: 0.0554)
- No LB submissions yet - need calibration
- Evaluator flagged CONCERNS: data leakage, minimal feature engineering

## Response to Evaluator
- **Technical verdict was CONCERNS**: Agree on leakage risk from combined train/test encoding. Will implement proper fold-wise target encoding.
- **Evaluator's top priority**: "Implement aggressive feature engineering treating all features as categorical" - This is CORRECT and aligns with winning solution analysis. Will prioritize this.
- **Key concerns addressed**:
  - **Leakage**: Implement proper 5-fold CV target encoding (fit on train fold only)
  - **Minimal features**: Add categorical treatment, interaction features, target encoding
  - **Metric mismatch**: Will optimize directly for MAP@3, not top-3 accuracy
  - **Model diversity**: Adding CatBoost to XGBoost/LightGBM
  - **Data augmentation**: Consider original dataset weighting (4:1 ratio per 2nd place)

## Data Understanding
- See `exploration/evolver_loop2_analysis.ipynb` for complete analysis
- **Critical finding**: All numerical features have low cardinality (14-43 unique values)
  - Temperature: 14 values, Humidity: 23, Moisture: 41, Nitrogen: 39, Potassium: 20, Phosphorous: 43
- **Winning solution insight**: Treat these as CATEGORICAL, not continuous
- TargetEncoder returns 2D array for multiclass - must flatten with .ravel() or .mean(axis=1)

## Recommended Approaches (Priority Order)

### 1. Categorical Treatment of Numericals (HIGHEST IMPACT)
**Why**: Winning solutions found this yields +0.015 improvement. Allows shallower trees (depth 7-8).
**How**: 
- Bin all numerical features into 10 categories using pd.cut()
- Convert to pandas category dtype
- Use XGBoost enable_categorical=True, LightGBM categorical_feature parameter
- Keep original numericals as backup features

### 2. Interaction Features (HIGH IMPACT)
**Why**: +0.010 estimated improvement. Captures non-linear relationships.
**What to add**:
- Environmental: Temp×Humidity, Temp×Moisture, Humidity×Moisture
- Nutrient-Environment: N×Temp, P×Humidity, K×Moisture  
- Soil-Crop: Soil_Type × Crop_Type (already have as 'Soil_Crop')
- Keep existing: NPK_product, NPK_ratio, N_to_P, K_to_N

### 3. Target Encoding with Leakage Prevention (MODERATE IMPACT)
**Why**: +0.008 estimated. Fixes evaluator's leakage concern.
**Implementation**:
- Use sklearn's TargetEncoder with target_type='multiclass'
- 5-fold CV: fit on train fold, transform validation fold
- Encode: Soil Type, Crop Type, Soil_Crop interaction
- Handle 2D output: use .mean(axis=1) to flatten to 1D
- CRITICAL: Never fit on test data, only transform

### 4. Model Diversity - Add CatBoost (MODERATE IMPACT)
**Why**: +0.005 estimated. Different algorithm = ensemble diversity.
**Config**: depth=7-8, learning_rate=0.05, iterations=500
**Advantage**: Native categorical support, handles high-cardinality well

### 5. Validation & Metric Fix (CRITICAL)
**Why**: Evaluator noted metric mismatch (top-3 accuracy vs MAP@3)
**Fix**: Optimize directly for MAP@3 in hyperparameter tuning
**CV Scheme**: Stratified 5-fold (keep this, it's correct)
**Monitoring**: Track both MAP@3 and per-fold variance (should increase from 0.1720-0.1728)

### 6. Data Augmentation (OPTIONAL, +0.003-0.005)
**Why**: 2nd place used 4:1 weighting (original:synthetic)
**Try**: Incorporate original Fertilizer Prediction dataset if available
**If not available**: Skip for now, focus on features first

## What NOT to Try
- ❌ Don't use combined train/test for encoding (leakage)
- ❌ Don't treat numericals as continuous (suboptimal per winners)
- ❌ Don't optimize for top-3 accuracy (wrong metric)
- ❌ Don't skip interaction features (missing signal)
- ❌ Don't use same hyperparameters as baseline (need tuning for categoricals)

## Validation Notes
- Use stratified 5-fold CV (already correct)
- Monitor fold variance - should increase from current 0.1720-0.1728 (too stable = leakage)
- Target encoding: fit on train_idx only, transform val_idx
- Categorical features: convert to category dtype before model training
- Expected CV: 0.360+ (0.030+ improvement from baseline)
- If CV < 0.350, revisit feature engineering before ensembling

## Expected Timeline
- This experiment should close ~0.030-0.035 of the 0.0554 gap
- Remaining gap (0.020-0.025) will need: stacking, more model diversity, data augmentation
- Submit to LB after this experiment to verify CV-LB correlation

## Success Criteria
- CV MAP@3 ≥ 0.360 (improvement ≥ 0.030)
- Fold variance increases (not all folds 0.172x)
- No data leakage in encoding pipeline
- CatBoost model performs competitively with XGBoost/LightGBM