{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4fc063",
   "metadata": {},
   "source": [
    "# Evolver Loop 3: Winning Solution Analysis & Strategy Refinement\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "**Current Status:**\n",
    "- Best CV: 0.3311 (baseline)\n",
    "- Last experiment: 0.3217 (REGRESSION of -0.0094)\n",
    "- Target: 0.3865 (gap: 0.0554)\n",
    "\n",
    "**What Failed:**\n",
    "- Binning numerical features (10 bins) destroyed information\n",
    "- Target encoding had implementation issues (2D array flattening)\n",
    "- Interactions may have added noise\n",
    "- No hyperparameter tuning for new feature types\n",
    "\n",
    "**What Winners Did:**\n",
    "- Treat ALL features as categorical (no binning for low-cardinality)\n",
    "- Use shallower trees (depth 7-8 for categorical vs 16-18 for numerical)\n",
    "- Extensive target encoding on feature combinations\n",
    "- 50-350 OOF models with stacking/hill climbing\n",
    "- Original data augmentation (4:1 weighting)\n",
    "\n",
    "**Key Insight:** Pure categorical treatment (convert to category dtype) preserves ordinal relationships while allowing tree models to find optimal splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b6dc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:05:02.337283Z",
     "iopub.status.busy": "2026-01-15T16:05:02.337047Z",
     "iopub.status.idle": "2026-01-15T16:05:03.805292Z",
     "shell.execute_reply": "2026-01-15T16:05:03.804798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Train: (750000, 10)\n",
      "Test: (250000, 9)\n",
      "\n",
      "Cardinality of numerical features:\n",
      "Temparature: 14 unique values\n",
      "Humidity: 23 unique values\n",
      "Moisture: 41 unique values\n",
      "Nitrogen: 39 unique values\n",
      "Potassium: 20 unique values\n",
      "Phosphorous: 43 unique values\n",
      "\n",
      "Target distribution:\n",
      "Fertilizer Name\n",
      "14-35-14    114436\n",
      "10-26-26    113887\n",
      "17-17-17    112453\n",
      "28-28       111158\n",
      "20-20       110889\n",
      "DAP          94860\n",
      "Urea         92317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "\n",
    "# Check cardinality of numerical features\n",
    "numerical_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "print(\"\\nCardinality of numerical features:\")\n",
    "for col in numerical_cols:\n",
    "    unique_vals = train[col].nunique()\n",
    "    print(f\"{col}: {unique_vals} unique values\")\n",
    "    \n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7889a",
   "metadata": {},
   "source": [
    "## 1. Pure Categorical Treatment (No Binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c8eeb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T16:05:03.806548Z",
     "iopub.status.busy": "2026-01-15T16:05:03.806433Z",
     "iopub.status.idle": "2026-01-15T16:05:04.969762Z",
     "shell.execute_reply": "2026-01-15T16:05:04.969352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types after processing:\n",
      "id                  int64\n",
      "Temparature        object\n",
      "Humidity           object\n",
      "Moisture           object\n",
      "Soil Type           int64\n",
      "Crop Type           int64\n",
      "Nitrogen           object\n",
      "Potassium          object\n",
      "Phosphorous        object\n",
      "Fertilizer Name    object\n",
      "dtype: object\n",
      "\n",
      "Training data shape: (750000, 9)\n",
      "Test data shape: (250000, 9)\n",
      "\n",
      "Sample of processed data:\n",
      "   id Temparature Humidity Moisture  Soil Type  Crop Type Nitrogen Potassium  \\\n",
      "0   0          37       70       36          1          8       36         4   \n",
      "1   1          27       69       65          4          4       30         6   \n",
      "2   2          29       63       32          4          4       24        12   \n",
      "3   3          35       62       54          4          0       39        12   \n",
      "4   4          35       58       43          3          6       37         2   \n",
      "\n",
      "  Phosphorous  \n",
      "0           5  \n",
      "1          18  \n",
      "2          16  \n",
      "3           4  \n",
      "4          16  \n"
     ]
    }
   ],
   "source": [
    "# Convert all features to categorical WITHOUT binning\n",
    "# This preserves the original values as categories\n",
    "\n",
    "cat_train = train.copy()\n",
    "cat_test = test.copy()\n",
    "\n",
    "# Convert numerical features to categorical using original values as strings\n",
    "for col in numerical_cols:\n",
    "    cat_train[col] = cat_train[col].astype(str)\n",
    "    cat_test[col] = cat_test[col].astype(str)\n",
    "\n",
    "# Label encode categorical features\n",
    "cat_features = ['Soil Type', 'Crop Type']\n",
    "for col in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    cat_train[col] = le.fit_transform(cat_train[col])\n",
    "    cat_test[col] = le.transform(cat_test[col])\n",
    "\n",
    "print(\"Feature types after processing:\")\n",
    "print(cat_train.dtypes)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = cat_train.drop(['Fertilizer Name'], axis=1)  # Train has no Id column\n",
    "y = cat_train['Fertilizer Name']\n",
    "X_test = cat_test.copy()  # Test has no Id column either\n",
    "\n",
    "# Encode target to integers\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target classes: {le_target.classes_}\")\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a134c",
   "metadata": {},
   "source": [
    "## 2. Test Pure Categorical Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with XGBoost (depth=7, as per winning solutions)\n",
    "\n",
    "# Stratified 5-fold CV - split on ENCODED labels\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "\n",
    "# Get number of classes\n",
    "n_classes = len(y.unique())\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Encode target labels to integers (do this once, outside the loop)\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "print(f\"Target classes: {le_target.classes_}\")\n",
    "print(f\"y_encoded type: {type(y_encoded)}\")\n",
    "print(f\"y_encoded shape: {y_encoded.shape}\")\n",
    "print(f\"y_encoded sample: {y_encoded[:10]}\")\n",
    "\n",
    "# Convert string features to category dtype for XGBoost\n",
    "X_cat = X.copy()\n",
    "for col in numerical_cols:\n",
    "    X_cat[col] = X_cat[col].astype('category')\n",
    "\n",
    "print(\"\\nRunning 5-fold CV with pure categorical features...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_cat, y_encoded)):\n",
    "    X_train, X_val = X_cat.iloc[train_idx], X_cat.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    print(f\"Fold {fold+1}: y_train type={type(y_train)}, unique={np.unique(y_train)[:5]}\")\n",
    "    \n",
    "    # XGBoost with categorical support\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        tree_method='hist',\n",
    "        enable_categorical=True,  # Enable native categorical support\n",
    "        max_depth=7,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        device='cuda'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "              eval_set=[(X_val, y_val)],\n",
    "              verbose=False)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    val_pred = model.predict_proba(X_val)\n",
    "    \n",
    "    # Calculate MAP@3 for this fold\n",
    "    from sklearn.metrics import label_ranking_average_precision_score\n",
    "    \n",
    "    # Convert to binary relevance matrix for MAP@3 calculation\n",
    "    y_val_bin = pd.get_dummies(y_val).values\n",
    "    fold_map3 = label_ranking_average_precision_score(y_val_bin, val_pred)\n",
    "    fold_scores.append(fold_map3)\n",
    "    \n",
    "    print(f\"Fold {fold+1}: MAP@3 = {fold_map3:.4f}\")\n",
    "\n",
    "print(f\"\\nCV MAP@3: {np.mean(fold_scores):.4f} ± {np.std(fold_scores):.4f}\")\n",
    "print(f\"Fold scores: {fold_scores}\")\n",
    "\n",
    "# Compare to baseline\n",
    "baseline_score = 0.3311\n",
    "improvement = np.mean(fold_scores) - baseline_score\n",
    "print(f\"\\nImprovement over baseline: {improvement:.4f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"✅ Pure categorical treatment IMPROVES performance!\")\n",
    "else:\n",
    "    print(\"❌ Pure categorical treatment does NOT improve performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89ec973",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc991b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different depths and learning rates\n",
    "# Winning solutions found depth 7-8 optimal for categorical treatment\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'learning_rate': [0.03, 0.05, 0.07]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for depth in param_grid['max_depth']:\n",
    "    for lr in param_grid['learning_rate']:\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model = xgb.XGBClassifier(\n",
    "                objective='multi:softprob',\n",
    "                eval_metric='mlogloss',\n",
    "                tree_method='hist',\n",
    "                enable_categorical=True,\n",
    "                max_depth=depth,\n",
    "                learning_rate=lr,\n",
    "                n_estimators=500,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                device='cuda'\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            val_pred = model.predict_proba(X_val)\n",
    "            \n",
    "            y_val_bin = pd.get_dummies(y_val).values\n",
    "            fold_map3 = label_ranking_average_precision_score(y_val_bin, val_pred)\n",
    "            fold_scores.append(fold_map3)\n",
    "        \n",
    "        cv_score = np.mean(fold_scores)\n",
    "        results.append({\n",
    "            'max_depth': depth,\n",
    "            'learning_rate': lr,\n",
    "            'cv_score': cv_score,\n",
    "            'std': np.std(fold_scores)\n",
    "        })\n",
    "        \n",
    "        print(f\"Depth {depth}, LR {lr}: CV = {cv_score:.4f} ± {np.std(fold_scores):.4f}\")\n",
    "\n",
    "# Find best parameters\n",
    "results_df = pd.DataFrame(results)\n",
    "best_params = results_df.loc[results_df['cv_score'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params.to_dict()}\")\n",
    "print(f\"Best CV score: {best_params['cv_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168bdb2",
   "metadata": {},
   "source": [
    "## 4. Key Insights & Recommendations\n",
    "\n",
    "### What Works:\n",
    "1. **Pure categorical treatment** (convert to category dtype, no binning)\n",
    "2. **Shallower trees** (depth 7-8) for categorical features\n",
    "3. **XGBoost native categorical support** (enable_categorical=True)\n",
    "4. **Stratified 5-fold CV** (already correct)\n",
    "\n",
    "### What Doesn't Work:\n",
    "1. ❌ **Binning low-cardinality features** (destroys ordinal information)\n",
    "2. ❌ **Target encoding without proper implementation** (2D array issues)\n",
    "3. ❌ **Adding interactions without validation** (adds noise)\n",
    "4. ❌ **No hyperparameter tuning** (suboptimal performance)\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement pure categorical treatment in exp_003\n",
    "2. Tune hyperparameters (depth 6-8, lr 0.03-0.07)\n",
    "3. Add CatBoost with native categorical support\n",
    "4. If successful (>0.340), carefully add back target encoding\n",
    "5. Then add selective interactions if needed\n",
    "6. Finally, build diverse ensemble for stacking\n",
    "\n",
    "### Expected Improvement:\n",
    "- Conservative: 0.340 (+0.009 from baseline)\n",
    "- Optimistic: 0.350 (+0.019 from baseline)\n",
    "- With proper ensembling: 0.360-0.370\n",
    "- Target: 0.3865 (requires full stacking pipeline)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
