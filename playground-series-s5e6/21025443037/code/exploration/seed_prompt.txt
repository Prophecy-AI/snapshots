## Competition Overview
This is a multi-class classification problem predicting Fertilizer Name based on soil and crop conditions. Evaluation metric is MAP@3 (Mean Average Precision at 3).

**Reference notebooks for data characteristics:**
- See exploration notebooks for feature distributions, target cardinality, and data characteristics
- Dataset has ~750K training samples, ~250K test samples
- Features include: Temperature, Humidity, Moisture, Soil Type, Crop Type, Nitrogen, Potassium, Phosphorous
- Target: Fertilizer Name (categorical)

## Winning Solution Patterns

### Model Diversity is Critical
Top solutions emphasize building diverse ensembles of 50-60+ models:
- **Primary models**: XGBoost (most important), LightGBM, CatBoost
- **Secondary models**: Neural Networks (various architectures), Logistic Regression, Multinomial NB, Gradient Boosted Trees
- **Key insight**: Diversity in model types beats minor hyperparameter tuning

### Feature Engineering
- **Minimal FE works best**: Most winners found extensive feature engineering hurt performance
- **Product features**: Some success with simple product features (e.g., N*P*K interactions)
- **Label encoding accidents**: One winner accidentally label encoded features and it worked well
- **Categorical treatment**: Treating all features as categorical often improved scores

### Data Augmentation
- **Original dataset**: Adding 1-6 copies of original Fertilizer prediction dataset significantly improved scores
- **Avoid over-weighting**: More than 6-8 copies biases away from competition data
- **Synthetic data awareness**: Competition data is synthetically generated from original dataset

### Ensembling Strategies

#### Ridge Regression Stacking (3rd place)
- **Method**: Ridge regression as final ensembler
- **Input shape**: Stack OOF predictions as (n_samples, n_models * n_classes)
- **Target encoding**: One-hot encode target, use multi-target regression
- **Advantages**: Fast (~1 min for 60 models), consistently improves with more models
- **Alternative**: LinearDiscriminantAnalysis as stacker showed good CV-LB alignment

#### Hill Climbing (5th, 6th, 21st place)
- **Method**: Use hill climbing to optimize ensemble weights
- **Implementation**: Save OOF predictions, then optimize weights to maximize MAP@3
- **Challenge**: Adding models can sometimes decrease score - requires careful validation
- **Best practice**: Start with strong base models, add incrementally

#### Simple Averaging
- **6th place**: Used mean between logistic regression and hill climbing
- **Weighting**: XGBoost (70%) + LightGBM (30%) based on OOF contributions

### Model-Specific Techniques

#### XGBoost Optimizations
- **sampling_method='gradient_based'**: Worked better even with high subsample
- **refresh_leaf=0**: Slight improvement, potential overfitting risk
- **Learning rate**: Most models used lr=0.05 for faster training
- **Categorical handling**: Treat features as categorical when possible

#### LightGBM Variants
- **GOSS and DART**: Used for diversity
- **Hyperparameter variations**: depth, estimators, max leaves, scoring functions

#### Neural Networks
- **All-embedding approach**: Embed all features including numerical
- **Libraries**: pytabkit, pytorch-tabular, rtdl_num_embeddings for diverse architectures
- **Regression approach**: Train with MSE loss on one-hot encoded targets

### Validation Strategy
- **Primary**: Stratified K-Fold (5-10 folds)
- **Key principle**: Trust CV scores, but watch for CV-LB misalignment
- **Best practice**: 10-fold CV used by multiple winners

### Hyperparameter Diversity
- **Vary systematically**: depth, estimators, max leaves, scoring metrics
- **GOSS/DART**: Use different LightGBM boosting types
- **Fold variations**: Different k-fold splits add diversity

### What Didn't Work
- **CatBoost**: Consistently underperformed for most winners
- **Extensive feature engineering**: Generally hurt performance
- **Over-reliance on public notebooks**: Single bad OOF can hurt ensemble
- **Too many original dataset copies**: >6-8 copies degrades performance

### Key Libraries
- **pytabkit**: For TabM, RealMLP models
- **pytorch-tabular**: For Gandalf NN and other architectures  
- **rtdl_num_embeddings**: For neural network embeddings
- **AutoGluon**: For automated model selection

### Computational Considerations
- **Training time**: Most models trained with lr=0.05 for speed
- **Local training**: 6th place trained all models locally on 4070 Super
- **CPU bottlenecks**: Logistic regression can be slow (3+ hours)

### Final Recommendations
1. **Start with XGBoost and LightGBM** as foundation
2. **Add diverse model types** (NNs, CatBoost, Logistic Regression)
3. **Use Ridge regression** for fast, effective ensembling
4. **Add 2-4 copies** of original dataset
5. **Treat features as categorical**
6. **Trust CV** but validate against LB
7. **Build ensemble incrementally** using Ridge to avoid overfitting
8. **Avoid complex feature engineering** - simple interactions work better