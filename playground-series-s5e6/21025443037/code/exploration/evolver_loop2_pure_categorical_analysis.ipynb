{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87af3c2c",
   "metadata": {},
   "source": [
    "# Evolver Loop 2: Pure Categorical Treatment Analysis\n",
    "\n",
    "This notebook validates the pure categorical approach recommended by the evaluator.\n",
    "We test whether treating numerical features as categorical WITHOUT binning improves performance.\n",
    "\n",
    "**Hypothesis**: Direct categorical treatment (no binning) preserves ordinal relationships and allows XGBoost/LightGBM to learn optimal splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd8288a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T14:06:07.649326Z",
     "iopub.status.busy": "2026-01-15T14:06:07.649082Z",
     "iopub.status.idle": "2026-01-15T14:06:08.148763Z",
     "shell.execute_reply": "2026-01-15T14:06:08.148343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Train: (750000, 10)\n",
      "Test: (250000, 9)\n",
      "\n",
      "Target distribution:\n",
      "Fertilizer Name\n",
      "14-35-14    114436\n",
      "10-26-26    113887\n",
      "17-17-17    112453\n",
      "28-28       111158\n",
      "20-20       110889\n",
      "DAP          94860\n",
      "Urea         92317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/data/train.csv')\n",
    "test = pd.read_csv('/home/data/test.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Test: {test.shape}\")\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train['Fertilizer Name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d11a3",
   "metadata": {},
   "source": [
    "## 1. Cardinality Analysis: Why Binning is Harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707e4307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T14:06:08.149809Z",
     "iopub.status.busy": "2026-01-15T14:06:08.149700Z",
     "iopub.status.idle": "2026-01-15T14:06:08.508491Z",
     "shell.execute_reply": "2026-01-15T14:06:08.508065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types after categorical conversion:\n",
      "Temparature    category\n",
      "Humidity       category\n",
      "Moisture       category\n",
      "Nitrogen       category\n",
      "Potassium      category\n",
      "Phosphorous    category\n",
      "Soil Type      category\n",
      "Crop Type      category\n",
      "dtype: object\n",
      "\n",
      "Number of categories per feature:\n",
      "Temparature : 14 categories\n",
      "Humidity    : 23 categories\n",
      "Moisture    : 41 categories\n",
      "Nitrogen    : 39 categories\n",
      "Potassium   : 20 categories\n",
      "Phosphorous : 43 categories\n",
      "Soil Type   : 5 categories\n",
      "Crop Type   : 11 categories\n",
      "\n",
      "Target classes: 7 - ['10-26-26', '14-35-14', '17-17-17', '20-20', '28-28', 'DAP', 'Urea']\n"
     ]
    }
   ],
   "source": [
    "# Test pure categorical treatment (no binning, no target encoding, no interactions)\n",
    "\n",
    "# Define features\n",
    "feature_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', \n",
    "                'Phosphorous', 'Soil Type', 'Crop Type']\n",
    "\n",
    "# Create a copy for categorical treatment\n",
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "# Convert numerical features to categorical dtype (NO BINNING - use original values as categories)\n",
    "for col in numerical_features:\n",
    "    train_cat[col] = train_cat[col].astype('category')\n",
    "    test_cat[col] = test_cat[col].astype('category')\n",
    "\n",
    "# Label encode categorical features (Soil Type, Crop Type)\n",
    "le_soil = LabelEncoder()\n",
    "le_crop = LabelEncoder()\n",
    "\n",
    "# Also label encode target for XGBoost\n",
    "le_target = LabelEncoder()\n",
    "\n",
    "train_cat['Soil Type'] = le_soil.fit_transform(train_cat['Soil Type'])\n",
    "test_cat['Soil Type'] = le_soil.transform(test_cat['Soil Type'])\n",
    "\n",
    "train_cat['Crop Type'] = le_crop.fit_transform(train_cat['Crop Type'])\n",
    "test_cat['Crop Type'] = le_crop.transform(test_cat['Crop Type'])\n",
    "\n",
    "# Convert to category dtype\n",
    "train_cat['Soil Type'] = train_cat['Soil Type'].astype('category')\n",
    "test_cat['Soil Type'] = test_cat['Soil Type'].astype('category')\n",
    "train_cat['Crop Type'] = train_cat['Crop Type'].astype('category')\n",
    "test_cat['Crop Type'] = test_cat['Crop Type'].astype('category')\n",
    "\n",
    "# Encode target\n",
    "train_cat['Fertilizer Name_encoded'] = le_target.fit_transform(train_cat['Fertilizer Name'])\n",
    "\n",
    "print(\"Feature types after categorical conversion:\")\n",
    "print(train_cat[feature_cols].dtypes)\n",
    "print(\"\\nNumber of categories per feature:\")\n",
    "for col in feature_cols:\n",
    "    print(f\"{col:12s}: {train_cat[col].nunique()} categories\")\n",
    "    \n",
    "print(f\"\\nTarget classes: {len(le_target.classes_)} - {list(le_target.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c1ee6",
   "metadata": {},
   "source": [
    "# Quick test to see if pure categorical treatment works\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Prepare data\n",
    "X = train_cat[feature_cols]\n",
    "y = train_cat['Fertilizer Name_encoded']  # Use encoded target\n",
    "\n",
    "# Define MAP@3 metric for XGBoost\n",
    "def map_at_3_xgboost(predt, dtrain):\n",
    "    \"\"\"MAP@3 metric for XGBoost\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    # Convert predictions to probabilities\n",
    "    # predt is already in the right format for multi:softprob\n",
    "    # Get top 3 predictions for each sample\n",
    "    top3_idx = np.argsort(predt, axis=1)[:, -3:][:, ::-1]\n",
    "    \n",
    "    map_scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        # Find where true label is in top 3\n",
    "        true_label = y_true[i]\n",
    "        pred_row = top3_idx[i]\n",
    "        \n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_row, 1):\n",
    "            if pred == true_label:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "        \n",
    "        if num_hits > 0:\n",
    "            map_scores.append(score / min(3, len(pred_row)))\n",
    "        else:\n",
    "            map_scores.append(0.0)\n",
    "    \n",
    "    return 'MAP@3', np.mean(map_scores)\n",
    "\n",
    "# Test with small number of trees first\n",
    "print(\"Testing pure categorical treatment with XGBoost...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Simple XGBoost model with categorical support\n",
    "model_test = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=7,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=100,  # Small number for quick test\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=4,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Single fold validation for quick test\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(skf.split(X, y))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Train\n",
    "model_test.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_proba = model_test.predict_proba(X_val)\n",
    "y_val_true = y_val.values\n",
    "\n",
    "# Calculate MAP@3\n",
    "top3_idx = np.argsort(y_pred_proba, axis=1)[:, -3:][:, ::-1]\n",
    "map_scores = []\n",
    "for i in range(len(y_val_true)):\n",
    "    true_label = y_val_true[i]\n",
    "    pred_row = top3_idx[i]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0\n",
    "    \n",
    "    for k, pred in enumerate(pred_row, 1):\n",
    "        if pred == true_label:\n",
    "            num_hits += 1\n",
    "            score += num_hits / k\n",
    "    \n",
    "    if num_hits > 0:\n",
    "        map_scores.append(score / min(3, len(pred_row)))\n",
    "    else:\n",
    "        map_scores.append(0.0)\n",
    "\n",
    "map3_score = np.mean(map_scores)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "print(f\"Quick test results (100 trees, 1 fold):\")\n",
    "print(f\"MAP@3: {map3_score:.4f}\")\n",
    "print(f\"Test time: {test_time:.1f} seconds\")\n",
    "print(f\"\\nBaseline CV: 0.3311\")\n",
    "print(f\"Difference: {map3_score - 0.3311:+.4f}\")\n",
    "\n",
    "if map3_score > 0.3311:\n",
    "    print(\"\\n‚úÖ PURE CATEGORICAL TREATMENT WORKS! (Better than baseline)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pure categorical treatment needs more tuning or features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30027909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T14:06:08.509446Z",
     "iopub.status.busy": "2026-01-15T14:06:08.509339Z",
     "iopub.status.idle": "2026-01-15T14:06:08.796625Z",
     "shell.execute_reply": "2026-01-15T14:06:08.796221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature types after categorical conversion:\n",
      "Temparature    category\n",
      "Humidity       category\n",
      "Moisture       category\n",
      "Nitrogen       category\n",
      "Potassium      category\n",
      "Phosphorous    category\n",
      "Soil Type      category\n",
      "Crop Type      category\n",
      "dtype: object\n",
      "\n",
      "Number of categories per feature:\n",
      "Temparature : 14 categories\n",
      "Humidity    : 23 categories\n",
      "Moisture    : 41 categories\n",
      "Nitrogen    : 39 categories\n",
      "Potassium   : 20 categories\n",
      "Phosphorous : 43 categories\n",
      "Soil Type   : 5 categories\n",
      "Crop Type   : 11 categories\n"
     ]
    }
   ],
   "source": [
    "# Test pure categorical treatment (no binning, no target encoding, no interactions)\n",
    "\n",
    "# Define features\n",
    "feature_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', \n",
    "                'Phosphorous', 'Soil Type', 'Crop Type']\n",
    "\n",
    "# Create a copy for categorical treatment\n",
    "train_cat = train.copy()\n",
    "test_cat = test.copy()\n",
    "\n",
    "# Convert numerical features to categorical dtype (NO BINNING - use original values as categories)\n",
    "for col in numerical_features:\n",
    "    train_cat[col] = train_cat[col].astype('category')\n",
    "    test_cat[col] = test_cat[col].astype('category')\n",
    "\n",
    "# Label encode categorical features (Soil Type, Crop Type)\n",
    "le_soil = LabelEncoder()\n",
    "le_crop = LabelEncoder()\n",
    "\n",
    "train_cat['Soil Type'] = le_soil.fit_transform(train_cat['Soil Type'])\n",
    "test_cat['Soil Type'] = le_soil.transform(test_cat['Soil Type'])\n",
    "\n",
    "train_cat['Crop Type'] = le_crop.fit_transform(train_cat['Crop Type'])\n",
    "test_cat['Crop Type'] = le_crop.transform(test_cat['Crop Type'])\n",
    "\n",
    "# Convert to category dtype\n",
    "train_cat['Soil Type'] = train_cat['Soil Type'].astype('category')\n",
    "test_cat['Soil Type'] = test_cat['Soil Type'].astype('category')\n",
    "train_cat['Crop Type'] = train_cat['Crop Type'].astype('category')\n",
    "test_cat['Crop Type'] = test_cat['Crop Type'].astype('category')\n",
    "\n",
    "print(\"Feature types after categorical conversion:\")\n",
    "print(train_cat[feature_cols].dtypes)\n",
    "print(\"\\nNumber of categories per feature:\")\n",
    "for col in feature_cols:\n",
    "    print(f\"{col:12s}: {train_cat[col].nunique()} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f8cfd",
   "metadata": {},
   "source": [
    "## 3. Quick Performance Test with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to see if pure categorical treatment works\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Prepare data\n",
    "X = train_cat[feature_cols]\n",
    "y = train_cat['Fertilizer Name']\n",
    "\n",
    "# Define MAP@3 metric for XGBoost\n",
    "def map_at_3_xgboost(predt, dtrain):\n",
    "    \"\"\"MAP@3 metric for XGBoost\"\"\"\n",
    "    y_true = dtrain.get_label()\n",
    "    # Convert predictions to probabilities\n",
    "    # predt is already in the right format for multi:softprob\n",
    "    # Get top 3 predictions for each sample\n",
    "    top3_idx = np.argsort(predt, axis=1)[:, -3:][:, ::-1]\n",
    "    \n",
    "    map_scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        # Find where true label is in top 3\n",
    "        true_label = y_true[i]\n",
    "        pred_row = top3_idx[i]\n",
    "        \n",
    "        score = 0.0\n",
    "        num_hits = 0\n",
    "        \n",
    "        for k, pred in enumerate(pred_row, 1):\n",
    "            if pred == true_label:\n",
    "                num_hits += 1\n",
    "                score += num_hits / k\n",
    "        \n",
    "        if num_hits > 0:\n",
    "            map_scores.append(score / min(3, len(pred_row)))\n",
    "        else:\n",
    "            map_scores.append(0.0)\n",
    "    \n",
    "    return 'MAP@3', np.mean(map_scores)\n",
    "\n",
    "# Test with small number of trees first\n",
    "print(\"Testing pure categorical treatment with XGBoost...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Simple XGBoost model with categorical support\n",
    "model_test = xgb.XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=7,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=100,  # Small number for quick test\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=4,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Single fold validation for quick test\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(skf.split(X, y))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Train\n",
    "model_test.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_proba = model_test.predict_proba(X_val)\n",
    "y_val_true = y_val.values\n",
    "\n",
    "# Calculate MAP@3\n",
    "top3_idx = np.argsort(y_pred_proba, axis=1)[:, -3:][:, ::-1]\n",
    "map_scores = []\n",
    "for i in range(len(y_val_true)):\n",
    "    true_label = y_val_true[i]\n",
    "    pred_row = top3_idx[i]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0\n",
    "    \n",
    "    for k, pred in enumerate(pred_row, 1):\n",
    "        if pred == true_label:\n",
    "            num_hits += 1\n",
    "            score += num_hits / k\n",
    "    \n",
    "    if num_hits > 0:\n",
    "        map_scores.append(score / min(3, len(pred_row)))\n",
    "    else:\n",
    "        map_scores.append(0.0)\n",
    "\n",
    "map3_score = np.mean(map_scores)\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "print(f\"Quick test results (100 trees, 1 fold):\")\n",
    "print(f\"MAP@3: {map3_score:.4f}\")\n",
    "print(f\"Test time: {test_time:.1f} seconds\")\n",
    "print(f\"\\nBaseline CV: 0.3311\")\n",
    "print(f\"Difference: {map3_score - 0.3311:+.4f}\")\n",
    "\n",
    "if map3_score > 0.3311:\n",
    "    print(\"\\n‚úÖ PURE CATEGORICAL TREATMENT WORKS! (Better than baseline)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pure categorical treatment needs more tuning or features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726184a",
   "metadata": {},
   "source": [
    "## 4. Why the Previous Approach Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae085ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze why exp_002 failed\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS: Why exp_002 (categorical_target_encoding) FAILED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. BINNING DESTROYED INFORMATION:\")\n",
    "print(\"   - Original numericals: 14-43 unique values (low cardinality)\")\n",
    "print(\"   - After binning: 10 bins (forced reduction)\")\n",
    "print(\"   - Result: Lost natural ordinal relationships\")\n",
    "print(\"   - Example: Temperature values 25, 26, 27... became bin 0, 1, 2...\")\n",
    "print(\"   - XGBoost couldn't learn optimal splits\")\n",
    "\n",
    "print(\"\\n2. TARGET ENCODING OVERWROTE ORIGINAL FEATURES:\")\n",
    "print(\"   - Replaced 'Soil Type', 'Crop Type' with encoded versions\")\n",
    "print(\"   - Lost original categorical information that XGBoost could leverage\")\n",
    "print(\"   - Flattened multiclass encoding with .mean(axis=1) destroyed class-specific info\")\n",
    "\n",
    "print(\"\\n3. INTERACTION FEATURES ADDED NOISE:\")\n",
    "print(\"   - Simple multiplicative interactions (Temp√óHumidity, etc.)\")\n",
    "print(\"   - No validation that these interactions actually help\")\n",
    "print(\"   - Added 12+ features without testing individual impact\")\n",
    "\n",
    "print(\"\\n4. NO HYPERPARAMETER TUNING:\")\n",
    "print(\"   - Used same hyperparameters as baseline (depth=7, lr=0.05)\")\n",
    "print(\"   - Categorical features may need different regularization\")\n",
    "print(\"   - No exploration of depth 6-8 range\")\n",
    "\n",
    "print(\"\\n5. HYBRID APPROACH CONFUSED MODELS:\")\n",
    "print(\"   - Mix of binned categoricals, original numericals, AND interactions\")\n",
    "print(\"   - No clear feature engineering strategy\")\n",
    "print(\"   - Too many changes at once (binning + interactions + target encoding + CatBoost)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SOLUTION: PURE CATEGORICAL TREATMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"- Convert numericals to categorical WITHOUT binning\")\n",
    "print(\"- Keep original features (no target encoding)\")\n",
    "print(\"- Remove interaction features (for now)\")\n",
    "print(\"- Tune hyperparameters specifically for categorical features\")\n",
    "print(\"- Add back complexity incrementally ONLY if it helps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf12c7",
   "metadata": {},
   "source": [
    "## 5. Recommended Next Experiment Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_experiment_design = \"\"\"\n",
    "## Experiment 003: Pure Categorical Treatment (No Binning, No Target Encoding)\n",
    "\n",
    "**Goal**: Test the core hypothesis that direct categorical treatment improves performance\n",
    "\n",
    "**Key Changes from exp_002:**\n",
    "1. ‚úÖ NO BINNING: Convert numericals to categorical using original values as categories\n",
    "2. ‚úÖ NO TARGET ENCODING: Keep original Soil Type, Crop Type features\n",
    "3. ‚úÖ NO INTERACTIONS: Remove all multiplicative interaction features\n",
    "4. ‚úÖ HYPERPARAMETER TUNING: Tune depth (6, 7, 8) and learning rate (0.03, 0.05, 0.07)\n",
    "5. ‚úÖ KEEP CATBOOST: Native categorical support is valuable\n",
    "\n",
    "**Feature Engineering Pipeline:**\n",
    "- Convert ALL numerical features to category dtype (use original values as categories)\n",
    "- Label encode Soil Type, Crop Type, then convert to category dtype\n",
    "- Total features: 8 (6 numerical-turned-categorical + 2 categorical)\n",
    "- NO target encoding\n",
    "- NO interaction features\n",
    "\n",
    "**Models:**\n",
    "- XGBoost: tree_method='hist', enable_categorical=True, depth=6-8, lr=0.03-0.07\n",
    "- LightGBM: categorical_feature=cat_cols, depth=6-8, lr=0.03-0.07  \n",
    "- CatBoost: native categorical support, depth=6-8, lr=0.03-0.07\n",
    "\n",
    "**Hyperparameter Search:**\n",
    "- Test depths: 6, 7, 8\n",
    "- Test learning rates: 0.03, 0.05, 0.07\n",
    "- Test regularization: reg_alpha=0, 0.1, 0.5\n",
    "\n",
    "**Validation:**\n",
    "- Stratified 5-fold CV\n",
    "- Monitor MAP@3 and fold variance\n",
    "- Compare against baseline (0.3311) and exp_002 (0.3217)\n",
    "\n",
    "**Expected CV:** 0.340-0.350 (improvement of 0.009-0.019 from baseline)\n",
    "**If successful**: Then carefully add back target encoding and selective interactions\n",
    "**If fails**: Reconsider entire strategy - maybe numerical treatment is better\n",
    "\"\"\"\n",
    "\n",
    "print(next_experiment_design)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf0a7f",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: Pure Categorical Treatment Strategy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ WHAT WORKS:\")\n",
    "print(\"- Low cardinality numerical features (14-43 unique values)\")\n",
    "print(\"- XGBoost native categorical support\")\n",
    "print(\"- CatBoost native categorical support\")\n",
    "print(\"- Stratified 5-fold CV\")\n",
    "\n",
    "print(\"\\n‚ùå WHAT DOESN'T WORK:\")\n",
    "print(\"- Binning low-cardinality features (destroys information)\")\n",
    "print(\"- Target encoding that overwrites original features\")\n",
    "print(\"- Flattening multiclass target encoding with .mean(axis=1)\")\n",
    "print(\"- Adding interactions without validation\")\n",
    "print(\"- No hyperparameter tuning for new feature types\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS:\")\n",
    "print(\"1. Implement pure categorical treatment (exp_003)\")\n",
    "print(\"2. Tune hyperparameters specifically for categorical features\")\n",
    "print(\"3. Validate CV improvement vs baseline (0.3311)\")\n",
    "print(\"4. If successful (>0.340), add back target encoding properly\")\n",
    "print(\"5. Then add selective interactions if needed\")\n",
    "print(\"6. Finally, ensemble diverse models and stack\")\n",
    "\n",
    "print(\"\\nüìä EXPECTED IMPROVEMENT:\")\n",
    "print(\"- Conservative: 0.340 (+0.009 from baseline)\")\n",
    "print(\"- Optimistic: 0.350 (+0.019 from baseline)\")\n",
    "print(\"- Target: 0.3865 (need additional strategies after this)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è RISK MITIGATION:\")\n",
    "print(\"- If pure categorical fails, test numerical + interactions approach\")\n",
    "print(\"- If still failing, research winning solutions more deeply\")\n",
    "print(\"- Consider data augmentation with original dataset\")\n",
    "print(\"- Explore alternative encoding strategies (target encoding per class)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
